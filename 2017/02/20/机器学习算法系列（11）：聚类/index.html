<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,聚类," />





  <link rel="alternate" href="/atom.xml" title="狗皮膏药" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico?v=5.1.0" />






<meta name="description" content="一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首">
<meta name="keywords" content="机器学习,聚类">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法系列（11）：聚类">
<meta property="og:url" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/index.html">
<meta property="og:site_name" content="狗皮膏药">
<meta property="og:description" content="一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%889.30.11.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8810.29.32.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.20.20.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.30.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.38.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.44.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.39.21.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%8812.19.49.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.16.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.32.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.46.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.58.36.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%886.00.14.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.04.17.png">
<meta property="og:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%884.11.42.png">
<meta property="og:updated_time" content="2017-04-11T02:28:57.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法系列（11）：聚类">
<meta name="twitter:description" content="一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首">
<meta name="twitter:image" content="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%889.30.11.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title> 机器学习算法系列（11）：聚类 | 狗皮膏药 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">狗皮膏药</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">在隆冬，我终于知道，我身上有一个不可战胜的夏天</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="狗皮膏药">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/img/v.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="狗皮膏药">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="狗皮膏药" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习算法系列（11）：聚类
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-20T23:14:45+08:00">
                2017-02-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2017/02/20/机器学习算法系列（11）：聚类/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/20/机器学习算法系列（11）：聚类/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。<br>给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都较前一次好。</p>
<a id="more"></a>
<p>聚类算法主要包括以下五类：</p>
<ul>
<li>基于分层的聚类（hierarchical methods）</li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</p>
<ul>
<li>基于划分的聚类（partitioning methods）</li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<em>K-means算法</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></p>
<ul>
<li>基于密度的聚类（density-based methods）</li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有：<em>DBSCAN（Density-Based Spatial Clustering of Applic with Noise）算法（1996）</em>、<em>OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></p>
<ul>
<li>基于网格的聚类（grid-based methods）</li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<em>STING（Statistical Information Grid）</em>、<em>CLIQUE（Clustering In Quest）算法（1998）</em>、<em>WaveCluster算法</em>。其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li>基于模型的聚类（model-based methods）</li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h2 id="二、相似度、距离计算方法"><a href="#二、相似度、距离计算方法" class="headerlink" title="二、相似度、距离计算方法"></a>二、相似度、距离计算方法</h2><ul>
<li>给定$n$维空间$R^n$中的两个向量$X=(x_1,x_2,···,x_n)^T$和$y=(y_1,y_2,···,y_n)^T$，$x,y$之间的距离可以反映两者的相似程度，一般采用$L_p$距离$$<br>dist\left( X,Y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^p} \right) ^{\frac{1}{p}}<br>$$，其中$p≥1$，也称为闵可夫斯基距离（Minkowski）距离。常用的$p$为$1,2,+\infty$，此时相应的距离公式分别为<ul>
<li>1.当$p=1$时，称为曼哈顿距离（Manhattan distance），改名字的由来起源于在纽约市去测量街道之间的距离就是由人不行的步数来确定的。<br>$$<br>d\left(x,y\right)=\sum_{i=1}^n{|x_i-y_i|}<br>$$</li>
<li>当$p=2$时，称为欧几里得距离（Euclidean distance）<br>$$<br>d\left(x,y\right)=\left(\sum_{i=1}^n{\left(x_i-y_i\right)^2}\right)^{\frac{1}{2}}<br>$$</li>
<li>当$p=+\infty$时，称为最大值距离（Maximum distance）<br>$$<br>d\left(x,y\right)=\underset{1\le i\le n}{\max}|x_i-y_i|<br>$$</li>
</ul>
</li>
</ul>
<ul>
<li><p>杰卡德相似系数（Jaccard）$$<br>J\left( A,B \right) =\frac{|A\cap B|}{|A\cup B|}<br>$$</p>
</li>
<li><p>余弦相似度（Cosine Similarity）$$<br>\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}<br>$$</p>
</li>
<li>pearson相似系数$$<br>\rho _{XY}=\frac{cov\left( X,Y \right)}{\sigma _x\sigma _y}=\frac{E\left[ \left( x-u_x \right) \left( y-u_y \right) \right]}{\sigma _x\sigma _y}<br>$$</li>
<li><p>相对熵（K-L）距离$$<br>D\left(p||q\right)=\sum_x{p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}}=E_{p\left(x\right)}\log\frac{p\left(x\right)}{q\left(x\right)}<br>$$</p>
</li>
<li><p>Hellinger距离$$<br>D_a\left(p||q\right)=\frac{2}{1-a^2}\left(1-\int{p\left(x\right)^{\frac{1+a}{2}}q\left(x\right)^{\frac{1-a}{2}}}dx\right)<br>$$</p>
</li>
<li><p>余弦相似度与pearson相似系数的比较</p>
</li>
</ul>
<p>$n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为：<br>$$<br>\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}=\frac{\sum_{i=1}^n{x_iy_i}}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}}·\sqrt{\sum_{i=1}^n{y_{i}^{2}}}}<br>$$</p>
<p>这两个向量的相关系数是：$$<br>\rho_{XY}=\frac{cov\left(X,Y\right)}{\sigma_x\sigma_y}=\frac{E\left[\left(x-u_x\right)\left(y-u_y\right)\right]}{\sigma_x\sigma_y}<br>$$$$<br>=\frac{\sum_{i=1}^n{\left(x_i-\mu_x\right)\left(y_i-\mu_y\right)}}{\sqrt{\sum_{i=1}^n{\left(x_i-\mu_x\right)^2}}\sqrt{\sum_{i=1}^n{\left(y_i-\mu_y\right)^2}}}<br>$$<br>相关系数即将$x,y$坐标向量各自平移到原点后的夹角余弦。这即揭示了为何文档间求距离使用夹角余弦，因为这个物理量表征了文档去均值化后的随机向量间的相关系数。</p>
<h2 id="三、K-Means算法"><a href="#三、K-Means算法" class="headerlink" title="三、K-Means算法"></a>三、K-Means算法</h2><h4 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h4><p>K-Means算法属于基于划分的聚类算法，对N 维欧氏空间中的点进行聚类，是一种最简单的无监督学习方法。它通过迭代来实现，其基本思想是：每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。</p>
<h4 id="3-2-基本假设"><a href="#3-2-基本假设" class="headerlink" title="3.2 基本假设"></a>3.2 基本假设</h4><p>K-Means聚类需要对数据进行一个基本假设：对于每一个 cluster ，我们可以选出一个中心点 (center) ，使得该 cluster 中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%889.30.11.png" alt="屏幕快照 2017-03-25 上午9.30.11"><br>由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点 2.5 来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是 2 ，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。</p>
<h4 id="3-3-算法步骤"><a href="#3-3-算法步骤" class="headerlink" title="3.3 算法步骤"></a>3.3 算法步骤</h4><p>假定输入样本为$S=x_1,x_2,···,x_n$，则算法步骤为：</p>
<ul>
<li>1、选择初始的K个类别中心$\mu_1,\mu_2,···,\mu_k$。这个过程通常是针对具体地问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑一个K-Means，并取其中最好的一次结果。</li>
<li>2、对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即：$$label_i=arg\underset{1\le j\le k}{\min}||x_i-\mu_j||$$</li>
<li>3、将每个类别中心更新为隶属于该类别的所有样本的均值$$<br>\mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}{x_i}<br>$$</li>
<li>4、重复前两步，直到类别中心的变化小于某阈值或者达到最大迭代次数</li>
</ul>
<h4 id="3-4-理论分析"><a href="#3-4-理论分析" class="headerlink" title="3.4 理论分析"></a>3.4 理论分析</h4><p>基于上述的假设，我们导出K-Means所要优化的目标函数：设我们一共有N个数据点需要分为K个Cluster，K-Means需要最小化的损失函数为：<br>$$<br>J=\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^K{r_{ij}||x_i-\mu_j||^2}}<br>$$<br>这个函数，其中$r_{ij}$在数据点$n$被归类到$Cluster(j) $的时候为1，否则为0.直接寻找$r_{ij}$和$\mu_j$来最小化$J$并不容易，不过我们可以通过反复迭代以下两步的方法来进行：</p>
<ul>
<li>1、先固定$\mu_j$，选择最优的$r_{ij}$，很容易看出，只要将数据点归类到离它最近的那个中心就能保证$J$最小，通俗来讲，因为每个样本点都有一个$r_{ij}$，不是0就是1，那么我们要想让$J$最小，就要保证当一个样本的$r_{ij=1}$时，与类别中心距离的平方和达到最小。这一步即</li>
<li>2、然后固定$r_{ij}$，再求最优的$\mu_j$。将$J$对$\mu_k$求导并令导数等于零，即令 $$<br>\frac{\partial J}{\partial\mu_j}=\sum_{i=1}^{N_j}{r_{ij}\left(x_i-\mu_j\right)}=0<br>$$<br>很容易得到$J$最小的时候$\mu_j$应该满足$$<br>\mu_j=\frac{\sum_i{r_{ij}x_i}}{\sum_i{r_{ij}}}<br>$$</li>
</ul>
<p>$\mu_j$的值是所有$Cluster(j)$中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$智慧不断地减小或者保持不变，而不会增加，这保证了K-Means最终或到达一个极小值。虽然K-Means并不能保证总是得到全局最优解，但是对于这样的问题，像K-Means这样复杂度的算法，这样的结果已经是很不错了。</p>
<h4 id="3-5-算法演练"><a href="#3-5-算法演练" class="headerlink" title="3.5 算法演练"></a>3.5 算法演练</h4><p>下面看一个来自WIKI的实例<br><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8810.29.32.png" alt="屏幕快照 2017-03-25 上午10.29.32"></p>
<ul>
<li>1、随机生成三个初始的中心点（这个中心点不一定是样本点），即图中红、绿、蓝三个小圈；</li>
<li>2、计算每个样本点与这三个中心店的距离，并将它们归属到离得最近的中心点对应的Cluster。此时图中分成了三个簇，分别是红色、绿色、蓝色部分；</li>
<li>3、重新分别计算三个簇中所有样本点的类别中心，指定为新的类别中心。此时红色、绿色、蓝色类的中点都发生了迁移。</li>
<li>4、反复迭代第2步和第3步，直至收敛。</li>
</ul>
<h4 id="3-6-总结"><a href="#3-6-总结" class="headerlink" title="3.6 总结"></a>3.6 总结</h4><ul>
<li>优点：<ul>
<li>是解决聚类问题的一种经典算法，简单、快速</li>
<li>对处理大数据集，该算法保持可伸缩性和高效率 </li>
<li>当簇近似为高斯分布时，它的效果较好</li>
</ul>
</li>
<li>缺点<ul>
<li>在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用</li>
<li>必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同</li>
<li>只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇</li>
<li>对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。比如数组1，2，3，4，100的均值为22，显然距离“大多数”数据1、2、3、4比较远，如果改成数组的中位数3，在该实例中更为稳妥，这种聚类也叫作K-mediods聚类</li>
</ul>
</li>
</ul>
<h4 id="3-7-Canopy算法"><a href="#3-7-Canopy算法" class="headerlink" title="3.7 Canopy算法"></a>3.7 Canopy算法</h4><h2 id="四、K-Mediods算法"><a href="#四、K-Mediods算法" class="headerlink" title="四、K-Mediods算法"></a>四、K-Mediods算法</h2><h2 id="五、层次聚类（Hierarchical-Clustering"><a href="#五、层次聚类（Hierarchical-Clustering" class="headerlink" title="五、层次聚类（Hierarchical Clustering"></a>五、层次聚类（Hierarchical Clustering</h2><p>）</p>
<h2 id="六、DBSCAN算法"><a href="#六、DBSCAN算法" class="headerlink" title="六、DBSCAN算法"></a>六、DBSCAN算法</h2><h4 id="6-1-密度聚类方法"><a href="#6-1-密度聚类方法" class="headerlink" title="6.1 密度聚类方法"></a>6.1 密度聚类方法</h4><p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。<br>其代表算法为DBSCAN算法和密度最大值算法。</p>
<h4 id="6-2-DBSCAN算法原理"><a href="#6-2-DBSCAN算法原理" class="headerlink" title="6.2 DBSCAN算法原理"></a>6.2 DBSCAN算法原理</h4><p>DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。</p>
<h4 id="6-3-若干概念"><a href="#6-3-若干概念" class="headerlink" title="6.3 若干概念"></a>6.3 若干概念</h4><ul>
<li>对象的$\varepsilon -$领域：给定对象在半径$\varepsilon$内的区域</li>
<li>核心对象：对于给定的数目$m$，如果一个对象的$\varepsilon -$领域至少包含$m$个对象，则称该对象为核心对象。</li>
<li><p>直接密度可达：给定一个对象集合$D$，如果p是在q的$\varepsilon -$领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。<br>如图$\varepsilon =1,m=5$，q是一个核心对象，从对象q出发到对象p是直接密度可达的。</p>
<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.20.20.png" alt="屏幕快照 2017-03-25 上午11.20.20"></center>
</li>
<li><p>密度可达：如果存在一个对象链$p_1p_2···p_n$，$p_1=q,p_n=p$，对$p_i\in D,(1≤i≤n)$,$p_{i+1}$是从$p_i$关于$\varepsilon$和$m$直接密度可达的，则对象$p$是从对象$q$和$m$密度可达的。</p>
<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.30.png" alt="屏幕快照 2017-03-25 上午11.30.30"></center></li>
<li><p>密度相连：如果对象集合$D$中存在一个对象$O$，使得对$p$和$q$是从$O$关于$\varepsilon $和$m$密度可达的，那么对象$p$和$q$是关于$\varepsilon $和$m$密度相连的。</p>
<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.38.png" alt="屏幕快照 2017-03-25 上午11.30.38"></center>
</li>
<li><p>簇：一个基于密度的簇是最大的密度相连对象的集合。</p>
</li>
<li>噪声：不包含在任何簇中的对象称为噪声。<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.44.png" alt="屏幕快照 2017-03-25 上午11.30.44"></center>

</li>
</ul>
<h4 id="6-4-算法步骤"><a href="#6-4-算法步骤" class="headerlink" title="6.4 算法步骤"></a>6.4 算法步骤</h4><p>下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤：</p>
<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.39.21.png" alt="屏幕快照 2017-03-25 上午11.39.21"></center>

<ul>
<li>1、首先随机选择A点为算法实施的切入点，我们将$\varepsilon $设置为图中圆的半径，对象个数$m（minPts）$设定为4。这里我们看到，A点的$\varepsilon - $领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。</li>
<li>2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点</li>
<li>3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。</li>
</ul>
<p>从上述算法可知：</p>
<ul>
<li>每个簇至少包含一个核心对象；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h5 id="6-5-总结"><a href="#6-5-总结" class="headerlink" title="6.5 总结"></a>6.5 总结</h5><ul>
<li><p>优点</p>
<ul>
<li>无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means. </li>
<li>可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.</li>
<li>对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers.</li>
<li>只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)</li>
<li>加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.</li>
<li><p>参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood.</p>
<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%8812.19.49.png" alt="屏幕快照 2017-03-25 下午12.19.49"></center></li>
</ul>
</li>
<li>缺点<ul>
<li>边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.</li>
<li>维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance.</li>
<li>不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters.</li>
<li>参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult. </li>
</ul>
</li>
</ul>
<h2 id="七、OPTICS算法"><a href="#七、OPTICS算法" class="headerlink" title="七、OPTICS算法"></a>七、OPTICS算法</h2><h2 id="八、密度最大值聚类"><a href="#八、密度最大值聚类" class="headerlink" title="八、密度最大值聚类"></a>八、密度最大值聚类</h2><h4 id="8-1-引言"><a href="#8-1-引言" class="headerlink" title="8.1 引言"></a>8.1 引言</h4><p>2014年6月，Alex Rodriguez和Alessandro Laio在$Science$上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。<br>在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。</p>
<h4 id="8-2-若干概念"><a href="#8-2-若干概念" class="headerlink" title="8.2 若干概念"></a>8.2 若干概念</h4><ul>
<li>局部密度$\rho_i$的定义为：$$\rho_i=\sum_j{\chi\left(d_{ij}-d_c\right)}$$，其中，$$<br>\chi\left(x\right)=\left\{\begin{array}{l}<br>  1 if x&lt;0\\<br>  0 if otherwise\\<br>\end{array}\right.<br>$$<br>其中$d_c$是一个截断距离，$\rho_i$即到对象$i$的距离小于$d_c$的对象的个数。由于该算法只对$\rho_i$的相对值敏感，所以对$d_c$的选择是比较稳健的。</li>
<li>高局部密度点距离$\delta_i$，其定义为：$$<br>\delta_i=\underset{j:\rho_j&gt;\rho_i}{\min}\left(d_{ij}\right)<br>$$<br>即在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离。<br>而极端地，对于密度最大的那个对象，我们设置$\delta=max(d_{ij})$；<br>只有那些密度是局部或者全局最大的点才会有远大于正常值的高局部密度点距离。</li>
</ul>
<h4 id="8-3-聚类过程"><a href="#8-3-聚类过程" class="headerlink" title="8.3 聚类过程"></a>8.3 聚类过程</h4><p>这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下：</p>
<ul>
<li><p>1、首先计算每一个点的局部密度$\rho_i$，如图中，$\rho_1=7,\rho_8=5,\rho_{10}=4$<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.16.png" alt="屏幕快照 2017-03-25 下午5.46.16"></p>
</li>
<li><p>2、然后对于每一个点$i$计算在局部密度高于对象$i$的所有对象中，到对象$i$<br>最近的距离$\delta$<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.32.png" alt="屏幕快照 2017-03-25 下午5.46.32"></p>
</li>
<li>3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.46.png" alt="屏幕快照 2017-03-25 下午5.46.46"></li>
<li>4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.58.36.png" alt="屏幕快照 2017-03-25 下午5.58.36"></li>
<li>5、将其他的点分配给距离其最近的有着更高的局部密度的簇。（Assign each point to the same cluster of its nearest neighbor of higher density）<img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%886.00.14.png" alt="屏幕快照 2017-03-25 下午6.00.14"></li>
</ul>
<p>左图是所有点在二维空间的分布，右图是以$\rho$为横坐标，以$\delta$为纵坐标绘制的决策图。容易发现，1和10两个点的$\rho_i$和$\delta_i$都比较大，作为簇的中心点。26、27、28三个点的$\delta$也比较大，但是$\rho比较小$，所以是异常点。</p>
<h4 id="8-4-一些关键点"><a href="#8-4-一些关键点" class="headerlink" title="8.4 一些关键点"></a>8.4 一些关键点</h4><ul>
<li><p>簇中心的识别</p>
<ul>
<li>那些有着比较大的局部密度$\rho_i$和很大的高局部密度$\delta_i$的点被认为是簇的中心；<br>而高局部密度距离$\delta_i$较大但局部密度$\rho_i$较小的点是异常点；<br>确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。<br>但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。在上图中可以分明地用肉眼判断聚类中心，但是有些情况下无法用肉眼来判断。不过，对于那些在决策图中无法用肉眼判断出聚类中心的情形，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将$\rho$值和$\delta$值综合考虑的量$$<br>\gamma_i=\rho_i\delta_i<br>$$，显然$\gamma$值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。<br>我们把排序后的$\gamma$在坐标平面（下标为横轴，$\gamma$值为纵轴）画出来，由图可见，非聚类中心的$gamma$值比较平滑，而从非聚类中心过渡到聚类中心时$\gamma$有一个明显的跳跃，这个跳跃用肉眼或数值检测应该可以判断出来。作者在文末还提到，对于人工随机生成的数据集，$\gamma$的分布还满足幂次定律，即$log\gamma$，且斜率依赖于数据维度。<center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.04.17.png" alt="屏幕快照 2017-03-25 下午5.04.17"></center></li>
</ul>
</li>
<li><p>截断距离$d_c$的选择</p>
<ul>
<li>一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。参数$d_c$的选取，从某种意义上决定这聚类算法的成败，取得太大或者太小都不行：如果取得太大，将使得每个数据点的$\rho$值都很大以致区分度不高，极端情况是取$d_c&gt;d_{max}$，则所有的数据点都归属于一个Cluster了；如果$d_c$取得太小，同一个Cluster中就可能被拆分成多个，极端情况是$d_c&lt;d_{min}$，则每个数据点都单独称为一个Cluster。作者将比例锁定在数据量的1%~2%，也是基于肉感数据集的经验值。</li>
</ul>
</li>
<li><p>选定簇中心之后</p>
<ul>
<li>在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$d_c$的点(这个区域由这样的数据点构成：它们本身属于该Cluster，但在与其距离不超过$d_c$的范围内，存在属于其他Cluster的数据点). 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$\rho_h$. 该类簇中所有局部密度大于$\rho_h$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下<br><center><img src="media/14844862016675/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%884.11.42.png" alt="屏幕快照 2017-03-25 下午4.11.42"></center><br>A图为生成数据的概率分布，B、C二图为分别从该分布中生成了4000，1000个点。D,E分别是B,C两组数据的决策图（decision tree），可以看到两组数据都只有五个点有比较大的$\rho_i$和很大的$\delta_i$，这些点作为类簇的中心，在确定了类簇的中心之后，每个点被划分到各个类簇（彩色点），或者划分到类簇光晕（黑色点），F图展示的是随着抽样点数量的增多，聚类的错误率在逐渐下降，说明该算法是鲁棒的。</li>
</ul>
</li>
</ul>
<h2 id="九、谱聚类（Spectral-Clustering）"><a href="#九、谱聚类（Spectral-Clustering）" class="headerlink" title="九、谱聚类（Spectral Clustering）"></a>九、谱聚类（Spectral Clustering）</h2><h2 id="十、降维（Dimensionality-Reduction）"><a href="#十、降维（Dimensionality-Reduction）" class="headerlink" title="十、降维（Dimensionality Reduction）"></a>十、降维（Dimensionality Reduction）</h2><h2 id="十一、聚类数目的选择（Deciding-the-Number-of-Clusterings）"><a href="#十一、聚类数目的选择（Deciding-the-Number-of-Clusterings）" class="headerlink" title="十一、聚类数目的选择（Deciding the Number of Clusterings）"></a>十一、聚类数目的选择（Deciding the Number of Clusterings）</h2><h2 id="高斯混合模型、EM"><a href="#高斯混合模型、EM" class="headerlink" title="高斯混合模型、EM"></a>高斯混合模型、EM</h2><h2 id="矢量量化（Vector-Quantization）"><a href="#矢量量化（Vector-Quantization）" class="headerlink" title="矢量量化（Vector Quantization）"></a>矢量量化（Vector Quantization）</h2>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/聚类/" rel="tag"># 聚类</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/02/27/机器学习算法系列（12）：SVM/" rel="next" title="机器学习算法系列（12）：SVM">
                <i class="fa fa-chevron-left"></i> 机器学习算法系列（12）：SVM
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/02/15/机器学习算法系列（10）：朴素贝叶斯/" rel="prev" title="机器学习算法系列（10）：朴素贝叶斯">
                机器学习算法系列（10）：朴素贝叶斯 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2017/02/20/机器学习算法系列（11）：聚类/"
     data-title="机器学习算法系列（11）：聚类"
     data-content=""
     data-url="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/02/20/机器学习算法系列（11）：聚类/"
           data-title="机器学习算法系列（11）：聚类" data-url="http://yoursite.com/2017/02/20/机器学习算法系列（11）：聚类/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/img/v.png"
               alt="狗皮膏药" />
          <p class="site-author-name" itemprop="name">狗皮膏药</p>
          <p class="site-description motion-element" itemprop="description">RUC机器学习在读 | 整理癖 |</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">24</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/zhang-hua-28-4/activities" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-zhihu"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、引言"><span class="nav-text">一、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、相似度、距离计算方法"><span class="nav-text">二、相似度、距离计算方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、K-Means算法"><span class="nav-text">三、K-Means算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-原理"><span class="nav-text">3.1 原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-基本假设"><span class="nav-text">3.2 基本假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-算法步骤"><span class="nav-text">3.3 算法步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-理论分析"><span class="nav-text">3.4 理论分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-算法演练"><span class="nav-text">3.5 算法演练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-总结"><span class="nav-text">3.6 总结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-7-Canopy算法"><span class="nav-text">3.7 Canopy算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、K-Mediods算法"><span class="nav-text">四、K-Mediods算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、层次聚类（Hierarchical-Clustering"><span class="nav-text">五、层次聚类（Hierarchical Clustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、DBSCAN算法"><span class="nav-text">六、DBSCAN算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-密度聚类方法"><span class="nav-text">6.1 密度聚类方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-DBSCAN算法原理"><span class="nav-text">6.2 DBSCAN算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-若干概念"><span class="nav-text">6.3 若干概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-算法步骤"><span class="nav-text">6.4 算法步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#6-5-总结"><span class="nav-text">6.5 总结</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七、OPTICS算法"><span class="nav-text">七、OPTICS算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八、密度最大值聚类"><span class="nav-text">八、密度最大值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-引言"><span class="nav-text">8.1 引言</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-若干概念"><span class="nav-text">8.2 若干概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-聚类过程"><span class="nav-text">8.3 聚类过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-一些关键点"><span class="nav-text">8.4 一些关键点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#九、谱聚类（Spectral-Clustering）"><span class="nav-text">九、谱聚类（Spectral Clustering）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十、降维（Dimensionality-Reduction）"><span class="nav-text">十、降维（Dimensionality Reduction）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十一、聚类数目的选择（Deciding-the-Number-of-Clusterings）"><span class="nav-text">十一、聚类数目的选择（Deciding the Number of Clusterings）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高斯混合模型、EM"><span class="nav-text">高斯混合模型、EM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矢量量化（Vector-Quantization）"><span class="nav-text">矢量量化（Vector Quantization）</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">狗皮膏药</span>
</div>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"plushunter"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  








  
  

  
  


  

  

  


</body>
</html>

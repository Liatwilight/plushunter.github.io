<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,SVM," />





  <link rel="alternate" href="/atom.xml" title="Free Will" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico?v=5.1.0" />






<meta name="description" content="线性可分支持向量机：硬间隔支持向量机 线性支持向量机：软间隔支持向量机学习一个线性分类器 非线性支持向量机：当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。">
<meta name="keywords" content="机器学习,SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法系列（12）：SVM">
<meta property="og:url" content="http://yoursite.com/2017/02/27/机器学习算法系列（12）：SVM/index.html">
<meta property="og:site_name" content="Free Will">
<meta property="og:description" content="线性可分支持向量机：硬间隔支持向量机 线性支持向量机：软间隔支持向量机学习一个线性分类器 非线性支持向量机：当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-19%20%E4%B8%8B%E5%8D%882.15.14.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8810.22.45.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8811.03.17.png">
<meta property="og:updated_time" content="2017-04-11T16:27:11.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法系列（12）：SVM">
<meta name="twitter:description" content="线性可分支持向量机：硬间隔支持向量机 线性支持向量机：软间隔支持向量机学习一个线性分类器 非线性支持向量机：当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。">
<meta name="twitter:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-19%20%E4%B8%8B%E5%8D%882.15.14.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title> 机器学习算法系列（12）：SVM | Free Will </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Free Will</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-yiyu">
          <a href="/yiyu" rel="section">
            
            呓语
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/27/机器学习算法系列（12）：SVM/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="狗皮膏药">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/img/v.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Free Will">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Free Will" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习算法系列（12）：SVM
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-27T23:14:45+08:00">
                2017-02-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>线性可分支持向量机：</strong>硬间隔支持向量机</p>
<p><strong>线性支持向量机：</strong>软间隔支持向量机学习一个线性分类器</p>
<p><strong>非线性支持向量机：</strong>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。</p>
<a id="more"></a>
<h2 id="一、线性可分支持向量机与硬间隔最大化"><a href="#一、线性可分支持向量机与硬间隔最大化" class="headerlink" title="一、线性可分支持向量机与硬间隔最大化"></a>一、线性可分支持向量机与硬间隔最大化</h2><h3 id="1-1-线性可分支持向量机"><a href="#1-1-线性可分支持向量机" class="headerlink" title="1.1 线性可分支持向量机"></a>1.1 线性可分支持向量机</h3><p>假设给定一个特征空间上的训练数据集<br>$$<br>T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}<br>$$<br>其中$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N<br>$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。</p>
<p>给定线性可分训练数据集，通过<strong>间隔最大化</strong>得到的<strong>分离超平面</strong>为$$<br>w^T·x+b=0<br>$$<br>以及相应的<strong>分类决策函数</strong>$$<br>f\left( x \right) =sign\left( w^T·x+b \right)<br>$$</p>
<p>该决策函数称为线性可分支持向量机</p>
<h3 id="1-2-函数间隔与几何间隔"><a href="#1-2-函数间隔与几何间隔" class="headerlink" title="1.2 函数间隔与几何间隔"></a>1.2 函数间隔与几何间隔</h3><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面确定的情况下$|w^T·x+b|$能够相对地表示点$x$距离超平面的远近。而$w^T·x+b$的符号与类标记的符号是否一致能够表示分类是否正确，所以可用$y(w^T·x+b)$来表示分类的正确性与确信度，这就是<strong>函数间隔</strong><code>functional margin</code>的概念</p>
<p>但是，函数间隔有一个不足之处，就是在选择分离超平面时，只要成比例地改变$w$和$b$，超平面并没有变化，而函数间隔却以同样比例变化了。因此，我们可以对分离超平面的法向量$w$加上某些约束，使得间隔确定，此时函数间隔成为<strong>几何间隔</strong><code>geometric margin</code>。</p>
<p>对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$w,b$关于样本点$(x_i,y_i)$的几何间隔为$$<br>\gamma _i=y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right)<br>$$</p>
<p>定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即<br>$$<br>\gamma =\underset{i=1,···,N}{\min}\gamma _i<br>$$</p>
<p>超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。</p>
<p>函数间隔与几何间隔的关系为$$<br>\gamma =\frac{\hat{\gamma}}{||w||}<br>$$若$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。</p>
<h3 id="1-3-间隔最大化"><a href="#1-3-间隔最大化" class="headerlink" title="1.3 间隔最大化"></a>1.3 间隔最大化</h3><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大的分离超平面是唯一的。</p>
<p><strong>间隔最大化</strong>的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据记性分类。即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<p><strong>最大间隔分离超平面</strong></p>
<p>接下来求一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，可以表示为下面的约束最优化问题：$$<br>\underset{w,b}{\max}\,\,\gamma<br>$$$$<br>s.t\,\,\,\,y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right) \geqslant \gamma \,\,,\,\,i=1,2,···,N<br>$$<br>即最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma $，约束条件表示的是超平面$w,b$关于每个训练样本点的几何间隔至少是$\gamma $</p>
<p>根据几何间隔和函数间隔的关系，可以将此问题改写为$$<br>\underset{w,b}{\max} \frac{\hat{\gamma}}{||w||}<br>$$$$<br>s.t  y_i\left( w·x_i+b \right) \geqslant \hat{\gamma} , i=1,2,···,N<br>$$<br>函数间隔$<br>\hat{\gamma}<br>$的取值不影响最优化的解。函数间隔因为$w$，$b$按比例改变为$<br>\lambda w\mathrm{，}\lambda b<br>$而成为$<br>\lambda \hat{\gamma}<br>$，但是对最优化问题中的不等式约束没有影响，对目标函数的优化也没有影响，即两者等价。这样，我们可以取$<br> \hat{\gamma}=1<br>$，代入后注意到最大化$<br>\frac{1}{||w||}<br>$和最小化$<br>\frac{1}{2}||w||^2<br>$是等价的。于是就得到下面的线性可分支持向量机学习的最优化问题。</p>
<p><code>线性可分支持向量机学习算法——最大间隔法</code></p>
<ul>
<li>输入：线性可分训练数据集$<br>T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}<br>$,其中,$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$</li>
<li>输出：最大间隔分离超平面和分类决策函数</li>
</ul>
<p>步骤如下：</p>
<ul>
<li>1）构造并求解约束最优化问题：$$<br>\underset{w,b}{\min}\frac{1}{2}||w||^2<br>$$<br>$$<br>                s.t  y_i\left( w·x_i+b \right) -1\geqslant 0, i=1,2,···,N<br>$$求得最优解$w^<em>$,$b^</em>$</li>
<li>2）由此得到分割超平面：$$w·x+b=0<br>$$<br>分类决策函数<br>$$f\left( x \right) =sign\left( w·x+b \right)<br>$$</li>
</ul>
<p>这其实是一个凸二次规划<code>convex quadratic programming</code> 问题，凸优化问题是指约束最优化问题$$<br>\underset{w}{\min}   f\left( w \right)<br>$$$$<br>                 s.t   s_i\left( w \right) \le 0 , i=1,2,···,k<br>$$$$<br>             h_i\left( w \right) =0 , i=1,2,···,l <br>$$<br>其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，上述凸优化问题成为凸二次规划问题。</p>
<p><strong>支持向量和间隔边界</strong></p>
<p>再线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例成为支持向量<code>support vector</code>。支持向量是使约束条件式等号成立的点，即$$<br>y_i\left( w·x_i+b \right) -1=0<br>$$对$y_i=+1$的实例点，支持向量在超平面$$<br>H_1\mathrm{：}w·x_i+b=1<br>$$<br>对$y_i=-1$的负例点，支持向量在超平面$$<br>H_2\mathrm{：}w·x_i+b=-1<br>$$</p>
<center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-19%20%E4%B8%8B%E5%8D%882.15.14.png" alt="屏幕快照 2017-03-19 下午2.15.14"></center>

<p>注意到$H_1$和$H_2$平行，并且没有实例点落在他们中间。在$H_1$和$H_2$之间形成一条长带，分离超平面与他们平行且位于他们中间。长带的宽度，即$H_1$与$H_2$之间的距离成为间隔<code>margin</code>，间隔依赖于分离超平面的法向量$w$，等于$<br>\frac{2}{||w||}<br>$。$H_1$和$H_2$称为间隔边界。</p>
<p>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在将俄边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量机的个数一般都很少，所以支持向量机由很少的“重要的”训练样本确定。</p>
<h3 id="1-4-学习的对偶算法"><a href="#1-4-学习的对偶算法" class="headerlink" title="1.4 学习的对偶算法"></a>1.4 学习的对偶算法</h3><p>为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法<code>dual algorithm</code>。</p>
<p>这样做的优点是，一是对偶问题往往更容易求解；二是引入核函数，进而推广到非线性分类问题。</p>
<p>首先构建拉格朗日函数，为此，对每一个不等式约束引进拉格朗日乘子$a_i≥0，i=1,2,···,N$定义拉格朗日函数：<br>$$<br>L\left( w,b,a \right) =\frac{1}{2}||w||^2-\sum_{i=1}^N{a_iy_i\left( w·x_i+b \right) +\sum_{i=1}^N{a_i}}<br>$$其中，$<br>a=\left( a_1,a_2,···,a_N \right) ^T<br>$<br>原问题是极小极大问题：<br>$$<br>\underset{w,b}{\min}\underset{a}{\max}L\left( w,b,a \right)<br>$$<br>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：<br>$$<br>\underset{a}{\max}\underset{w,b}{\min}L\left( w,b,a \right)<br>$$</p>
<p>为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$a$的极大。</p>
<ul>
<li>1)    求$<br>\underset{w,b}{\min}L\left( w,b,a \right)<br>$</li>
</ul>
<p>将拉格朗日函数$L(w,b,a)$分别对$w,b$求偏导数并令其为$0$。<br>$$<br>\nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i}=0<br>$$$$<br>\nabla _bL\left( w,b,a \right) =\sum_{i=1}^N{a_iy_i}=0<br>$$得到$$<br>w=\sum_{i=1}^N{a_iy_ix_i}<br>$$$$<br>\sum_{i=1}^N{a_iy_i=0}<br>$$将其代入拉格朗日函数，得到<br>$$<br>L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_iy_i\left( \left( \sum_{j=1}^N{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^N{a_i}}}}<br>$$$$<br>-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}<br>$$</p>
<ul>
<li>2)求$<br>\underset{w,b}{\min}L\left( w,b,a \right)<br>$对$a$的极大，即是对偶问题<br>$$<br>\underset{a}{\max} \sum_{i=1}^N{a_i}-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}<br>$$$$<br>s.t. \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>a_i\geqslant 0,  i=1,2,···,N<br>$$</li>
</ul>
<p>将目标函数由求极大转换为极小，就得到下面与之等价的对偶最优化问题。<br>$$<br>\underset{a}{\min}   \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}-\sum_{i=1}^N{a_i}<br>$$$$<br>s.t. \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>a_i\geqslant 0,  i=1,2,···,N<br>$$<br>考虑原始最优化问题和对偶最优化问题，原始问题满足定理C.2的条件，所以存在$w,α,β$,使$w$是原始问题的解，$α,β$是对偶问题的解。这意味着求解原始问题可以转换为求解对偶问题。</p>
<p>对线性可分训练数据集，假设对偶最优化问题对$a$的解为$a=(a_1,a_2,···,a_N)$，可以由$a$求得原始最优化问题对$(w,b)$的解$w,b$。得到下面的定理。</p>
<p>设$a=(a_1,a_2,···,a_l)^T$是对偶最优化问题的解，则存在下标$j$，使得$a_j&gt;0$，并可按下式求得原始最优化问题的解$w,b$:<br>$$<br>w=\sum_{i=1}^N{a_{i} y_ix_i}<br>$$$$<br>b=y_i-\sum_i^N{a_{i} y_i\left( x_i·x_j \right)}<br>$$</p>
<p>证明  根据定理C.3,KKT条件成立，即得<br>$$\nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i=0}<br>$$$$<br>\nabla _bL\left( w,b,a \right) =-\sum_{i=1}^N{a_{i} y_i=0}<br>$$$$<br>a_{i} \left( y_i\left( w·x_i+b \right) -1 \right) =0 , i=1,2,···,N<br>$$$$<br>y_i\left( w·x_i+b \right) -1\geqslant 0 , 1,2,···,N<br>$$$$<br>a_{i} \geqslant 0 , i=1,2,···,N<br>$$<br>由此得<br>$$<br>w=\sum_i^{}{a_{i} y_ix_i}<br>$$<br>其中至少有一个$a_j&gt;0$(反证法，假设$a^*=0$，由上可知$w=0$，而$w=0$不是原始最优化问题的解，产生矛盾)，对此$j$有<br>$$<br>y_j\left( w·x_j+b \right) -1=0<br>$$$$<br>a_{j} y_jx_j·x_i+b=1/y_j=y_j<br>$$$$<br>b=y_j-\sum_{i=1}^N{a_{i} y_i\left( x_i·x_j \right)}<br>$$</p>
<p>综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题的解$a$;再利用求得原始问题的解$w^<em>,b^</em>$,从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。</p>
<p><code>线性可分支持向量机学习算法</code></p>
<ul>
<li>输入：线性可分训练数据集$<br>T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}<br>$,其中,$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$</li>
<li>输出：最大间隔分离超平面和分类决策函数</li>
</ul>
<p>步骤如下</p>
<ul>
<li>1）构造并求解约束最优化问题</li>
</ul>
<p>$$<br>\underset{a}{\min}\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_i}}}<br>$$$$<br>s.t.    \sum_{i=1}^N{a_iy_j=0}<br>$$$$<br>a_i\geqslant 0, i=1,2,···,N<br>$$</p>
<p>求得最优解$a=(a_1,a_2,···,a_N)$</p>
<ul>
<li><p>2）计算<br>$$<br>w =\sum_i^{}{a_{i} y_ix_i}<br>$$并选择$a$的一个正分量$a_j&gt;0$，计算<br>$$b=y_j-\sum_{i=1}^N{a_{i} y_i\left( x_i·x_j \right)}<br>$$</p>
</li>
<li><p>3） 求得分离超平面<br>$$w·x+b=0<br>$$分类决策函数：<br>$$f(x)=sign(w·x+b)$$</p>
</li>
</ul>
<p>在线性可分支持向量机中，$w和b$只依赖于训练数据中对应于$a_i&gt;0$的样本点$x_i,y_i$,而其他样本点对$w和b$没有影响。我们将训练数据中对应于$a_i&gt;0$的实例点$x_i\in R^n$称为<strong>支持向量</strong>。</p>
<p>对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。</p>
<h2 id="二、线性支持向量机与软间隔最大化"><a href="#二、线性支持向量机与软间隔最大化" class="headerlink" title="二、线性支持向量机与软间隔最大化"></a>二、线性支持向量机与软间隔最大化</h2><h3 id="2-1-线性支持向量机"><a href="#2-1-线性支持向量机" class="headerlink" title="2.1 线性支持向量机"></a>2.1 线性支持向量机</h3><p>通常情况是，训练数据中有一些特异点<code>outlier</code>，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。</p>
<p>线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$<br>\xi \geqslant 0<br>$，使函数间隔加上松弛变量大于等于1.这样，约束条件变成$$<br>y_i\left( w·x_i+b \right) \geqslant 1-\xi _i<br>$$同时，对每个松弛变量$<br>\xi \geqslant 0<br>$，支付一个代价$<br>\xi \geqslant 0<br>$。当然，如果我们允许$<br>\xi \geqslant 0<br>$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $<br>\xi \geqslant 0<br>$的总和也要最小：目标函数由原来的$<br>\frac{1}{2}||w||^2<br>$变成$$<br>\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}<br>$$<br>这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，控制目标函数中两项（“寻找 $margin$ 最大的超平面”和“保证数据点偏差量最小”）之间的权重，$C$越大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$<br>\frac{1}{2}||w||^2<br>$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。</p>
<p><code>线性支持向量机</code><br>对于给定的线性不可分的训练数据集，通过求解凸二次规划问题：<br>$$<br>\underset{w,b,\xi}{\min} \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}<br>$$$$<br>s.t.  y_i\left( w·x_i+b \right) \geqslant 1-\xi _i , i=1,2,···,N<br>$$$$<br>\xi _i\geqslant 0, i=1,2,···\mathrm{，}N<br>$$<br>可证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间。</p>
<p>用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数：<br>$$<br>L\left( w,b,\xi ,a,u \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i-\sum_{i=1}^N{a_i\left( y_i\left( w·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^N{u_i\xi _i}}}<br>$$</p>
<p>首先求拉格朗日函数针对$<br>w,b,\xi<br>$的极小。$$<br>\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^N{a_iy_ix_i}<br>$$$$<br>\frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>\frac{\partial L}{\partial \xi _i}=0\Rightarrow C-a_i-u_i=0，i=1,2,3···,N<br>$$<br>将它们代入拉格朗日函数，得到和原来一样的目标函数。<br>$$<br>\underset{a}{\max}  -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}<br>$$$$<br>s.t. \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>C-a_i-u_i=0<br>$$$$<br>a_i\geqslant 0<br>$$$$<br>u_i\geqslant 0<br>$$</p>
<p>不过，由于我们得到$C-a_i-u_i=0$，而又有$u_i&gt;0$（作为拉格朗日乘子的条件）,因此有$a_i≤C$,所以整个<code>dual</code>问题现在写作：<br>$$<br>\underset{a}{\max}  -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}<br>$$$$<br>s.t. \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>0\le a_i\le C ,  i=1,2,···,N<br>$$<br>和之前的结果对比一下，可以看到唯一的区别就是现在拉格朗日乘子$a$多了一个上限$C$。</p>
<p>构造并求解上述二次规划问题后求得最优解<br>$$<br>a=\left( a_{1},a_{2},···,a_{N} \right) ^T<br>$$<br>然后计算$$<br>w=\sum_{i=1}^N{a_{i}y_ix_i}<br>$$<br>选择$a$的一个分量$a_i$适合约束条件$0&lt;a_i&lt;C$,计算$$<br>b=y_j-\sum_{i=1}^N{y_ia_{i} \left( x_i·x_j \right)}<br>$$<br>对任一适合条件都可求得一个$b$，但是由于原始问题对$b$的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。</p>
<h3 id="2-2-支持向量"><a href="#2-2-支持向量" class="headerlink" title="2.2 支持向量"></a>2.2 支持向量</h3><p>再现性不可分的情况下，将对偶问题的解中对应于$a_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。如图所示，这时的支持向量要比线性可分时的情况复杂一些。</p>
<center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8810.22.45.png" alt="屏幕快照 2017-03-20 上午10.22.45"></center>

<p>图中，分离超平面由实线表示，间隔边界由虚线表示。正例点由$。$表示，负例点由$×$表示。图中还标出了实例$x_i$到间隔边界的距离$<br>\frac{\xi _i}{||w||}<br>$。</p>
<p>软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。</p>
<p>若$a_i^*&lt;C$，则$<br>\xi _i=0<br>$，支持向量恰好落在间隔边界上；</p>
<p>若$a_i^*=C,0&lt;<br>\xi _i&lt;1<br>$，则分类正确，$x_i$在间隔边界与分离超平面之间；</p>
<p>若$a_i^*=C，\xi _i=1$则$x_i$在分隔超平面上；</p>
<p>若$a_i^*=C,\xi _i&gt;1$，则$x_i$位于分离超平面误分一侧。</p>
<h2 id="三、非线性支持向量机与核函数"><a href="#三、非线性支持向量机与核函数" class="headerlink" title="三、非线性支持向量机与核函数"></a>三、非线性支持向量机与核函数</h2><h3 id="3-1-核技巧"><a href="#3-1-核技巧" class="headerlink" title="3.1 核技巧"></a>3.1 核技巧</h3><p>前面我们介绍了线性情况下的支持向量机，他通过寻找一个现行的超平面来达到对数据线性分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。</p>
<p>例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要他是线性的，就没有办法处理，SVM也不行。因为这样的数据本身就是线性不可分的。</p>
<center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8811.03.17.png" alt="屏幕快照 2017-03-20 上午11.03.17"></center>

<p>此数据集为两个半径不同的圆圈加上了少量的噪音得到，所以一个理想的分界应该是一个圆圈而不是一条直线。如果用$X_1和X_2$来表示这个二维平面的两个坐标的话，则此方程可以写作<br>$$<br>a_1X_1+a_2X_{1}^{2}+a_3X_2+a_4X_{2}^{2}+a_5X_1X_2+a_6=0<br>$$</p>
<p>注意上面的形式，如果我们构造另外一个无谓的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1·X_2$</p>
<p>那么显然，上面的方程在新的坐标系下可以写作：<br>$$<br>\sum_{i=1}^5{a_iZ_i+}a_6=0<br>$$</p>
<p>如果我们做一个映射$<br>\phi :R^2\rightarrow R^5<br>$，将$X$按照上面的规则映射为$Z$那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推倒的线性分类算法就可以进行处理了。这正是核方法处理非线性问题的基本思想。</p>
<p>总结一下，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。此即核技巧</p>
<p>核技巧应用到支持向量机，其基本思想就是通过一个非线性变换将输入空间（欧式空间$R^n或离散集合$）对应于一个特征空间（希尔伯特空间），使得在输入空间$R^n$中的超曲面模型对应于特征空间中的超平面模型（支持向量机），这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<p>现在回到SVM的情形，假设原始的数据是非线性的，我们通过一个映射$<br>\phi \left( · \right)<br>$将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量$w$，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的高斯核函数），要表示一个无穷维的向量描述起来就比较麻烦。</p>
<p>我们似乎可以这样做，拿到非线性数据，就找一个映射$<br>\phi（·）<br>$，然后一股脑把原来的数据映射到新空间，再做线性SVM即可。但是在之前对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；但如果原始空间是三维，我们就会得到19维的新空间，这个数目是呈爆炸性增长的，这给映射的计算带来了很大困难，而且如果遇到无穷维的情况，就根本无从计算了，所以就需要核函数出马了。</p>
<p>核技巧的想法是，再学习与预测中只定义核函数$<br>K\left( x,z \right)<br>$，而不显式地定义映射函数$<br>\phi（·）<br>$。不像之前是映射到高维空间中，然后再根据内积公式进行计算，现在我们直接在原来的低维空间中进行计算，而不需要显式的写出映射后的结果。通常，直接计算$K(x,z)$比较容易，而通过$<br>\phi \left( x \right) \mathrm{和}\phi \left( z \right)<br>$计算$K(x,z)$并不容易。</p>
<p>最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的$<br>\phi（·）<br>$，然后通过这个$<br>\phi（·）<br>$得到对应的$K(·，·)$进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到的，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常是“胡乱”选择一个核函数即可——我们直到她对应了某个映射，虽然我们不知道这个映射具体是什么，由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。</p>
<p>我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i·x_j)=\phi \left( x_i \right) ·\phi \left( x_j \right) $来代替，此时对偶问题的目标函数成为：<br>$$<br>\underset{a}{\max}    \sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}}<br>$$</p>
<p>同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为$$<br>sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)<br>$$</p>
<p>在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐性地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的奇数。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。</p>
<h3 id="3-2-常用核函数"><a href="#3-2-常用核函数" class="headerlink" title="3.2 常用核函数"></a>3.2 常用核函数</h3><p>通常人们会从一些常用的核函数中选择，根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数。</p>
<ul>
<li><p>多项式核函数<code>polynomial kernel function</code><br>$$<br>K\left( x,z \right) =\left( x·z+1 \right) ^p<br>$$对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为$$<br>f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\left( x_i·x+1 \right) ^p+b} \right)<br>$$</p>
</li>
<li><p>高斯核函数<code>gaussian kernel function</code><br>$$<br>K\left( x,z \right) =\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right)<br>$$<br>这个核就是会将原始空间映射为无穷维空间的那个家伙。不过如果$\sigma$选得很大的话，高次特征上的权重实际上衰减的非常块=快，所以实际上相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分，当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最为广泛的核函数之一。<br>它对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数称为</p>
</li>
</ul>
<p>$$<br>f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right) +b} \right)<br>$$</p>
<ul>
<li>字符串核函数</li>
</ul>
<p>核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上，比如，字符串核实定义在字符串集合上的核函数，字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。</p>
<p><code>非线性支持向量机学习算法</code></p>
<ul>
<li>输入：线性可分训练数据集$<br>T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}<br>$,其中,$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$</li>
<li>输出：分类决策函数</li>
</ul>
<p>步骤如下：</p>
<ul>
<li><p>1）选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题$$<br>\underset{a}{\max}\,\,\,\,\,\,\,\,\sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}}<br>$$$$<br>s.t.    \sum_{i=1}^N{a_iy_i=0}<br>$$$$<br>0\le a_i\le C  ,  i=1,2,···,N<br>$$<br>求得最优解$$<br>a^<em>=\left( a_{1}^{</em>},a_{2}^{<em>},···,a_{N}^{</em>} \right) ^T<br>$$</p>
</li>
<li><p>2）选择$a^<em>$的一个正分量$a_i^</em>$适合约束条件$0&lt;a_i&lt;C$,计算$$<br>b^<em>=y_j-\sum_{i=1}^N{y_ia_{i}^{</em>}K\left( x_i·x_j \right)}<br>$$</p>
</li>
<li><p>3）构造决策函数：<br>$$<br>f(X)=sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)<br>$$<br>当$K(x,z)$是正定核函数时，该问题为凸二次规划问题，解是存在的。</p>
</li>
</ul>
<h2 id="四、序列最小最优化算法（SMO）"><a href="#四、序列最小最优化算法（SMO）" class="headerlink" title="四、序列最小最优化算法（SMO）"></a>四、序列最小最优化算法（SMO）</h2><p>通常对于优化问题，我们没有办法的时候就会想到最笨的办法，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以$Gradient  Descent$和$Gradient Ascend$并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的$Gradient Descent$并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于二次规划问题，极值只有一个，所以是没有局部极值的问题。</p>
<p>另外还有一种叫做$Coordinate Descend$的变种，它每次只选择一个维度，例如$a=(a_1,···,a_n)$，它每次选取$a_i$为变量，而将其他都看成是常数，从而原始的问题在这一步编程一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。$Coordinate Descend$的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当$a$只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。</p>
<p>然后这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的二次规划问题，也是很好求解的。但是有了约束条件，结果让$Coordinate Descend$变得很尴尬了，直接根据第二个约束条件$<br>\sum_{i=1}^N{a_iy_i=0}<br>$,$a_1$的值立即就可以定下来，事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切停留在初始值。</p>
<p>所以SMO一次选取了两个坐标来进行优化。例如，我们假设现在选取$a_1$和$a_2$为变量，其余为常量，则根据约束条件我们有：$$<br>\sum_{i=1}^N{a_iy_i=0}\Longrightarrow a_2=\frac{1}{y_2}\left( -\sum_{i=3}^N{a_iy_i-a_1y_1} \right) \Longleftrightarrow y_2\left( K-a_1y_1 \right)<br>$$<br>其中那个从3到n的作和都是常量，我们统一记作K。将这个式子代入原来的目标函数中，可以消去$a_2$，从而变成一个一元二次函数。总之现在变成了一个带区间约束的一元二次函数极值问题。唯一要注意的就是这里的约束条件，一个就是$a_1$本身需要满足$0≤a_i≤C$,然后由于$a_2$也要满足同样的约束，即：<br>$0≤y_2(K-a_1y_1)≤C$，可以得带$a_1$的一个可行区间，同$[0,C]$交集即可得到最终的可行区间。投影到$a_1$轴上所对应的区间即是$a_1$的取值范围，在这个区间内求二次函数的最大值即可完成SMO的一步迭代。</p>
<p>同$Coordinate Descent$一样，SMO也会选取不同的两个$coordinate$维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO也并不是一次或随机地选取两个坐标函数极值问题，而是有一些启发式的策略来选取最优的两个坐标维度。</p>
<h2 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h2><p><a href="http://blog.pluskid.org/?p=632" target="_blank" rel="external"><code>pluskid支持向量机系列</code></a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/SVM/" rel="tag"># SVM</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/05/机器学习算法系列（13）：推荐系统/" rel="next" title="机器学习算法系列（13）：推荐系统">
                <i class="fa fa-chevron-left"></i> 机器学习算法系列（13）：推荐系统
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/02/20/机器学习算法系列（11）：聚类/" rel="prev" title="机器学习算法系列（11）：聚类">
                机器学习算法系列（11）：聚类 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/img/v.png"
               alt="狗皮膏药" />
          <p class="site-author-name" itemprop="name">狗皮膏药</p>
          <p class="site-description motion-element" itemprop="description">在隆冬，我终于知道，我身上有一个不可战胜的夏天</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">47</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">64</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、线性可分支持向量机与硬间隔最大化"><span class="nav-text">一、线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-线性可分支持向量机"><span class="nav-text">1.1 线性可分支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-函数间隔与几何间隔"><span class="nav-text">1.2 函数间隔与几何间隔</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-间隔最大化"><span class="nav-text">1.3 间隔最大化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-学习的对偶算法"><span class="nav-text">1.4 学习的对偶算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、线性支持向量机与软间隔最大化"><span class="nav-text">二、线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-线性支持向量机"><span class="nav-text">2.1 线性支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-支持向量"><span class="nav-text">2.2 支持向量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、非线性支持向量机与核函数"><span class="nav-text">三、非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-核技巧"><span class="nav-text">3.1 核技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-常用核函数"><span class="nav-text">3.2 常用核函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、序列最小最优化算法（SMO）"><span class="nav-text">四、序列最小最优化算法（SMO）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、参考资料"><span class="nav-text">五、参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">狗皮膏药</span>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	




  
  

  
  


  

  

  


</body>
</html>

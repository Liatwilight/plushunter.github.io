<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[数据分析系列（5）：窗口函数]]></title>
      <url>/2018/08/25/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>这篇文章梳理了几个关于分组排序的SQL语句，在查询的时候常会遇到分组排序的场景，比如找出每个群体中中排名前十的用户，找出某一类商品销量最高的几个商品等等。它们分别是：</p>
<ul>
<li>row_number() over()</li>
<li>rank( ) over( )</li>
<li>dense_rank( ) over( )</li>
</ul>
<p>由此引申开来，讲述一下窗口函数的功用。</p>
<a id="more"></a>
<h2 id="一、row-number-over-排序功能："><a href="#一、row-number-over-排序功能：" class="headerlink" title="一、row_number() over()排序功能："></a>一、row_number() over()排序功能：</h2><h3 id="1-1-row-number-over-无分组排序"><a href="#1-1-row-number-over-无分组排序" class="headerlink" title="1.1 row_number( ) over( )无分组排序"></a>1.1 row_number( ) over( )无分组排序</h3><p>若不对其进行分组，则此时row_number() over()的功能与rownum一致，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">SELECT </div><div class="line">    empno,</div><div class="line">    WORKDEPT,</div><div class="line">    SALARY, </div><div class="line">    Row_Number() OVER (ORDER BY salary desc) rank </div><div class="line">FROM </div><div class="line">    employee   </div><div class="line">--------------------------------------  </div><div class="line">000010  A00 152750  1  </div><div class="line">000030  C01 98250   2  </div><div class="line">000070  D21 96170   3  </div><div class="line">000020  B01 94250   4  </div><div class="line">000090  E11 89750   5  </div><div class="line">000100  E21 86150   6  </div><div class="line">000050  E01 80175   7  </div><div class="line">000130  C01 73800   8  </div><div class="line">000060  D11 72250   9</div></pre></td></tr></table></figure>
<p>但row_number() over()更强大的地方在于可以进行分组排序。</p>
<h3 id="1-2-row-number-over-分组排序"><a href="#1-2-row-number-over-分组排序" class="headerlink" title="1.2 row_number( ) over( )分组排序"></a>1.2 row_number( ) over( )分组排序</h3><ul>
<li>partition by <em> ： 根据</em>字段进行分组</li>
<li>order by <em>  desc：在分组后的各组内根据</em>字段进行降序排序</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">SELECT </div><div class="line">    empno,</div><div class="line">    WORKDEPT,</div><div class="line">    SALARY, </div><div class="line">    Row_Number() OVER (partition by workdept ORDER BY salary desc) rank </div><div class="line">FROM</div><div class="line">    employee   </div><div class="line">--------------------------------------  </div><div class="line">000010  A00 152750  1  </div><div class="line">000110  A00 66500   2  </div><div class="line">000120  A00 49250   3  </div><div class="line">200010  A00 46500   4  </div><div class="line">200120  A00 39250   5  </div><div class="line">000020  B01 94250   1  </div><div class="line">000030  C01 98250   1  </div><div class="line">000130  C01 73800   2</div></pre></td></tr></table></figure>
<ul>
<li><p>注意：使用 row_number() over()函数时候，over()里头的分组以及排序的执行晚于 where group by  order by 的执行</p>
</li>
<li><p>如果没有指定partition by，那么它把整个结果集作为一个分组，与聚合函数不同的地方在于它能够返回一个分组中的多条记录，而聚合函数一般只有一个反映统计值的记录</p>
</li>
<li><p>row_number() over()返回的排名是没有重复值的，也就是即使遇到相同的值其排名也有先后，但具体哪个在前，具有不确定性，详细可见该<a href="https://docs.microsoft.com/zh-cn/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-2017" target="_blank" rel="noopener">链接1</a>和<a href="https://docs.microsoft.com/zh-cn/sql/t-sql/functions/row-number-transact-sql?view=sql-server-2017" target="_blank" rel="noopener">链接2</a>，具体的例子可以看文末的对比。</p>
</li>
</ul>
<h3 id="1-3-使用row-number-over-去重"><a href="#1-3-使用row-number-over-去重" class="headerlink" title="1.3 使用row_number( ) over( )去重"></a>1.3 使用row_number( ) over( )去重</h3><p>假设表TAB中有a,b,c三列，可以使用下列语句删除a,b,c都相同的重复行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">select </div><div class="line">    year,</div><div class="line">    QUARTER,</div><div class="line">    RESULTS,</div><div class="line">    row_number() over(partition by YEAR,QUARTER,RESULTS order by YEAR,QUARTER,RESULTS) AS ROW_NO </div><div class="line">FROM </div><div class="line">    SALE </div><div class="line">WHERE </div><div class="line">    ROW_NO=1</div></pre></td></tr></table></figure>
<p>所以除了使用distinct进行去重，还可以用row_number( ) over( )去重。需要注意的有以下几点：</p>
<ul>
<li>使用关键字 distinct 去重，其作用于单个字段和多个字段的时候是不同的，作用于单个字段时，其“去重”的是表中所有该字段值重复的数据；作用于多个字段的时候，其“去重”的表中所有字段值都相同的数据。</li>
<li>在使用函数 row_number() over() 的时候，其是按先分组排序后，再取出每组的第一条记录来进行“去重”的</li>
</ul>
<h2 id="二、rank-over"><a href="#二、rank-over" class="headerlink" title="二、rank( ) over( )"></a>二、rank( ) over( )</h2><p>这是跳跃排序，在同一个分组内，若有两个第一名时接下来就是第三名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">select </div><div class="line">    workdept,</div><div class="line">    salary,</div><div class="line">    rank() over(partition by workdept order by salary) as dense_rank_order </div><div class="line">from </div><div class="line">    emp </div><div class="line">order by </div><div class="line">    workdept</div><div class="line">------------------  </div><div class="line">A00 39250   1  </div><div class="line">A00 46500   2  </div><div class="line">A00 49250   3  </div><div class="line">A00 66500   4  </div><div class="line">A00 152750  5  </div><div class="line">B01 94250   1  </div><div class="line">C01 68420   1  </div><div class="line">C01 68420   1  </div><div class="line">C01 73800   3</div></pre></td></tr></table></figure>
<h2 id="三、dense-rank-over"><a href="#三、dense-rank-over" class="headerlink" title="三、dense_rank( ) over( )"></a>三、dense_rank( ) over( )</h2><p>这是连续排序，在同一个分组内，有两个第一名时仍然跟着第二名。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">select </div><div class="line">    workdept,</div><div class="line">    salary,</div><div class="line">    dense_rank() over(partition by workdept order by salary) as dense_rank_order </div><div class="line">from </div><div class="line">    emp </div><div class="line">order by </div><div class="line">    workdept</div><div class="line">------------------  </div><div class="line">A00 39250   1  </div><div class="line">A00 46500   2  </div><div class="line">A00 49250   3  </div><div class="line">A00 66500   4  </div><div class="line">A00 152750  5  </div><div class="line">B01 94250   1  </div><div class="line">C01 68420   1  </div><div class="line">C01 68420   1  </div><div class="line">C01 73800   2  </div><div class="line">C01 98250   3</div></pre></td></tr></table></figure>
<h2 id="四、三者对比"><a href="#四、三者对比" class="headerlink" title="四、三者对比"></a>四、三者对比</h2><p>这个例子可以直观地看到，rank() over()是跳跃排序，dense_rank() over()是连续排序，row_number() over()是非重复排序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">select </div><div class="line">    region_id, </div><div class="line">    customer_id, </div><div class="line">    sum(customer_sales) total,</div><div class="line">    rank() over(order by sum(customer_sales) desc) rank,</div><div class="line">    dense_rank() over(order by sum(customer_sales) desc) dense_rank,</div><div class="line">    row_number() over(order by sum(customer_sales) desc) row_number</div><div class="line">from </div><div class="line">    user_order</div><div class="line">group by </div><div class="line">    region_id, customer_id;</div><div class="line"></div><div class="line"> REGION_ID CUSTOMER_ID      TOTAL       RANK DENSE_RANK ROW_NUMBER</div><div class="line">---------- ----------- ---------- ---------- ---------- ----------</div><div class="line">            </div><div class="line">8          18           1253840         11         11         11</div><div class="line">5          2            1224992         12         12         12</div><div class="line">9          23           1224992         12         12         13</div><div class="line">9          24           1224992         12         12         14</div><div class="line">10         30           1216858         15         13         15</div></pre></td></tr></table></figure>
<ul>
<li>在这个例子中也可以看到order by 的关键字可以是group by之后的聚合函数。</li>
</ul>
<h2 id="五、窗口函数"><a href="#五、窗口函数" class="headerlink" title="五、窗口函数"></a>五、窗口函数</h2><p>以上我们介绍了RANK、DENSE_RANK、ROW_NUMBER这几个函数，其实在数据库中，它们被称为窗口函数，窗口函数可以进行排序、生成序列号等一般的聚合函数无法完成的操作。它也称为OLAP函数。OLAP是OnLine Analytical Processing的简称，意思是对数据库进行实时分析处理。窗口函数就是为了实现OLAP而添加的标准SQL功能。</p>
<p>它的基本语法是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&lt;窗口函数&gt; OVER ( [PARTITION BY &lt;列清单&gt;]</div><div class="line">                       ORDER BY &lt;排序用列清单&gt;)</div></pre></td></tr></table></figure>
<p>其中重要的关键字是PARTITON BY 和ORDER BY，理解这两个关键字的作用是帮助我们理解窗口函数的关键。</p>
<p>窗口函数大致可以分为两种：</p>
<ul>
<li>RANK、DENSE_RANK、ROW_NUMBER等专用窗口函数</li>
<li>能够作为窗口函数的聚合函数 （SUM, AVG,COUNT,MAX,MIN）</li>
</ul>
<p>我们上面已经介绍过RANK、DENSE_RANK、ROW_NUMBER，可以看到它们兼具了GROUP BY子句的分组功能以及ORDER BY子句的排序功能。但是PARTITION BY不具备GROUP BY子句的汇总功能。所以使用RANK函数不会减少原表中记录的行数。</p>
<p>通过PARTITION BY分组后的记录集合称为窗口。此处的窗口表示范围。</p>
<p>目前为止我们学过的函数大多数都没有使用位置的限制，最多也就是在WHERE子句不能使用聚合函数。但是，使用窗口函数的位置却有很大的限制，确切的说，窗口函数只能在SELECT子句中使用。</p>
<p>所有的聚合函数都能用作窗口函数，且使用语法与专用窗口函数完全相同。</p>
<h3 id="5-1-窗口函数——SUM"><a href="#5-1-窗口函数——SUM" class="headerlink" title="5.1 窗口函数——SUM"></a>5.1 窗口函数——SUM</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">--将SUM函数作为窗口函数使用</div><div class="line"> SELECT </div><div class="line">    product_id, </div><div class="line">    product_name, </div><div class="line">    sale_price,        </div><div class="line">    SUM(sale_price) OVER (ORDER BY product_id) AS current_sum   </div><div class="line">FROM </div><div class="line">    Product;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">product_id | product_name | sale_price | current_sum------------+--------------+------------+-------------</div><div class="line"> 0001       | T衫          |       1000 |        1000</div><div class="line"> 0002       | 打孔器       |        500 |        1500</div><div class="line"> 0003       | 运动T衫      |       4000 |        5500</div><div class="line"> 0004       | 菜刀         |       3000 |        8500</div><div class="line"> 0005       | 高压锅       |       6800 |       15300</div><div class="line"> 0006       | 叉子         |        500 |       15800</div><div class="line"> 0007       | 擦菜板       |        880 |       16680</div><div class="line"> 0008       | 圆珠笔       |        100 |       16780</div><div class="line">(8 行记录)</div></pre></td></tr></table></figure>
<p>使用聚合函数作为窗口函数时，需要在其括号内指定相应的列。像上例中，使用sale_price(销售单价)作为累加的对象, current_sum的结果为在它之前的销售单价的合计。这种统计方法称为累计。</p>
<h3 id="5-2-窗口函数——AVG"><a href="#5-2-窗口函数——AVG" class="headerlink" title="5.2 窗口函数——AVG"></a>5.2 窗口函数——AVG</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">--将AVG函数作为窗口函数使用</div><div class="line">SELECT </div><div class="line">    product_id, </div><div class="line">    product_name, </div><div class="line">    sale_price,        </div><div class="line">    AVG(sale_price) OVER (ORDER BY product_id) AS current_avg   </div><div class="line">FROM </div><div class="line">    Product;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">product_id | product_name | sale_price |      current_avg------------+--------------+------------+-----------------------</div><div class="line"> 0001       | T衫          |       1000 | 1000.0000000000000000</div><div class="line"> 0002       | 打孔器       |        500 |  750.0000000000000000</div><div class="line"> 0003       | 运动T衫      |       4000 | 1833.3333333333333333</div><div class="line"> 0004       | 菜刀         |       3000 | 2125.0000000000000000</div><div class="line"> 0005       | 高压锅       |       6800 | 3060.0000000000000000</div><div class="line"> 0006       | 叉子         |        500 | 2633.3333333333333333</div><div class="line"> 0007       | 擦菜板       |        880 | 2382.8571428571428571</div><div class="line"> 0008       | 圆珠笔       |        100 | 2097.5000000000000000</div><div class="line">(8 行记录)</div></pre></td></tr></table></figure>
<p>current_avg的结果为在它之前的销售单价的平均值。像这样以“自身记录”（当前记录）作为基准进行统计，就是将聚合函数作为窗口函数使用时的最大特征。</p>
<h3 id="5-2-移动平均"><a href="#5-2-移动平均" class="headerlink" title="5.2 移动平均"></a>5.2 移动平均</h3><p>窗口函数就是将表以窗口为单位进行分割，并在其中进行排序的函数。其中还包含在窗口中指定更详细的汇总范围的备选功能，这种备选功能中的汇总范围称为框架。</p>
<p>例如，指定“最靠近的3行”作为汇总对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">--指定“最靠近的3行”作为汇总对象</div><div class="line"> SELECT </div><div class="line">    product_id, </div><div class="line">    product_name, </div><div class="line">    sale_price,        </div><div class="line">    AVG(sale_price) OVER (ORDER BY product_id  ROWS 2 PRECEDING) AS moving_avg   </div><div class="line">FROM </div><div class="line">    Product;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">product_id | product_name | sale_price |      moving_avg------------+--------------+------------+-----------------------</div><div class="line"> 0001       | T衫          |       1000 | 1000.0000000000000000</div><div class="line"> 0002       | 打孔器       |        500 |  750.0000000000000000</div><div class="line"> 0003       | 运动T衫      |       4000 | 1833.3333333333333333</div><div class="line"> 0004       | 菜刀         |       3000 | 2500.0000000000000000</div><div class="line"> 0005       | 高压锅       |       6800 | 4600.0000000000000000</div><div class="line"> 0006       | 叉子         |        500 | 3433.3333333333333333</div><div class="line"> 0007       | 擦菜板       |        880 | 2726.6666666666666667</div><div class="line"> 0008       | 圆珠笔       |        100 |  493.3333333333333333</div><div class="line">(8 行记录)</div></pre></td></tr></table></figure>
<p>上例中，我们使用了ROWS（行）和PRECEDING(之前)两个关键字，将框架指定为“截止到之前~行”，因此，“ ROWS 2 PRECEDING”意思就是将框架指定为“截止到之前2行”，也就是“最靠近的3行”。如果将条件中的数字改为“ROWS 5 PRECEDING”，就是“截止到之前5行”（最靠近的6行）的意思。这样的统计方法称为移动平均。</p>
<p>使用关键字FOLLOWING(之后)替换PRECEDING，就可以指定“截止到之后~行”作为框架。</p>
<p>如果希望将当前记录的前后行作为汇总对象，可以同时使用PRECEDING(之前)和FOLLOWING（之后）关键字来实现。<br>例，将当前记录的前后行作为汇总对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> --将当前记录的前后行作为汇总对象</div><div class="line">SELECT </div><div class="line">    product_id, </div><div class="line">    product_name, </div><div class="line">    sale_price,       </div><div class="line">    AVG(sale_price) OVER (ORDER BY product_id ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avg   </div><div class="line">FROM </div><div class="line">    Product;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">product_id | product_name | sale_price |      moving_avg------------+--------------+------------+-----------------------</div><div class="line"> 0001       | T衫          |       1000 |  750.0000000000000000</div><div class="line"> 0002       | 打孔器       |        500 | 1833.3333333333333333</div><div class="line"> 0003       | 运动T衫      |       4000 | 2500.0000000000000000</div><div class="line"> 0004       | 菜刀         |       3000 | 4600.0000000000000000</div><div class="line"> 0005       | 高压锅       |       6800 | 3433.3333333333333333</div><div class="line"> 0006       | 叉子         |        500 | 2726.6666666666666667</div><div class="line"> 0007       | 擦菜板       |        880 |  493.3333333333333333</div><div class="line"> 0008       | 圆珠笔       |        100 |  490.0000000000000000</div><div class="line">(8 行记录)</div></pre></td></tr></table></figure>
<p>当前记录的前后行的具体含义就是：</p>
<ul>
<li>之前1行的记录</li>
<li>自身（当前记录）</li>
<li>之后1行的记录</li>
</ul>
<p>如果能够熟练掌握框架功能，就可以称为窗口函数高手了。</p>
]]></content>
      
        <categories>
            
            <category> 数据分析 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> row_number( ) over( ) </tag>
            
            <tag> rank() over() </tag>
            
            <tag> dense_rank() over() </tag>
            
            <tag> distinct </tag>
            
            <tag> 窗口函数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析系列（4）：基于ARMA模型的资金渠道流入流出预测]]></title>
      <url>/2018/05/23/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%9F%BA%E4%BA%8EARMA%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B5%84%E9%87%91%E6%B8%A0%E9%81%93%E6%B5%81%E5%85%A5%E6%B5%81%E5%87%BA%E9%A2%84%E6%B5%8B/</url>
      <content type="html"><![CDATA[<p>该数据为某资金渠道从2017/01/01至2017/07/29这段时间内的清算金额汇总信息，每5分钟按流入流出汇总交易数据，type为SE表示流入，type为RE表示流出，其中2017/01/01至2017/07/23这段时间的每天各时段的流入流出汇总数据是齐全的，而2017/07/24至2017/07/29这六天时间只有0点到早上10点的数据。现在要求我们根据这些数据来预测这六天每天从当日0点到24点的流入流出轧差值（总流入-总流出）。</p>
<a id="more"></a>
<p>通过观察我们可以得知这些资金流入流出数据是各时间点上形成的数值序列，而时间序列分析就是通过观察历史数据预测未来的值，适用于我们这次预测。</p>
<p>自回归移动平均模型(ARMA(p，q))是时间序列中最为重要的模型之一，它主要由两部分组成： AR代表p阶自回归过程，MA代表q阶移动平均过程，其公式如下：</p>
<script type="math/tex; mode=display">Z_t=\varphi _1z_{t-1}+\varphi _2z_{t-2 }+···+\varphi _pz_{t-p}+a_t-\theta_1a_{t-1}-···-\theta_qa_{t-q}</script><p>可以对其简化：</p>
<script type="math/tex; mode=display">\varphi_p(B)z_t=\theta_q(B)a_t\\其中\varphi_p(B)=1-\varphi_1B-\varphi_2B^2-···-\varphi_pB^p\\\theta_q(B)=1-\theta_1B-\theta_2B^2-···-\theta_qB^p</script><h2 id="一、分析过程"><a href="#一、分析过程" class="headerlink" title="一、分析过程"></a>一、分析过程</h2><h3 id="1-1-数据预处理"><a href="#1-1-数据预处理" class="headerlink" title="1.1 数据预处理"></a>1.1 数据预处理</h3><p>首先要对数据进行转换处理，type为SE的amount保持不变，type为RE的amount取负数，对缺失值记为0，这样可以方便之后处理，将数据存入字典中，key为时间，value为资金（有正负）。处理脚本如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">Data_Process</span><span class="params">(input_file,header= True)</span>:</span></div><div class="line">    amount_dict=&#123;&#125;</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(input_file):</div><div class="line">        term = line.strip().split(<span class="string">','</span>)   </div><div class="line">        <span class="keyword">if</span> header:</div><div class="line">            header =<span class="keyword">False</span></div><div class="line">            <span class="keyword">continue</span></div><div class="line">        <span class="keyword">if</span> term[<span class="number">1</span>]==<span class="string">'SE'</span>: </div><div class="line">            <span class="keyword">if</span> term[<span class="number">2</span>] ==<span class="string">''</span>:</div><div class="line">                amount =<span class="number">0</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                amount = float(term[<span class="number">2</span>])</div><div class="line">        <span class="keyword">elif</span> term[<span class="number">1</span>]==<span class="string">'RE'</span>:</div><div class="line">            <span class="keyword">if</span> term[<span class="number">2</span>] ==<span class="string">''</span>:</div><div class="line">                amount =<span class="number">0</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                amount = <span class="number">-1</span>*float(term[<span class="number">2</span>])</div><div class="line">        time = datetime.strftime(datetime.strptime(term[<span class="number">0</span>],<span class="string">'%Y/%m/%d %H:%M'</span>),<span class="string">'%Y/%m/%d %H:%M'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> time <span class="keyword">in</span> amount_dict:</div><div class="line">            amount_dict[time] = round(amount,<span class="number">2</span>)</div><div class="line">        <span class="keyword">elif</span> time <span class="keyword">in</span> amount_dict:</div><div class="line">            amount_dict[time]+=round(amount,<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span>  amount_dict</div></pre></td></tr></table></figure>
<p>因为我们要对天级别的数据进行预测，可以对资金按天进行聚合：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataframe_trans_series</span><span class="params">(amount_dict,freq=<span class="string">'D'</span>)</span>:</span></div><div class="line">    data_dict=&#123;&#125;</div><div class="line">    data_dict[<span class="string">'amount'</span>] = amount_dict</div><div class="line">    data = pd.DataFrame(data_dict)</div><div class="line">    data.index = pd.to_datetime(data.index) </div><div class="line">    df = pd.DataFrame(data[<span class="string">'amount'</span>].resample(freq).sum())</div><div class="line">    time_series =  df[<span class="string">'amount'</span>]</div><div class="line">    <span class="keyword">return</span> time_series</div></pre></td></tr></table></figure>
<p>得到的中间数据如下所示，每一天对应一条数据，表示这一天的流入流出轧差值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>    <span class="number">123081.02</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>    <span class="number">150143.02</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-03</span>    <span class="number">161112.42</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>    <span class="number">138823.62</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>    <span class="number">130364.32</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>     <span class="number">85308.56</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>     <span class="number">77484.16</span></div><div class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>     <span class="number">93860.22</span></div><div class="line">······</div></pre></td></tr></table></figure>
<h3 id="1-2-绘制时序图"><a href="#1-2-绘制时序图" class="headerlink" title="1.2 绘制时序图"></a>1.2 绘制时序图</h3><p>按天聚合之后，我们可以简单看一下时序图、移动平均时序图和加权移动平均时序图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_trend</span><span class="params">(timeSeries, size)</span>:</span></div><div class="line">    f = plt.figure(facecolor=<span class="string">'white'</span>)</div><div class="line">    </div><div class="line">    rol_mean = timeSeries.rolling(window=size).mean()</div><div class="line">    rol_weighted_mean = pd.ewma(timeSeries, span=size)</div><div class="line">    timeSeries.plot(color=<span class="string">'blue'</span>, label=<span class="string">'Original'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">    plt.title(<span class="string">'Original'</span>)    </div><div class="line">    rol_mean.plot(color=<span class="string">'red'</span>, label=<span class="string">'Rolling Mean'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">    plt.title(<span class="string">'Rolling Mean'</span>)    </div><div class="line">    rol_weighted_mean.plot(color=<span class="string">'black'</span>, label=<span class="string">'Weighted Rolling Mean'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">    plt.title(<span class="string">'Weighted Rolling Mean'</span>)    </div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://omu7tit09.bkt.clouddn.com/15261985846825.jpg" alt=""></p>
<p>可以从时序图看出，该时间序列存在着波动，尚无法确定是否平稳，需要进一步的统计检验。</p>
<h3 id="1-3-平稳性检验"><a href="#1-3-平稳性检验" class="headerlink" title="1.3 平稳性检验"></a>1.3 平稳性检验</h3><p>序列的平稳性是进行时间序列分析的前提条件，单位根检验（ADF）是一种常用的平稳性检验方法，他的原假设为序列具有单位根，即非平稳，对于一个平稳的时序数据，就需要在给定的置信水平上显著，拒绝原假设。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_Stationarity</span><span class="params">(ts)</span>:</span></div><div class="line">    test = adfuller(ts)</div><div class="line">    output = pd.Series(test[<span class="number">0</span>:<span class="number">4</span>], index=[<span class="string">'Test Statistic'</span>,<span class="string">'p-value'</span>,<span class="string">'Lags Used'</span>,<span class="string">'Number of Observations Used'</span>])</div><div class="line">    <span class="keyword">for</span> key,value <span class="keyword">in</span> test[<span class="number">4</span>].items():</div><div class="line">        output[<span class="string">'Critical Value (%s)'</span>%key] = value</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<p>以下为检验结果，其p值小于0.01，说明拒绝原假设，得出此序列具有平稳性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Test Statistic                  <span class="number">-3.779937</span></div><div class="line">p-value                          <span class="number">0.003119</span></div><div class="line">Lags Used                        <span class="number">2.000000</span></div><div class="line">Number of Observations Used    <span class="number">201.000000</span></div><div class="line">Critical Value (<span class="number">5</span>%)             <span class="number">-2.876029</span></div><div class="line">Critical Value (<span class="number">1</span>%)             <span class="number">-3.463309</span></div><div class="line">Critical Value (<span class="number">10</span>%)            <span class="number">-2.574493</span></div></pre></td></tr></table></figure>
<p>当然也可以观察其自相关图，可以看到，其自相关图快速衰减，体现了平稳性序列的特点，可认为他是平稳序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_acf_pacf</span><span class="params">(ts, lags=<span class="number">31</span>)</span>:</span></div><div class="line">    f = plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>),facecolor=<span class="string">'white'</span>)</div><div class="line">    ax1 = f.add_subplot(<span class="number">211</span>)</div><div class="line">    plot_acf(ts, lags=<span class="number">31</span>, ax=ax1)</div><div class="line">    ax2 = f.add_subplot(<span class="number">212</span>)</div><div class="line">    plot_pacf(ts, lags=<span class="number">31</span>, ax=ax2)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://omu7tit09.bkt.clouddn.com/15261974529979.jpg" alt=""></p>
<h3 id="1-4-对数处理"><a href="#1-4-对数处理" class="headerlink" title="1.4 对数处理"></a>1.4 对数处理</h3><p>对数变换主要是为了减小数据的振动幅度，使其线性规律更加明显。对数变换相当于增加了一个惩罚机制，数据越大其惩罚越大，数据越小惩罚越小。虽然上一步我们验证了序列是平稳的，取对数也可以在该数据集上使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">Log_Process</span><span class="params">(time_series)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.log(time_series+<span class="number">1</span>)</div></pre></td></tr></table></figure>
<h3 id="1-5-纯随机性检验"><a href="#1-5-纯随机性检验" class="headerlink" title="1.5 纯随机性检验"></a>1.5 纯随机性检验</h3><p>经过平稳性检验之后，我们得到该序列为平稳性序列，接下来对其进行纯随机性检验，只有当时间序列不是一个白噪声即纯随机序列的时候，我们才可以对该序列做进一步的分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_stochastic</span><span class="params">(ts)</span>:</span></div><div class="line">    p_value = acorr_ljungbox(ts, lags=<span class="number">1</span>)[<span class="number">1</span>] </div><div class="line">    <span class="keyword">return</span> p_value</div></pre></td></tr></table></figure>
<p>得到的p值如下，可知，该序列为非白噪声序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">6.09097901e-21</span></div></pre></td></tr></table></figure>
<h3 id="1-6-确定ARMA的阶数"><a href="#1-6-确定ARMA的阶数" class="headerlink" title="1.6 确定ARMA的阶数"></a>1.6 确定ARMA的阶数</h3><p>由前面的平稳性检验和纯随机性检验可知，该序列为平稳序列且非白噪声序列，我们可以用ARMA(p,q)模型对其建模，这里关键的是p和q的定阶，我们可以通过观察自相关图ACF和偏自相关图PACF来识别，也可以借助AIC、BIC统计量来确定。</p>
<h4 id="1-6-1-BIC识别"><a href="#1-6-1-BIC识别" class="headerlink" title="1.6.1 BIC识别"></a>1.6.1 BIC识别</h4><p>以下是根据设定的maxlag，循环对p、q赋值，选出拟合BIC最小的p、q值作为模型的参数，并输出BIC最小的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_better_model</span><span class="params">(time_series, maxLag=<span class="number">5</span>)</span>:</span></div><div class="line">    init_bic = sys.maxint</div><div class="line">    init_p = <span class="number">0</span></div><div class="line">    init_q = <span class="number">0</span></div><div class="line">    init_properModel = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> np.arange(maxLag):</div><div class="line">        <span class="keyword">for</span> q <span class="keyword">in</span> np.arange(maxLag):</div><div class="line">            model = ARMA(time_series, order=(p, q))</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                results_ARMA = model.fit(disp=<span class="number">-1</span>, method=<span class="string">'css'</span>)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            bic = results_ARMA.bic</div><div class="line">            <span class="keyword">if</span> bic &lt; init_bic:</div><div class="line">                init_p = p</div><div class="line">                init_q = q</div><div class="line">                init_properModel = results_ARMA</div><div class="line">                init_bic = bic</div><div class="line">    <span class="keyword">return</span> init_bic, init_p, init_q, init_properModel</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init_bic = <span class="number">33.0754219416592</span>  init_p = <span class="number">1</span>  init_q = <span class="number">1</span></div></pre></td></tr></table></figure>
<p>得到使BIC最小的模型，bic = 33.1，p、q分别为1。</p>
<h4 id="1-6-2-ACF、BCF识别"><a href="#1-6-2-ACF、BCF识别" class="headerlink" title="1.6.2 ACF、BCF识别"></a>1.6.2 ACF、BCF识别</h4><p>通过观察自相关图ACF和偏自相关图PACF来识别。观察下图，发现自相关和偏相系数都存在拖尾的特点，并且他们都具有明显的一阶相关性，所以我们设定p=1, q=1。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15262026757996.jpg" alt=""></p>
<h3 id="1-7-拟合ARAM模型"><a href="#1-7-拟合ARAM模型" class="headerlink" title="1.7 拟合ARAM模型"></a>1.7 拟合ARAM模型</h3><p>选出BIC最小的模型，对训练集进行拟合。这里添加了一步将对数处理和差分处理还原的操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bic, p, q, properModel = choose_better_model(time_series_diff)    </div><div class="line">predict_time_series = properModel.predict()</div><div class="line">predict_time_series = recover(predict_time_series, time_series_train, best_diff)</div></pre></td></tr></table></figure>
<h3 id="1-8-训练集的拟合优度"><a href="#1-8-训练集的拟合优度" class="headerlink" title="1.8 训练集的拟合优度"></a>1.8 训练集的拟合优度</h3><p>用拟合好的模型对训练集进行预测，并使用均方根误差对效果进行衡量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">original_time_series = time_series[predict_time_series.index]  </div><div class="line">plt.figure(facecolor=<span class="string">'white'</span>)</div><div class="line">predict_time_series.plot(color=<span class="string">'blue'</span>, label=<span class="string">'Predict'</span>)</div><div class="line">original_time_series.plot(color=<span class="string">'red'</span>, label=<span class="string">'Original'</span>)    </div><div class="line">RMSE = np.sqrt(sum((predict_time_series-original_time_series)**<span class="number">2</span>)/original_time_series.size)</div><div class="line">plt.title(<span class="string">'RMSE: %.4f'</span>% RMSE)</div><div class="line">plt.show()</div><div class="line">print(<span class="string">"训练集的RMSE为："</span>+str(RMSE))</div></pre></td></tr></table></figure>
<p>训练集预测值和真实值拟合图如下：<img src="http://omu7tit09.bkt.clouddn.com/15261978277574.jpg" alt="">训练集的均方根误差为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">训练集的RMSE为：<span class="number">40024.97</span></div></pre></td></tr></table></figure>
<h3 id="1-9-预测结果"><a href="#1-9-预测结果" class="headerlink" title="1.9 预测结果"></a>1.9 预测结果</h3><p>用拟合好的模型对测试集进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">test_predict = properModel.predict(start=<span class="string">'2017-07-24'</span>, end=<span class="string">'2017-07-29'</span>)</div><div class="line">test_predict = recover(test_predict, time_series_train, best_diff)</div><div class="line"><span class="keyword">print</span> test_predict</div></pre></td></tr></table></figure>
<p>得到最终的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-24</span>    <span class="number">244854.689637</span></div><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-25</span>    <span class="number">222634.079094</span></div><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-26</span>    <span class="number">206351.989143</span></div><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-27</span>    <span class="number">194213.093708</span></div><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-28</span>    <span class="number">185037.296689</span></div><div class="line"><span class="number">2017</span><span class="number">-07</span><span class="number">-29</span>    <span class="number">178024.480144</span></div></pre></td></tr></table></figure>
<h2 id="二、问题"><a href="#二、问题" class="headerlink" title="二、问题"></a>二、问题</h2><h3 id="2-1-请给出你的分析过程、预测结果、建模脚本（要求可复现），包含必要的说明以及截图"><a href="#2-1-请给出你的分析过程、预测结果、建模脚本（要求可复现），包含必要的说明以及截图" class="headerlink" title="2.1 请给出你的分析过程、预测结果、建模脚本（要求可复现），包含必要的说明以及截图"></a>2.1 请给出你的分析过程、预测结果、建模脚本（要求可复现），包含必要的说明以及截图</h3><p>分析过程和预测结果如上所示，建模脚本见<a href="https://pan.baidu.com/s/1zPYrhyxNjVmlwht27ot01w" target="_blank" rel="noopener">百度网盘</a>，密码：9okf，包括两个文件，一个为jupyter notebook【SERE_PREDICT.ipynb】，一个为python脚本【SERE_PREDICT.py】，两者均可复现。实验环境为python2.7，实验所依赖的package包括：</p>
<ul>
<li>pandas</li>
<li>numpy</li>
<li>datetime</li>
<li>matplotlib</li>
<li>statsmodels</li>
</ul>
<h3 id="2-2-你认为你的模型预测结果如何，附上关键说明和截图"><a href="#2-2-你认为你的模型预测结果如何，附上关键说明和截图" class="headerlink" title="2.2 你认为你的模型预测结果如何，附上关键说明和截图"></a>2.2 你认为你的模型预测结果如何，附上关键说明和截图</h3><p>在本实验中，我使用均方根误差来评价模型预测效果，得到在训练集上的训练集的均方根误差为：40024.97。因为该数据集的数值均较大，日流入流出均达到了十万级别，这样的均方根误差相对来说效果较好，但有较大的提升空间。在分析过程中已有具体的呈现和说明。</p>
<h3 id="2-3-如果需要提高预测准确率，可以从哪些方面入手，简单列举即可"><a href="#2-3-如果需要提高预测准确率，可以从哪些方面入手，简单列举即可" class="headerlink" title="2.3 如果需要提高预测准确率，可以从哪些方面入手，简单列举即可"></a>2.3 如果需要提高预测准确率，可以从哪些方面入手，简单列举即可</h3><ol>
<li><p>在时间序列中存在的噪声，会干扰序列中每个点的波动，给预测造成难度，可以使用卡尔曼滤波来过滤这个噪声。将卡尔曼滤波算法与ARMA模型结合时，卡尔曼滤波算法可以在当获得一个新的数据点时，递归地更新状态变量（预测值）的信息，可以起到对ARMA模型的修正作用，在一定程度上提高ARMA模型的预测精度。</p>
</li>
<li><p>注意到我们需要预测的那几天在0点至10点是存在资金流动数据的，我们可以将这一部分数据作为我们预测全天数据的先验信息，提高预测精度。</p>
</li>
<li><p>如果我们具有有关资金流动的其他信息，诸如资金流动的来源、股市波动信息、宏观政策的变动、市场波动的信息等等，就可以构建一个特征集，用LSTM（长短时循环神经网络）来对序列进行建模。当然要是没有这些信息，也可以直接用历史记录 $[y_{t-1}, y_{t-2},…]$ 以及他们的各种统计量如均值、最大最小值、一阶差分、二阶差分等对 $x_t$ 做特征工程，以此丰富其维度，再使用LSTM对其进行建模。</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 数据分析 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 时间序列 </tag>
            
            <tag> ARMA </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习系列（13）：QCon北京2018 深度学习分享总结]]></title>
      <url>/2018/04/24/2018-04-24/</url>
      <content type="html"><![CDATA[<p>参加了历时三天的QCon北京的技术分享大会，一个码农云集的盛会，各大厂拿出自己的看家技术，真的是干货满满。特别感谢金主爸爸给的免费通票和树苗苗姐姐的联络，让我这样的小白加穷学生可以有机会参加这样规模的大会，还加了很多业界大佬的微信，见到了之前膜拜已久的洪强宁老师和张俊林老师，技术交流真的获益匪浅，扩宽了眼界，看到了自己的不足，有很多不懂的地方需要自己慢慢去补课。</p>
<p>下面是一些个人收获比较大的分享内容记录和整理，之后还会慢慢更新。</p>
<a id="more"></a>
<h2 id="一、达观数据-陈运文-《文本智能处理的深度学习技术》"><a href="#一、达观数据-陈运文-《文本智能处理的深度学习技术》" class="headerlink" title="一、达观数据 陈运文 《文本智能处理的深度学习技术》"></a>一、达观数据 陈运文 《文本智能处理的深度学习技术》</h2><p>人工智能目前的三个主要细分领域为图像、语音和文本，老师分享的是达观数据所专注的文本智能处理领域。文本智能处理，亦即自然语言处理，试图让机器来理解人类的语言，而语言是人类认知发展过程中产生的高层次抽象实体，不像图像、语音可以直接转化为计算机可理解的对象，它的主要应用主要是在智能问答，机器翻译，文本分类，文本摘要，标签提取，情感分析，主题模型等等方面。</p>
<p>自然语言的发展历程经历了以下几个阶段。这里值得一提的是，关于语言模型，早在2000年，百度IDL的徐伟博士提出了使用神经网络来训练二元语言模型，随后Bengio等人在2001年发表在NIPS上的文章《A Neural Probabilistic Language Model》，正式提出神经网络语言模型（NNLM），在训练模型的过程中也能得到词向量。2007年，Mnih和Hinton在神经网络语言模型（NNLM）的基础上提出了log双线性语言模型（Log-Bilinear Language Model，LBL），同时，Hinton在2007年发表在 ICML 上的《Three new graphical models for statistical language modelling》初见其将深度学习搬入NLP的决心。2008年，Ronan Collobert等人 在ICML 上发表了《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》，其中的模型名字叫C&amp;W模型，这是第一个直接以生成词向量为目标的模型。LBL与NNLM的区别正如它们的名字所示，LBL的模型结构是一个log双线性结构；而NNLM的模型结构为神经网络结构。这些积淀也成就了Mikolov创造了实用高效的Word2Vec工具，起初，他用循环神经网络RNNLM来做语言模型，发表paper《Recurrent neural network based language model》，之后就是各种改进，博士论文研究的也是用循环神经网络来做语言模型，《Statistical Language Models based on Neural Networks》。2013年，Mikolov等人同时提出了CBOW和Skip-gram模型。使用了Hierarchial Softmax和Negative Sampling两种trick来高效获取词向量。当然这个模型不是一蹴而就的，而是对于前人在NNLM、RNNLM和C&amp;W模型上的经验，简化现有模型，保留核心部分而得到的。同时开源了Word2Vec词向量生成工具，深度学习才在NLP领域遍地开花结果。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 上午11.41.04.png" alt="屏幕快照 2018-04-23 上午11.41.04"></p>
<p>一般地，文本挖掘各种类型应用的处理框架如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午1.05.40.png" alt="屏幕快照 2018-04-23 下午1.05.40"><br>文本数据经过清洗、分词等预处理之后，传统方法通过提取诸如词频、TF-IDF、互信息、信息增益等特征形成高维稀疏的特征集合，而现在则基本对词进行embedding形成低维稠密的词向量，作为深度学习模型的输入，这样的框架可用于文本分类、情感分析、机器翻译等等应用场景，直接端到端的解决问题，也无需大量的特征工程，无监督训练词向量作为输入可带来效果的极大提升。</p>
<h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><p>对于文本分类，以下列出了几种典型的深度学习模型：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午1.13.24.png" alt="屏幕快照 2018-04-23 下午1.13.24"></p>
<h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>序列标注的任务就是给每个汉字打上一个标签，对于分词任务来说，我们可以定义标签集合为：$LabelSet=\{ B,M,E,S\}$。B代表这个汉字是词汇的开始字符，M代表这个汉字是词汇的中间字符，E代表这个汉字是词汇的结束字符，而S代表单字词。下图为中文分词序列标注过程：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午6.51.46.png" alt="屏幕快照 2018-04-23 下午6.51.46"></p>
<p>中文分词转换为对汉字的序列标注问题，假设我们已经训练好了序列标注模型，那么分别给每个汉字打上标签集合中的某个标签，这就算是分词结束了，因为这种形式不方便人来查看，所以可以增加一个后处理步骤，把B开头，后面跟着M的汉字拼接在一起，直到碰见E标签为止，这样就等于分出了一个单词，而打上S标签的汉字就可以看做是一个单字词。于是我们的例子就通过序列标注，被分词成如下形式：{跟着  Tfboys 学 左手 右手 一个 慢动作}</p>
<p>对于序列标注，传统的方法基本是使用大量的特征工程，进入CRF模型，但不同的领域需要进行相应的调整，无法做到通用。而深度学习模型，例如Bi-LSTM+CRF则避免了这样的情况，可以通用于不同的领域，且直接采用词向量作为输入，提高了泛化能力，使用LSTM和GRU等循环神经网络还可以学习到一些较远的上下文特征和一些非线性特征。</p>
<p>经典的Bi-LSTM+CRF模型如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午1.18.20.png" alt="屏幕快照 2018-04-23 下午1.18.20"></p>
<h3 id="生成式摘要"><a href="#生成式摘要" class="headerlink" title="生成式摘要"></a>生成式摘要</h3><p>对于生成式摘要，采用Encode-Decoder模型结构，两者都为神经网络结构，输入原文经过编码器编码为向量，解码器从向量中提取关键信息，组合成生成式摘要。当然，还会在解码器中引入注意力机制，以解决在长序列摘要的生成时，个别字词重复出现的问题。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午1.24.26.png" alt="屏幕快照 2018-04-23 下午1.24.26"></p>
<p>此外，在生成式摘要中，采用强化学习与深度学习相结合的学习方式，通过最优化词的联合概率分布，即MLE(最大似然)，有监督进行学习，在这里生成候选的摘要集。模型图如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午1.27.25.png" alt="屏幕快照 2018-04-23 下午1.27.25"><br>模型图中的ROUGE指标评价是不可导的，所以无法采用梯度下降的方式训练，这样我们就考虑强化学习，鼓励reward高的模型，通过给予反馈来更新模型。最终训练得到表现最好的模型。</p>
<h3 id="知识图谱关系抽取"><a href="#知识图谱关系抽取" class="headerlink" title="知识图谱关系抽取"></a>知识图谱关系抽取</h3><p>对于知识图谱的关系抽取，主要有两种方法，一个是基于参数共享的方法，对于输入句子通过共用的 word embedding 层，然后接双向的 LSTM 层来对输入进行编码。然后分别使用一个 LSTM 来进行命名实体识别 (NER)和一个 CNN 来进行关系分类(RC)；另一个是基于联合标注的方法，把原来涉及到序列标注任务和分类任务的关系抽取完全变成了一个序列标注问题。然后通过一个端对端的神经网络模型直接得到关系实体三元组。</p>
<p>如下图所示，我们有三类标签，分别是 ①单词在实体中的位置{B(begin),I(inside),E(end),S(single)}、②关系类型{CF,CP,…}和③关系角色{1(entity1),2(entity2)}，根据标签序列，将同样关系类型的实体合并成一个三元组作为最后的结果，如果一个句子包含一个以上同一类型的关系，那么就采用就近原则来进行配对。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午7.01.06.png" alt="屏幕快照 2018-04-23 下午7.01.06"></p>
<hr>
<h2 id="二、爱奇艺-方非-《爱奇艺视频信息流推荐的深度学习之路》"><a href="#二、爱奇艺-方非-《爱奇艺视频信息流推荐的深度学习之路》" class="headerlink" title="二、爱奇艺 方非 《爱奇艺视频信息流推荐的深度学习之路》"></a>二、爱奇艺 方非 《爱奇艺视频信息流推荐的深度学习之路》</h2><p>爱奇艺的短视频内容生态主要由OGC（Occupationally-generated Content，职业生产内容）、PGC（Professionally-generated Content，专业生产内容）、UGC（User-generated Content，用户生产内容），布局到PC端、移动端、TV端和VR，通过个性化推荐并使用feed流或者多列瀑布流进行高效内容分发，它的整个内容平台如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午7.42.12.png" alt="屏幕快照 2018-04-23 下午7.42.12"></p>
<p>爱奇艺的深度学习架构设计主要分为深度召回模型和深度排序模型，</p>
<h3 id="深度召回模型"><a href="#深度召回模型" class="headerlink" title="深度召回模型"></a>深度召回模型</h3><p>召回，即给定上下文{ User， Context }，从所有推荐集合中过滤出具备推荐价值的内容{ Item List }，推荐价值使用兴趣相关性、热门、好友都在看、关注、LBS等等来定义。</p>
<p>协同过滤方法是基于用户行为来 预测/推荐 的一类算法，它基于这样一种假设：相似的视频更大概率会被同一个用户(相似的用户)看过；相似兴趣的用户会更大概率看同一个视频(相似的视频)；历史会重复发生。</p>
<p>基于近邻的协同过滤方法有Item-based CF和User-based CF，虽然实现简单，不依赖内容本身，能得到基本稳定可靠的结果，可解释性强；但是对稀疏性非常敏感，没有考虑行为顺序和上下文变化，难以加入其它特征，高维空间建模，难以和其他模型结合。</p>
<p>基于协同过滤思想的还有其他很多模型，分别如下</p>
<ul>
<li>Matrix Factorization ：SVD &amp; ALS</li>
<li>Bayesian Model</li>
<li>Classification &amp; Clustering</li>
<li>Embedding：Item2Vec、Neural CF</li>
<li>Deep Learning：RBM、MLP、LSTM、Attention</li>
</ul>
<p>概括一下基于Embedding和基于Deep Learning的协同过滤模型</p>
<h4 id="Item2Vec"><a href="#Item2Vec" class="headerlink" title="Item2Vec"></a>Item2Vec</h4><p>由Barkan O和Koenigstein N于2016年在论文《Item2Vec: Neural Item Embedding for Collaborative Filtering》中提出， 作者受nlp中运用embedding算法学习word的latent representation的启发，特别是参考了google发布的的word2vec（Skip-gram with Negative Sampling，SGNS），利用item-based CF 学习item在低维 latent space的 embedding representation，优化 item2item的计算。其模型结构如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午8.10.08.png" alt="屏幕快照 2018-04-23 下午8.10.08"></p>
<p>Item2vec中把用户浏览的商品集合等价于word2vec中的word的序列，即句子（忽略了商品序列空间信息spatial information） ，出现在同一个集合的商品对视为 positive。对于集合$w_{1}, w_{2}, …,w_{K}$，模型的目标函数是最大化平均的log概率：</p>
<script type="math/tex; mode=display">\frac{1}{K}\sum_{i=1}^{K}\sum_{j≠i}^K logp(w_j|w_i)</script><p>同word2vec，利用负采样，将$p(w_{j}|w_{j})$定义为：<script type="math/tex">P(I_j|I_i)=\sigma (\mu_i^T v_j) \prod _{k=1}^N\sigma (-\mu_i^Tv_k)</script><br>subsample的方式也类似于word2vec，丢弃某一个集合$w_i$的概率为：</p>
<script type="math/tex; mode=display">P(discard|w_i)=1-\sqrt{\frac{t}{f(w_i)}}</script><p>最终，利用SGD方法学习参数最大化目标函数，任意Item都能 得到U和V两个一定维度的向量，这样我们得到每个商品的embedding representation，商品之间可以两两计算cosine相似度，即为商品的相似度。这个模型的缺点是相似度的计算只利用到了item的共现信息，忽略了user行为序列信息，且没有建模用户对不同item的喜欢程度高低。</p>
<h4 id="Neural-CF"><a href="#Neural-CF" class="headerlink" title="Neural CF"></a>Neural CF</h4><p>Xiangnan He , Lizi Liao , Hanwang Zhang , Liqiang Nie , Xia Hu , Tat-Seng Chua, Neural Collaborative Filtering, Proceedings of the 26th International Conference on World Wide Web, April 03-07, 2017, Perth, Australia</p>
<p>基于MLP的协同过滤模型<br>模型结构如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午8.39.02.png" alt="屏幕快照 2018-04-23 下午8.39.02"></p>
<p>这个模型增加了网络深度，但缺乏时序性考虑，没有其他的特征信息。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>基于LSTM的协同过滤模型，加入时序性的考虑，但缺乏对历史项重要性的学习，且没有其他的特征信息。模型结构如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午8.42.02.png" alt="屏幕快照 2018-04-23 下午8.42.02"></p>
<h4 id="LSTM-Attention"><a href="#LSTM-Attention" class="headerlink" title="LSTM + Attention"></a>LSTM + Attention</h4><p>基于LSTM + Attention的协同过滤模型加入时序性考虑，且在LSTM的基础上加入对历史项Attention的学习，但还是存在没有其他的特征信息的问题。模型结构如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午8.42.35.png" alt="屏幕快照 2018-04-23 下午8.42.35"></p>
<h4 id="深度召回模型-1"><a href="#深度召回模型-1" class="headerlink" title="深度召回模型"></a>深度召回模型</h4><p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午8.44.09.png" alt="屏幕快照 2018-04-23 下午8.44.09"></p>
<h3 id="深度排序模型"><a href="#深度排序模型" class="headerlink" title="深度排序模型"></a>深度排序模型</h3><p>排序算法，即从召回算法选取的内容集合中，找到最符合推荐预定目标的K个结果。主要有三种，分别是Pointwise、Pairwise、Listwise。<br>排序模型的四个要素：目标、样本、特征和模型。</p>
<ul>
<li>目标</li>
</ul>
<p>最终的最大化目标有以下这些，目标有长期与短期之分，也有主次之分，每一个目标都有各自的优化偏重：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>目标</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>点击率</td>
<td>简单直接，偏短时长</td>
</tr>
<tr>
<td>时长</td>
<td>更能反应用户的兴趣，偏长时长</td>
</tr>
<tr>
<td>互动</td>
<td>社交属性，稀疏</td>
</tr>
<tr>
<td>关注订阅</td>
<td>社交属性，内容生态，稀疏</td>
</tr>
<tr>
<td>留存</td>
<td>最接近用户满意度，直接优化难度高</td>
</tr>
</tbody>
</table>
</div>
<p>当然还会受到许多约束，诸如内容价值观、多样性、时效性、长尾分发、标题党、反作弊等</p>
<ul>
<li>样本</li>
</ul>
<p>然后根据目标仔细设计样本，将点击、点赞、评论、转发、关注、留存等定义为正样本，把展示不点击、不喜欢、负面评论等定义为负样本，并根据播放时长、反馈类型、样本有效性对样本进行加权。此外，处于样本有效性的考虑，会将无效样本判断过滤掉</p>
<ul>
<li>特征</li>
</ul>
<p>关于特征，根据特征向量的特点可分为三种：稠密特征、稀疏特征和Embedding特征。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>稠密特征</th>
<th>稀疏特征</th>
<th>Embedding特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>点击率</td>
<td>视频 ID</td>
<td>标题 Embedding</td>
</tr>
<tr>
<td>播放量</td>
<td>视频标签</td>
<td>Item2Vec Embedding</td>
</tr>
<tr>
<td>播放时长</td>
<td>用户标签</td>
<td>图片 Embedding</td>
</tr>
<tr>
<td>年龄</td>
<td>订阅作者 ID</td>
<td>社交 Embedding</td>
</tr>
<tr>
<td>性别</td>
<td>LBS</td>
<td>End2End Embedding</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>模型</li>
</ul>
<p>关于模型这一块，已经有很成熟的模型可以使用，主要有以下几种：LR、FM、GBDT、GBDT + FM(LR)、DNN+FM<br>（DeepFM: A Factorization-Machine based Neural Network for CTR Prediction）</p>
<p>这里使用的是基于Wide And Deep思想的多目标分类，模型结构如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午9.57.47.png" alt="屏幕快照 2018-04-23 下午9.57.47"></p>
<p>最后是整个排序服务平台的搭建，其组成部分如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-23 下午10.07.01.png" alt="屏幕快照 2018-04-23 下午10.07.01"></p>
<hr>
<h2 id="三、优酷-刘尚堃-《深度学习在视频搜索中的应用》"><a href="#三、优酷-刘尚堃-《深度学习在视频搜索中的应用》" class="headerlink" title="三、优酷   刘尚堃 《深度学习在视频搜索中的应用》"></a>三、优酷   刘尚堃 《深度学习在视频搜索中的应用》</h2><h3 id="视频搜索的挑战"><a href="#视频搜索的挑战" class="headerlink" title="视频搜索的挑战"></a>视频搜索的挑战</h3><p>视频搜索中存在以下这些挑战：</p>
<ul>
<li>非结构化、无组织，这就提升了召回难度</li>
<li>短文本、信息不充分，带来语义上的困难</li>
<li>海量短视频，给用户选择带来困难</li>
</ul>
<p>针对这些挑战，提出了一些利用深度学习来解决问题的对策，分别是</p>
<ul>
<li>基于视频内容理解的召回</li>
<li>语义模型、语义表征</li>
<li>个性化表征</li>
</ul>
<h3 id="视频内容理解—召回"><a href="#视频内容理解—召回" class="headerlink" title="视频内容理解—召回"></a>视频内容理解—召回</h3><h4 id="基于类目标签的召回"><a href="#基于类目标签的召回" class="headerlink" title="基于类目标签的召回"></a>基于类目标签的召回</h4><p>输入任意的视频，通过内容理解的方法对视频进行类目和标签的预测。采用的方法是CNN+LSTM的端到端预测方法。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午9.56.53.png" alt="屏幕快照 2018-04-24 上午9.56.53"></p>
<h4 id="基于事件-场景的召回"><a href="#基于事件-场景的召回" class="headerlink" title="基于事件/场景的召回"></a>基于事件/场景的召回</h4><p>给定不定长视频，定位感兴趣行为发生的时间段并给出对应行为类标。采用的方法是Convolution 3D+Gated Recurrent Unit(GRU)算法，结合Single Shot Detector（SSD）框架实现行为检测功能。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午9.59.46.png" alt="屏幕快照 2018-04-24 上午9.59.46"></p>
<h4 id="基于物体-人物的召回"><a href="#基于物体-人物的召回" class="headerlink" title="基于物体/人物的召回"></a>基于物体/人物的召回</h4><p>定位和识别视频中的特定目标，并在目标生命周期内进行跟踪。采用的方法是Region fully convolution network（R-FCN）的深度学习框架，对于小物体在feature map进行了优化；跟踪采用DCF框架，结合颜色模型，并使用BACF进行候选区域扩充。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.03.55.png" alt="屏幕快照 2018-04-24 上午10.03.55"></p>
<h4 id="视频智能封面图"><a href="#视频智能封面图" class="headerlink" title="视频智能封面图"></a>视频智能封面图</h4><p>UGC视频智能缩略图，目的是通过对视频进行结构化分析，对关键帧、视频镜头进行筛选和排序，选择最优的关键帧、关键片段来作为视频的展示。采用的方法是用关键帧提取+MMR优化+美学评分等方法，选择视频中最优关键帧作为该视频的首图。</p>
<h3 id="语义搜索—语义表征"><a href="#语义搜索—语义表征" class="headerlink" title="语义搜索—语义表征"></a>语义搜索—语义表征</h3><p>人工审核标注ground truth，训练集测试集比例为7：3，加入语义特征后，NDCG提升1%的绝对值。</p>
<ul>
<li>固定数据尝试不同的模型</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>NDCG</th>
</tr>
</thead>
<tbody>
<tr>
<td>双向LSTM+Attention</td>
<td>达到0.9以上</td>
</tr>
<tr>
<td>BiGRU dropout</td>
<td>达到0.8以上</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>固定模型尝试term embedding初始化方式</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>初始化方式</th>
<th>长尾query NDCG</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bi-LSTM+Attention</td>
<td>随机</td>
<td>baseline</td>
</tr>
<tr>
<td>Bi-LSTM+Attention</td>
<td>embedding</td>
<td>低于baseline 1%</td>
</tr>
<tr>
<td>Bi-LSTM+Attention</td>
<td>UC/豆瓣的FastText Embedding</td>
<td>高于baseline 1%</td>
</tr>
</tbody>
</table>
</div>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.26.24.png" alt="屏幕快照 2018-04-24 上午10.26.24"></p>
<ul>
<li><p>用户体验优化：</p>
<ul>
<li>在长尾query和语义层面实现了特征增益，在长尾query相关性上有较大改善</li>
<li>ground truth测试集NDCG提升了1%</li>
</ul>
</li>
<li><p>技术创新突破</p>
<ul>
<li>基于FastText无监督embedding</li>
<li>细粒度+字切分 优酷title语料覆盖度99%</li>
<li>训练数据集 billion级别，模型参数量千万级别</li>
<li>Bi-LSTM+Attention</li>
<li>基于pai-tensorflow的分布式训练</li>
</ul>
</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.38.11.png" alt="屏幕快照 2018-04-24 上午10.38.11"></p>
<h3 id="排序—个性化表征"><a href="#排序—个性化表征" class="headerlink" title="排序—个性化表征"></a>排序—个性化表征</h3><p>特征按照field进行组织，按照特征重要性和关联性进行分域，分为query field、user field、video field、id field、统计field、用户观看序列、标签兴趣、文本等，形成超高维的稀疏编码来表征独立个体，使得深度特征的组合表达能力达到极致，其模型结构如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.38.45.png" alt="屏幕快照 2018-04-24 上午10.38.45"></p>
<p>但这样对在线计算形成了挑战，特征维度高，模型存储空间大，离线训练计算时间成本高，在线实现资源占用高，前向网络计算不能满足RT的要求。就用了一系列的技术，诸如随机编码、挂靠编码、抽样技术等。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.53.04.png" alt="屏幕快照 2018-04-24 上午10.53.04"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-24 上午10.53.47.png" alt="屏幕快照 2018-04-24 上午10.53.47"></p>
<h2 id="百度-孙宇-《神经网络语义匹配技术及其百度搜索实践》"><a href="#百度-孙宇-《神经网络语义匹配技术及其百度搜索实践》" class="headerlink" title="百度 孙宇 《神经网络语义匹配技术及其百度搜索实践》"></a>百度 孙宇 《神经网络语义匹配技术及其百度搜索实践》</h2><h2 id="腾讯-苏函晶-《深度学习在广告投放中的应用》"><a href="#腾讯-苏函晶-《深度学习在广告投放中的应用》" class="headerlink" title="腾讯 苏函晶 《深度学习在广告投放中的应用》"></a>腾讯 苏函晶 《深度学习在广告投放中的应用》</h2><h2 id="PayPal-张彭善-《构建高效的风控机器学习平台》"><a href="#PayPal-张彭善-《构建高效的风控机器学习平台》" class="headerlink" title="PayPal 张彭善 《构建高效的风控机器学习平台》"></a>PayPal 张彭善 《构建高效的风控机器学习平台》</h2><h2 id="新浪微博-刘博-《深度学习在微博信息流排序的应用》"><a href="#新浪微博-刘博-《深度学习在微博信息流排序的应用》" class="headerlink" title="新浪微博 刘博 《深度学习在微博信息流排序的应用》"></a>新浪微博 刘博 《深度学习在微博信息流排序的应用》</h2><h2 id="腾讯-王辉-《小Q机器人的诞生之路》"><a href="#腾讯-王辉-《小Q机器人的诞生之路》" class="headerlink" title="腾讯 王辉 《小Q机器人的诞生之路》"></a>腾讯 王辉 《小Q机器人的诞生之路》</h2><h2 id="爱因互动-吴金龙-《深度学习在对话机器人中的应用》"><a href="#爱因互动-吴金龙-《深度学习在对话机器人中的应用》" class="headerlink" title="爱因互动 吴金龙 《深度学习在对话机器人中的应用》"></a>爱因互动 吴金龙 《深度学习在对话机器人中的应用》</h2><h2 id="阿里-黄丕培-《万物皆向量—双十一淘宝首页个性化推荐的秘密》"><a href="#阿里-黄丕培-《万物皆向量—双十一淘宝首页个性化推荐的秘密》" class="headerlink" title="阿里 黄丕培 《万物皆向量—双十一淘宝首页个性化推荐的秘密》"></a>阿里 黄丕培 《万物皆向量—双十一淘宝首页个性化推荐的秘密》</h2><h2 id="商汤科技-陈宇恒-《未来都市—智慧城市与基于深度学习的机器视觉》"><a href="#商汤科技-陈宇恒-《未来都市—智慧城市与基于深度学习的机器视觉》" class="headerlink" title="商汤科技 陈宇恒 《未来都市—智慧城市与基于深度学习的机器视觉》"></a>商汤科技 陈宇恒 《未来都市—智慧城市与基于深度学习的机器视觉》</h2><h2 id="百分点-黄伟-《AI认知技术帮助公共安全行业》"><a href="#百分点-黄伟-《AI认知技术帮助公共安全行业》" class="headerlink" title="百分点 黄伟 《AI认知技术帮助公共安全行业》"></a>百分点 黄伟 《AI认知技术帮助公共安全行业》</h2>]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 视频推荐 </tag>
            
            <tag> 视频搜索 </tag>
            
            <tag> 语义匹配 </tag>
            
            <tag> 广告投放 </tag>
            
            <tag> 风险控制 </tag>
            
            <tag> 信息流排序 </tag>
            
            <tag> 对话机器人 </tag>
            
            <tag> 机器视觉 </tag>
            
            <tag> 公共安全 </tag>
            
            <tag> 个性化推荐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人大应统部落（3）：2018年中国人民大学应用统计专业课真题与解析]]></title>
      <url>/2018/04/05/%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E9%83%A8%E8%90%BD%EF%BC%883%EF%BC%89%EF%BC%9A2018%E5%B9%B4%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%9C%9F%E9%A2%98%E5%8F%8A%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>2018年中国人民大学应用统计初试专业课真题及详细解答，文末附专业课复试笔试真题回忆版。</p>
<p>如有疑问或建议，可添加微信：zhanghua63170140</p>
<a id="more"></a>
<h3 id="第一题【10分】"><a href="#第一题【10分】" class="headerlink" title="第一题【10分】"></a>第一题【10分】</h3><p>1.1 请说明雷达图和箱线图的基本要点。<br>1.2 下面的数据集为8个同学的数学、语文和英语的成绩，如何利用雷达图和箱线图来描述这个数据集？</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>姓名</th>
<th>数学</th>
<th>语文</th>
<th>英语</th>
</tr>
</thead>
<tbody>
<tr>
<td>甲</td>
<td>83</td>
<td>86</td>
<td>82</td>
</tr>
<tr>
<td>乙</td>
<td>93</td>
<td>89</td>
<td>93</td>
</tr>
<tr>
<td>丙</td>
<td>85</td>
<td>79</td>
<td>90</td>
</tr>
<tr>
<td>丁</td>
<td>79</td>
<td>81</td>
<td>75</td>
</tr>
<tr>
<td>戊</td>
<td>79</td>
<td>81</td>
<td>75</td>
</tr>
<tr>
<td>己</td>
<td>75</td>
<td>70</td>
<td>94</td>
</tr>
<tr>
<td>庚</td>
<td>69</td>
<td>62</td>
<td>94</td>
</tr>
<tr>
<td>辛</td>
<td>67</td>
<td>62</td>
<td>94</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>【解答】</p>
<blockquote>
<p>1.1 </p>
</blockquote>
<p>箱线图由一组数据的最大值、最小值、中位数、上下四分位数这五个特征值绘制而成。不仅反映一组数据分布的特征，如分布是否对称、是否存在离群点等，还可以对多组数据的分布特征进行比较。</p>
<p>雷达图，也称为蜘蛛图，是展示多个变量的常用方法，主要用于比较研究多个样本的相似程度。</p>
<blockquote>
<p>1.2 </p>
</blockquote>
<p>以数学组数据为例绘制箱线图，步骤如下：</p>
<ol>
<li><p>画一只箱子，箱子两端分别位于上四分位数和下四分位数。四分位数是指一组数据排序后（67 69 75 79 79 83 85 93）处于$25\%$和$75\%$位置上的值，对于数学组数据来说,下四分位数$Q_1 = 69$以及上四分位数$Q_3 = 83$。这个箱子包括中间$50\%$的数据。</p>
</li>
<li><p>在箱子中位数的位置画一条垂直线，中位数是指一组数据排序后处于中间位置上的变量值。对于数学组数据来说，中位数$M_e=79$。</p>
</li>
<li><p>用四分位数全距$IQR = Q_3 − Q_1$，确定限制线的位置。箱线图的上、下限制线分别在比$Q_1$低$1.5×IQR$和比$Q_3$高$1.5×IQR$的位置上。对于数学组数据来说，$IQR = Q_3 − Q_1 = 83-69 = 14$。因此，限制线的位置在$69− 1.5×14 = 48$和$83 + 1.5×14 =104$处。两条限制线以外的数据可以认为是异常值。但一般限制线不绘制在图上。</p>
</li>
<li><p>绘制触须线。触须线从箱子两端开始绘制，直至最小值和最大值。因此，数学组数据的触须线分别在67和93处结束。</p>
</li>
<li><p>绘制异常值。即处于内限以外位置的点。</p>
</li>
</ol>
<p>以此类推，绘制语文组和英语组的箱线图，得到的箱线图如下：<br><img src="http://omu7tit09.bkt.clouddn.com/15152253704365.jpg" alt=""></p>
<p>其中中位数位置代表平均水平；箱子长短代表离散程度；分布形状可以看出是否对称。既可以研究单独分析组内数据，也可以对语文、数学、英语三组进行比较研究。我们可以看到，八位同学的英语成绩平均水平最高，数学和语文成绩的离散程度较高，三组成绩都不存在异常值。</p>
<p>雷达图从一个点出发，找到八条射线分别代表八位同学。八个变量的数据点链接起来，围成一个区域，三门课程围成三个区域，研究语文、数学、英语三组成绩的相关比较。从图中可知，多位同学英语成绩的高于其他两门，数学和语文成绩都比较接近。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-01-06 下午4.22.46.png" alt="屏幕快照 2018-01-06 下午4.22.46"><br>也可以找三条射线，分别代表数学英语和语文三个变量，比较每个学生三门课的成绩。乙同学的三门成绩都比较高，辛同学的三门成绩都较低，需要加倍努力哦。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-01-06 下午4.21.27.png" alt="屏幕快照 2018-01-06 下午4.21.27"></p>
<h3 id="第二题【20分】"><a href="#第二题【20分】" class="headerlink" title="第二题【20分】"></a>第二题【20分】</h3><p>2.1 说明在方差已知的条件下，正态总体均值区间估计的宽度与样本量的关系。</p>
<p>2.2 现在有一组来自正态总体的随机样本，可以由此得到在方差已知和方差未知两种条件下的置信区间，请分析这两个置信区间的中点和宽度的异同。</p>
<p>【解答】</p>
<blockquote>
<p>2.1 </p>
</blockquote>
<p>在方差已知的条件下，正态总体均值区间估计的宽度随样本量增大而减小。</p>
<p>在方差已知的条件下，总体分布为正态分布的均值区间估计(无论是大样本还是小样本)为</p>
<script type="math/tex; mode=display">\bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}</script><p>其中$\bar {x}-z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$称为置信下限，$\bar {x}+z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}} $称为置信上限;$a$是事先所确定的一个概率值，也被称为风险值，是总体均值不包括在置信区间的概率;$1-a$称为置信水平; 是标准正态分布右侧面积为$\frac{a}{2}$的 Z 值;$z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$是估计总体均值的估计误差。 总体均值的置信区间由两部分组成:点估计值和描述估计量精度的±值，这个±值称为估计误差。</p>
<p>由估计的区间可知，当样本量$n$增大时，区间估计的宽度减小。</p>
<blockquote>
<p>2.2 </p>
</blockquote>
<p> 如果是大样本的情况下，方差已知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$ ，方差未知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{s}{\sqrt{n}}$ 。两个置信区间的中点相同，但是方差未知的情况下是用样本方差替代总体方差，宽度存在差异。</p>
<p> 如果在小样本的情况下，方差已知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$，方差未知的公式为$\bar {x}±t_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$ 。两个置信区间的中点相同，但是方差未知的置信区间的宽度会更宽一些。因为选择的t分布与正态分布形状接近，但是两侧的尾端会更宽一些，所构造的置信区间宽度也会更宽。随样本量增大，宽度差异会缩小。</p>
<h3 id="第三题【20分】"><a href="#第三题【20分】" class="headerlink" title="第三题【20分】"></a>第三题【20分】</h3><p>3.1 给出一个列联表，写出可以描述上述数据的所有的图形，并说明这些图形的用途，<br>3.2 写出可以分析上述数据所有可能的方法，并说明用途。</p>
<blockquote>
<p>3.1 </p>
</blockquote>
<p>对于分类型数据可以选择以下图形进行描述性统计分析</p>
<ol>
<li>复式条形图：用宽度相同的条形来表述数据的多少，观察不同类型数据的分布状况。比条形图更便于比较分析</li>
<li>帕累托图：按各类别出现的频数多少排序后绘制的条形图，通过排序后，容易看出哪类数据出现的多，哪类出现的少</li>
<li>复式饼图：用圆形及圆内扇形的角度来表示数值大小的图形，用于表示各类别的频数占全部频数的比例，适用于研究结构性问题。比之饼图更适用于展示两个或者多个类别变量的构成比较。</li>
<li>环形图：显示多个样本各类别频数所占的相应比例，有利于构成的比较研究</li>
</ol>
<blockquote>
<p>3.2 </p>
</blockquote>
<p>因为以上列联表中有两个分类变量，我们关心这两者是否有关联，可以进行独立性检验，也即进行列联分析。独立性检验就是分析列联表中行变量与列变量是否相互独立，也就是检验性别分布与满意度之间是否存在依赖关系。具体分析步骤如下： </p>
<ol>
<li>提出原假设和备择假设<br>H0:性别与满意度之间是独立的<br>H1：性别与满意度之间不独立</li>
<li>计算检验统计量<br>计算$f_o-f)e$，其中$f_e=\frac{RT}{n}×\frac{CT}{n}×n=\frac{RT×CT}{n}$<br>计算$(f_o-f_e)^2$<br>计算$\frac{(f_o-f_e)^2}{f_e}$<br>计算$\sum\frac{(f_o-f_e)^2}{f_e}$</li>
<li>做出决策<br>若$\chi ^2&gt;\chi_a^2$，则拒绝原假设，认为性别与满意度两者之间存在依赖关系。<br>如果$\chi ^2&lt;\chi_a^2$，则不拒绝原假设，认为两者之间不存在依赖关系。</li>
</ol>
<p>若拒绝原假设，可继续使用C相关系数或V相关系数测定两者之间的相关关系到底为多少。 通过上面的方法，可以判断两个分类变量是否独立，而当拒绝原假设后，我们可以接着运用对应分析方法来了解两个分类变量即分类变量各个状态之间的相关关系。对应分析利用降维的思想进行简化数据结构，同时对数据表中的行与列进行处理，可把样本点和变量点同时反映到相同的因子轴上，用低维图形简洁明了地揭示两个分类变量之间及分类变量各状态之间的相关关系。</p>
<h3 id="第四题【20分】"><a href="#第四题【20分】" class="headerlink" title="第四题【20分】"></a>第四题【20分】</h3><p>设因变量为$y$ ,自变量为$x_1,x_2,x_3·····x_k$，写出建立多元线性回归建模的基本思路。</p>
<p>【解答】</p>
<ol>
<li>提出因变量和自变量：首先根据具体问题选择合适的因变量，然后选择合理的自变量和结合问题的实际意义和专业理论知识，运用逐步回归法等选择自变量。</li>
<li>收集整理数据：这是一个重要环节，他直接影响模型的质量。 </li>
<li>做相关分析，构造多元线性理论回归模型：用SPSS软件计算增广相关阵。 </li>
<li>用软件计算，输出计算结果（参数估计有最小二乘法和极大似然法等方法）：用SPSS软件对原始数据作回归分析。</li>
<li>回归诊断：（1）诊断基本假定是否成立（2）相关分析：由复相关系数或决定系数判断回归方程是否显著（3）方差分析表：判断回归方程是否显著（4）回归方程系数显著性检验是否显著（5）检验异常值：判断是否符合实际意义 </li>
<li>若未通过回归诊断，返回第一步，否则可进入回归应用，主要应用于结构分析、预测和控制三个方面。 </li>
</ol>
<h3 id="第五题【20分】"><a href="#第五题【20分】" class="headerlink" title="第五题【20分】"></a>第五题【20分】</h3><p>5.1 方差分析有哪些基本假定？</p>
<p>5.2 简要说明检验这些假定的方法。</p>
<p>【解答】</p>
<blockquote>
<p>5.1 </p>
</blockquote>
<p>方差分析有三大假定</p>
<ol>
<li>正态性:要求每个处理所对应的总体都应服从正态分布</li>
<li>方差齐性：要求各处理的总体方差必须想等</li>
<li>独立性：要求每个样本数据是来自不同处理的独立样本</li>
</ol>
<blockquote>
<p>5.2 </p>
</blockquote>
<ol>
<li><p>正态性检验:检验正态性可以选择图示法去绘制因变量的正态概率图。包括PP图、qq图或者绘制箱线图、直方图、茎叶图。</p>
<ul>
<li>其中绘制箱线图、直方图、茎叶图观察与正态曲线是否接近；QQ图是根据观测值的实际分位数与理论的分位数绘制的；PP图是根据观测数据的累积概率与理论分布的累积概率的符合程度绘制的；检验正态性还包括检验法包括Shapiro-Wilk和K-S正态性检验，其中Shapiro-Wilk适用于小样本。K-S既可以用于大样本也可以用于小样本</li>
</ul>
</li>
<li><p>方差齐性检验检验：方差齐性的图示法包括箱线图和残差图。检验法包括Bartlett检验和Levene检验等</p>
</li>
<li>独立性检验：独立性可以在实验设计之前予以确定，不需要检验<h3 id="第六题【20分】"><a href="#第六题【20分】" class="headerlink" title="第六题【20分】"></a>第六题【20分】</h3>在同一个概率空间中是否存在三个随机事件$A,B,C$使得同时成立下面三个不等式：<script type="math/tex; mode=display">P(A|B,C)≤P(A|\bar {B},C) \\\ P(A|B,\bar {C})<P(A|\bar {B}, \bar {C})\\\ P(A|B)>P(A|\bar B)</script>如果存在，请列举一个例子；若不存在，证明你的结论。</li>
</ol>
<p>【解答】</p>
<p>存在。该题实际是考察辛普森悖论，大家可以自行了解一下。作如下假设：</p>
<p>A事件：被录取<br>B事件：是男生<br>C事件：左撇子</p>
<p> B事件和C事件同时发生，即是男生，左撇子有1人<br> B事件发生，C事件不发生，即是男生，右撇子有4人<br> B事件不发生，C事件发生，即是女生，左撇子的有4人<br> B事件不发生，C事件不发生，即是女生，右撇子的有1人</p>
<script type="math/tex; mode=display">P(A|BC)=0</script><script type="math/tex; mode=display">P(A|B\bar C)=\frac{3}{4}</script><script type="math/tex; mode=display">P(A|\bar BC)=\frac{1}{4}</script><script type="math/tex; mode=display">P(A|\bar B \bar C)=1</script><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>男生B</th>
<th>女生$\bar B$</th>
</tr>
</thead>
<tbody>
<tr>
<td>左撇子C</td>
<td>1(录取0人)</td>
<td>4(录取1人)</td>
</tr>
<tr>
<td>右撇子$\bar C$</td>
<td>4（录取3人）</td>
<td>1(录取1人)</td>
</tr>
</tbody>
</table>
</div>
<p> 那么</p>
<script type="math/tex; mode=display">P(A|B)=\frac{3}{5}</script><script type="math/tex; mode=display">P(A|\bar B)=\frac {2}{5}</script><p> 所以</p>
<script type="math/tex; mode=display">P(A|B)>P(A|\bar B)</script><h3 id="第七题【20分】"><a href="#第七题【20分】" class="headerlink" title="第七题【20分】"></a>第七题【20分】</h3><p>设$x_1,x_2······,x_n$为一个来自均值为$\mu$，方差为$\sigma^2$的分布的样本，$\mu$和$\sigma ^2$未知，考虑均值为$\mu$的线性无偏估计类</p>
<script type="math/tex; mode=display">
L=\{T\left(X\right)：T\left(X\right)=\sum_{i=1}^n{x_i}.c_i \}  \\\ 其中c_i是常数</script><p>求出$L$中$T(X)$为$\mu$的无偏估计的充要条件，并求出无偏估计类中方差一致最小的估计。</p>
<p>【解答】</p>
<script type="math/tex; mode=display">E[T(X)]=E(\sum_{i=1}^n x_ic_i)\\=\sum_{i=1}^nEx_ic_i=\sum_{i=1}^nc_i Ex_i\\=\mu \sum_{i=1}^nc_i=\mu</script><p>所以$\sum_{i=1}^nc_i=1$</p>
<script type="math/tex; mode=display">var[T(X)]=\sum_{i=1}^nc_i^2var(x_i)\\=\sigma ^2 \sum_{i=1}^nc_i^2≥\frac{\sigma^2}{n}</script><p>当且仅当$c_i=\frac{1}{n}, \qquad i=1,2，····，n$时,等号成立，即$var[T(X)]=\frac{1}{n}\sigma^2$<br>所以无偏估计类方差一致最小的估计为$T(X)=\frac{1}{n}\sum_{i=1}^n x_i$</p>
<h3 id="第八题【10分】"><a href="#第八题【10分】" class="headerlink" title="第八题【10分】"></a>第八题【10分】</h3><p>设$X$是一个正值随机变量，方差有界，证明：对于$\forall 0&lt;\lambda &lt;1$, 有</p>
<script type="math/tex; mode=display">P(X>\lambda EX)\geqslant\left(1-\lambda\right)^2\frac{\left(EX\right)^2}{EX^2}</script><p>【解题】<br>要证明$P(X&gt;\lambda EX)\quad \geqslant\quad { (1-\lambda ) }^{ 2 }\frac { { (EX) }^{ 2 } }{ E{ X }^{ 2 } } $</p>
<p>即证明$P(X&gt;\lambda EX)·{ EX }^{ 2 }≥\quad { (1-\lambda ) }^{ 2 }·{ (EX) }^{ 2 }$</p>
<p>不等式左边，根据示性函数$I(X-Y)=\begin{cases} 1\quad X-Y&gt;0 \\ 0\quad X-Y≤0 \end{cases}$和定义$P(X&gt;Y)=E[I(X&gt;Y)]$可得</p>
<script type="math/tex; mode=display">P(X>\lambda EX)·EX^2 = E[I(X>\lambda EX)]·EX^2 =E[I^2(X>\lambda EX)]·EX^2  \qquad ①</script><p>根据柯西不等式$(EXY)^2≤EX^2·EY^2$，得</p>
<script type="math/tex; mode=display">①≥{E[X·I(X>\lambda EX)]}^2 \qquad ②</script><p>又因为$X·I(X&gt;\lambda EX)=\begin{cases} X·1\quad X&gt;\lambda EX\quad  \\ X·0\quad X≤\lambda EX \end{cases}≥X-\lambda EX$<br>所以</p>
<script type="math/tex; mode=display">②≥[E(X-\lambda EX)]^2=(EX-\lambda EX)^2=(1-\lambda)^2(EX)^2  \qquad ③</script><p>综合①和③，得到</p>
<script type="math/tex; mode=display">P\left\{X>\lambda EX\right\}\geqslant\left(1-\lambda\right)^2\frac{\left(EX\right)^2}{EX^2}</script><h3 id="第九题【10分】"><a href="#第九题【10分】" class="headerlink" title="第九题【10分】"></a>第九题【10分】</h3><p> 设地区生产总之（亿元）为因变量，固定资产投资（亿元）、社会消费品零售总额（亿元）、出口总额（亿美元）、地方财政收入（亿元）、电力消费量（亿千瓦时）、居民消费水平（元）为自变量，根据31个样本数据得到回归结果如下：</p>
<p>Coefficients </p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Estimate</th>
<th>Std. Error</th>
<th>t value</th>
<th>Pr(t)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>-2.377 e+03</td>
<td>1.166 e+03</td>
<td>-2.038</td>
<td>0.05270</td>
<td></td>
</tr>
<tr>
<td>固定资产投资</td>
<td>4.504 e-01</td>
<td>8.166 e-02</td>
<td>5.515</td>
<td>1.14 e-05</td>
<td><em>*</em></td>
</tr>
<tr>
<td>社会消费品零售总额</td>
<td>1.110 e+00</td>
<td>1.572 e-01</td>
<td>7.060</td>
<td>2.68 e-0.7</td>
<td><em>*</em></td>
</tr>
<tr>
<td>出口总额</td>
<td>1.887 e+01</td>
<td>6.379 e+00</td>
<td>2.958</td>
<td>0.00686</td>
<td>**</td>
</tr>
<tr>
<td>地方财政收入</td>
<td>9.596 e-01</td>
<td>6.959 e-01</td>
<td>1.379</td>
<td>0.18061</td>
<td></td>
</tr>
<tr>
<td>电力消费量</td>
<td>6.683 e-01</td>
<td>5.671 e-01</td>
<td>1.178</td>
<td>0.25016</td>
<td></td>
</tr>
<tr>
<td>居民消费水平</td>
<td>1.194 e-01</td>
<td>6.949 e-02</td>
<td>1.718</td>
<td>0.09868</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>Residual standard error:</th>
<th>1526</th>
<th>自由度</th>
<th>24</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multiple R-Squared:</td>
<td>0.9944</td>
<td>Adjusted R-squared</td>
<td>0.993</td>
</tr>
<tr>
<td>F -statistic:</td>
<td>708.8</td>
<td>P-Value</td>
<td>&lt; 2.2 e-16</td>
</tr>
</tbody>
</table>
</div>
<p>对该回归模型进行综合分析，评价是否需要改进，并给出思路。【10分】</p>
<p>【解答】</p>
<ol>
<li>写出模型,并对每个回归系数进行解释</li>
<li>通过了f检验认为因变量与自变量之间线性关系显著</li>
<li>t检验回归系数是否显著，拒绝原假设时，表明回归系数显著</li>
<li>调整的多重判定系数R2=0.993，表示在用样本量和模型中自变量的个数调整后，因变量y的总变差中被多个自变量多共同解释的比例为99.3%</li>
<li>通过回归系数的显著性检验和回归系数的正负号判定可能存在多重共线性。还可以计算容忍度$1-R^2$、方差扩大因子$VIF=\frac{1}{1-R^2}$和特征根进一步判断，容忍度越小，多重共线性越严重，容忍度小于1时，存在严重多重共线性，VIF越大，多重共线性越严重，VIF大于10时，存在严重多重共线性；或者对模型中各对自变量之间的相关系数进行显著性检验，通常认为大于0.8高度相关，0.5-0.8是较强的相关关系</li>
<li>处理多重共线性可以选择以下方法</li>
</ol>
<p>a）增加样本容量<br>b）自变量筛选：根据容忍度和方差因子进行筛选；向前选择；向后剔除；逐步回归</p>
<ul>
<li>前进法：  思想是变量由少到多，每次增加一个，直到没有可以引入的变量为止</li>
<li>后退法：首先用全部m个变量建立一个回归方程，然后在这m个变量中选择一个最不重要的变量，将它从方程中剔除</li>
<li>逐步回归：思想是有进有出。做法是将变量一个一个引入，每引入一个自变量后，对已选入的变量要进行逐个检验，当原引入的变量优于后面变量的引入而变得不在显著时，要将其剔除</li>
</ul>
<p>c）放弃无偏估计选择有偏回归（岭回归；主成分回归；偏最小二乘回归）</p>
<ul>
<li>岭回归</li>
<li>主成分回归: 构造前M个主成分Z1,…,Zm,然后以这些主成分作为预测变量，用最小二乘拟合线性回归模型</li>
<li>偏最小二乘：将原始变量的线性组合Z1,…,Zm作为新的变量集，然后用这M个新变量拟合最小二乘模型</li>
</ul>
<h3 id="专业课复试笔试真题回忆版"><a href="#专业课复试笔试真题回忆版" class="headerlink" title="专业课复试笔试真题回忆版"></a>专业课复试笔试真题回忆版</h3><ol>
<li>给定30个数值型数据，请问如何检验其正态性？</li>
<li>给出40个男女生考试分数，其中男生25人，女生15人，问男女生考试分数是否有显著差异，说出检验思路以及该方法的假设条件。</li>
<li>多元线性回归模型的基本假定。</li>
<li>从获取数据到得出分析结果的过程中会出现误差和干扰，举例说出几种误差以及解决办法。</li>
<li>有些统计量中分母会出现总体或样本的方差或标准差，从z检验、t检验和方差分析的角度思考，尽量用文字表述你对这种情况的理解</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 人大应统部落 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 人大应统 </tag>
            
            <tag> 考研真题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人大应统部落（2）：2018年人大应统专业硕士经验贴合集]]></title>
      <url>/2018/04/04/%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E9%83%A8%E8%90%BD%EF%BC%882%EF%BC%89%EF%BC%9A2018%E7%BA%A7%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E7%A1%95%E5%A3%AB%E7%BB%8F%E9%AA%8C%E8%B4%B4%E5%90%88%E9%9B%86/</url>
      <content type="html"><![CDATA[<p>2018年中国人民大学应用统计专业硕士经验贴合集</p>
<a id="more"></a>
<h2 id="第一篇：2018人大应统排名第三经验贴-by-肖洁琳"><a href="#第一篇：2018人大应统排名第三经验贴-by-肖洁琳" class="headerlink" title="第一篇：2018人大应统排名第三经验贴 by 肖洁琳"></a>第一篇：2018人大应统排名第三经验贴 by 肖洁琳</h2><p>2018年考研终于落下帷幕，如愿被人大应统拟录取，写篇经验贴还愿。背景:一战人大应统，本科应用统计学，初试总分401，政治68，英语89，数学129，专业课115，排名第三。复试后综合排名第三。</p>
<h3 id="一、复习阶段和时间安排"><a href="#一、复习阶段和时间安排" class="headerlink" title="一、复习阶段和时间安排"></a>一、复习阶段和时间安排</h3><p>2018年考研我准备的时间是是3月-12月，历时10个月。6-7月忙着期末和搬家，复习进度完全打乱了，真正的的复习时间是3-5月和8-12月。总体来说，整个复习跨度分为三个阶段:3-6月基础阶段，8-10月强化阶段，11-12月冲刺阶段。</p>
<blockquote>
<p>3-6月:</p>
</blockquote>
<p>考研前期准备阶段，我给这个阶段的定位是，消除对考研的恐惧感以及打好数学和英语的基础。主要任务是翻看经验帖，了解考研应该怎样规划(做好心理建设);数学三基础复习(听课+笔记+练习);英语一基础复习(单词+长难句)。这时学校还有课，没课的时候就去图书馆坐着学习，时间不固定，每天学习3-5小时，看看视频做做题，大多数时候有种能捡到分数的满足感。</p>
<blockquote>
<p>8-10月:</p>
</blockquote>
<p>正式进入严肃备考状态(7月回家了，基本没复习)，掰掰手指还有五个月，有了紧迫感，但还是蜜汁自信。这一阶段我认为以提升自己各科的硬实力为要。主要任务是数学三强化(听课+笔记+习题+真题);英语一练习阅读，巩固单词;专业课复习1~2遍；政治整体复习第一遍。具体时间安排如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间</th>
<th>学科</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>8:30-11:30</td>
<td>数学</td>
<td>听课，看书，做题，改题</td>
</tr>
<tr>
<td>11:30-12:30</td>
<td>午饭</td>
<td></td>
</tr>
<tr>
<td>12:30-14:00</td>
<td>趴桌子上午休</td>
<td>不午休的话，下午还是会睡着</td>
</tr>
<tr>
<td>14:00-16:00</td>
<td>英语</td>
<td>阅读题</td>
</tr>
<tr>
<td>16:00-17:00</td>
<td>政治</td>
<td>英语学累了，学学政治放松放松</td>
</tr>
<tr>
<td>17:00-18:00</td>
<td>晚餐</td>
<td>去操场上散散步</td>
</tr>
<tr>
<td>18:00-19:00</td>
<td>政治</td>
<td></td>
</tr>
<tr>
<td>19:00-22:00</td>
<td>专业课</td>
<td>参考书目不少，还是会有危机感~</td>
</tr>
</tbody>
</table>
</div>
<p>大概是这么个时间安排，但不会严丝合缝地按照这个来，数学学累了的时候用软件刷刷单词，有时候会扣扣手机看看视频~算下来一天正经学习时间7~10小时。会用forest计时，看到有效学习时间增长很有成就感。</p>
<blockquote>
<p>11-12月：</p>
</blockquote>
<p>一进入十一月，冲刺阶段开始，需要各科都形成清晰的脉络以供按图索骥;查漏补缺，学过的基础内容丢分岂不是很可惜;做冲刺练习，临阵磨枪，不快也光。主要任务是数学三查漏补缺，做冲刺卷;英语一解决完型，新题型，翻译，作文;政治准备第二轮客观题，准备主观题，准备时政，考前抱佛脚;专业课形成自己的知识脉络，操练解题技巧。具体时间安排:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间</th>
<th>学科</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>8:30-11:30</td>
<td>数学</td>
<td>刷题，改题，抡笔记</td>
</tr>
<tr>
<td>11:30-12:30</td>
<td>午饭</td>
<td></td>
</tr>
<tr>
<td>12:30-14:00</td>
<td>趴桌子上午休</td>
<td>午休前会看看整理的数学公式啥的</td>
</tr>
<tr>
<td>14:00-16:00</td>
<td>英语</td>
<td></td>
</tr>
<tr>
<td>16:00-17:00</td>
<td>政治</td>
<td></td>
</tr>
<tr>
<td>17:00-18:00</td>
<td>晚餐</td>
<td>去操场上散散步(改善心情，不过走太久会有负罪感)</td>
</tr>
<tr>
<td>18:00-19:00</td>
<td>政治</td>
<td></td>
</tr>
<tr>
<td>19:00-22:00</td>
<td>专业课</td>
<td></td>
</tr>
<tr>
<td>22:00-24:00</td>
<td></td>
<td>图书馆闭馆了，去教学楼背诵</td>
</tr>
</tbody>
</table>
</div>
<p>每天学习9-12小时，背单词也是利用早上图书馆排队或其他科目厌倦了的时间空隙。</p>
<h3 id="二、数学三"><a href="#二、数学三" class="headerlink" title="二、数学三"></a>二、数学三</h3><p>我本科学的数分高代，理解能力不错，反应能力比较快，计算能力一般，数三129，事实证明我栽在计算上。</p>
<h4 id="2-1-复习资料和时间安排"><a href="#2-1-复习资料和时间安排" class="headerlink" title="2.1 复习资料和时间安排"></a>2.1 复习资料和时间安排</h4><blockquote>
<p>参考书</p>
</blockquote>
<p>-《李永乐复习全书》(大红皮):</p>
<p>知识点很全面细致，初次做会有点困难，一定要耐心全部刷完，初次做不要直接看例题答案。刷了三遍。也有人用范培华李正元的粉皮全书，据说比较难，数学基础好的小伙伴可以用这本书。</p>
<ul>
<li>李永乐《线代辅导讲义》:</li>
</ul>
<p>五星推荐!搞透这本书，线代满分没啥问题。大概刷了两遍。《李永乐660题》:全是选择题和填空题，需要对知识点有深入的掌握和良好的计算能力，把高数部分完整地刷了一遍，线代和概统没写完。</p>
<ul>
<li>张宇《高数18讲》:</li>
</ul>
<p>有很多很有意思的题，重视解题思路，还蛮好玩的，比较适合思维灵活，基础好的同学。我只写了七八章的内容，配合张宇老师的视频课食用更佳。</p>
<ul>
<li>2000-2017年数三真题:</li>
</ul>
<p>好像是买啥书送的试卷，解析看的是李正元的历年真题解析汤家凤最后八套卷:计算量巨大张宇最后四套卷:挑战解题思路，计算量比汤家凤八套卷稍小</p>
<h4 id="2-2-辅导课程"><a href="#2-2-辅导课程" class="headerlink" title="2.2 辅导课程"></a>2.2 辅导课程</h4><p>汤家凤高数基础班，强化班，概统基础班视频张宇高数强化班视频:张宇和汤家凤都是很好的老师，各有侧重，汤家凤讲的内容更细致，注重系统性，适合基础少薄弱的或者思维习惯比较规矩的同学，张宇注重数学思维，适合思维比较跳跃的同学。李永乐线代基础班，强化班视频:虽然讲课有点沉闷，但是思路清晰，内容完整，深入浅出，李永乐老师不愧被称为线代王。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>时间</th>
<th>书目</th>
<th>辅导课</th>
</tr>
</thead>
<tbody>
<tr>
<td>基础</td>
<td>3.1-6.30</td>
<td>李永乐复习全书;本科数分，高代，概统课本</td>
<td>汤家凤高数基础班，李永乐线代基础班，张宇概统基础班视频</td>
</tr>
<tr>
<td>强化</td>
<td>7.24-9.30</td>
<td>李永乐《线代辅导讲义》，张宇《高数18讲》、《660题》、《全书》</td>
<td>张宇高数强化班(一半)，汤家凤高数强化班，李永乐线代强化班，汤家凤概统强化班视频(挑着看了看)</td>
</tr>
<tr>
<td>真题</td>
<td>10.1-11.05</td>
<td>2000-2017年数三真题</td>
<td>无</td>
</tr>
<tr>
<td>冲刺</td>
<td>11.05-12.22</td>
<td>汤家凤8套卷，张宇4套卷</td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-3-复习方法"><a href="#2-3-复习方法" class="headerlink" title="2.3 复习方法"></a>2.3 复习方法</h4><blockquote>
<p>3-6月</p>
</blockquote>
<p>花了两个月复习高数，线代和概统各一个月。我是一边听视频课一边自己用笔记本做笔记，每学完一章，把全书上对应的例题都做一遍，有疑问的题做上标记，再刷全书的时候重点关注标记的题。注意，这一阶段不要图快，一定要扎实。我当时要求自己笔记尽量写得美观整齐些，留出一定的空间以备日后补充修正，后来证实这对逐渐形成对每个知识点的全方位的理解以及构建各个模块的联系很有帮助。听视频课的时候，遇到记忆不准确的概念和难以理解的知识点时，暂停视频，参考数学教材和网上的资料，也是为了对知识点有尽量全面的理解。这一阶段下来，全书写完第一遍，写了一本自己的数学笔记，基础阶段就算完结了。(其实概统半个月就够了，但是我六月份要准备期末，所以没太沉得下心来，进度很慢)</p>
<blockquote>
<p>8-9月</p>
</blockquote>
<p>全书复习完了第一遍，进入攻坚克难期。还是听课，做笔记，写练习题，再加上把新的想法补充到基础阶段写的笔记上去，按照高数-线代-概统的顺序复习。练习，学习思路，总结，再练习。练习的材料是《660》，《张宇高数18讲》，《全书》(二刷)，《线代辅导讲义》。其中二刷《全书》和《线代辅导讲义》是我规定自己必须完成的任务(我是重基础型选手，厌恶风险，宁愿难题只能做出一部分也不能容忍基础知识出错)，后来由于时间来不及了，就只把《660》的高数部分选择填空和线代概统选择题写了一次，《高数18讲》写了七八讲的样子，没有强求。</p>
<blockquote>
<p>10-11月</p>
</blockquote>
<p>开始真题，两天一个周期，每个周期解决一套试卷，第一天模拟考试写真题并批改打分(时间和时长建议按照考研时间来)，第二天订正错题。养成写试卷的手感。(分数大部分在140+，也有130+或者120+)</p>
<blockquote>
<p>11-12月</p>
</blockquote>
<p>汤家凤八套卷，张宇四套卷。和真题一样的打法，模拟分数会比真题的分数低，正常(分数一般在110-130)。另外，趁着10-12月，三刷全书(屡次做错的题)，二刷《线代辅导讲义》，进一步完善基础阶段的笔记(让它尽量体现你对知识结构的把我，同时又有全面细致的知识点供查询)。</p>
<p>18年数三考得比较难，客观题主观题都不是善茬，思考量和计算量都不小。写完客观题，再写完第一个主观题我就有点慌了，还好我有旺盛的求生欲，坚持到最后一秒2333，结果是客观题没扣分，所有主观题我都知道出题意图是啥，该用啥方法。但是有一半主观只写了一半，就是算不出来或者算错最终结果，想来还是没多练难题和计算能力的锅。</p>
<h3 id="三、英语一"><a href="#三、英语一" class="headerlink" title="三、英语一"></a>三、英语一</h3><h4 id="3-1-总体经验"><a href="#3-1-总体经验" class="headerlink" title="3.1 总体经验"></a>3.1 总体经验</h4><p>我英语基础还行，四级裸考630+，六级裸考530+这样。我一开始的目标是75，冲一下80，一直按照80的预期准备的，考完预期分数80-85，真实分数89，也许有一定运气成分。主要用的参考书是朱伟的《恋练有词》，张剑黄皮书(历年真题)，张剑的黄皮英语作文书(买书送的17年版)。个人认为考研英语最基础的是词汇，次基础的是语法和长难句理解，在此基础上掌握各种题型的套路就大功告成了。我从三月份开始复习单词和长难句。单词用的朱伟的《恋词》和配套视频，因为我光背单词表会感觉很枯燥，听视频课调节一下，大概三月到五月磨磨蹭蹭地把恋词的三十个视频听完了(最后十个串讲视频太长了，懒得看了)，个人觉得词根词缀对猜词还是很有好处的，不需要刻意去记忆，听课的时候跟着理解一下形成印象就行了，以后碰到眼熟但是不确定的词根词缀或者单词，反复查几次就行了。系统地学完《恋词》之后，可以用单词软件背单词，每天坚持背，不能停下来，坚持到考研。我用的是扇贝，有的阶段一天背500个，有时候300个，有时候200个，根据其他计划酌情调整，只要每天背，坚持到最后一天就行。长难句用的是何凯文的《长难句解密》，每天翻20页左右，三四月份看完的，学怎么断句，怎样整体理解句子。不必刻意去记东西。</p>
<h4 id="3-2-各个题型经验"><a href="#3-2-各个题型经验" class="headerlink" title="3.2 各个题型经验"></a>3.2 各个题型经验</h4><blockquote>
<p>阅读:</p>
</blockquote>
<p>应该算复习性价比最高的题型?(当然也是重难点题型了)分值高，套路性特别强，英语练习我都是直接用的真题，没有另外买模拟题或者专门的书，12月最后一段时间，直接拿最近五年的真题当模拟题练。<br>大概到七月底才拿起真题练阅读。总共约80篇真题阅读，每天干掉两篇阅读(96-11年)，大概花了40-50天。建议先读文章做题，批改，尝试理解错题，再通读全文，查单词，看全文解析，看题目解析，理解出题和答题套路，做标记，如果实在觉得理解有问题，可以听听阅读专项的视频课(我听的是唐迟的阅读强化课大概04-08年的真题，基本上包括了考研英语阅读的所有套路)。十月到十一月中旬我把阅读刷了第二次(96-11年)，直接通读全文，看标记的单词和标记的文章解析，看题目和题目解析，做标记。十一月中旬到考前，整套刷最近五年真题，三刷96-11年的阅读(直接看标记部分)。第一遍刷阅读边看解析边思考边划记，耗时会久一些，大概一篇阅读45-60分钟，第二轮第三轮就会快很多，因为已经有了适合自己的重点生词和重点解析了。不建议全文翻译，做阅读重在逻辑关系和情感倾向。</p>
<blockquote>
<p>新题型:</p>
</blockquote>
<p>据说新题型很简单，但是我做新题型很烂，不知道啥原因，于是我听了王晟老师的专项强化课，再不行就只能看命了。新题型放在10-11月刷完阅读后复习(因为觉得阅读是大头，其他题型都是小菜)，先做真题，再批改，再听课学思路。但是18年英语一的新题型当时我做的时候，直觉给出的答案还是和用王晟课程中的思路给出的答案有出入，最后直觉赢了，所以语感还是很重要的。</p>
<blockquote>
<p>翻译:</p>
</blockquote>
<p>长难句的基础在这里很重要，翻译要求句子结构切分正确，内容完整正确。我看的是唐迟的专项强化班，每次练习完一篇翻译就看一节课，放在十一月中旬到十二月刷阅读之后练习。</p>
<blockquote>
<p>作文:</p>
</blockquote>
<p>分两块掌握吧，一块是论述结构，一块是表达。论述的结构就那么几种，基本都是三段式，看几篇范文就能领悟，表达就需要自己整理用得顺手的句型和搭配，同时词汇量也要过关。最重要的就是练习，心里有一个论述的结构，向里面填充具体的内容(论述或例子)。作文我从十一月开始准备，从十二月才开始练习(后来还是觉得有点虚，最后主观题只扣了9分，出乎意料)，看的张剑黄皮的作文书和真题的作文解析，积累其中用得顺手的表达。</p>
<blockquote>
<p>完型:</p>
</blockquote>
<p>我从12月才开始准备，当时想着来不及复习就放弃，完型应该是最考验英语功底的题型了，分值小，范围广，做真题是时候可以注意积累完型的常用搭配，完型还是可以做做模拟题的。</p>
<h3 id="四、政治"><a href="#四、政治" class="headerlink" title="四、政治"></a>四、政治</h3><p>政治分为客观题和主观题。客观题是拉分的关键。68分，说不上经验，说说个人的复习用书和复习感想。</p>
<h4 id="4-1-复习用书"><a href="#4-1-复习用书" class="headerlink" title="4.1 复习用书"></a>4.1 复习用书</h4><p>肖秀荣强化三件套(精讲精练，1000题，真题)，《任燕翔主观题背诵宝典》《肖八》《肖四》《肖秀荣命题人时事与政策》《任燕翔四套卷》</p>
<h4 id="4-2-时间安排"><a href="#4-2-时间安排" class="headerlink" title="4.2 时间安排"></a>4.2 时间安排</h4><p>8-10月中旬，听政治网课，看精讲精练，做1000题。10月中旬-11月中旬看第二遍精讲精练，刷第二遍1000题(只做了一半)，11月中旬-考前，背《任燕翔主观题背诵宝典》(边看视频边背，其实没有完全背下来)，做模拟卷选择题，读肖八主观题，背肖四客观题。政治最后悔的就是10月中旬-11月中旬想二刷1000题，对知识点纠结得过细，浪费了很多时间，但效果一般，导致最后一段时间政治比较被动。客观题我觉得还是快刷，多刷为好，建议最后各个政治老师的模拟卷都买来做做客观题见见世面，模拟卷的客观题错题可以考前强记一波。主观题不要太恐慌，看看任燕翔的主观题直播视频，背好肖四，考试的时候抄抄材料，背背理论就不会崩。</p>
<h3 id="五、432统计学"><a href="#五、432统计学" class="headerlink" title="五、432统计学"></a>五、432统计学</h3><p>今年突然出了50分计算证明题，大概写出来了20分，拿了115分，对准备的部分还算满意，没准备的数理统计部分也怨不得人了。</p>
<h4 id="5-1-参考书目"><a href="#5-1-参考书目" class="headerlink" title="5.1 参考书目"></a>5.1 参考书目</h4><ul>
<li>《统计学》贾俊平(第六版)及配套辅导书</li>
<li>《统计学》贾俊平(经管版)(有第六版中没有的内容，如实验设计部分内容，时间序列部分模型)</li>
<li>圣才的《统计学》考研辅导书(蓝白皮的)(用来刷选择题熟悉概念)何晓群《应用回归分析》(第四版)</li>
<li>王燕《应用时间序列》(第四版)</li>
<li>何晓群《多元统计分析》(第四版)</li>
</ul>
<h4 id="5-2-关于真题"><a href="#5-2-关于真题" class="headerlink" title="5.2 关于真题"></a>5.2 关于真题</h4><p>真题很重要，真题直接反映专业课的考察套路:考哪些内容，考哪些方面，应该怎样作答。研究历年真题，会发现有很多知识点是反复考察的，那就是复习的重点。</p>
<h4 id="5-3-关于笔记"><a href="#5-3-关于笔记" class="headerlink" title="5.3 关于笔记"></a>5.3 关于笔记</h4><p>不得不说学长学姐的讲义还是挺重要的，可以提供一个基本的整理框架和思路，让我不至于跑偏，但是不能完全依赖学长学姐的笔记，自己整理的东西用起来是最顺手的。我是参考学长的笔记结构和历年真题，按照自己思路整理的总结笔记，补充上我的薄弱项。最后整理了主要的八个模块，回归分析全过程的思维图，时间序列的模型总结，多元统计各个方法的联系图。到最后一个月基本就直接背笔记了，很少翻书。</p>
<h4 id="5-4-复习过程"><a href="#5-4-复习过程" class="headerlink" title="5.4 复习过程"></a>5.4 复习过程</h4><p>专业课复习从八月开始，我把上面的这几本教材通看了两遍，第一遍是把所有内容自己理解一次(感觉比较难的章节会做笔记加深一下印象)，第二遍注意整理知识结构和在此理解难以理解的部分，这两遍是为了给后续整理自己的笔记和记忆打基础。进入十月下旬，开始一边做真题，一边整理自己的笔记。《统计学》中，概念的辨析和联系是很重要的，如假设检验中第一类第二类错误的概念，p值的含义，假设检验的思想等等。而《回归》《时序》《多元》中，各种方法的适用范围和优缺点，还有分析思路是很重要的。比如回归分析的整个过程，就要从收集数据，数据清洗开始，到设定模型形式，到参数估计拟合模型，到模型评价，到模型改进和模型选择，再到预测控制，各个环节有各自对应的不同评估指标和处理方法，整个一套过程下来理清楚，就不容易出现记忆错乱的情况;时间序列中理清楚不同类型时间序列的不同处理方法以及具体模型是主线…</p>
<h4 id="5-5-关于2018年专业课"><a href="#5-5-关于2018年专业课" class="headerlink" title="5.5 关于2018年专业课"></a>5.5 关于2018年专业课</h4><p>今年专业课一反常态，出了三个概统的题，50分，难倒一片。第一个是辛普森悖论，第二个是证估计量的性质，第三个是证概率不等式，用到示性函数和柯西不等式的小技巧。可能专硕对数统的要求会有所提高，建议以后大家复习书目加上茆诗松的《概率论与数理统计》…</p>
<p>最后，决定好了的事情就用心去做，祝愿大家得偿所愿。</p>
<h2 id="第二篇：2018人大应统初试第六经验贴-by-肖健一"><a href="#第二篇：2018人大应统初试第六经验贴-by-肖健一" class="headerlink" title="第二篇：2018人大应统初试第六经验贴 by 肖健一"></a>第二篇：2018人大应统初试第六经验贴 by 肖健一</h2><p>一战人大应统，政治67，英语67，数学139，专业课119，初试成绩排名第六，专业课和数学总分排名第一，重点写一下数学三和432统计学专业课的学习经验，英语和政治则稍微简要一点。(以下我说的每一句话都是重中之重，知道同学们疲于观看冗长的经验贴，我已经尽量精简了)</p>
<h3 id="一、432统计学专业课"><a href="#一、432统计学专业课" class="headerlink" title="一、432统计学专业课"></a>一、432统计学专业课</h3><p>我们将准备考研的过程分为三个阶段，暑假前(3-6月)暑假(7-8月)暑假后(9-12月)我的专业课复习过程贯穿于全程，但是方法不同。9月份之前，我仅仅只是将贾俊平老师的统计学看了两遍，9月份之后开始陆续把回归，时序，多元三本书看完，此时脑中有一个初步的知识架构:专业课总共有八个专题，统计学独占前五个专题(数据图表展示以及度量，统计量与抽样分布，假设检验与参数估计，方差分析，列联分析)回归，时序，多元分别是第六七八专题。但是我对八个专题的细节还不清楚，这时候我开始自己拿一个新本子，整理自己的专业课笔记，六七八专题作为重点，一至五专题则作为次要，写完了一个本子。在这里我要特别感谢人大学长学姐为我辅导专业课，由于专业课考试范围过于宏大，你很需要过来人指出考试重点以及考场感受!到了11月中旬，我已经将八个专题的笔记整理完全，之后就开始了整个复习过程中最重要的一环:默写。可以说之前你所做的全是铺垫工作，构架了知识体系，了解了各专题中常见的问题，但是我们还是需要在考场上写出大概12面A4纸的内容，八个题目，每个题目题干一两句话，答案全需要你牢牢记在脑子里并且能完整默写出来!所以我的最后一个月全在不停的默写，对照笔记本，拿出白纸，直接默写出每个专题的常见问题答案，再对照自己的笔记本，做往年真题，默写出答案，默写默写再默写，再对照自己的笔记本，总之就是默写以及围绕自己笔记本!就这样笔记本已经翻烂，笔芯不知写过多少，八个专题已经牢牢印入脑海，各种问题可以说是倒背如流了。上了考场，感觉全是老朋友，只恨不得自己能多长手，全部一顿默写，那种熟悉感觉大大增加了我的信心，学弟学妹们都知道18年专业课有50分证明题，相当于诈和，但是由于信心爆棚，思维打开，我还是做出来了近30分，做完卷子还剩5分钟，大汗淋漓，直呼痛快!</p>
<h3 id="二、数学三-1"><a href="#二、数学三-1" class="headerlink" title="二、数学三"></a>二、数学三</h3><p>最有感触的一门，详情见知乎回答:如何评价2018年考研数学三?<br><a href="https://www.zhihu.com/question/264535482/answer/283393765" target="_blank" rel="noopener">https://www.zhihu.com/question/264535482/answer/283393765</a></p>
<p>从七月份开始正式准备考研以来，数学3一直是优势科目，先是把全书做了一遍，然后15年数3真题，之后660题，然后15年数1真题，这时大概到11月份了，一时兴起又补上了数三从87年到04年的真题，之后到了十二月份是每天一套模拟题，张宇12套，超越135，合工大超越卷，我做题的过程有一个特点，就是从九月份开始，每天都是以模拟考的形式做数学，每天上午8.30到11.30雷打不动，全真模拟，就是那种不准拉的模拟=(虽然好像每次都没憋住…</p>
<p>就这样一直做到12.22，我记得大概是从11月开始，我的模拟成绩就稳定在真题145加，模拟题130左右的水平</p>
<p>上了考场，看到第一个题导数不存在就看出来这套卷子不简单，第二题卡住，看到二阶可导确实想到了泰勒，但是当时也没有再深入思考，再bd中徘徊，然后随手画了个图选了d，那个时候已经有点慌了，但是由于平时模拟考场次数很多，特别是最后的各种模拟题给了我很大历练，就是那种在考场上碰到不会做的题至少不慌的能力，所以强忍着看第三题，万幸看出来了那个中间量是1，所以也就成功做出来，从第三题开始，进入了我平时模拟的状态，一路绿灯到差分方程，其实这里我要多说一句，因为我考的是应用统计，所以我是知道二阶差分的，因为时间序列分析中有多阶差分消除趋势成分的内容，但也只限于我认识那个二阶符号，我当时第一个想到的就是这题超纲了，陈独秀啊…于是没敢耽误一分一秒赶紧跳，最后花了一小时做完选择填空，平时我模拟大概也是这样的时间，于是继续往下</p>
<p>往下是大题，当时我应该是已经进入了状态了，后面几道高数都比较顺利，提一下级数展开an那个题，当时不知道要怎么处理x的2n次方这一项，但是想想了应该是不能再化简了，于是就暴力合并，分奇偶数情况写上了答案，答案复杂到我都不敢写，但是做过的模拟题告诉我真的有正确答案是很复杂的，所以就这么过了那道题。</p>
<p>之后就到了数列递推这道题，这个时候我陈独秀同志真的有话说了，因为其实在考前，我在做往年数3真题的时候，已经发现了一个特别恐怖的规律，特别是到了1617年，我发现最近的数3真的有一种怎么偏怎么考的感觉，比如我在做17年真题的时候，当时有三个点很怪：</p>
<ol>
<li>差分方程</li>
<li>级数那道大题，第一问就不说了，玄学证明收敛半径大于1。。。第二问在合并的时候，考到了级数下角标以及n要变的情况，也就是n=0还是n=1要变化的情况，而以前从没有出过，数1中有考到过，但数3没有</li>
<li>概率最后一道大题一反常规，不是直接考估计，而是先让你推出来概率密度，也是数3以前没有考到过的</li>
</ol>
<p>因为我当时发现了17年已经有点不对了，于是就特别注意一个冷门的考点：数列递推，单调有界准则。这一类题目，数3从来没有考过，数1倒是有考过，所以我在做模拟题的时候碰到这种题就很注意了一下，知道一般证有界需要用数学归纳法，于是在考场上那道题也就顺利做出来。</p>
<p>之后到了线代，线代是真的不考常规，说好的正交变换是重点，反而考了配方法，我相信我们考生最熟悉的关于二次型的题就是让你常规求正交变换，但是万幸的是，我之前有一段时间专攻了二次型，这还要归功于模拟题，因为真题大部分都是让你直接正交变换的，而不会深入考你二次型，所以第一时间我就想到了要分a的情况讨论，来看是不是可逆线性变换，于是这题也过了。</p>
<p>之后三道题就是真的比较常规了，但是我在做的时候也有一些小插曲。。。在做线代第二道的时候，第一问我把a算错了，到第二问的时候发现A矩阵是可逆的，我心想AP=B求P直接左乘A逆不就行了吗！！太弱智了，但是在算A逆的时候突然发现A好像不可逆！！于是又返回去算A行列式，发现第一问的a算错了，于是开始大面积划掉答案，改的时候手都在抖，心想差点，差点就凉了，10分啊！！还好悬崖勒马！然后两道概率就中规中矩了。</p>
<p>这个时候还剩5分钟，之前我有说过我选择题第二题是蒙的，差分方程没有做，于是这时返回去看这两个题，这时猪操作来了，因为我考前总结了大概14-17的选择题答案，发现ABCD是2231的分布，也就是说会有三个是选一样，然后是两个一样的，两个一样的，最后剩一个，但是到底是ABCD哪一个选三个是不确定的，于是我当时自以为很聪明的开始看我的其他七个确定的题，发现已经有3个A，2个D，1个B，1个C，如果第二题我再选D，就会有3个D，3个A，正好第二题我又在BD中徘徊，于是我当机立断马上改为B，没跑了！还很美滋滋。。觉得自己很帅。。。再回来看差分方程那个题，已经犯迷糊了，因为看那个yx很不爽，潜意识里告诉自己这题超纲！再加上当时就两分钟了，所以就没有做。</p>
<p>加一条线，显示我终于把整个考试过程说清楚了，昨天上午对完了答案，发现选择错一个（那个自以为很秀的第二题）填空错一个（差分）之后的大题好像没有错。</p>
<p>做一个总结吧，2018的数3，总体来说是难的，而且是历史最难，没有之一，从1987年到2017年但是我想强调点是，如果你的考研数学复习，一直是限于做真题，全书，而且做很多遍的情况下，来做2018的题，是绝对不够的，因为它和往年的真题真的很不一样，基本上是一种“怎么偏我怎么考，考的常规算我输”的状态，但是如果你有做过大量的模拟题，有经历过在难题堆里挣扎的情形，在考场上至少不会慌张，不会心态爆炸，我觉得我就是这么一种情况，当时从第二题不会做开始，就拼命告诉自己，稳住稳住，什么难题没见过之类话，所以总的来说，我认为2018年题首先是难，然后那道差分方程确实是败笔，但是其他的题，都在考纲范围内，只是比较冷门的知识点，所以说没做出来只能说自己功夫不到家，我到现在都认为我那道选择题是自己没学好才错了的。</p>
<p>其实在考场上，只要不出现大的失误，类似于题目抄错，考生信息填错等原则性错误，其实都属于自己的正常发挥，没考好也是自己的某方面知识薄弱。</p>
<h3 id="三、英语一-1"><a href="#三、英语一-1" class="headerlink" title="三、英语一"></a>三、英语一</h3><p>如果你像我一样基础不太好，可以做精翻，也就是把往年阅读做完后再翻译出来，不要做模拟题，反复做真题，做他五六次，然后每天记单词，不要断，一直到考研那天，最重要的是，不要忽视新题型，18年就是英语一新题型诈和，我估摸着我应该是全错了，不然不会这么低。我英语花的时间仅次于数学，感觉还是没考好，不然可以冲一下400分，可惜。</p>
<h3 id="四、政治-1"><a href="#四、政治-1" class="headerlink" title="四、政治"></a>四、政治</h3><p>考前诈和，周末考试，我周一到周四都在发烧，所以完全没有背分析题，还好上了考场发现答案全在题里，相信这也是政治考试以后的发展趋势吧。不急着复习政治，结合我身边的同学复习情况来看:我女朋友五道口第17名，考前可以说是把所有类型模拟题都做了，政治投入极大，最后也没有上70，一个朋友是今年北大非法学法硕初试第一，他反而确有70多分，我室友连肖4肖8都没有买，也有66??所以我认为考研政治完全是一门玄学，你的复习时间和分数关系不大，分数高低全看天，选择题一定要多做，所有人各种版本的模拟题都可以做一做，反正也就35个题，二十分钟搞定一套，这样虽然不能保证你上70，但65肯定是有的，上不上70看天吧。</p>
<p>最后总结一下，既然决定了考研就要一心一意，从我身边的同学来看，考研很简单，你认真了，就能上，没有认真，就一定上不了。暑假之前，暑假，暑假之后，这三个阶段中学习强度可以逐渐加强，循序渐进，但这些都是次要的，最重要的是心诚则灵，送两个字给你们:专注。你的心要属于考研，一心向学，必然成功。最后愿所有真正努力过的同学都能取得理想的成绩！</p>
<h2 id="第三篇：2018-年人大应用统计考研经验贴-by-曾诚"><a href="#第三篇：2018-年人大应用统计考研经验贴-by-曾诚" class="headerlink" title="第三篇：2018 年人大应用统计考研经验贴 by 曾诚"></a>第三篇：2018 年人大应用统计考研经验贴 by 曾诚</h2><p>在 2018 年的考研中，我的分数不算太高，但也算是我自己比较没有遗憾的 一个成绩吧——375 分，也算是给两年的艰辛的考研旅程画上了一个圆满的句号。 各科分数为政治 64，英语 65，数学三 135，统计学 111。之所以写下此经验贴， 一是对自己两年考研历程的一个总结，二也是希望能够给今后考人大的师弟师妹 们提供一些帮助吧。</p>
<h3 id="一、我的大致情况"><a href="#一、我的大致情况" class="headerlink" title="一、我的大致情况"></a>一、我的大致情况</h3><p>本人就读于天津一个非 985、211 院校，读的是统计专业精算方向。对于专 业的选择，其实没有考虑太多，由于没有读博的打算，所以直接就选择了专硕。 整个大学期间读书还算认真，但碍于非 985、211 院校，保研资格实在是太少太 少，而且就算能够保研，可能所就读的院校也不是特别特别的拔尖(除非从很早 很早就开始准备，刚进校那会就听说有个学姐保送了北大应统，据说从大一就开 始准备，发表过很多论文作品，而且大学期间就拿下了中国准精算师证书，膜拜 膜拜)，所以当时就了解了一下应统比较好的院校，这里说一下我大概了解的情 况，因为本科是在天津，所以更多的还是愿意选择北京的学校，一来北京离天津 很近，二来北京作为三大一线城市之一，就业机会远大于其他城市，其实读本科 的时候就有些后悔，应该选择一线城市的学校去读，且不谈学校如何，城市给你 的感觉截然不同。就拿天津和北京为例，天津的氛围相较于北京，实在是太过于 安逸，只能说天津是个适合安家的城市，但北京的环境所给你带来的无论是眼界、 机遇绝对是天津所给予不了的。所以当时就把目标定在了人大和北大，当然南方 也有许多不错的院校，例如说厦大，听大学一个关系不错的老师说华东师范的精 算很不错，想考精算的童鞋可以了解一下。当然这里我就重点从客观的角度说一 下人大和北大，不用说从院校实力来比较呢，自然是北大要强啦，考的难度自然 也是北大难度大些。从初试分数线来看，北大和人大的初试分数线分别是 380 和 360。从专业课来看，北大偏数理统计，而人大比较偏应用，这也是统计专业 课为数不多的专业课相对来说比较简单的院校了，但可能也是这个原因，从今年 开始人大专业课也来了个 360 度大转弯，把我们打得措手不及，估计以后就不会 那么容易了哦。从招生规模来看，都有小幅度扩招，但人大的招生人数还是远远大于北大，近年人大统招 45 人，而北大只有 17 人，招的人少，这就说明你要和 更多数学大神，专业课大神去拼，不能混入低分飘过的队伍中去了哦。虽然现在 人大的拟录取名单还没出，但从初试名单上的分数来看，低分飘过的小伙伴还是 不占少数，而北大最低的初试分数也达到了 385。从复试来看，北大的复试分差 还是相当恐怖的，最高的有 96 分的大神，在这里膜拜一下，低的才 65 分，初试 分数高的也不一定稳，这点人大还是非常占优势的，人大复试比例只占 30%，相 比其他院校还是低了很多。总之，清华北大这个大神级的学校，要考还是要多三 思。</p>
<p>再说说我的工作经历吧，在数学难度大增还有专业课改革的大形势下，相信 今年考研的许多小伙伴们都碰了灰。去年的这个时候我也面临着和你们一样的困 境，实习?工作?二战?这似乎是考研失败的小伙伴们避不开的选择。这里直接 给出几条建议吧。要是在纠结工作还是二战的朋友，我真的劝你好好想一想你是 否真的值得再花一年宝贵的时间来学习?今年成绩不理想的原因在哪?再给你 一年时间你在哪科能有巨大的提升?不妨再问一下自己，这个研究生是否值得读， 是否比两年或者三年的工作经历来的更重要?重新审视之后再做出决定，可以先 去认认真真地实习两个月，体会一下工作是一个什么样的状态，准备二战的也可 以实习，毕竟二战暑假再开始准备也是完全来得及的。再就是纠结全职二战还是 专心二战，本人是全职二战，不得不说当时的考虑就是想着自己毕业了，也不能 老靠着家里，就找了一个相对轻松的工作，一个月能赚足自己的生活费。但我必 须要提醒大家的是，要全职二战可是会非常的辛苦了，即使是一份轻松的工作， 你一天的学习时间也绝对不会超过 8 个小时，这可是比第一年的学习时间直接砍 掉了将近一半，而且有时候下班累了，就真的已经不想在学习了。所以我对二战 的小伙伴们的建议就是，要么就找个好点的实习，实习几个月然后专心考研;要 么就找个相对轻松的工作，但是后者会非常辛苦，建议慎重考虑。</p>
<h3 id="二、各科复习进度与复习方法"><a href="#二、各科复习进度与复习方法" class="headerlink" title="二、各科复习进度与复习方法"></a>二、各科复习进度与复习方法</h3><p>我主要还是讲一下我的备考经历，按时间顺序，里面会加上我的复习心得。</p>
<h4 id="2-1-数学三"><a href="#2-1-数学三" class="headerlink" title="2.1 数学三"></a>2.1 数学三</h4><blockquote>
<p>复习用书</p>
</blockquote>
<p>复习全书、概率论与数理统计计提方法与技巧、微积分解题与技巧、线性代数辅导讲义、数学基础过关 660 题、历年真题、模拟题(李永乐)、 合工大三套卷</p>
<blockquote>
<p>复习进度</p>
</blockquote>
<p>由于第二年考研时间比较短，复习的时间规划不太详细，以第一年时间规划为例。</p>
<ul>
<li>第一阶段:7 月-9 月(基础阶段)</li>
</ul>
<p>每天 3-4 小时 建议使用李永乐复习全书、概率论与数理统计计提方法与技巧、微积分解题与技巧、线性代数辅导讲义。第一本比较系统，但是有些许定理会直接摆在上面， 并没有给予证明，可能第一遍会看不太懂，不过这也正常，可以去翻看本科的数 学课本，第一遍看的时候尽量把知识点都弄得详尽一些，最近几年对于细节的考 点越来越多，以前看似不会考的考点，比如说数列收敛性的证明，差分方程，都 成为近两年数三试卷上的宠儿(差分方程的题目连错两年，不是难，平时这类题 目练得少，公式总记不住)。后三本是分别针对数三的微积分、线代和概率论三 部分的书。其中，微积分解题技巧这本书比较难，当时也是看了 14 年考上北大 的学长经验贴买的，内容要比全书上多很多，要看不懂可以酌情跳过，但是最近 数三的微积分部分越考越难，我觉得有必要推荐一下，上面有很多题目的解题方 法。线性代数的讲义写的很有条理，思路很清晰。概率论的这本书真的是相当推 荐，有很多方法相当实用，看完这本书，基本上概率论这部分就没什么问题了。</p>
<p>这个阶段大家尽量将每个知识点都理解透彻，不要放过每一个细节，最好自 己列一个知识点框架，把一些重要的公式记下来，下次再记不住，就用记号笔重 点画出。刷题的时候，不会的题和猜对但不知道原理的题都要用记号标出，不要 糊弄，最后都会直接体现在你的考研分数中，这部分时间长，一定要把知识点学 扎实。</p>
<ul>
<li>第二阶段:10 月上旬(提升阶段) 每天 3 小时</li>
</ul>
<p>建议基础过关 660 题，这本书 20 天左右绝对刷的完。我第一年选择的是张宇 1000 题，第二年选的 660，这两本书我觉得 660 更贴近真题一些，张宇 1000 题偏难，对答案的时候非常扎心。660 只有选择和填空两种题型，1000 题所有题 型都有，但讲真，有些大题还真是有点难，大家根据自身情况进行选择。</p>
<ul>
<li>第三阶段:</li>
</ul>
<p>10 月下旬-12 月底(冲刺阶段) 每天 3 小时 建议历年真题、模拟题(李永乐)、合工大三套卷，很多一战的人都喜欢先</p>
<p>刷历年真题，再刷模拟题，没错我一战也是这样，我这个人喜欢总结套路，由于 真题简单，我就开始总结哪些题会考哪些题不会考，结果 2017 考研的时候我就 傻眼了，以为不会考的差分和数列出现在试卷上。所以大家一定要吸取我的教训， 千万不要以为某个知识点是一定不会考的，近几年的数学题是一年比一年刁钻。 我刷完真题之后发现我好像只会那么真题的那么几类题型了，做模拟题的时候好 多题型完全想不起来怎么做。所以第二年我先从模拟题刷起，把你的水平先拔高， 再去对待简单的真题。强烈推荐合工大三套卷，感觉它的出题风格非常像今年的 数学真题了，有点难，但还挺锻炼你的临场发挥能力的。每套卷子一定要掐着时 间做。我当时还专门买了考研专用的答题卡来模拟训练，因为我这个人还有个毛 病，一看见答题卡就紧张，当然我觉得这也是有必要的，因为毕竟将答案誊写到 答题卡上还要一定的时间。</p>
<p>12 月份我就没有再练习新的题了，这个时候在数学上的时间可以缩减到 2 个甚至 1 个小时，因为这段时间对你数学的提分作用不大，应该多放时间在其他 三门记忆性的学科上。这段时间可以看看你的错题，总结总结自己在哪方面还比 较薄弱、公式重点记忆、查缺补漏，千万不要在做题了，只会越做越紧张!</p>
<h4 id="2-2-英语一"><a href="#2-2-英语一" class="headerlink" title="2.2 英语一"></a>2.2 英语一</h4><p>复习用书:扇贝 app，历年真题，王江涛高分写作</p>
<p>这门学科我就不说太多了，17 年考了 76 分，18 年才考了 65 分，分析一下 分数降低的原因吧，也给不了太多的建议。</p>
<p>今年分数下来看见自己的英语成绩，是不能接受的。真的未免太低了，不过 后来想想也就该得这个分。近年可以说真的是一个单词都没背，这能充分说明背 单词的重要性了。学了很多阅读的技巧什么的，但终归抵不过单词不认识，句子 看不懂。所以说不要妄想走捷径，根基不稳，终究是不行滴!</p>
<p>英语的重心还是放在阅读上吧，阅读占的分值实在太大。历年真题即可，无 需其他的辅导书。05 年之前的真题和现在出题风格不太一样，时间不多的话就 可以不用刷了。10 年后英语开始变难。一篇一篇阅读认真刷，最好弄懂每篇文 章的每个单词和句子，对后面的翻译也是有好处的。</p>
<p>再就是写作，这部分真的最好要提前准备，千万不要等到 12 月份才开始准备。小作文非常有套路，我第二年因为时间不够，就把 10 年到 17 年所有小作文 考的题型列出来，发现常考的就那么 5、6 类，而且邀请信出现次数最少，结果 还真就考了，哈哈哈。大作文也是有套句的。</p>
<p>作文一定要背句子，不要整篇整篇的背，因为有些句子是可以通用的，文章 背完了可能只适合那一类题材，而且容易被判卷老师看出来是背的模板。平时也 要有意识的训练自己写句子的能力，毕竟不能所有的句子都写套句。</p>
<p>其他三部分真的是听天由命了，考的非常惨淡，就不给出建议了。</p>
<h4 id="2-3-政治"><a href="#2-3-政治" class="headerlink" title="2.3  政治"></a>2.3  政治</h4><p>复习用书:肖秀荣命题人知识点精讲，肖秀荣 1000 题，肖 8，肖 4</p>
<p>这种学科要考好真的是完全靠天赋了，两年都是 60 几分，总之，告诫各位 前面时间真的别用太长，9 月开始准备足矣。最后一个月肖 8 肖 4，尤其是肖 4 努力背，使劲背，背着全是你的，哈哈，2017 年政治大题全压中了，可怕。其 他的不多说了~</p>
<h4 id="2-4-432-统计学"><a href="#2-4-432-统计学" class="headerlink" title="2.4 432 统计学"></a>2.4 432 统计学</h4><p>复习用书:统计学第六版、统计学第四版、应用时间序列分析第四版、多元 统计分析第四版、应用回归分析第四版，今年考的证明据说是茆诗松的概率论与 数理统计的书上的，具体也不是很清楚，最好好好了解一番，不过这本书是很 多学校统计考研都会考的书，偏理论证明。</p>
<p>今年考研专业课画风突变，今年之前从来没有考过定理的证明，但今年考了 三道大题，一道简单，另外两道比较难，感觉有一道跟切比雪夫不等式比较像， 但怎么也证不出来，另外一道连题目都看不懂。而且今年不知道为什么初试包括 复试都只考了统计学，不知道明年还是不是这样。不管考不考，其他三本书该复 习还是得复习，再加上一本偏理论的数理统计教程。</p>
<p>建议大家先把历年真题都过一遍，知道大概每道题都考什么知识点，因为真 的不是所有书上的内容都会考到的，这样做非常省时间，不然毫无目的看完一遍 之后什么也记不下来。人大的专业课考研是分模块的，考试一般考 8-9 个题，全 是大题，一般分八个模块，都是按统计方法分的，每个模块一到两道题，你可以 把每个模块的题都放一起看，看看都考什么知识点。比如说第一个模块是统计图 表，知识点就是数据的分类、每类数据适用的图表以及特点。通过这些问题去找答案，看书效率事半功倍。我第二年书都没看，直接看的笔记，仅仅只花了 2 个月的时间复习专业课，最后考了 111 分，还算可以吧。不过证明题这块个人真 的不知道怎么复习，以前也没考过，这里就不给出建议了。</p>
<p>另外强调一下，大家在做历年真题的时候一定要自己亲自动手做，我第一年没动手做，结果到考场上专业术语全都想不起来，写的语言特别大白话，最后分 也不是很理想。不要嫌麻烦，真的，想的和写出来的不太一样的，写一遍，再对 照书本或是学长学姐的资料或笔记重新改一遍，重点记忆的部分用记号笔做标记。 另外，一个小细节，关于字母的书写的，凡是涉及字母的题目都要在开始标注清 楚每个字母都代表什么，保证做题的严谨。其他的有关于每本书的复习思路相信 在前几届学长学姐的经验贴中已经说的非常详细了，此处不再赘述。</p>
<h3 id="三、结语"><a href="#三、结语" class="headerlink" title="三、结语"></a>三、结语</h3><p>考研除了复习方法之外，保持身体强健与心态平和也尤为重要。要知道复习 时间紧迫，只要一病就有可能耽误好几天，所以一定要保持自己身体的健康，每 天给自己半个小时或一个小时的休息锻炼时间，劳逸结合。心态平和更是重要， 第一年由于自己的心态问题导致数学相当惨烈，不仅紧张，还没休息好，考数学 前差点睡着，考试大脑一片空白，想想都可怕。考试前一定就不要熬夜了，也不 要花太多时间在复习上了，保证良好的休息时间才能有清醒的头脑对待考试。<br>数学和专业课真的相当重要了，特别特别拉分，建议多花点时间。<br>以上就是全部关于两年的考研历程的总结，希望大家能够多多吸取我的教训 吧，都是血和泪的教训啊，希望能对考人大的学弟学妹们有所帮助。也祝愿大家 能在 2019 年的考研中取得一个自己满意的成绩!</p>
<h2 id="第四篇：2018人大应统考研经验帖-by-王雪楠"><a href="#第四篇：2018人大应统考研经验帖-by-王雪楠" class="headerlink" title="第四篇：2018人大应统考研经验帖 by 王雪楠"></a>第四篇：2018人大应统考研经验帖 by 王雪楠</h2><p>大家好，我是 18 考研中国人民大学应用统计专业的小王，初试分数为政治 73、英语一 78、 数学三 124、专业课 103、总分 378 分，初试排名 15/54，复试成绩未出，成绩不算很高， 备考经验仅供参考。(注:18 考研指的是 2018 年研究生入学那一拨人的考研，实际是在 2017 年末考的)</p>
<h3 id="一、-背景介绍"><a href="#一、-背景介绍" class="headerlink" title="一、 背景介绍"></a>一、 背景介绍</h3><ol>
<li>学业背景<br>本人为应届生，本科读统计学专业;<br>六级裸考520+(听力和写作较弱)，高中英语语法尚可(后面会提到)。</li>
<li>目标院校考研情况 人大应用统计专业最近几年的分数水涨船高，报的人多，收的人多，分数线也很高; 去年校线 390，今年校线 360，30 分主要差在专业课上，18 考研人大应统专业课难度突然 提升，导致分数线下降，个人认为分数线的下降与数学今年较难无关; 专业课按照以往的经验全部是论述题，但 18 考研突然出了 50 分的证明题，确实有些措手不 及，其中有 30 分(我猜)大部分人都不太会，所以分数线明显下降，至于明年会不会再考 证明不好说。</li>
</ol>
<h3 id="二、-备考经验"><a href="#二、-备考经验" class="headerlink" title="二、 备考经验"></a>二、 备考经验</h3><h4 id="2-1-第一阶段"><a href="#2-1-第一阶段" class="headerlink" title="2.1 第一阶段"></a>2.1 第一阶段</h4><p>大三下学期即 2017.03-2017.07 大三下我还有课要上，而且课还不少，所以没有具体规划每天的时间，有大块时间就会去图 书馆，计划也是定的至少一个月以内的计划，并没有单日或单周计划。</p>
<blockquote>
<p>数学三 </p>
</blockquote>
<ul>
<li>复习资料:</li>
</ul>
<p>李永乐复习全书(红皮)及配套分阶习题、李永乐线性代数辅导讲义、张宇高数 基础班视频 复习进度:因为我有课，所以目标是期末(2017.07)之前刷过一遍全书和分阶习题即可， 实际进度也是如此。</p>
<ul>
<li>复习安排:</li>
</ul>
<p>按照高数、线性代数、概率论的顺序复习。</p>
<p>高数部分: 按照章节看张宇高数基础班视频并做笔记。张宇老师的课讲的非常精彩，前期复习比较迷 茫的话可以一听，个人认为听了该视频做过全书的话那么可以不必再看十八讲。 每看完一章视频，去做一章全书加一章配套的分阶习题。 在全书上，会且做对的题目标1，会但做错/不太会瞎蒙对的标2，不会的题目标3，其他 部分也同样如此，方便后续复习。</p>
<p>线代部分: 直接做了线性代数辅导讲义。个人认为做了线代辅导讲义那么全书的线代部分和分阶习题的 线代部分可不做。</p>
<p>概率论:刷全书+分阶习题。</p>
<blockquote>
<p>英语一 </p>
</blockquote>
<p>简单看了一下考研英语的语法视频，发现讲课内容主要集中在从句部分。而从句我的高中英 语老师讲得很好，我掌握得还可以，所以没有单独学习考研英语语法部分。如果同学觉得自 己从句部分掌握得不是很好，那么可以专攻一下英语的各种从句，不必听全套英语语法视 频浪费时间。</p>
<ul>
<li>复习资料</li>
</ul>
<p>扇贝 app 复习目标:没有具体目标，只希望自己能坚持每天打卡背单词，没有开始做英语真题。 复习安排:每天在扇贝上背 200-400 个单词，一个学期过去应该是背过了考研单词一轮。 </p>
<p>其他:我没有使用单词书(如新东方大绿皮)和看恋练有词视频的原因: 我一开始用大绿皮背单词，每天两个新的 list，还要复习。用单词书时会看的非常细致，每 一个意思都希望自己能够记住，先背再检验是否背了下来有时可能会用一整个上午的时间， 我觉得有点浪费时间，所以最后不看单词书选择用 APP 背单词; 恋练有词里面朱伟废话连篇，虽然可以快进但是体验不太好;另一个比较主要的原因是我看 过视频后不会立刻去背单词和他讲的词根词缀等东西，过了好几天再翻发现已经忘了，最后 索性就不看视频了。如果有同学能够坚持看下来的话应该效果也不错。</p>
<blockquote>
<p>432 统计学</p>
</blockquote>
<ul>
<li>复习资料</li>
</ul>
<p>人大出版的 统计学第四版、统计学第六版及配套学习指导书、应用回归分析、 时间序列分析、多元统计分析</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-04 上午12.03.27.png" alt="屏幕快照 2018-04-04 上午12.03.27"></p>
<ul>
<li>复习目标</li>
</ul>
<p>期末前看完上面所有书第一遍，不做笔记只是读书。 复习安排:有时间就看看，没有单日计划，能在暑假开始前看完即可。</p>
<blockquote>
<p>政治</p>
</blockquote>
<p>没看。</p>
<h4 id="2-2-第二阶段"><a href="#2-2-第二阶段" class="headerlink" title="2.2 第二阶段"></a>2.2 第二阶段</h4><p>暑假即 2017.07-2017.08在家，并没有每天学习，进度不快。</p>
<blockquote>
<p>数学三</p>
</blockquote>
<ul>
<li>复习资料</li>
</ul>
<p>李永乐复习全书(红皮)、李永乐 660 题、李永乐线性代数辅导讲义、第一轮自己做的笔记</p>
<ul>
<li>复习进度</li>
</ul>
<p>暑假内二刷完全书，做完了 660 题</p>
<ul>
<li>复习安排</li>
</ul>
<p>按照章节看全书和自己的笔记。看全书时，第一遍标1的题目不再看，只看标2和3的题 目，从前不会做的如今会做且做对那么将标号改为1，以此类推。 每看过一章全书和笔记做一章 660 题，我当时的目标是每天 50 道。</p>
<blockquote>
<p>英语一</p>
</blockquote>
<ul>
<li>复习资料</li>
</ul>
<p>张剑黄皮书三本，扇贝 APP</p>
<ul>
<li>复习目标</li>
</ul>
<p>继续每天背单词，阅读随缘</p>
<ul>
<li>复习安排:</li>
</ul>
<p>每天早上在扇贝上背 700 个单词再起床，大约用时 1h; 从张剑黄皮书最早的年份开始，只做阅读题。当时是(尽量)一天精读一篇，全篇落实笔头 翻译，进度非常慢。开学前没有做完第一本阅读。</p>
<ul>
<li>其他:</li>
</ul>
<p>为什么不直接做第二本第三本(题型更贴合的年份)? 学习方法需要慢慢摸索，用第一本书来大胆试错，省得把第二本和第三本浪费了。当时我全 篇翻译很费时间，所以进度很慢，后来只翻译阅读中的长难句，再后来只在心里默默翻译并 在卷子上标明句子结构，慢慢才找到适合我自己的方法。全篇翻译有必要吗?个人认为没有必要。</p>
<blockquote>
<p>432 统计学 </p>
</blockquote>
<p>资料同上，依然是每天默默看书，希望大家能够在暑假时把专业课笔记自己整理完。</p>
<blockquote>
<p> 政治</p>
</blockquote>
<p>没看。</p>
<h4 id="2-3-第三阶段"><a href="#2-3-第三阶段" class="headerlink" title="2.3 第三阶段"></a>2.3 第三阶段</h4><p>2017.09-2017.12</p>
<blockquote>
<p>数学三</p>
</blockquote>
<p>2017.09-2017.10</p>
<ul>
<li>复习资料</li>
</ul>
<p>李永乐复习全书(红皮)、李永乐 660 题、李永乐线性代数辅导讲义、第一轮自己做的笔记、李永乐数学三历年真题 复习进度:三刷全书(包括笔记和线代讲义)，看完了 660 的错题，做完了 97-17 的真题。 复习安排: 大约花了一个月的时间三刷全书和从前的错题，个人认为这段时间有点浪费，错题不必太纠 结; 三刷全书以后大约以每天一套数学三真题的速度往下刷，每做五套休息一到两天，整理真题 错题，易错的知识点及时复习。个人认为真题没有必要刷两三遍，因为比较简单。 2017.11-2017.12</p>
<ul>
<li>复习资料:</li>
</ul>
<p>李永乐 6+2 模拟题，张宇八套卷，张宇四套卷，合工大模拟题，</p>
<ul>
<li>复习安排</li>
</ul>
<p>每一套模拟题都定好闹钟在三小时内完成，做一套或两套休息一天来整理，及时回顾易错知 识点直到考前。模拟题没有二刷。 (即使是现在我也可以说数学是我复习的最好用时最长的一门，没想到考试的时候考砸分数 不高，对我来说特别遗憾。但我觉得复习的过程还是可以借鉴的，因为当时真的数学复习得 最用心 T^T)</p>
<blockquote>
<p>英语一</p>
</blockquote>
<p>阅读、完型、新题型、翻译部分:</p>
<ul>
<li>复习资料:</li>
</ul>
<p>张剑黄皮书，扇贝 APP</p>
<ul>
<li>复习安排:</li>
</ul>
<p>单词数自己决定，不要低于 200 也不要超过 600; 第一轮:每天精读两篇阅读，不包括完型新题型，全做过一遍后进入第二轮; 第二轮:每天精读两篇阅读，不包括张剑第一本，不包括完型新题型，全做过一遍后进入第 三轮; 第三轮:每天精读四篇阅读(即一年的)，不包括张剑第一本，每天一篇完型/新题型/翻译。 如果还有时间那么继续读真题阅读和完型新题型，没有时间三轮足矣。</p>
<ul>
<li>作文部分:</li>
</ul>
<p>12 月份开始每天背王江涛大小作文各一篇;12 月中旬看何凯文的作文模板视频并背诵。 (英语一我完型错了一个，阅读没错，新题型五个空错了四个，最后 78 分。个人认为阅读 的复习经验可以借鉴，新题型我也很无奈，作文和翻译(扣了 13.5 分)在北京的极旱区给 分我个人觉得还算可以，但是作文准备的并不是很充分，我考前一篇作文都没写过，不要学 我。)</p>
<blockquote>
<p>432 统计学 </p>
</blockquote>
<p>整理完笔记就是背诵，考前能够背过五轮就可以了。至于证明题由于我当时没有复习所以我 没办法给出比较好的实践过的建议。 除去背诵记得看真题。真题重要，真题参考答案也很重要。我联系了人很好的章华师兄，买了真题的参考答案，给自己做题方面指引了一些方向。 总之专业课就是一直背诵直到考前。</p>
<blockquote>
<p>政治 </p>
</blockquote>
<ul>
<li>复习资料</li>
</ul>
<p>肖秀荣精讲精练、肖秀荣一千题、肖秀荣时事政治小册子、肖秀荣八套卷、肖秀 荣四套卷、腿姐复习宝典</p>
<ul>
<li>复习安排</li>
</ul>
<p>2017.09-2017.11<br>看精讲精练做一千题，反正看完了做完了啥也没记住。<br>2017.11-2017.12<br>花了两天时间集中看了肖秀荣时事政治小册子，把时事政治部分搞定; 问答题背诵肖秀荣八套卷四套卷的问答题即可，18 考研肖秀荣全部命中; 选择题，看腿姐的复习宝典，有时间搭配视频来看，选择题效果拔群。</p>
<ul>
<li>其他:</li>
</ul>
<p>我为什么不看风中劲草: 风中劲草知识点太全了，把所有知识点都拿出来=什么都没说，背不下来浪费时间，不如多 看几遍腿姐的复习宝典(12 月份会出)(徐涛老师的也可以，总之选择题我认为不要靠风中 劲草或者一千题或者八套卷四套卷来提高，看各位老师总结的重点知识点比较有用)。<br>我</p>
<p>个人的复习经验大致如此，不一定适合每个人。成绩不高写这个经验帖感觉比较羞耻，希 望大家能够多看经验帖找到适合自己的学习方法而不是照搬他人。本人自制力较差，即使是 考前一周也会因为心情不好而在宿舍躺尸一天，相信大家严格要求自己肯定可以取得不错的 成绩。</p>
<h2 id="第五篇：18-年人大应用统计经验贴-by-冯亚宁"><a href="#第五篇：18-年人大应用统计经验贴-by-冯亚宁" class="headerlink" title="第五篇：18 年人大应用统计经验贴 by 冯亚宁"></a>第五篇：18 年人大应用统计经验贴 by 冯亚宁</h2><p>我本科就读于武汉某高校经济统计学专业，说实话，当初经历了专业调剂， 转专业的风波，最后也是阴差阳错选择了统计学。起初真的没抱太多的期望， 随着接触了越来越多的统计学知识，我渐渐的迷上了统计，这大概就是所谓的 “日久生情”吧。但是，随着毕业的尾声接近，我越来越觉得自己掌握的知识 有限，因此我决定继续进行统计方面的学习。在选择学校方面，我丝毫没有犹 豫的选择了人大，一是源自我高考的执念，二是人大统计真的非常让人向往。 讲了这么多，介绍一下我的基本情况，我是二战，两年都是应用统计学专业， 去年总分 388，今年 362，刚刚过线。答应了师兄要写经验贴，想想了一想，经验谈不上，分享一下我的一点心得吧，希望对师弟师妹们有所帮助。</p>
<h3 id="一、心态问题"><a href="#一、心态问题" class="headerlink" title="一、心态问题"></a>一、心态问题</h3><p>首先，我觉得一定不能偏科，要合理分配自己的时间。即使某一科学习不 太好，也不要让它成为你明显的短板。因为即使你有一科特别出彩，也很容易 被差的一科拉下来。我 17 年，政治 74，英语 62，数学 122，专业课 130，很明 显英语上就相对要低得多。我的经历就给大家作为一个反面教材，提个醒，希 望大家一定要各科均衡发力。其次，数学和专业课是大头。总看这几年的录取 名单，我们可以明显的发现数学和专业课好的同学成绩都比较靠前。希望大家 能够注重数学和专业课的学习。最后，一定要坚持。既然选择了考研，就一定 要记住自己的初心，坚持下去。期间很多同学保研、出国、工作，但是请一定 不要受他们影响。既然选择了远方，<br>那便只顾风雨兼程。</p>
<h3 id="二、时间安排"><a href="#二、时间安排" class="headerlink" title="二、时间安排"></a>二、时间安排</h3><p>我是 8 月份在人大附近租房准备的，如果有二战的小伙伴，有住宿或者相 关的问题可以问我，具体是作息时间安排，我借鉴一位师兄(花旗先生)的时 间表。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-04 上午12.01.05.png" alt="屏幕快照 2018-04-04 上午12.01.05"></p>
<p>这只是我个人的时间安排，仅提供参考，希望对大家有所帮助。</p>
<h3 id="三、复习进度安排"><a href="#三、复习进度安排" class="headerlink" title="三、复习进度安排"></a>三、复习进度安排</h3><p>因为是二战，前期复习的比较快，后期主要是整理背记，查漏补缺。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-04 上午12.01.27.png" alt="屏幕快照 2018-04-04 上午12.01.27"><br>这些仅是个人的一点规划，虽然没有严格执行，但是基本上相差不大，学 弟学妹可以借鉴一下，每个人的学习能力都不尽相同，还是要结合自己的实际 情况，合理的安排计划。</p>
<h3 id="四、参考书目"><a href="#四、参考书目" class="headerlink" title="四、参考书目"></a>四、参考书目</h3><p>我用的书基本上跟学长的经验是一致的，详细的我就不介绍了，仅提供一 个书单，给大家参考一下。</p>
<ul>
<li>数学:</li>
</ul>
<p>《数学全书》 、《660 题》 《真题》、张宇《8 套卷》 、《1000 题》等;</p>
<ul>
<li>英语:</li>
</ul>
<p>张剑《英语一》全套，何凯文作文，《红宝书》等;</p>
<ul>
<li>专业课: </li>
</ul>
<p>《统计学》第六版、经管类《统计学》，《应用回归分析》，《多元统计分析》，《时间序列分析》等。注:今年专业课出现了证明题，大家可能要多看一本《概率与数理统计》， 关注一下证明题。</p>
<h3 id="五、感悟、感谢"><a href="#五、感悟、感谢" class="headerlink" title="五、感悟、感谢"></a>五、感悟、感谢</h3><p>考研的路上受到了很多人的关心和帮助，有家人的支持和关爱，有一个会 拿着糖葫芦坐十几站公交车来看“探视”我的闺蜜，还有学长的帮助，研友舍 友的陪伴，深深地情谊和感谢不是用言语可以表达的，只希望未来，大家都可 以心想事成，幸福安康!也希望我的经验可以为大家带来一点帮助，祝愿即将 考研的小伙伴们心想事成，金榜题名!</p>
<h2 id="第六篇：2018人大应用统计考研经验分享——by-wtt"><a href="#第六篇：2018人大应用统计考研经验分享——by-wtt" class="headerlink" title="第六篇：2018人大应用统计考研经验分享——by wtt"></a>第六篇：2018人大应用统计考研经验分享——by wtt</h2><p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.43.26.png" alt="屏幕快照 2018-04-11 上午12.43.26"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.43.38.png" alt="屏幕快照 2018-04-11 上午12.43.38"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.43.51.png" alt="屏幕快照 2018-04-11 上午12.43.51"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.44.03.png" alt="屏幕快照 2018-04-11 上午12.44.03"><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.44.16.png" alt="屏幕快照 2018-04-11 上午12.44.16"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.44.16.png" alt="屏幕快照 2018-04-11 上午12.44.16"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-04-11 上午12.44.28.png" alt="屏幕快照 2018-04-11 上午12.44.28"></p>
<h2 id="第七篇：2018-年人大应用统计考研经验贴-by-GGG"><a href="#第七篇：2018-年人大应用统计考研经验贴-by-GGG" class="headerlink" title="第七篇：2018 年人大应用统计考研经验贴 by GGG"></a>第七篇：2018 年人大应用统计考研经验贴 by GGG</h2><p>大家好!我本人是二战的考生，本科就读于中央财经大学应用统计专业，这 次初试成绩是 381 分，政治 67，英语 76，数学 125，统计学 113。关于时间安排 的问题，其实我也看过许多学长学姐的经验贴，写得很详细。不过我认为只要把 握好大体的复习阶段和节奏就好，具体的要看个人习惯，自己尝试几天，发现最 适合的时间段，分配好各科的复习，毕竟计划赶不上变化，要随机应变。这个一 会说每一科的时候再单独说。</p>
<p>在开始之前，其实我最想说的是复习的心态问题。我本人一战失败的原因有 两点:1.忙于上新东方的考研班，结果忽视了自己做题练习的重要性;2.没有搞 清楚主要矛盾，一些该暂时放下的不太重要的事情没有放下比如一些娱乐项 目….;3.拖延症，把计划想的很好，然后不执行。所以可以说，我一战失败是 必然的，查到初试成绩是 375 分，去年的分数线大家都知道是 390 分，几乎不用 等通知就知道肯定是没戏了。失落消沉是肯定有过的，这些都是很正常的情绪， 大家在今后的一段漫长的复习历程中可能也难免会有。一定不要否定自己当下的 某些负面情绪!不要压抑它，一切都是正常的流露，没有人能够做到一直斗志昂 扬从来不沮丧的。你想要开始好好复习，并不需要先调整好心态。事实上，什么 样的心态并不是最重要的，对复习效果也没什么影响，只要你在做、在学习、在 努力，你就在朝着目标前进，并且不知不觉中，你就会忘记沮丧。我妈和我说: 有担心的时间，还不如多看两页书多做两道题呢!一战失败后，我决定去换个环 境换个心态，就去银河证券实习了 2 个月，一边实习一边复习，也和其他的实习 生聊聊人生。当然一战失败的同学们可以选择别的调整方式，能走出来就好。</p>
<p>啰啰嗦嗦说了很多，就是希望各位学弟学妹在复习过程中感到有些孤独无助 的时候能知道，其实大家都是这么度过的。每天都要在心里告诉自己，再努力一 下，再坚持一会儿，你就能胜利了。</p>
<h3 id="一、数学三"><a href="#一、数学三" class="headerlink" title="一、数学三"></a>一、数学三</h3><p>我本人大一时学的是数分和高代，主要侧重定理的证明和推导，和考研数学 的侧重点是有所不同。所以，如果大学期间也是学的数分和高代的同学，不要觉 得高数和线代这本书相对简单就忽视它。数学的复习时间我喜欢安排在上午，英 语在午后。政治在之后的下午，晚上安安静静地看专业课。因为是二战，可能和 大部分同学的情况不一样，我个人的复习进度是:</p>
<p>3 月中旬到 6 月底，按章节，先看课本做例题(没做课后习题)，然后看《李 永乐考研数学复习全书》的相应章节，做例题，做错的题标注出来，认真研究。 注意，标号章节和题号，可以拿三个本，高数、线代、概率论各一个，方便后期 看错题。这本书编得不错的，我上过新东方的考研数学班，讲得也就是这本书上 的内容。</p>
<p>7 月到 8 月中旬，刷《数学基础过关 660 题》，我是按科目刷的，先高数、然 后线代、最后概率论。660 题，如果基础知识比较牢固(3 月到 6 月打好基础哦) 的话其实做起来也很快的，因为就是选择和填空，但是建议要留下简单的解题步 骤。我大概是每天刷 30 道题，后面线代和概率论部分可以每天多刷一点。<br>从 8 月中旬或者 9 月初开始，(1)首先，把之前的所有错题(大家不要忘记 标记哦)都再捋一遍，可以新找个本留下点痕迹。其实也不用抄错题，我的做法 是把每道错题都总结成一句或一小段简短的文字或公式，然后时时翻看。这个工 作可以坚持到最后一天，且各科都适用!</p>
<p>(2)然后，就可以开始每天早 8 点到 11 点的每日一套卷刷题时光啦~回想起这 段时光、真是忙碌又充实又很有成就感!注意自己要掐好时间，一定要习惯在有 紧张感的条件下做题。一开始会觉得时间紧张，但是慢慢就学会怎么安排时间了。 用 3 个小时做完一套卷子之后，大概还要用 1 个多小时的时间来改错和总结，大 家千万不要做完就完了!至于做什么卷子，先做近 15 年的真题，然后差不多《张 宇 8 套卷》就出了!(注意，张宇的卷子会比较难、刚一接触会有受挫感，但是 没关系都这样!大家其实可以去关注一波张宇的微博，还挺正能量的。至于为什 么要推荐张宇的卷子而不是李永乐的，大家也可以买来李永乐的卷子看看，我个 人认为含金量不高。)之后还有会一段时间才会出 4 套卷，这段时间好好要回顾 一下，也可以二刷真题之类的。做完张宇的卷子再做真题就会发现真题 so easy! 真题很重要，一定要好好研究。但同时多做模拟题也是很有好处的，谁做谁知道。 注意:在此之前一直是数学占用的时间比较多，到后期可能别的科目会有很多 要背的东西比如政治。但是!大家一定不要把数学放下!必须要保持做题的手 感的，张宇老师说过，数学这东西一个月不做题就基本废了。所以，真心建议 每天一套卷子，一直到考研的前一天!</p>
<h3 id="二、政治"><a href="#二、政治" class="headerlink" title="二、政治"></a>二、政治</h3><p>关于用书，感觉没什么好说的，就是肖秀荣的一系列全套。</p>
<p>4 月中旬到 7 月底，看《命题人知识点精讲精练》，看完一章，做章节后的仿 真模拟题。注意精讲精练上的题只有答案，有错的要自己回到书中去找到出处， 然后画出来。建议看书的顺序是:马原——史纲——毛中特——思修。马原比较 晦涩难懂，没关系也不用太懂，靠做题分清什么事重点!毛中特可以结合着史纲 来看，因为两者讲得基本上是同一历史时期的事件，只不过史纲偏重事件的历史 意义，毛中特偏重政策理论。思修比较简单，考试也只靠重点几章，这些肖秀荣 老师都会考诉你的~形势和当代部分可以先不看。</p>
<p>8 月到 10 月，刷《命题人 1000 题》。我这年的题我数了一下，选择 1514 道 (没错!命题人大约 1000 题)，分析题 43 道(分析题可以先不做)。选择题做起 来很快的，我自己是每天 80 道(马原、史纲、毛中特、思修，一共 4 部分嘛， 每部分做单选多选各 10 道)，20 天就刷完一遍了。然后，看错题，刷第二遍! 注意只做一遍是肯定不行的，按肖秀荣老师的说法，最好能刷 3 遍呢(虽然我只 刷了 2 遍，但注意我是二战的)。另外，近 10 年的真题的选择题部分(时政不用 管它)也应该做一下。注意选择的错题一定要好好看答案，搞清楚为什么!</p>
<p>10 月到 11 月，那本《命题人形式与政策以及当代世界经济与政治》的小册 子就出了。上编《形势与政策》包括 116 道选择题，大家看一遍然后把后边单选 题做了，时政部分就基本 OK 了。下编《当代世界经济与政治》的每个专题都有 一些分析题，这些题就可以适当地看看了，理解一下答题的逻辑，因为 10 月份 热点事件差不多都发生了，和后边的 8 套卷里的当代部分的题也都比较像了的。 之前做过的选择题的错题再好好看看!然后，看近 3 年的真题分析题，看看分析 题是怎么出的，答案应该怎么回答。<br>11 月到 12 月，自己做《命题人 8 套卷》的选择题，选择题的答案会编的比较用心，建议都看一遍。熟读分析题答案!(能背一背最好)</p>
<p>12 月份(从 4 套卷出了开始，记得提前预定!)到考试前，一定要把分析题的答 案背下来!选择部分的建议和 8 套卷一样。很多人觉得难背，其实…就是很难背!<br>我也是靠前头天晚上才差不多可以背下来。</p>
<p>总之，政治我考得也不算高，但是这么复习下来应该也不会考得太低。</p>
<h3 id="三、英语一-2"><a href="#三、英语一-2" class="headerlink" title="三、英语一"></a>三、英语一</h3><p>怎么说呢，我没有像很多人一样拿本书或者手机 app 刷单词，我看的单词都 是我在做阅读的时候自己总结的。我自认为这个方法挺好的，毕竟单词还是要在 文章中学习。用的书就是新东方的《考研英语历年真题详解及复习指南》，到后 期 10 月份以后我有刷过阅读的模拟题。</p>
<p>英语的复习我认为就分成两个阶段吧:</p>
<p>8 月底之前，做 1999-2010 的历年真题的阅读理解，每天一到两篇，也不用 严格计时，做完之后每道题都认真看答案，如果做错了，一定要搞清楚自己当时 是怎么错的。然后，仔细阅读文章，也不一定要非要翻译，但是一点要做到看懂 每一句话。这个其实好好看新东方那本书就行了，书上已经把单词、短语总结得 很好了。找个小本本，有不认识的单词一定要写下来，记好词性，至于单词对应 的中文，只写你在文中碰到的那个，不要一下写一堆中文意思，不好记的。记单 词的那个本要经常翻看!</p>
<p>9 月份开始，继续做 2011-2017 年的真题阅读理解，可以计时用 50 分钟做 完 4 篇阅读，对个答案，然后第二天第三天再精度文章，练习考试的感觉;也可 以一天做一到两篇，还和之前一样。总之阅读也是要保持感觉得，单词也要坚持 看，一直到考试前一天。其次，要开始练习写作了。建议先找一本比如说新东方 的写作书，背几篇范文，熟悉下考研英语写作的套路，然后再自己写。注意，自 己动手写作很重要!</p>
<h3 id="四、432-统计学"><a href="#四、432-统计学" class="headerlink" title="四、432 统计学"></a>四、432 统计学</h3><p>专业课的复习各路大神已经总结得尽善尽美了，我只是把我复习时参考的<br>ZH 学长的经验贴贴出来!就是如下的 2000 多字，总结得可以说非常详细了! 复习用书:</p>
<ul>
<li>贾俊平《统计学》第四版(经管类) </li>
<li>贾俊平《统计学》第六版(21 世纪统计学系列教材) </li>
<li>何晓群《多元统计分析》</li>
<li>王燕 《时间序列分析》</li>
<li>何晓群《应用回归分析》</li>
</ul>
<p>复习方案: </p>
<blockquote>
<p>贾俊平《统计学》:</p>
</blockquote>
<p>一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架 体系也比较清晰，基本上一个章节一天就 OK，看了两遍，然后整理了自己的笔 记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下， 做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是 万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六 版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析(这俩个属于多元统计分析) 都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复 试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的， 幸好看了第四版。</p>
<blockquote>
<p>何晓群《多元统计分析》</p>
</blockquote>
<p>吐槽一句，这本书写得像哲学，个人感觉是直接 从英文版翻译过来的，很多表述都没有做到像贾俊平那本书那么通俗易懂。我只 看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小 概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去 对应可以解决的问题，不需要去死抠推导过程，这不是 432 需要重视的，但是对 于理解还是有帮助的，有兴趣的可以推推看。16 年没考这部分内容， 但不能预计 17 年会不会考，要复习的全面一些。一些问答题都要结合历年真题 自己根据课本进行总结。</p>
<blockquote>
<p>王燕《时间序列分析》:</p>
</blockquote>
<p>这本书的编写就相对好得多，条理很清晰，思路引 导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道 08 年学硕考过的题:有趋势有季节变动可建立的模型，写出模型形式并简要说明。 可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候 也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点， 只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。</p>
<blockquote>
<p>何晓群《应用回归分析》:</p>
</blockquote>
<p>一直以为何晓群老师是个女老师，后来复试的时候 才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多 次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测 了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就 考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些 题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。</p>
<blockquote>
<p>关于真题:</p>
</blockquote>
<p>真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作 用。我当时的做法就是把真题整理成八个专题，分别是:《专题一:图表展示与 概括性度量》、《专题二:统计量与抽样分布》、《专题三:参数估计与假设检验》、 《专题四:分类数据分析》、《专题五:方差分析与实验设计》、《专题六:回归分 析》、《专题七:时间序列分析》、《专题八:多元统计分析》，学硕和专硕的历年 真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就 是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏 数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像 今年时序和回归的题都是曾经考过的，几乎一模一样。</p>
<blockquote>
<p>关于笔记:</p>
</blockquote>
<p>自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立 在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反 复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当 时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到 了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识 的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思， 但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知 识域相对来说也会完善一些。</p>
<blockquote>
<p>关于背诵</p>
</blockquote>
<p>心理学中有一个广为认可的记忆机制，即:我们在记忆的时候将 许多线索(诸如对一个原理的发散性理解、当时联想的事物的多样性)一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于 这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背 诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、 生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间 具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面， 搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考 研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出 入，但是关键词必须要锁定，大致意思要接近。</p>
<h2 id="第八篇：2018人大应用统计经验贴——by-XXX"><a href="#第八篇：2018人大应用统计经验贴——by-XXX" class="headerlink" title="第八篇：2018人大应用统计经验贴——by XXX"></a>第八篇：2018人大应用统计经验贴——by XXX</h2><p>看到今年还有去年的经验贴，我的复习策略跟他们大致相同。但也希望我的经验，能给学弟学妹们一点不一样的启示。个人基本情况：一战人大应统，总分370+ 本科北京某双非大学 数学专业。跟大多数考上的同学比，也算是个逆袭了。所以双非同学要有信心，只要持之以恒坚持下去，考上人大是可以的。<br>复习过程大致分为三个阶段。</p>
<h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p>大三下学期，每天大约学习考研知识5-6个小时（除了上课）。大三下学期一开始我就已经确定要考人大了。所以一切就全部针对人大应统而复习了。政治一点也没看，专业课看到人大出题相对简单，所以就把复习重点放到选修课 ： 回归分析 跟 多元统计分析，其它除了上课就没在复习专业课的知识了。英语这个学期就是把恋恋有词看了两三遍遍，感觉单词都差不多了解了，然后一个小时做了05年真题阅读错了三个，可以说这一学期英语学习取得很好效果吧。数学呢，一学期做完了36讲跟三分之二的复习全书（其实选择一个就可以了），做了17年真题大约125吧，虽然有的题之前做过，但也满意复习结果</p>
<h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><p>暑假期间（业内人士称为考研期间的遵义会议）先回家呆了一个星期看完了人民的名义。回来后就开启了全面复习计划（但从这个角度而言遵义会议好像不太合适了）。</p>
<p>每天学8-9小时。数学结束复习全书的内容，还完成了660题（这个跟1000题选一个就好了）。英语就是一个星期做一年真题，有听网上老师的讲解。政治一天学也就一个小时或者不到，看哲学跟政经部分。专业课在认真看课本，也买了几家学长的资料，总之，专业课下的资本比较大，舍不得孩子套不着狼，能考上就值了。</p>
<h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p>大四上学期，每天6-7；9-10月由于开学有课学校也来了好多人加之略显疲惫，颓废了。数学上了学校里面老师讲的关于考研的课，再复习了一下前面学过的内容。政治看完了近代史，毛概的一部分（因为要开19大好多就留到后面复习了）。英语还是按着暑假的节奏走。专业课，主要复习回归跟多元部分了。10-12.23，这个时间是考研的冲刺时间，每天大概学习9-10个小时吧。数学每天一套历年真题或者模拟题，3小时左右，市面上大部分模拟题做过了。 政治，学完基础知识后，刷了好多好多选择题各种模拟题，每天学2-3小时，最后大题就是往死里背肖八肖四。专业课，3小时左右，基础知识刷好几遍，然后研究历年真题，总结出题规律。英语，时间压缩到一个小时左右，结果证明这是 一个错误之举。最后按照考试时间模拟了几场考试，为考试找感觉。</p>
<p>考研成绩与努力程度成正比，加油！</p>
]]></content>
      
        <categories>
            
            <category> 人大应统部落 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 考研 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[计算机视觉系列（1）：从LeNet到DenseNet]]></title>
      <url>/2018/03/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E4%BB%8ELeNet%E5%88%B0DenseNet/</url>
      <content type="html"><![CDATA[<p>ImageNet Large Scale Visual Recognition Challenge，简称ILSVRC，它是图像分类领域的比赛，每年举办一次，下图为每年的得奖模型及其top-5错误率，这篇文章主要梳理一下这些模型的结构：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15309420500223.png" alt=""></p>
<a id="more"></a>
<h2 id="一、LeNet-5"><a href="#一、LeNet-5" class="headerlink" title="一、LeNet-5"></a>一、LeNet-5</h2><p>LeNet-5是LeCun等人于1998年在论文 <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a> 提出的一个用于手写字体识别的卷积神经网络模型，是卷积神经网络的开山之作。它的模型结构相对于现在流行的网络来说略显简单，但其组成部分卷积层、池化层、全连接层都是后来的CNN网络的一部分。但在当时由于计算能力的局限且与支持向量机分类能力旗鼓相当，没有引起大范围的深入研究和关注。先看一下它的结构示意图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-05-28 下午6.21.32.png" alt="屏幕快照 2018-05-28 下午6.21.32"></p>
<ul>
<li>INPUT：输入为32×32的的单通道图片像素数值矩阵；</li>
<li>C1：卷积层，共6个卷积核，卷积核大小均为5×5，卷积操作步长为1，得到6×28×28的feature maps；</li>
<li>S2：池化层，池化窗口大小为2×2，最大池化，得到6×14×14的maps；</li>
<li>C3：卷积层，卷积核大小均为5×5，卷积操作步长为1，关键的地方是S2是6层，而C3有16层，中间是怎么变换过去的呢？这里有一个Locally Connect的方法；</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-05-28 下午6.18.00.png" alt="屏幕快照 2018-05-28 下午6.18.00"><br>举几个例子，C3层的第0个map是有S2层的第0、1、2个map经过3个滤波器后相加得到，以此类推。</p>
<ul>
<li>S4：池化层，池化窗口大小为2×2，最大池化，得到16×5×5的maps；</li>
<li>C5：卷积层，卷积核大小均为5×5，卷积操作步长为1，得到120个1*1的maps；</li>
<li>F6：全连接层，84个神经元。</li>
<li>OUTPUT：输出层神经元核函数(作用函数)是高斯函数，对输入信息进行空间映射的变换，最后输出属于各个类别（一共是10个）的分布。</li>
</ul>
<p><a href="https://www.zhihu.com/question/44328472" target="_blank" rel="noopener">RBF神经网络和BP神经网络有什么区别？</a></p>
<h2 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h2><p>AlexNet是深度学习从沉寂走向繁盛的一个关节点，在2012年的ImageNet图像分类竞赛上，相比上一年的冠军，AlexNet的top-5错误率下降了十个百分点，远远超过当年的第二名。它的网络结构示意图如下所示：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-05-28 下午6.58.52.png" alt="屏幕快照 2018-05-28 下午6.58.52"></p>
<p>网络结构的简化版如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/1041527908463_.pic_hd.jpg" alt="1041527908463_.pic_hd"></p>
<p>AlexNet 之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：</p>
<ol>
<li><a href="https://plushunter.github.io/2017/05/12/深度学习系列（8）：激活函数/" target="_blank" rel="noopener">ReLU</a></li>
<li><a href="https://plushunter.github.io/2017/05/20/深度学习系列（11）：神经网络防止过拟合的方法/" target="_blank" rel="noopener">Dropout</a></li>
<li><a href="https://plushunter.github.io/2017/05/20/深度学习系列（11）：神经网络防止过拟合的方法/" target="_blank" rel="noopener">Data augmentation</a></li>
<li>局部响应归一化层（Local Response Normalization Layer）:存在于第一层卷积层和第二层卷积层的激活函数后面，引入这一层的主要目的，主要是为了防止过拟合，增加模型的泛化能力。对当前层的输出结果做平滑处理。这个策略贡献了1.2%的Top-5错误率。前后几层（对应位置的点）对中间这一层做一下平滑约束，计算方法是：<img src="http://omu7tit09.bkt.clouddn.com/15279112446518.jpg" alt=""></li>
<li>重叠池化（Overlapping Pooling）：Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。</li>
<li>大数据训练：百万级ImageNet图像数据</li>
<li>多GPU训练：每个GPU上并行地、分别地运行AlexNet的一部分（例如将4096个神经元的全连接层拆分为两个并行的2048个神经元的全连接层，第一个卷积层有96个feature map而在一块GPU上只有48个feature map等），两块GPU只在特定的层上有交互。</li>
<li>学习率衰减：</li>
</ol>
<h2 id="三、VGGNet"><a href="#三、VGGNet" class="headerlink" title="三、VGGNet"></a>三、VGGNet</h2><p>网络结构如下所示：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15279114581250.jpg" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/15279116151588.jpg" alt=""><br>VGGNet与AlexNet不同的地方主要在于卷积核大小、卷积步数以及网络深度。</p>
<p>VGGNet将小卷积核带入人们的视线，AlexNet中第一个卷积层使用的kernel大小为 $11\times11$  ，stride为4，C3和C5层中使用的都是 $5\times5$ 的卷积核；而出现在VGGNet中大多数的卷积核都是大小为$3\times3 $，stride为1的。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15279117977701.jpg" alt=""></p>
<p>直观上我们会觉得大的卷积核更好，因为它可以提取到更大区域内的信息，但是实际上，大卷积核可以用多个小卷积核进行代替。例如，一个 $5\times5$ 的卷积核就可以用两个串联的 $3\times3$ 卷积核来代替，一个 $7\times7$ 的卷积核就可以用三个串联的 $3\times3$ 卷积核来代替。这样的替代方式有两点好处：</p>
<ol>
<li>减少了参数个数：两个串联的小卷积核需要$3\times3\times2=18$ 个参数，一个 $5\times5 $的卷积核则有25个参数；三个串联的小卷积核需要$3\times3\times3=27$个参数，一个 $7\times7$的卷积核则有49个参数。大大减少了参数的数量。</li>
<li>引入了更多的非线性：多少个串联的小卷积核就对应着多少次激活(activation)的过程，而一个大的卷积核就只有一次激活的过程。引入了更多的非线性变换，也就意味着模型的表达能力会更强，可以去拟合更高维的分布。</li>
</ol>
<p>值得一提的是，VGGNet结构的C里面还用到了 $1\times1 $的卷积核。但是这里对这种卷积核的使用并不是像Inception里面拿来对通道进行整合，模拟升维和降维，这里并没有改变通道数，所以可以理解为是进一步的引入非线性。</p>
<p>VGGNet的出现让我们知道CNN的潜力无穷，并且越深的网络在分类问题上表现出来的性能越好，并不是越大的卷积核就越好，也不是越小的就越好，就VGGNet来看， $3\times3$ 卷积核是最合理的。</p>
<h2 id="四、GoogLeNet"><a href="#四、GoogLeNet" class="headerlink" title="四、GoogLeNet"></a>四、GoogLeNet</h2><p>GoogLeNet的核心思想是：将全连接，甚至卷积中的局部连接，全部替换为稀疏连接。这样既能保持网络结构的稀疏性，又能利用密集矩阵计算的高效性的方法。大量研究表明，可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，Inception应运而生。Inception结构如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15309398227239.jpg" alt=""></p>
<p>这里的稀疏性是指卷积运算过程中，kernel的尺寸远远小于图片的尺寸，一个feature map仅与少数的输入单元相连接；而密集矩阵计算是指在Inception模块中的四个分支可以看作是较为稀疏的部分，但是拼接之后又成为一个大的密集矩阵。这种基本模块使用了3种不同的卷积核，那么提取到的应该是3种不同尺度的特征，既有较为宏观的特征又有较为微观的特征，增加了特征的多样性。池化层则保留较为原始的输入信息。在模块的输出端将提取到的各种特征在channel维度上进行拼接，得到多尺度的特征。</p>
<p>GoogleNet将上述模块进行堆积，整个GoogleNet的网络结构如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15309400522807.jpg" alt=""></p>
<p>此外，还有升级版本的Inception，如下所示：</p>
<ul>
<li>Inception v2 &amp; v3</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15309402630277.jpg" alt=""></p>
<ul>
<li>Inception-v4 &amp;Inception-ResNet-v2</li>
</ul>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://omu7tit09.bkt.clouddn.com/15309403995213.jpg"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://omu7tit09.bkt.clouddn.com/15309403639180.jpg"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://omu7tit09.bkt.clouddn.com/15309403720288.jpg"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://omu7tit09.bkt.clouddn.com/15309403770874.jpg"></div></div></div></div>
<h2 id="五、ResNet"><a href="#五、ResNet" class="headerlink" title="五、ResNet"></a>五、ResNet</h2><p>ResNet，深度残差网络，在top-5上的错误率为3.6%，是2015年ILSVRC的冠军，其网络结构如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15309308732790.jpg" alt=""></p>
<p>残差网络的设计理念归纳起来就是：</p>
<ul>
<li>保持简洁</li>
<li>VGG-style</li>
<li>没有最深，只有更深，目前已突破1k层<br><img src="http://omu7tit09.bkt.clouddn.com/15309309330003.jpg" alt=""></li>
</ul>
<p>接下来讲讲网络中最关键的残差结构：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15309317415559.jpg" alt=""><br>如上图所示，这个网络的提出本质上还是要解决层次比较深的时候无法训练的问题。它借鉴了Highway Network的思想，在旁边专门开个通道使得输入可以直达输出，而优化的目标由原来的拟合输出H(x)变成输出和输入的差H(x)-x，其中H(X)是某一层原始的的期望映射输出，x是输入，箭头称作shortcut connection。</p>
<p>作者的解读是这样的：</p>
<ul>
<li>if identity were optimal,easy to set weights as 0;</li>
<li>if optimal mapping is closer to identity,easier to find small fluctuations</li>
</ul>
<p>如果identity是最优的，它就是我们想要的理想映射，那么我们可以直接将F(x)的参数全部置零；如果identity和H(x)很相近，那么我们就可以通过学习残差来修正这种差别。</p>
<p>但为什么深度残差网络没有因为网络的加深而带来梯度消失或者梯度爆炸的问题呢？下面这篇文章分析了其为何性能优越，而且提出了一种新的残差单元，并且比较了各种残差单元的性能优劣，如下图所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15309313107650.jpg" alt=""></p>
<p><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a></p>
<p>归纳地说就是因为残差结构的特殊性，在由loss对输入求导的时候，导数项会被分解为两个，而有一个直接对输入的导数项并不会消失，所以梯度一直存在。</p>
<h2 id="六、DenseNet"><a href="#六、DenseNet" class="headerlink" title="六、DenseNet"></a>六、DenseNet</h2><p>众所周知，最近几年卷积神经网络提高效果的方向，要么深（比如ResNet，解决了网络深时候的梯度消失问题）要么宽（比如GoogleNet的Inception），而作者则是从feature入手，通过对feature的极致利用达到更好的效果和更少的参数。归纳起来，DenseNet有以下几个优点：</p>
<ol>
<li>减轻了vanishing-gradient（梯度消失） </li>
<li>加强了feature的传递 </li>
<li>更有效地利用了feature </li>
<li>一定程度上较少了参数数量</li>
</ol>
<p>DenseNet保证网络中层与层之间最大程度的信息传输的前提下，直接将所有层连接起来。在传统的卷积神经网络中，如果你有L层，那么就会有L个连接，但是在DenseNet中，会有L(L+1)/2个连接。简单讲，就是每一层的输入来自前面所有层的输出。如下图：$x_0$是input，$H_1$的输入是$x_0$（input），$H_2$的输入是$x_0$和$x_1$（$x_1$是$H_1$的输出）<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-07-07 下午12.15.05.png" alt="屏幕快照 2018-07-07 下午12.15.05"></p>
<p>梯度消失问题在网络深度越深的时候越容易出现，原因就是输入信息和梯度信息在很多层之间传递导致的，而现在这种dense connection相当于每一层都直接连接input和loss，因此就可以减轻梯度消失现象，这样更深网络不是问题。另外作者还观察到这种dense connection有正则化的效果，因此对于过拟合有一定的抑制作用。</p>
<p>文章与ResNet进行了对比，如下：</p>
<ul>
<li>ResNet：$X_l= H(x_{l-1})+x_{l-1}$</li>
</ul>
<p>这里的l表示层，$x_l$表示l层的输出，$H_l$表示一个非线性变换。所以对于ResNet而言，l层的输出是$l-1$层的输出加上对$l-1$层输出的非线性变换。</p>
<ul>
<li>DenseNet：$x_l = H_l([x_0,x_1,···,x_{l-1}])$</li>
</ul>
<p>$[x_0,x_1,…,x_{l-1}]$表示将0到l-1层的输出feature map做concatenation。concatenation是做通道的合并，就像Inception那样。而前面ResNet是值的相加，通道数是不变的。</p>
<p>下图是整个网络结构，这个结构图中包含了3个Dense Block，其中加入了bottleneck layer和transition layer操作，每个dense block的3×3卷积前面都包含了一个1×1的卷积操作，就是所谓的bottleneck layer，目的是减少输入的feature map数量，既能降维减少计算量，又能融合各个通道的特征；另外作者为了进一步压缩参数，在每两个dense block之间又增加了1×1的卷积操作，表示增加了这个Translation layer，该层的1×1卷积的输出channel默认是输入channel到一半。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-07-07 下午12.17.12.png" alt="屏幕快照 2018-07-07 下午12.17.12"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>绝佳的介绍文章：<a href="https://zhuanlan.zhihu.com/p/31006686" target="_blank" rel="noopener">从LeNet-5到DenseNet</a></li>
<li>LeNet-5：<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a> </li>
<li>AlexNet：<a href="https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li>
<li>VGGNet：<a href="https://arxiv.org/abs/1409.1556v6" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></li>
<li>VGGNet：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.00567v3.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a></li>
<li>GoogleNet：<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a></li>
<li>ResNet：<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 计算机视觉 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> LeNet </tag>
            
            <tag> AlexNet </tag>
            
            <tag> VGGNet </tag>
            
            <tag> GoogLeNet </tag>
            
            <tag> ResNet </tag>
            
            <tag> DenseNet </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[日知录（5）：认知偏误列表]]></title>
      <url>/2018/03/02/%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%885%EF%BC%89%EF%BC%9A%E8%AE%A4%E7%9F%A5%E5%81%8F%E8%AF%AF%E5%88%97%E8%A1%A8/</url>
      <content type="html"><![CDATA[<h2 id="一、认知偏误简介"><a href="#一、认知偏误简介" class="headerlink" title="一、认知偏误简介"></a>一、认知偏误简介</h2><p>认知偏误（Cognitive biases）是一种倾向，它认为在某些方面，认知会与理性标准（Standard of rationality）或者良好的判断相偏离，它经常应用于心理学（Psychology）和行为经济学（Behavioral economics）的研究中。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/15145188999131.jpg" alt=""></p>
<p>认知偏误的存在虽然被可重复的研究（Replicable research）证实，但是目前仍然有许多关于如何对这些偏误进行分类或如何解释它们的争论。部分偏误受到大脑用于处理判断与决策的启发式（Heuristics）信息处理规则（如心理捷径Mental shortcuts）的影响，这些影响统称为认知偏误。偏误拥有各种形式的认知（“冷性质”），比如心理噪音（Mental noise），或者动机（“热性质”）的解释（Motivational explanations），比如当理念被一厢情愿（Wishful thinking）歪曲的时候，以上两种性质的影响都可能在同时出现。</p>
<p>也有一些争论探讨这里面的部分偏误是否是无用的、不合理的（Irrational）或者可能会引致有效益的态度或行为。例如，当试图去了解别人的时候，人们倾向于提出确认自己关于对方的假设性引导性的问题（Leading questions）。有人认为这种确认偏误（Confirmation bias）是一种社会技能（Social skill），为了用于与其他人建立联系。</p>
<p>对这些偏误的研究绝大多数是关于人的，然而，一些调查显示部分偏误也存在于非人类的动物身上。例如，双曲贴现（Hyperbolic discounting）的情况存在于老鼠、鸽子和猴子身上。</p>
<h2 id="二、决策、信念与行为的偏误"><a href="#二、决策、信念与行为的偏误" class="headerlink" title="二、决策、信念与行为的偏误"></a>二、决策、信念与行为的偏误</h2><div class="table-container">
<table>
<thead>
<tr>
<th>中文名称</th>
<th>英文名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>模糊效应</td>
<td>Ambiguity effect</td>
<td>也称为不明确效应，倾向于选择信息量更大的选项，避免选择缺少信息或者未知的选择项</td>
</tr>
<tr>
<td>锚定效应</td>
<td>Anchoring or focalism</td>
<td>人们在对某人某事做出判断时，易受到第一印象或第一信息的支配，思想就像沉入海底的锚 一样被固定在某个点</td>
</tr>
<tr>
<td>注意力偏误</td>
<td>Attentional bias</td>
<td>人们的知觉受到再现的想法的影响，如做决定时受制于环境或情绪，忽略一些相关信息而更关注某些特定信息</td>
</tr>
<tr>
<td>自动化偏误</td>
<td>Automation bias</td>
<td>过度依赖自动化系统，这可能导致错误的自动化信息取代正确的决定</td>
</tr>
<tr>
<td>叠加效应</td>
<td>Availability ca</td>
<td>也称效用层叠，某一观点不断重复确认后会加强，如集体观念通过其在公共话语中不断地重复获得越来越多的合理性，自我认知也将随之产生强化。换言之重复足够多那么它将变成“真的”，有点像三人成虎，以及“重要的事情说三遍”</td>
</tr>
<tr>
<td>可得性启发</td>
<td>Availability heuristic</td>
<td>在记忆中高估事件可能性的倾向，它受到记忆的远近程度、事件的罕见程度以及情绪化的影响，因而在作评价时更多依赖记忆中记住的信息而不是全部信息。在使用启发法进行判断时，人们往往会依赖最先想到的经验和信息，并认定这些容易知觉到或回想起的事件更常出现，以此作为判断的依据</td>
</tr>
<tr>
<td>逆火效应</td>
<td>Backfire effect</td>
<td>也称逆反效应，人们通过强化他们的信仰来为驳斥现象做出反应。如当一个错误的信息被更正后，如果更正的信息与其原本的看法相违背，它反而会加深人们对原本错误信息的信任</td>
</tr>
<tr>
<td>从众效应</td>
<td>Bandwagon effect</td>
<td>同羊群效应，人们从事或者相信某件事时依赖于其他相同情况的人做出的选择，相关的是群体思想（Groupthink）和从众行为（Herd behavior）</td>
</tr>
<tr>
<td>基率谬误/忽视</td>
<td>Base rate fallacy / neglect</td>
<td>忽视基本的比率信息（通用的、一般的），而倾向于在某些特定情况下才适用的特定的信息，可参考统计学中的贝叶斯统计（Bayesian Statistics）理论</td>
</tr>
<tr>
<td>信念偏误</td>
<td>Belief bias</td>
<td>或称信仰偏误，人们评价论据是否合乎逻辑的可信程度受到其结论可信度的影响，因为相信结论所以相信其推理过程</td>
</tr>
<tr>
<td>偏见盲点</td>
<td>Bias blind spot</td>
<td>认为自己的偏见比别人更少，或者说认为别人的偏见比自己多的倾向</td>
</tr>
<tr>
<td>拉拉队效应</td>
<td>Cheerleader effect</td>
<td>人们在团体里面比在孤立状态下表现得更有吸引力的倾向，就像在拉拉队中更倾向于表现自己</td>
</tr>
<tr>
<td>选择支持性偏误</td>
<td>Choice-supportive bias</td>
<td>认为自己做出的选择是更好的而事实上可能并非如此（忽略不利的地方），稍有自我安慰之意</td>
</tr>
<tr>
<td>聚集性错觉</td>
<td>Clustering illusion</td>
<td>也称丛集错觉或聚类错觉，人们倾向于将大量随机小样本中不可避免的“条纹”或“聚簇”状的随机分布考虑为某种具有统计学意义的“规律”（幻象模式Phantom patterns），此处可参考知乎问题“打麻将的时候，为什么会有连续一段时间运气特别好的现象？”下面的回答</td>
</tr>
<tr>
<td>舒适区效应</td>
<td>Comfort zone effect</td>
<td>对于过去常用的方案，高估其效益或成功机会；对于过去少用的方案，低估其效益或成功机会</td>
</tr>
<tr>
<td>确认偏误</td>
<td>Confirmation bias</td>
<td>也称证实偏误，人们偏向于关注、寻找、解释和记忆能够证实自身预想的信息，而忽视事实去支持自己的成见</td>
</tr>
<tr>
<td>一致性偏误</td>
<td>Congruence bias</td>
<td>也称相合性偏误，对假设的验证完全依赖于直接的测试，而不对其替代性假设的可能性做测试；或者说只相信直接证明假说的实验，而忽视间接的证明或是证明其他假说的可能</td>
</tr>
<tr>
<td>合取谬误</td>
<td>Conjunction fallacy</td>
<td>也称联合谬误，认为特殊情况比一般情况更可能发生；认为单个条件发生的概率要小于多个条件联合发生的概率。如对某人的描述很像女性主义者时，认为某人是替妇女辩护的律师的可能性比认为某人是律师的可能性更高</td>
</tr>
<tr>
<td>保守主义</td>
<td>Conservatism</td>
<td>在新的证据出现时仍然难以改变某人观念（Revise one’s belief）的现象，相对于贝叶斯信念修正（Bayesian Belief-revision）来说，人们看重先验分布（Prior distribution）而轻视新的样本证据（Sample evidence）</td>
</tr>
<tr>
<td>对比效应</td>
<td>Contrast effect</td>
<td>同一刺激因背景不同而产生的感觉差异的现象，或者说当与最近观察到的有所差异的对象相比较时，加深或降低感知刺激的现象</td>
</tr>
<tr>
<td>知识的诅咒</td>
<td>Curse of knowledge</td>
<td>见多识广的人很难从并不那么博识的人的角度看问题，或者说知情者很难从不知情者的角度看问题</td>
</tr>
<tr>
<td>数据迁就偏误</td>
<td>Data-snooping bias</td>
<td>也称数据探查偏误、数据拟合偏误，指在基于先前得到的实证经验后对历史数据进行分析后所引起的偏误，由于模型“迁就”历史数据（即与历史数据拟合得太好），反而导致“预测”结果十分糟糕（与未来有偏差）</td>
</tr>
<tr>
<td>诱饵效应</td>
<td>Decoy effect</td>
<td>人们对两个不相上下的选项进行选择时，因为第三个新选项（诱饵）的加入，会使某个旧选项显得更有吸引力。有时“诱饵”并不需要真的存在，即“幽灵诱饵”</td>
</tr>
<tr>
<td>辩护人谬误</td>
<td>Defendant’s fallacy</td>
<td>泛指多种根据不相关资讯认定被告“犯罪的概率”很小的情况。另参见检察官谬误（Prosecutor’s fallacy）</td>
</tr>
<tr>
<td>似曾相识效应</td>
<td>Déjà vu effect</td>
<td>对某些事物有强烈的熟悉感，似乎曾经接触过，且能预先想到接下来会发生什么事</td>
</tr>
<tr>
<td>面额效应</td>
<td>Denomination effect</td>
<td>相对于大面额的钞票来说，小面额的钞票会花费得更快</td>
</tr>
<tr>
<td>处置效应</td>
<td>Disposition Effect</td>
<td>倾向于出售价值增长的资产，而抵制出售价值下降的资产</td>
</tr>
<tr>
<td>区分偏误</td>
<td>Distinction bias</td>
<td>两个事物作为备选项一起考虑时比单独考虑时区别更大。另见越少越好效应（Less-is-better effect）</td>
</tr>
<tr>
<td>丹宁克鲁格效应</td>
<td>Dunning-Kruger effect</td>
<td>也称达克效应，指无知要比知识更容易产生自信。如能力欠缺的人在自己认识不足的基础上得出错误结论，但是无法正确认识到自身的不足，察觉到自己的错误行为</td>
</tr>
<tr>
<td>时长忽视</td>
<td>Duration neglect</td>
<td>也称过程时间忽视，在确定某事件的价值时忽视该事件的持续时间。心理观察发现，人们判断某件痛苦经历的不愉快程度很少依赖于时长因素，但是经历中痛苦的最高点（Peak）和痛苦的消减速度却是两个最主要因素，一般来说消减得越快，会觉得这段经历更加痛苦</td>
</tr>
<tr>
<td>同理心断层</td>
<td>Empathy gap</td>
<td>也称移情差异、移情隔阂，人们低估情感的影响或者力量的倾向，“子非鱼，焉知鱼之乐？”</td>
</tr>
<tr>
<td>禀赋效应</td>
<td>Endowment effect</td>
<td>也称敝帚自珍效应，当个人一旦拥有某项物品，那么他对该物品价值的评价要比未拥有之前大大增加，总认为自己的东西是最好的；放弃某件物品比希望拥有它更有难度</td>
</tr>
<tr>
<td>本质主义</td>
<td>Exaggerated expectation</td>
<td>期望超出估计和现实，世界不是那么戏剧化，理想很热血，现实很平淡。相反的是保守主义偏误（Conservatism bias）</td>
</tr>
<tr>
<td>排除偏误</td>
<td>Exclusion bias</td>
<td>也称剔除偏误，研究进行时由于排除某些看似不符预期的样本产生的偏误</td>
</tr>
<tr>
<td>试验偏误或预期偏误</td>
<td>Experimenter’s or expectation bias</td>
<td>实验者相信、证实并发布那些支持他们预期结果的数据，相反地，对相矛盾的结果持怀疑、抛弃的态度或者对它们进行降权。推荐书籍《推理的迷宫》</td>
</tr>
<tr>
<td>聚焦效应</td>
<td>Focusing effect</td>
<td>对事物的某一方面过分注重的倾向，这可能导致错误的预期</td>
</tr>
<tr>
<td>福勒效应/巴纳姆效应</td>
<td>Forer effect / Barnum effect</td>
<td>人们常常认为一种笼统的、一般性的人格描述十分准确地揭示了自己的特点，而这些描述往往十分模糊及普遍，以至于能够适用到很多人身上（放之四海而皆准）。这种效应可以提供关于普遍接受的一些观念和做法的解释，如占星（Astrology）、算命（Fortune telling）、笔迹学（Graphology）和某些类型的性格测试</td>
</tr>
<tr>
<td>正向偏置</td>
<td>Forward Bias</td>
<td>模型是基于历史数据建立的，因此仅对过去的数据进行了验证，很难预测未知</td>
</tr>
<tr>
<td>框架效应</td>
<td>Framing effect</td>
<td>一个问题两种在逻辑意义上相似的说法却导致了不同的决策判断。人际交往中指关键不在于说什么，而在于怎么说；经济学中指当消费者感觉某一价格带来的是“损失”而不是“收益”时，他们对价格就越敏感</td>
</tr>
<tr>
<td>频率错觉</td>
<td>Functional fixedness</td>
<td>也称功能固着，认知限制某人只能在传统的方式上使用某种物品或对象的倾向；当需要解决问题时，思维阻断了使用新方法的可能性</td>
</tr>
<tr>
<td>经费偏误</td>
<td>Funding bias</td>
<td>也称赞助偏误，选择研究方法或诠释研究结果时，倾向于迎合经费提供者或赞助商的立场</td>
</tr>
<tr>
<td>赌徒谬误</td>
<td>Gambler’s fallacy</td>
<td>也称蒙地卡罗谬误，一种负近因效应（Negative recency），认为某事物在未来的几率会因过去的事件发生改变，事实上并非如此，统计规律不适用于个体，有些已经发生的事不会影响将要发生的事。概率上认为随机序列中一个事件发生的机率与之前发生的事件有关，即其发生的机率会随着之前没有发生该事件的次数而上升。典型的概念为大数定律（Law of large numbers）。如在某事件的一面连续发生后，人们会认为另一面在下一次“风水轮流转”或“翻盘”的几率更大。另参见逆赌徒谬误（Inverse gambler’s fallacy）</td>
</tr>
<tr>
<td>难度效应</td>
<td>Hindsight bias</td>
<td>也称后见之明偏误、后视偏误，常说的“事后诸葛亮”，事情已经发生了却说自己早已预料到</td>
</tr>
<tr>
<td>敌对媒体效应</td>
<td>Hostile media effect</td>
<td>也称敌意媒介效应，可以理解为意识形态偏误，由于强烈的阵容观念的存在，总是认为立场与自己不同的媒体有偏见、不客观</td>
</tr>
<tr>
<td>热手谬误</td>
<td>Hot-hand fallacy</td>
<td>也称热手效应、热手现象（Hot hand phenomenon），一种正近因效应（Postive recency），认为某事多次发生则未来发生的概率会较大；认为经历过成功的人物在额外的尝试之下有更大的机会获得进一步成功的可能。如人们会以为随机的连赢来自杰出的表现</td>
</tr>
<tr>
<td>双曲贴现</td>
<td>Hyperbolic discounting</td>
<td>更关注眼前利益而不是长远利益，相对于未来的回报，更倾向于现在的回报，这将导致人们的选择基于相同的原因却会随着时间的推移而发生改变。也作现时偏误（Current moment bias / Present-bias），可以参考动态不一致（Dynamic inconsistency）理论</td>
</tr>
<tr>
<td>可识别受害者效应</td>
<td>Identifiable victim effect</td>
<td>相对于处于危难中的一大群人来说，人们更倾向于为单个的可辨识的人做出响应，这里还涉及到同理心（Empathy）的问题</td>
</tr>
<tr>
<td>宜家效应</td>
<td>IKEA effect</td>
<td>当人们亲自组装、构建自己的东西（如宜家IKEA家具）时，无论最终结果如何，都会对它评价甚高，而且投入越多的劳动或情感就越容易产生依恋感和自豪感而高估物品的价值</td>
</tr>
<tr>
<td>控制错觉</td>
<td>Illusion of control</td>
<td>高估对外部事件的影响程度，人们以为自己对事物的控制能力或控制权超过实际上所拥有的控制能力，个体对自己成功的可能性的估计远高于其客观可能性的一种不合理的期望</td>
</tr>
<tr>
<td>有效性错觉</td>
<td>Illusion of validity</td>
<td>认为通过进一步地获取信息，将产生更多相关的辅助预测的数据，虽然可能并非如此，因为不一定有用</td>
</tr>
<tr>
<td>错觉关联</td>
<td>Illusory correlation</td>
<td>或称相关性错觉，错误地认为两个毫不相关的事物之间存在联系</td>
</tr>
<tr>
<td>影响偏误</td>
<td>Impact bias</td>
<td>也称影响力偏误，高估事件对未来情绪状态的影响力，如认为中大奖后会永远开心却并非如此</td>
</tr>
<tr>
<td>信息偏误</td>
<td>Information bias</td>
<td>也称资讯偏误，指人们为了获取信息会搜寻那些并不一定起作用的信息的倾向。如人们为了更好地决策，会搜集尽可能多的信息，但是并不用得上</td>
</tr>
<tr>
<td>样本大小不敏感性</td>
<td>Insensitivity to sample size</td>
<td>小样本中的变化容易被忽视的现象，人们判断样本统计（Sample statistic）中的概率时不考虑样本大小；评估统计数据时，未考虑小样本比大样本更容易观察到极端结果</td>
</tr>
<tr>
<td>逆赌徒谬误</td>
<td>Inverse gambler’s fallacy</td>
<td>认为概率很小的事发生了，一定是做了很多次。另参见赌徒谬误（Gambler’s fallacy）</td>
</tr>
<tr>
<td>不理性增值</td>
<td>Irrational escalation</td>
<td>也称沉没成本谬误（Sunk cost fallacy），人们基于累积的前期投资来证明某增加投资的决策是正确的，尽管新证据表明该决策可能是错误的；增加投资时关注已有业绩而不关注当前更有价值的信息</td>
</tr>
<tr>
<td>妄下结论</td>
<td>Jumping to conclusions</td>
<td>根据少许的信息即做出判断与决策。如诛心、预言、贴标签等等</td>
</tr>
<tr>
<td>公平世界假定</td>
<td>Just-world hypothesis</td>
<td>或称公平世界原则，人们倾向于相信自己生活在一个公平的世界里，每个人都得到他应得</td>
</tr>
<tr>
<td>领先时间偏误</td>
<td>Lead-time bias</td>
<td>也称超前时间偏误，经常出现在疾病检测中，当比较新近的或实验的测试与之前的测试时，其结果没有变化的现象，因为新近的或实验的测试只是比之前更早确定疾病，因此给人以病症时间延长的错觉</td>
</tr>
<tr>
<td>时长偏误</td>
<td>Length-time bias</td>
<td>也称时距偏误，选择偏误的一种，由于统计结果失真造成数据结论错误。当随机选择的时间点或空间点间隔参与到时长间隔的分析中时将会造成此偏误，这一分析过程倾向于选择较长的间隔，从而影响数据的准确性</td>
</tr>
<tr>
<td>越少越好效应</td>
<td>Less-is-better effect</td>
<td>当判断的时候，不联合在一起，倾向于分离出较小的集合而不是较大的集合加以考虑</td>
</tr>
<tr>
<td>损失厌恶</td>
<td>Loss aversion</td>
<td>也称损失规避，倾向于避免损失而获得收益，因为放弃某事物的负效用大于拥有它的正效用，差距可能超出1倍。可参考沉没成本效应（Sunk cost effects）和禀赋效应（Endowment effect）</td>
</tr>
<tr>
<td>戏局谬误</td>
<td>Ludic fallacy</td>
<td>一种不相干的谬误，指错误地将游戏的机率模式套用到现实世界，以及过度使用统计与概率预测未来</td>
</tr>
<tr>
<td>麦纳马拉谬误</td>
<td>McNamara fallacy</td>
<td>一种非形式谬误，指描述过度使用数据评估事情的现象，过度相信数据、依赖数据评估事情，忽略难以量化的事</td>
</tr>
<tr>
<td>单纯曝光效应</td>
<td>Mere exposure effect</td>
<td>也称曝光/熟悉/多看/重复/暴露/接触效应等，因为仅仅熟悉或亲近某一人事物而表现出对TA过分喜爱的倾向。某人事物在TA首次出现时如果没有带来厌恶感，那么随着TA出现的次数越多，对其产生的好感度也越高</td>
</tr>
<tr>
<td>货币错觉</td>
<td>Money illusion</td>
<td>也称金钱错觉，注重于货币上的名义面额价值，而不是其购买力的实际价值</td>
</tr>
<tr>
<td>道德凭证效应</td>
<td>Moral credential effect</td>
<td>道德会导致无意识的偏误，良好而平等的历史记录或看法增加了后续出现不平等偏误的可能性，受影响的人可能意识不到先前思维中已经建立的道德凭证或者说对某个事物的看法。如某人由于得到了某些高道德的评价或认证，而认为自己做得够好，反而在其他面向做了相反的事</td>
</tr>
<tr>
<td>多重比较谬误</td>
<td>Multiple Comparisons Fallacy</td>
<td>广泛比较二个群体的各种特征，从中找出有明显差异的几个，宣称它就是造成二个群体不同的原因</td>
</tr>
<tr>
<td>消极偏误</td>
<td>Negativity bias</td>
<td>也称负面/消极偏误或负性认知偏误，是一种心理现象，相比于积极的记忆，人们更容易回想起那些不愉快的记忆；相比积极的事物，人们更加注重消极的</td>
</tr>
<tr>
<td>负性效应</td>
<td>Negativity effect</td>
<td>也称负向效果，当人们评估一个他们不喜欢者的行为的原因时，倾向于将积极行为归因于周边环境，而将消极行为归因于个人的本性。人们在印象形成过程中，消极信息的作用往往大于积极信息的作用，人们根据他人的消极品质形成的印象或评价很难改变，而且更愿意相信</td>
</tr>
<tr>
<td>可能性忽视</td>
<td>Neglect of probability</td>
<td>或称概率忽视，做决定时人们会忽略掉一些可能性，如决策时完全不理会不确定条件下的可能性或几率</td>
</tr>
<tr>
<td>反安慰剂效应</td>
<td>Nocebo effect</td>
<td>也称反伪药效应，给予有效的药物或治疗，病人却相信或觉得病情有所恶化。另参见安慰剂效应（Placebo effect）</td>
</tr>
<tr>
<td>正常化偏误</td>
<td>Normalcy bias</td>
<td>高估事物正常化发展的趋势，因而在发生变故时拒绝为之前从来没有发生过的灾祸、失败（disaster）做出计划、反应或者推卸责任；对突如其来的灾难人们更难接受，更想恢复原状</td>
</tr>
<tr>
<td>非本地发明</td>
<td>Not invented here</td>
<td>也称非我发明，一种心理现象，拒绝接触、使用外部开发的产品、研究、标准和知识。另参见宜家效应</td>
</tr>
<tr>
<td>选择性观察偏误</td>
<td>Observation selection bias</td>
<td>观察时不可避免受到前置条件的限制而筛选了样本，因而得出不适当的结论。例如问卷调查到的人是个热心、愿意填问卷的人，因而其结果未必能反映不热心、不愿意填问卷的人的想法</td>
</tr>
<tr>
<td>选择性观察偏误</td>
<td>Observation selection bias</td>
<td>观察时不可避免受到前置条件的限制而筛选了样本，因而得出不适当的结论。例如问卷调查到的人是个热心、愿意填问卷的人，因而其结果未必能反映不热心、不愿意填问卷的人的想法</td>
</tr>
<tr>
<td>观察者期望效应</td>
<td>Observer-expectancy effect</td>
<td>研究员在期待给定的结果时，会因此不自觉地操纵实验或者曲解数据，以便找到它。另参见受试者期望效应（Subject-expectancy effect）</td>
</tr>
<tr>
<td>忽略偏误</td>
<td>Omission bias</td>
<td>也称忽视偏误、不作为偏误，认为错误的行为比不作为更糟糕。相比将同样有害的行为忽略或者不作为来说，将有害的行为付诸行动被看成是使情况更加糟糕的或者不道德的，因为行动比不作为更加明显</td>
</tr>
<tr>
<td>遗漏变量偏误</td>
<td>Omitted-variable bias</td>
<td>遗漏了有关变量而产生的估计量的偏误</td>
</tr>
<tr>
<td>乐观偏误</td>
<td>Optimism bias</td>
<td>也称乐观主义倾向，态度过于乐观，高估了有利和令人愉快的结果，过分相信事情会向好的一方面发展。另参见一厢情愿（Wishful thinking），情价效应（Valence effect），正面结果偏误（Positive outcome bias）</td>
</tr>
<tr>
<td>鸵鸟效应</td>
<td>Ostrich effect</td>
<td>不理会明显的消极的情形或条件，是一种逃避现实的心理，也是一种不敢面对问题的懦弱行为，就像鸵鸟被逼得走投无路时，就把头钻进沙子里。相关成语掩耳盗铃</td>
</tr>
<tr>
<td>结果偏误</td>
<td>Outcome bias</td>
<td>也称结果效应/偏好，它是发生在决策评估中的一种偏误式判断，即当决策结果与决策质量不存在实质性联系时，评估者仍根据结果信息评估决策质量，是一种不考量决策当时的状况，而以结果论成败的倾向</td>
</tr>
<tr>
<td>自负效应</td>
<td>Overconfidence effect</td>
<td>在判断、解答、决策问题时，自己往往过度自信。如人们对某些问题，回答者“99%肯定”很有可能40%是错误的</td>
</tr>
<tr>
<td>过度诊断偏误</td>
<td>Overdiagnosis bias</td>
<td>也称过度诊断偏倚，指诊断出受试者（或患者）并不会造成症状或死亡的“疾病”，经常出现于早期疾病筛查中</td>
</tr>
<tr>
<td>空想性视错觉</td>
<td>Pareidolia</td>
<td>也称空想性错视、幻想性视错觉，将模糊随机的外界刺激（声音、图像等）看成是显著的并赋予实际意义，如像动物的云朵、月球上的人脸、听到倒播影音（Records played in reverse）中不存在的隐藏信息等</td>
</tr>
<tr>
<td>悲观主义偏误</td>
<td>Pessimism bias</td>
<td>人，尤其是经历过不幸或消极的人，认为不好的事情发生在自己身上的可能性更大</td>
</tr>
<tr>
<td>安慰剂效应</td>
<td>Placebo effect</td>
<td>也称伪药效应，给予无效的药物或治疗，病人却相信或觉得病情有所改善。另参见反安慰剂效应（Nocebo effect）</td>
</tr>
<tr>
<td>计划谬误</td>
<td>Planning fallacy</td>
<td>或称规划谬误，计划时低估任务完成时间的倾向。如人们在计划未来时，往往为未来安排过多的事物，设定过于理想化的目标，而实际实施时却很难完成计划，计划赶不上变化</td>
</tr>
<tr>
<td>正面结果偏误或情价效应</td>
<td>Positive outcome bias or Valence effect</td>
<td>认为积极正面的结果比负面的更容易发生</td>
</tr>
<tr>
<td>购后合理化</td>
<td>Post-purchase rationalization</td>
<td>或称买入后理性化论、“买家的斯德哥尔摩症候群”，人们在买入某东西之后为劝说自己而加入对购买行为合理化的解释，即使买下的产品太过昂贵或发现瑕疵，以此来化解认知失调</td>
</tr>
<tr>
<td>预筛选偏误</td>
<td>Pre-screening bias</td>
<td>也称预审偏误，筛选样本时预先排除了某些不应排除的样本</td>
</tr>
<tr>
<td>首因效应</td>
<td>Primacy effect</td>
<td>也称首位效应、起始效应、第一印象作用、先入为主效应，是一种开头刺激或信息的记忆过于引人注目的认知偏误。通过“第一印象”最先输入的信息对客体以后的认知会产生显著的影响，它是由第一印象所引起的一种心理倾向，许多人习惯称之为“第一感”。如人们更容易回想起序列中起始的项目而不是后面的项目</td>
</tr>
<tr>
<td>创新偏误</td>
<td>Pro-innovation bias</td>
<td>对于发明或者创新在社会上的作用持过分乐观倾向，同时往往忽视其局限性和弱点。如只看重新意而忽视其具体应用状况</td>
</tr>
<tr>
<td>检察官谬误</td>
<td>Prosecutor’s fallacy</td>
<td>泛指多种根据不相关资讯认定被告“无辜的概率”很小的情况。另参见辩护人谬误（Defendant’s fallacy）</td>
</tr>
<tr>
<td>假确定性效应</td>
<td>Pseudocertainty effect</td>
<td>当预期的结果是正的（收益）时候作风险规避（Risk-averse）的选择，当结果是负的（损失）时候作风险寻求（Risk-seeking）的选择，在选择项内容一致的情况下，其选择受描述结果的方式影响的趋向。另参见损失厌恶（Loss aversion）</td>
</tr>
<tr>
<td>对抗心理</td>
<td>Reactance</td>
<td>也称感应抵抗、抗拒心理，当他人想要限制你自由选择或做不想做之事的时候，产生站在对立面或者持反对意见的一种冲动。另参见逆反心理（Reverse psychology）</td>
</tr>
<tr>
<td>反冲性贬低</td>
<td>Reactive devaluation</td>
<td>也称反冲性贬抑，只是因为源自对手而认为其价值不大</td>
</tr>
<tr>
<td>近因偏误</td>
<td>Recency bias</td>
<td>也称近因效应、新颖效应，与首因效应相反，它更重视近期的数据或经验，忽视早期的数据或经验（参见“峰终定律”Peak-end rule）；在多种刺激一次出现的时候，印象的形成主要取决于后来出现的刺激</td>
</tr>
<tr>
<td>新近错觉</td>
<td>Recency illusion</td>
<td>或称新词错觉，认为某词语或语法是新近才出现的，事实上它们已经存在很久了。另参见眼球效应（Frequency illusion）</td>
</tr>
<tr>
<td>复原谬误</td>
<td>Regression fallacy</td>
<td>也称还原谬误，非常态的甲事发生以后，用乙措施处理后甲事扭转，便断定乙措施可扭转甲事。然而非常态的事发生后，本来就比较容易发生较接近常态的事</td>
</tr>
<tr>
<td>可能性贝叶斯回归</td>
<td>Regressive Bayesian likelihood</td>
<td>对条件概率的估计偏于保守而不是偏激的</td>
</tr>
<tr>
<td>回归偏误</td>
<td>Regressive bias</td>
<td>或称退缩偏误、逆行偏误，思维中将高价值高可能性的情况低估，反之高估的倾向</td>
</tr>
<tr>
<td>报告偏误</td>
<td>Reporting bias</td>
<td>也称报告选择偏误，指在反映情况时，选择性披露或隐藏信息的现象</td>
</tr>
<tr>
<td>自制偏误</td>
<td>Restraint bias</td>
<td>也称压制偏误，人们高估自己面对诱惑的抵抗力</td>
</tr>
<tr>
<td>押韵效应</td>
<td>Rhyme as reason effect</td>
<td>或称押韵有理效应，押韵的语句被认为是更加真实的。典型的案例出现在辛普森审判（O.J Simpson trial）一案中，辩词说道“If the gloves don’t fit, then you must acquit”（如果手套是不合手的，那么你就是无罪的）</td>
</tr>
<tr>
<td>风险补偿/佩兹曼效应</td>
<td>Risk compensation / Peltzman effect</td>
<td>在安全性增加的情况下，人们往往倾向于做出更加危险的举动。诸如很多安全产品的发明反而增加了相关活动的风险系数</td>
</tr>
<tr>
<td>选择偏误</td>
<td>Selection bias</td>
<td>一种系统误差，来源于研究对象的选择过程以及影响研究对象参与的因素，由于选入的研究对象与未选入的研究对象在某些特征上存在差异而引起的误差，常发生于研究的设计阶段</td>
</tr>
<tr>
<td>选择性感知</td>
<td>Selective perception</td>
<td>也称预期感知，事先的期望影响了事物的感知。如打针时事先觉得很疼就真觉得很疼</td>
</tr>
<tr>
<td>自我选择偏误</td>
<td>Self-selection bias</td>
<td>也称志愿者偏误（Volunteer bias），是指由于普查组与对照组人员自身条件差异造成偏误的情况，由于志愿者的自我选择而造成统计结果产生异常或偏误</td>
</tr>
<tr>
<td>塞默尔维斯反射</td>
<td>Semmelweis reflex</td>
<td>拒绝接受与已建立的规范、信仰或价值观相矛盾的新证据，仅凭证据并不能改变心理现状</td>
</tr>
<tr>
<td>社会比较偏误</td>
<td>Social comparison bias</td>
<td>在雇佣人才的时候，倾向于选择那些在某些方面不如自己的候选者</td>
</tr>
<tr>
<td>社会期望偏误</td>
<td>Social desirability bias</td>
<td>或称社会赞许性偏误，倾向于向外人展示自己社会期许的或者社会友好型的能力和行为，而隐藏社会不太看重的或者不利的方面</td>
</tr>
<tr>
<td>现状偏误</td>
<td>Status quo bias</td>
<td>期待事物保持不变，安于现状的倾向。另参见损失厌恶（Loss aversion），禀赋效应（Endowment effect），系统正当化（System justification）</td>
</tr>
<tr>
<td>刻板印象</td>
<td>Stereotyping</td>
<td>也称定型化效应，人们对某事物具体特点形成的一种固定的看法，并把这种观看法推而广之，认为这个事物或者整体都具有该特征，而忽视个体差异，有如将心中刻好的模板套到每个个体身上</td>
</tr>
<tr>
<td>次可加性效应</td>
<td>Subadditivity effect</td>
<td>或称分开加总效应，认为整体的可能性比部分的可能性（之和）更低</td>
</tr>
<tr>
<td>受试者期望效应</td>
<td>Subject-expectancy effect</td>
<td>由于受试者期待某种结果，因而下意识地扭曲了回报内容。另参见观察者期望效应（Observer-expectancy effect），案例安慰剂效应（Placebo effect）</td>
</tr>
<tr>
<td>主观验证</td>
<td>Subjective validation</td>
<td>也称主观确认，相信某事是对的，就感觉它是对的。也会把巧合的事当作有关联</td>
</tr>
<tr>
<td>幸存者偏误</td>
<td>Survivorship bias</td>
<td>也称存活者偏误、“死人不会说话”等。当取得资讯之渠道仅来自于幸存者时（因为无法从死者获得来源），此资讯可能会存在与实际情况不同的偏误。联系成语“兼听则明，偏信则暗”</td>
</tr>
<tr>
<td>德州神枪手谬误</td>
<td>Texas sharpshooter fallacy</td>
<td>即“先射箭再画靶”，原用以形容流行病学上的群集错觉，后衍生泛指统计研究做出结果后，把其中的群集独立出来当作有统计意义，然而实际上此集群更可能是随机产生</td>
</tr>
<tr>
<td>省时偏误</td>
<td>Time-saving bias</td>
<td>低估从低速加速（减速）行进时节省（损失）的时间；而高估从高速加速（减速）行进时节省（损失）的时间</td>
</tr>
<tr>
<td>单位偏误</td>
<td>Unit bias</td>
<td>认为计量单位反映合理程度，人们倾向于认为给定的一个单位、一瓶、一罐、一盘或更多其他精细度量的单位食物，就是适当的食用份量。这个偏误强烈影响到分装食物的消费，如果分装的食物看上去应该就是一顿或一人份，那么他们就会一直吃到见底</td>
</tr>
<tr>
<td>熟悉路线效应</td>
<td>Well travelled road effect</td>
<td>低估走熟悉路线的持续时间；高估走陌生路线的时间</td>
</tr>
<tr>
<td>只看整体效应</td>
<td>Whole only effect</td>
<td>选项为整套方案时，只关注整体，而忽略个别部分有协商的可能</td>
</tr>
<tr>
<td>零风险偏误</td>
<td>Zero-risk bias</td>
<td>偏向于将小的风险降为零，虽然说高风险降低的程度可能更大。如偏向将2%降到0%，而不是20%降到5%</td>
</tr>
<tr>
<td>零和直观推断</td>
<td>Zero-sum heuristic</td>
<td>也称零和捷思，直观地判断某情形是零和的（收益和损失之和为零）或者说总体平衡的，但事实未必如此。零和（Zero-sum）概念来自博弈论（Game theory）。这种偏误的产生可能与社会主导倾向（Social dominance orientation）的个人因素有关</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三、社会偏误"><a href="#三、社会偏误" class="headerlink" title="三、社会偏误"></a>三、社会偏误</h2><p>这里的大部分偏误被称为归因偏误（Attributional biases）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>中文名称</th>
<th>英文名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>观察者偏误</td>
<td>Actor-observer bias</td>
<td>也称行为者-观察者偏误，解释其他个体的行为时过分强调内在个性的影响，而轻视当时的外在情况（参见基本归因错误Fundamental attribution error），而在解释自己行为时则与此相反</td>
</tr>
<tr>
<td>防御性归因假定</td>
<td>Defensive attribution hypothesis</td>
<td>也称防御性归因，当后果变得越严重或者自己与受害者的情况越相似时，对加害者的责怪或谴责也就越多</td>
</tr>
<tr>
<td>自我中心偏误</td>
<td>Egocentric bias</td>
<td>相比于外部归结的责任来说，人们更倾向于将合作行动中的责任归结于自己；人们作为参与者要比作为观察者更能承担责任</td>
</tr>
<tr>
<td>外在激励偏误</td>
<td>Extrinsic incentives bias</td>
<td>一种基本归因错误，认为他人要靠外在动机如情境才能做好；而自己可以靠内在动机如个人特质做好</td>
</tr>
<tr>
<td>错误共识效应</td>
<td>False consensus effect</td>
<td>也称错误共识理论，指人们高估其他人对自己认同程度的倾向，认为别人跟自己的想法是一致的</td>
</tr>
<tr>
<td>基本归因错误</td>
<td>Fundamental attribution error</td>
<td>也叫基本归因谬误，人们在评判他人的行为时倾向于强调人格特质的影响，而低估环境在其中的角色以及给其带来的影响。另参见观察者偏误（Actor-observer bias），群体归因错误（Group attribution error），积极效应（Positivity effect），消极效应（Negativity effect）</td>
</tr>
<tr>
<td>群体归因错误</td>
<td>Group attribution error</td>
<td>认为群组成员的个体特征反映了群组整体的特征，认为群组的表现反映了群组成员个体的偏好，虽然事实所说明的可能是不同的情况</td>
</tr>
<tr>
<td>光晕效应</td>
<td>Halo effect</td>
<td>也称晕轮效应、光圈效应，人们对某件事物的总体映象会影响到人们对其具体特征或属性的看法。另参见外貌魅力偏见（Physical attractiveness stereotype）、刻板印象（Stereotyping）</td>
</tr>
<tr>
<td>非对称认知错觉</td>
<td>Illusion of asymmetric insight</td>
<td>或称非对称洞察力错觉，人们认为自己对别人的了解超过别人对自己的了解，认为自己比其他人懂得更多</td>
</tr>
<tr>
<td>外部媒介错觉</td>
<td>Illusion of external agency</td>
<td>人们通常低估自己对未来结果产生满意度的能力，当人们体会到这种自我生成的满意度时，他们会错误地认为这些是由外部的影响力（influence）、洞察力（insight）、善意（benevolence）等造成的</td>
</tr>
<tr>
<td>透明度错觉</td>
<td>Illusion of transparency</td>
<td>也称洞悉错觉，认为别人很容易看透（理解）自己，自己也很容易看透（理解）别人</td>
</tr>
<tr>
<td>优越感错觉</td>
<td>Illusory superiority</td>
<td>也称优越感偏误（Superiority bias）、乌比冈湖效应（Lake Wobegon effect）、优于常人效应（Better-than-average effect）等，相对于其他人来说，自己对个人的优秀品质评价过高，而对不良品质评价过低，即高估自己的优点，低估自己的缺点，认为自己更有水平</td>
</tr>
<tr>
<td>群内偏误</td>
<td>Ingroup bias</td>
<td>或称派系偏误，人们对认为是属于自己这一方的成员有优先或者较好待遇的倾向，即偏向某个人自己的群体</td>
</tr>
<tr>
<td>道德运气</td>
<td>Moral luck</td>
<td>基于事件结果的道德立场影响人们对事件的评判，而没有考虑到事件的意图、过程及情境</td>
</tr>
<tr>
<td>朴素犬儒主义</td>
<td>Naïve cynicism</td>
<td>认为别人的自我中心偏误或者利己行为比自己更多</td>
</tr>
<tr>
<td>朴素现实主义</td>
<td>Naïve realism</td>
<td>也称朴素实在论，相信我们的所见所闻就是真实的、客观的、不带偏见的，事实是显而易见的，理性的人一定会赞成我们，不赞成我们的人一定是无知的、懒惰的、无理的、有偏见的</td>
</tr>
<tr>
<td>外群体同质性偏误</td>
<td>Outgroup homogeneity bias</td>
<td>或称外群同质性偏误、组外一致性偏误，人们认为自己群体内的成员是比其他群体的成员更加多样化的，外部群体的成员之间彼此极其相似，比内群体成员更加“同质化”，不是自己这一方的都是一样的</td>
</tr>
<tr>
<td>投射偏误</td>
<td>Projection bias</td>
<td>也称投射效应，不自觉地以为他人（或未来的自己）和（现在的）自己有相似的情感、思想与价值观。以己度人，认为自己具有某种特性，他人也一定会有与自己相同的特性，是一种把自己的感情、意志、特性投射到他人身上并强加于人的认知障碍</td>
</tr>
<tr>
<td>自利偏误</td>
<td>Self-serving bias</td>
<td>或称自利性偏误、利己偏误，相比失败，更多地将成功归因于自己，在评价模棱两可的信息时，也倾向于描述得对自己有利。另参见利群偏误（Group-serving bias）</td>
</tr>
<tr>
<td>共有信息偏误</td>
<td>Shared information bias</td>
<td>群体成员倾向于花费更多地时间和精力讨论大家都熟悉的信息（Shared information），而对只有部分人了解的信息（Unshared information）则讨论得更少</td>
</tr>
<tr>
<td>系统正当化</td>
<td>System justification</td>
<td>也称体制合理化，倾向于保卫和巩固现状，现有的社会、经济、政治状况往往是首选，而其他方案往往遭到贬抑，即使牺牲个人或者集体利益。另参见现状偏误（Status quo bias）</td>
</tr>
<tr>
<td>性格归属偏误</td>
<td>Trait ascription bias</td>
<td>也称个性归属偏误，认为自己的个性、行为、情绪是多变的，而他人是一成不变且容易预测的</td>
</tr>
<tr>
<td>终极归因错误</td>
<td>Ultimate attribution error</td>
<td>也称最终归因错误，类似于基本归因错误（Fundamental attribution error），只是这里人们倾向于将问题归因于整个群体，而不是群内的个体</td>
</tr>
<tr>
<td>差于常人效应</td>
<td>Worse-than-average effect</td>
<td>在任务困难的时候认为自己比别人更差，与优于常人效应（Better-than-average effect）相反</td>
</tr>
</tbody>
</table>
</div>
<h2 id="四、记忆的错误与偏误"><a href="#四、记忆的错误与偏误" class="headerlink" title="四、记忆的错误与偏误"></a>四、记忆的错误与偏误</h2><p>心理学与认知科学（Cognitive science）中，记忆偏误（Memory bias）是一种认知偏误（Cognitive bias），它将强化或者削弱记忆，其中包括记忆回想（recall）起来的可能性以及记忆回想起来的所需时间，或者是改变记忆的内容。</p>
<p>以下是各种记忆偏误：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>中文名称</th>
<th>英文名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>怪异效应</td>
<td>Bizarreness effect</td>
<td>怪异的事物或情况比正常的更容易记住</td>
</tr>
<tr>
<td>改变偏误</td>
<td>Change bias</td>
<td>在参与到事物的变化中后，很难回忆起其之前的状况</td>
</tr>
<tr>
<td>童年期遗忘</td>
<td>Childhood amnesia</td>
<td>也称童年失忆症，人们很难回忆起四岁之前的情况</td>
</tr>
<tr>
<td>选择支持性偏误</td>
<td>Choice-supportive bias</td>
<td>也称支持选择偏误，在回顾自己做过的选择时，认为是明智的</td>
</tr>
<tr>
<td>保守主义或回归偏误</td>
<td>Conservatism or Regressive bias</td>
<td>记忆中将高价值高可能性的情况记成比实际低，而低价值低可能性的记成比实际高。基于证据的记忆情况是非极端化的，参见回归偏误（Regressive bias）</td>
</tr>
<tr>
<td>一致性偏误</td>
<td>Consistency bias</td>
<td>记忆中错误地将某人过去的态度和行为看成像是现在的态度和行为</td>
</tr>
<tr>
<td>情境效应</td>
<td>Context effect</td>
<td>也称语境效应，认知与记忆依赖于所处情境，回忆脱离情境的事件比处于情境内中的更难，某些情境的记忆在其他情境中也不容易回想，如在家里回忆工作内容，它所需的时间更长而准确性也更低</td>
</tr>
<tr>
<td>跨种族效应</td>
<td>Cross-race effect</td>
<td>相比辨认种族内的人来说，辨认其他种族成员的难度更高</td>
</tr>
<tr>
<td>潜隐记忆</td>
<td>Cryptomnesia</td>
<td>也称隐藏记忆，错误归因（Misattribution）的一种，当一个被遗忘的记忆无意识地出现时，会以为是自己新的原创的，是灵感的涌现。人们可能因此错误地回想或产生某个想法、观念，这在某方面可能会造成剽窃的争议</td>
</tr>
<tr>
<td>自我中心偏误</td>
<td>Egocentric bias</td>
<td>回忆中存在自利或自我美化的倾向，例如记得考试成绩比实际的好或者记得抓住的鱼比实际的大</td>
</tr>
<tr>
<td>衰退效应偏误</td>
<td>Fading affect bias</td>
<td>也称情感衰退偏误，负面情绪、不愉快的记忆比正面的积极的衰退得更快</td>
</tr>
<tr>
<td>虚假记忆/虚构症</td>
<td>False memory or Confabulation</td>
<td>也称错误记忆、伪记忆、虚谈症，记忆障碍和错误归因（Misattribution）的一种，记忆中产生空想、虚构，并非有意欺骗地捏造、歪曲、曲解关于某件事物的记忆，区别于撒谎，因为自己都不知道那些信息是虚假的</td>
</tr>
<tr>
<td>谷歌效应</td>
<td>Google effect</td>
<td>人们对于很容易用搜索引擎（如谷歌）就能从网络上搜集到的信息很容易淡忘</td>
</tr>
<tr>
<td>幽默效应</td>
<td>Humor effect</td>
<td>幽默的事物更加容易记住，可能的原因解释是幽默的事物会增加认知处理的时间以及激发情感</td>
</tr>
<tr>
<td>真相错觉效应</td>
<td>Illusion of truth effect</td>
<td>即使没有意识到听说过，人们也倾向认为听过的是真的，而不管该陈述实际上是否正确。换言之，人容易相信熟悉的话胜过陌生的</td>
</tr>
<tr>
<td>错觉关联</td>
<td>Illusory correlation</td>
<td>或称相关性错觉，错误地认为两个毫不相关的事物之间存在联系</td>
</tr>
<tr>
<td>延迟效应</td>
<td>Lag effec</td>
<td>也称滞后效应、间隔效应（Spacing effect），相对于短时间内多次接触的信息来说，人们容易学习记住那些长时间跨度但是接触少的信息。这一效应表明突击式的考前复习并不能带来很好的学习效果（质量），但是短时间内能够带来良好的记忆表现（Memory performance）</td>
</tr>
<tr>
<td>钝化与锐化</td>
<td>Leveling and Sharpening</td>
<td>可理解成强化与弱化，记忆随着时间的推移发生扭曲丢失细节的现象，特别重要的事情会得到锐化或者被选择性地想起，而其中的细节和某些方面的信息将被钝化或者遗忘。随着时间的推移以及回忆次数的增加，这两种趋势将进一步加强</td>
</tr>
<tr>
<td>处理深度效应</td>
<td>Levels-of-processing effect</td>
<td>深层次的分析产生更精细、持久、强大的记忆痕迹，且记忆中用不同方式“编码”的信息拥有不同的效力等级</td>
</tr>
<tr>
<td>列表长度效应</td>
<td>List-length effect</td>
<td>列表的长度越长，所能记住的项目的比例越少，但是随着列表长度的增加，所能记住的绝对数量也将增加</td>
</tr>
<tr>
<td>误导信息效应</td>
<td>Misinformation effect</td>
<td>也称错误信息效应，由于事后信息（Post-event information）的干扰，记忆变得愈加模糊或不准确</td>
</tr>
<tr>
<td>通道效应</td>
<td>Modality effect</td>
<td>也称模态效应，通过口语传达出来的信息，最后听到的东西会记忆得比较深刻，而书写的则相对更低</td>
</tr>
<tr>
<td>心境一致性记忆偏误</td>
<td>Mood-congruent memory bias</td>
<td>与当前心境相合的记忆更加容易回想</td>
</tr>
<tr>
<td>轮流发言效应</td>
<td>Next-in-line effect</td>
<td>当人们一个接着一个发言时，后面发言的人不太容易记住前一个人说过的话</td>
</tr>
<tr>
<td>部分项目提示效应</td>
<td>Part-list cueing effect</td>
<td>当记忆的内容是一整组的时候，提示其中的一项会使回忆起其他项目的难度加大</td>
</tr>
<tr>
<td>峰终定律</td>
<td>Peak-end rule</td>
<td>体验一项事物之后，所能记住的大约只有在高峰期与最终时的体验，而在过程中好与不好的比重和时间长短、总体的感觉，对记忆影响不大</td>
</tr>
<tr>
<td>创伤的持续性</td>
<td>Persistence of traumatic event</td>
<td>记忆中往往对创伤事件（Traumatic event）进行反复不断地回忆。另参见“创伤后压力症候群”</td>
</tr>
<tr>
<td>图画优异性效应</td>
<td>Picture superiority effect</td>
<td>也称图优效应，通过图片来传达的信息记忆效果要比通过文字传达的更好</td>
</tr>
<tr>
<td>积极效应</td>
<td>Positivity effect</td>
<td>或称正面效应，老年人的记忆偏向积极美好的倾向</td>
</tr>
<tr>
<td>首因效应、近因效应和序位效应</td>
<td>Primacy effect, Recency effect &amp; Serial position effect</td>
<td>在序列末尾的项目最容易想起，接着是起始的，而处于中间的项目最不容易想起</td>
</tr>
<tr>
<td>处理难度效应</td>
<td>Processing difficulty effect</td>
<td>处理难度更高的信息时，由于花费更多时间阅读与思考而更容易想起</td>
</tr>
<tr>
<td>怀旧凸显</td>
<td>Reminiscence bump</td>
<td>回忆人生事件时，青春期和成年早期的事比其他时期的事更容易想到</td>
</tr>
<tr>
<td>玫瑰色回忆</td>
<td>Rosy retrospection</td>
<td>也称美好的回忆，将过去事件的回忆美化得比实际更好</td>
</tr>
<tr>
<td>自我生成效应</td>
<td>Self-generation effect</td>
<td>人们记忆自己生成的信息更加清楚，而别人给出的信息则容易淡忘</td>
</tr>
<tr>
<td>自我关联效应</td>
<td>Self-relevance effect</td>
<td>与自己有关的记忆比与他人有关的记忆更加容易回想</td>
</tr>
<tr>
<td>来源混淆</td>
<td>Source confusion</td>
<td>记忆中把偶然发生的事件与其他信息相混淆，造成记忆歪曲</td>
</tr>
<tr>
<td>间隔效应</td>
<td>Spacing effect</td>
<td>长时间跨度重复暴露的信息比短时间跨度重复暴露的信息要更容易想起；比起无间隔的重复接触，有间隔的重复接触有较好的记忆与学习效果。另参见延迟效应（Lag effect）</td>
</tr>
<tr>
<td>焦点效应</td>
<td>Spotlight effect</td>
<td>也称聚光灯效应，高估别人关注自己表现和行为的错觉，认为自己是瞩目的焦点</td>
</tr>
<tr>
<td>陈规偏误</td>
<td>Stereotypical bias</td>
<td>记忆向着陈规或刻板映象扭曲，如种族和性别偏见，又如错误地认为某些名字是罪犯的</td>
</tr>
<tr>
<td>后缀效应</td>
<td>Suffix effect</td>
<td>也称附加效应，在不太需要回想起来的主题或条目后面加入合理的项目，会减少近因效应的影响，这是近因效应的一种应用</td>
</tr>
<tr>
<td>可暗示性</td>
<td>Suggestibility</td>
<td>错误归因（Misattribution）的一种，想法容易受到暗示者的提醒而发生扭曲</td>
</tr>
<tr>
<td>伸缩效应</td>
<td>Telescoping effect</td>
<td>记忆中将近期的事情往更早移，遥远的事情往更近移的倾向。因此近期的事情变得更加遥远，而遥远的事情变得更加临近</td>
</tr>
<tr>
<td>测试效应</td>
<td>Testing effect</td>
<td>或称测验效应，人们更容易记住自己反复书写过的信息，而不是反复阅读的信息</td>
</tr>
<tr>
<td>舌尖现象</td>
<td>Tip of the tongue phenomenon</td>
<td>能够想起一件事情的一部分或者相关的信息，但是难以回忆起全部。当多个相似的记忆同时出现时，会产生干扰或者阻塞，这将造成人们说话时欲言又止或者说到一半卡住</td>
</tr>
<tr>
<td>逐字效应</td>
<td>Verbatim effect</td>
<td>人们说过的话语中的大意比逐字的词语或完整的句子更容易记住，因为记忆记住的是表述而不是直接复制</td>
</tr>
<tr>
<td>雷斯多夫效应</td>
<td>Von Restorff effect</td>
<td>也称莱斯托夫效应、梵•雷斯托夫效应，突出的、醒目的、特别强调的事物更容易记住</td>
</tr>
<tr>
<td>蔡格尼克效应</td>
<td>Zeigarnik effect</td>
<td>也称蔡格尼克记忆效应、蔡格尼效应，未完成或者中断了的事物比完成了的事物更容易被记住</td>
</tr>
</tbody>
</table>
</div>
<h2 id="五、认知偏误的成因理论"><a href="#五、认知偏误的成因理论" class="headerlink" title="五、认知偏误的成因理论"></a>五、认知偏误的成因理论</h2><blockquote>
<p>有限理性（Bounded rationality）——理性与优化的限制1.</p>
</blockquote>
<ol>
<li>前景理论（Prospect theory）</li>
<li>心理账户（Mental accounting）</li>
<li>适应偏误（Adaptive bias）：基于有限的信息作出决策，并因错误的代价而有所偏误</li>
</ol>
<blockquote>
<p>属性替代（Attribute substitution）——无意识地用简单的判断替代了复杂的、困难的判断<br>归因理论（Attribution theory）</p>
</blockquote>
<ol>
<li>显著性（Salience）</li>
<li>朴素现实主义（Naïve realism）</li>
</ol>
<blockquote>
<p>认知失调（Cognitive dissonance），相关的是：</p>
</blockquote>
<ol>
<li>印象管理（Impression management）</li>
<li>自我感知理论（Self-perception theory），也称自我知觉理论</li>
</ol>
<blockquote>
<p>启发式判断与决策（Heuristics in judgment and decision making），包括：</p>
</blockquote>
<ol>
<li>可得性启发（Availability heuristic）：记忆中容易想起的东西偏向于生动、不寻常和充满感情</li>
<li>代表性启发（Representativeness heuristic）：基于相似性来判断可能性</li>
<li>情绪性启发（Affect heuristic）：通过情绪反应来作出决策，而不是风险效益的考量</li>
</ol>
<blockquote>
<p>关于情绪（Emotion）的其他理论：</p>
</blockquote>
<ol>
<li>情绪二因素理论（Two-factor theory of emotion），也称情绪二因论</li>
<li>躯体标记假说（Somatic markers hypothesis）</li>
</ol>
<blockquote>
<p>内省错觉（Introspection illusion）<br>统计（Statistics）的误解（Misinterpretations）与误用（Misuse），数字盲（Innumeracy）</p>
</blockquote>
<p>2012年一份《心理学公报》（Psychological Bulletin）指出，至少有8种看似无关的偏误是由同一种信息理论（Information-theoretic）生成机制产生，它认为在人类记忆中存储与提取（Retrieval）信息时存在的杂乱信息处理过程造成了偏误。</p>
<p>转载自silensea的<a href="http://silensea.org/2015/09/13/List-of-cognitive-biases-Wikipedia/" target="_blank" rel="noopener">个人博客</a></p>
]]></content>
      
        <categories>
            
            <category> 日知录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 行为经济学 </tag>
            
            <tag> 认知偏误 </tag>
            
            <tag> Cognitive biases </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（21）：维特根斯坦论语言的限度（7）—— 哲学的意义]]></title>
      <url>/2018/02/17/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8821%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%887%EF%BC%89%E2%80%94%E2%80%94%20%E5%93%B2%E5%AD%A6%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      <content type="html"><![CDATA[<p>只有把哲学当作天下公器的时候，我们的哲学才有生命力，所以我理解的哲学是可以处理一切问题的，它不仅仅可以处理我们所面对的问题，也可以处理我们所没有面对到的问题。所以一定要把哲学看作是一个可以被适用在不同的对象上的一种工作方法，它是我们思考问题的一种方式，会带动我们去分析问题，甚至带动我们去想办法解决问题</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈 女子（1）.jpg" alt="莫奈 女子（1）"><br>其实我研究维特根斯坦的目的并不是要去让大家都像维特根斯坦一样，我们研究一个哲学家的目的，第一个是要了解他，第二个要通过他去了解他所给我们提供的这样一种哲学的思想方法，我们能够从中学到什么。我们当然也可以批评质疑他的一些观点，但是一定要有根据，因为维特根斯坦所有的思想都有自己的理路，如果不懂他的理路去批评他，最后可能就是一个堂吉诃德式的风车之战，你根本不是真的在跟维特根斯坦作战，所以一定要了解他，这是最基本的。但是很少有人真的去了解维特根斯坦，或者说我是按照维特根斯坦的方式去做，很少有人这样，所以这是我为什么要通过各种途径来讲解维特根斯坦，让维特根斯坦让成为大家所知道的人物，并且让大家去阅读维特根斯坦，让维特根斯坦的思想方式成为我们所熟知的方式。我们不一定按照维特根斯坦的方式去思考问题，但是我们一定要了解维特根斯坦的思维方式，这是我们了解当代哲学一个重要的组成部分，这也是我的目的之一。</p>
<p>别人问我江老师你是做分析哲学吗？我说我不知道。也有人问我，如果你是做分析哲学，为什么不像分析哲学家那样对一些具体的问题做出一些解读，对一些概念做逻辑上的推理？我其实做了很多的概念分析，包括真理、意义这些概念我都发过文章。他们之所以问这个问题，是因为从我的文章当中，他们看出我的思想倾向，认为我可能不像搞分析哲学的，更像搞大陆哲学的。一个搞分析的人怎么会讲拓扑学，做大陆哲学才会讲到拓扑概念。现在英国有一些哲学家他们专门做欧洲大陆的哲学家，他们做海德格尔拓扑学，有的做伽德默尔拓扑学，他们做的风格跟我做的风格不一样，我是做Philosophical Topology，我做的是哲学拓扑学，哲学拓扑学跟我们通常讲的数学的、几何学的、集合论的，或者说代数拓扑有直接关系，我更多是科学意义上来谈拓扑。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  女子（2）.jpg" alt="莫奈  女子（2）"></p>
<p>我对于整个哲学史的把握要比现在纯粹做分析的人要有优势，因为这么多年从古希腊到当代，我一直在给学生讲课，所以我对历史非常熟悉，我自己也正在写西方哲学史。我对当代哲学，欧洲大陆哲学也是非常熟的，从胡塞尔一直到德里达、福柯，读他们作品，通过阅读更多地了解他们的工作性质，我可以不同意他们的观点，但是绝不能够不读他们的书。但是现在很多搞分析的不读这些书，这就是最大的麻烦，也是有失公允的一种做法，所以我们首先需要了解。我觉得这是我们做学问的一种方法，通过广泛的了解再深入地进入讨论问题的方式。这些所有的哲学家的观点其实都是我们的一种资源，它虽然并不能够成为我们思想的一个出发点，但是通过这种思想资源我们可以了解我们究竟应当如何去做哲学。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  女子（3）.jpg" alt="莫奈  女子（3）"></p>
<p>我是一个无立场的哲学家，我既不是一个维特根斯坦的哲学家，也不是一个分析哲学家，也不是欧洲大陆哲学家，更不是中国哲学家，我是一个没有国籍没有立场的哲学家，因为哲学我把它当作天下公器，并且只有把哲学当作天下公器的时候，我们的哲学才有生命力，所以我理解的哲学是可以处理一切问题的，它不仅仅可以处理我们所面对的问题，也可以处理我们所没有面对到的问题，因为哲学只有在这个高度上，我们才能够真正理解哲学的价值。如果把哲学只限于解决某些具体的问题上，哲学的生命力或者价值就非常有限的，所以一定要把哲学看作是一个可以被适用在不同的对象上这样一种工作的方法，所以哲学是一种工作方法，或者说它是我们思考问题的一种方式，而这种思考问题的方式，会带动我们去分析问题，甚至带动我们去想办法解决问题。所以哲学不仅仅是对问题的讨论，甚至不仅仅是提出问题，我个人理解哲学应当能够解决一些问题。并不是像维特根斯坦在这本书里面说他一劳永逸地解决了哲学问题，但是它应当能够解决一些我们所讨论的某一些问题，解决的意思并不是说我能够彻底把这个问题完全让它成为在现实中，实践当中可以加以考量的问题，而是说能够消除我们思想上的困难，有一点像是每一次我的课程结束的时候，学生都会找我提问题，每一次我给学生解读完了我的理解之后，学生满意的离开了，这个时候我就想，问题解决了。哲学就是干这个事情，你能够让他的思想上解决问题，这个时候哲学的价值就出来了。当然会不断有新的问题产生，解决完这个问题，又有新的问题出来，可以进一步来解决，所以哲学是一个不断解决问题的活动。所以维特根斯坦说的很好，哲学是一种活动。这一点我接受，哲学一定是一种活动，它不是一种理论体系，不是一种观念体系，它应当是一种我们共同来践行的思想活动，我想这是最重要，只有这样我们的哲学才能有发展的可能性，这也是我力图想推广的一种哲学的观念。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  女子（4）.jpg" alt="莫奈  女子（4）"></p>
<p>并且，哲学不是一个可以用理论的方式构造出来的体系，哲学它一定是用来考察我们使用语言，使用概念，甚至是我们使用某一些思想来解决问题的活动和能力，只有在这个活动当中才能够验证哲学的价值。如果没有这样的活动，哲学就变成理论家能说会道的一种本事，而不能够成为大家普遍接受的某一些真实观念，我觉得这样的哲学就变的没有意义了。我从来认为社会对哲学的拒绝，是哲学家的耻辱，而不是社会的耻辱。换句话说，哲学在社会中的消沉是哲学家的失职，而不是社会的冷漠。你不能够把这个罪责归咎于社会，你给别人提供了什么？你不提供一个创新性的思想，一个解决问题的方案，一个能够让我们深入思考某一些重要现实问题的解决方式，甚至连一个问题都没有提出来，凭什么让人家接受？所以这个时候哲学家们需要主动的要为社会做点贡献，这是问题所在。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（20）：维特根斯坦论语言的限度（6）—— 维特根斯坦与罗素]]></title>
      <url>/2018/02/16/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8820%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E4%B8%8E%E7%BD%97%E7%B4%A0/</url>
      <content type="html"><![CDATA[<p>维特根斯坦的思想不是逻辑的，是形而上学的。他通过对语言的了解是要说明,其实我们人类有很多东西是隐藏在语言背后没法说出来的，而这恰恰是一种推动我们用语言去表达的动力。但是对于罗素来说是很难理解的，因为罗素是一个经验主义者，他所有的观念都是建立在我们对经验的了解和认识之上，而维特根斯坦不是一个经验主义者，他是一个真正的理性主义者，他的观念具有很强的形而上学的根据，这是罗素很难理解的。</p>
<a id="more"></a>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  塞纳河（1）.jpg" alt="莫奈  塞纳河（1）"></p>
<p>他跟罗素的师生关系其实保持的时间并不长久，1912年他正式跟随罗素学习，到1918年以后基本就开始逐渐疏远了。他去农村教书基本上就跟学术界失去了联系，前后加起来有将近有十年的时间，那时候跟罗素也没有直接的往来。跟哥德尔的具体历史过程我不是太熟悉，不知道他到底跟哥德尔交往得有多深，不仅仅是哥德尔，在当时的时代里面有很多后来成为大家的学者们，包括经济学家凯恩斯，米塞斯他们都有很多的交往，还有图灵,本来就跟他有亲戚关系，所以他当时对这些人是有影响的，乃至于后来他们讨论说图灵从维特根斯坦那里得到什么东西.图灵肯定不会说他受了维特根斯坦的影响，但是从他们的时代来看他们两个是有交集的，我们就能够知道维特根斯坦在那个年代，他的思想的活跃程度是受益于当时他跟这些人有这样一些接触，也保证了他的思想总是站在前面，具有更广泛的影响力。</p>
<p>罗素对他的前期哲学是推崇有佳，这本书的导言是罗素写的，罗素写完以后维特根斯坦并不满意，我在我的《〈逻辑哲学论〉导读》里详细的分析了为什么维特根斯坦不满意罗素的导言，罗素是借用这个机会表达自己的观点，所以很多地方理解的根本不到位。到现在为止我们基本上可以判断，罗素真的没有读懂维特根斯坦。因为维特根斯坦的思想不是逻辑的，甚至不是语言的，是形而上学的。他通过对语言的了解，是要说明其实我们人类有很多东西是隐藏在语言背后的，这个隐藏在语言背后的东西是没法说出来，而这种没法说出来恰恰是一种推动力，推动我们用语言去表达，就像我们说话，其实有的时候不是我们想说话，而是我们不得不说话，当我们说我想说话的时候，其实不是你主观愿望想说话，而是有一个东西在推动你让你去说。所以海德格尔说，不是我说语言，是语言让我说。维特根斯坦实际上已经意识到这个问题了，但是对于罗素来说是很难理解的，因为罗素是一个真正的经验主义者，他所有观念的建立都是基于我们对经验的了解，所有这些都是根据我们对当下经验活动的认识，而维特根斯坦不是一个经验主义者，他是一个真正的理性主义者，或者叫逻辑主义者，所以他的这种观念具有很强的形而上学的根据，这是罗素很难理解的。罗素对他的后期哲学的评价很低，虽然前期他还能够推崇他，但是到后期哲学他就完全不理解维特根斯坦在干什么，所以罗素在他的晚年1949年所写的《我的哲学的发展》里评价维特根斯坦后期哲学的时候说，维特根斯坦后期哲学只能作为我们茶余饭后的谈资，而不能够当作深刻的思想。显然他就根本没有读懂，虽然那个时候维特根斯坦的书还没有出来，但是其实他很多的讲稿已经在外面传播开来了，所以很多人对维特根斯坦的了解不是《哲学研究》出版以后才了解的，而是之前很多人就已经知道了，只是到了他去世以后，《哲学研究》那本书才被正式的编辑出版。<br>  <img src="http://omu7tit09.bkt.clouddn.com/莫奈  塞纳河（2）.jpg" alt="莫奈  塞纳河（2）"><br>不仅仅是罗素，G. E. Moore这样的哲学家也完全不能理解维特根斯坦，G. E. Moore和他差了好几个档次。他从来没有觉得罗素厉害，他去找罗素也是不得已而为之，是因为弗雷格推荐，他觉得弗雷格比罗素厉害得多，弗雷格的思想实际上在很大程度上给他提供了方向，维特根斯坦所做的工作其实是沿着弗雷格的工作往前推进的，跟罗素一点关系都没有，罗素只是帮助他完成了推进，所以罗素在他的心目当中并没有那么重要，为什么一直不把罗素放在眼里就是这个原因，他仅仅把他当作一个我思考哲学问题的助手，可想而知他有多么傲气，以及他的心智水平有多么高，这不是一般人能够达到的。罗素让他读哲学史，先看历史上的那些哲学家他们是怎么来讨论问题的，维特根斯坦拒绝，罗素震惊了，说那你回去写一篇东西，维特根斯坦就利用圣诞节期间写了几篇文章，罗素看了就说，这是天才。虽然罗素自己不一定能够达到这么高的心智水平，但他还是能够判断出来维特根斯坦的确是一个心智很高的人，这一点超出了常人。所以他跟维特根斯坦的姐姐写信说，你的弟弟将会成为我们中间最棒的一个人，他将会成为伟大的哲学家。这就是罗素当年给他下的一个判断。</p>
<p>罗素在1918年的时候他在发表他的代表作的时候，在伦敦做了一个系列的演讲，叫“逻辑原子主义哲学”演讲，开场白就说，我以下要讲的思想不是我的，是来自于我的学生和我的朋友维特根斯坦，但是我现在不知道他的死活。因为1918年第一次世界大战结束了，维特根斯坦是被作为俘虏被收编到意大利的俘虏营。后来维特根斯坦从俘虏营里寄了个明信片告诉罗素，他写了一本书叫《逻辑哲学论》，希望他们有时间能够见面聊一聊。1918年12月份他们俩商定在海牙见面，两个人在海牙整整谈了一个星期，讨论他这本《逻辑哲学论》，最后罗素答应给这本书写一个序言，这就是整个的维特根斯坦跟他的关系。到了1919年的时候，维特根斯坦就去了奥地利南部当小学教师了，维特根斯坦跟罗素之间他们相处的时间并不是很长，但是维特根斯坦仍旧表现出他自己的那种特殊性质，这种特殊的品质就是他自己的心智之高已经完全不把罗素放在自己的考量之内。他出于尊重在书里会提到罗素，这本书提到两个哲学家，弗雷格和罗素，但是他对罗素的评价远不如对弗雷格评价高，但是罗素毕竟是他的老师。1930年维特根斯坦重返剑桥以后，罗素要给他一个博士学位，但因为剑桥大学对于所有任教的老师是要求有学位的，而维特根斯坦走的时候连硕士都没有毕业，1912年进校1914年就已经去参军了，没有毕业就参军了。<br>    <img src="http://omu7tit09.bkt.clouddn.com/莫奈 塞纳河（3）.jpg" alt="莫奈 塞纳河（3）"><br>1929年重返剑桥，到了1930年初的时候，维特根斯坦参加了剑桥大学组织的博士答辩，G. E. Moore向他提议用《逻辑哲学论》当博士论文，当时这本书已经是风靡全球成为经典著作了，维特根斯坦就答应了。但其实他这个时候已经放弃了这本书里的很多观点，不再把这本书当作他主要的思想，但是为了拿到博士学位，所以就以这个作为博士论文申请去答辩，答辩的时候只有两个答辩教授，一个外请一个是自己的，一个是G. E. Moore一个是罗素，两个人简单的对个话，就算是答辩了。很快他就在学校里面任教了，后来为了拿到更高的教席，必须要有东西，所以他当时就很快就完成了《哲学评论》跟《哲学语法》这两本书，这两本书都是作为正式的出版物已经做好编辑了，但是没有付印，只是作为提交申报教职的材料，因为虽然有学位但是没有东西不行。所以他就写了这两本书，大概1930年到1932年完成的，在这个过程当中，他的思想就开始发生剧烈的变化。从1929年到1936年，大概就这五六年时间里面，他的思想就已经开始成型了，从1936年开始写《哲学研究》，一直到1945年完成，前后十年的时间完成了《哲学研究》的工作，但到他去世之前仍然是没有被出版的，本来出版社已经接受了他的出版合同，并且跟他签了约，结果他最后要出版之前又撤回来，认为好多想法还不太成熟，还希望能够再补充或者修订，结果一直到他去世这本书也没有正式出版。他去世以后，安斯康作为他的遗嘱执行人最后给他编辑完成了这本书。</p>
<p>维特根斯坦对自己的要求非常严格，不可能随随便便就可以把一本书交到出版社去出版，他去世之后整理他所有的笔记本，大概有几十本，全部整理出来应该有上千页的内容。但是后来实在量太大就不再整理了，全部做成微缩胶片，现在这个手稿保存在挪威，但微缩胶片在全国各个图书馆都有，有一些部分被编辑、翻译。因为他的原书大部分都是用德文写的，翻译成英文出版。如果我们要更多的了解维特根斯坦的话，需要把维特根斯坦所有的东西看过，才能够知道他在晚年的时候到底还在思考什么问题。<br>   <img src="http://omu7tit09.bkt.clouddn.com/莫奈  塞纳河（4）.jpg" alt="莫奈  塞纳河（4）"></p>
<p>在2017年年初我们翻译出版了维特根斯坦跟他另外一个学生的谈话纪要，这个谈话纪要是从1932年一直持续到1946年，在过去没有出版过，后来在2015年的时候被英国的一个著名的杂志《Mind》出版，后来我们就花了一年时间翻译，一年时间校对，今年年初就全部出版出来了，通过这个对话里面就可以看出，他在这一段历史发展过程当中所考虑的问题涉及的面是相当广的，这些问题他的学生给他记录下来，后来这些东西被整理成文字。我们可以看出维特根斯坦直到去世之前都一直在思考哲学，甚至是说他认为自己还有精力（energy）,还有能力去做哲学的工作，只是因为最后身体越来越差。所以，维特根斯坦本人其实是把自己整个生命献给哲学。一个人的经历虽然是很有限的，但是如果一个人他什么事都不管，他只做哲学，你想想这个人会做出多大成就出来。但这个人得有能力去做这件事情，上天给了维特根斯坦这样的能力，只有维特根斯坦这样的人才能够真正做到这一点，这是很奇妙的一件事情，一个人只干一件事情，一辈子只做一件事情，而且把它做的这么好，觉得这个事情唯有他能做，别人真的做不了这个事情，让我们几代人可能都觉得从中受益，我觉得这种哲学家真是了不起。</p>
<p>我给学生放维特根斯坦照片的时候，大家都说这简直是男神，他的眼睛特别深邃，一看就是思想家，而且不是在看着现在，是望着未来，他的思想也是看着人类未来可能发生什么，这是我们常人很难达到跟做到的，这也是维特根斯坦过人之处。所以维特根斯坦英文的传记――《天才之为责任：维特根斯坦传》写的真好，揭示了他是个天才的哲学家，虽然我们讲哲学家是可以后天慢慢形成，但是像维特根斯坦这样的哲学家，大概真是很少，难得一见，空前绝后。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  塞纳河（5）.jpg" alt="莫奈  塞纳河（5）"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（19）：维特根斯坦论语言的限度（5）—— 尘世生活]]></title>
      <url>/2018/02/15/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8819%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%20%E5%B0%98%E4%B8%96%E7%94%9F%E6%B4%BB/</url>
      <content type="html"><![CDATA[<p>维特根斯坦的生活是非常简单的，没有特别高的要求，但他对于思想和哲学上的要求却非常高，他是一个完美的理想主义者。他追求的目标是，所有的东西一定要做到最好，这是一般人做不到的。</p>
<a id="more"></a>
<p>维特根斯坦的生活经历是比较复杂的，甚至可以用坎坷这两个字，所谓坎坷说他的生活过程实际上是颠沛流离的状态。他居无定所，自从1945年他辞去了剑桥大学的教授职位以后，他就没有自己固定的住所，经常借住在他的学生家里面，或者到挪威，在山的边上盖了一个小房子，所以他基本上没有一个自己固定的生活住所，在剑桥大学的时候住自己的宿舍，宿舍陈设也非常简单、简陋。他是一个不追求物质也不追求生活情趣的人，虽然他对艺术，文学，以及宗教都有很高的鉴赏跟认识水平，但是尘世的生活并不是一个让他特别眷恋的东西，所以他也没有表现出窘迫，他仅仅表达一种个人的情绪，他在给朋友的信当中描写他当时某一种状况下心情很糟糕写不出东西，或者他觉得自己的思想已经枯竭了已经不能够做哲学了，他经常表示这样的担心。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  圣拉扎尔火车站（1）.jpg" alt="莫奈  圣拉扎尔火车站（1）"></p>
<p>他经常会表现出高傲的姿态，对很多人来说他是一个难以接近的人，他经常跟很多人搞不好关系，性格也比较孤僻敏感，虽然跟普通人还是比较容易融洽的，但他对跟他有思想关系的那些人反而表现出自己的独特，因为他老是怀疑他人在剽窃他的思想，尽管这些人其实在宣传他的思想，不断地在解释他的思想，但是他老觉得这些人不怀好意。</p>
<p>他最好的朋友罗素是他的老师，最后跟他分道扬镳，最好的朋友石里克跟卡尔纳普，当年在维也纳的时候，请大师一样的把他请去讲《逻辑哲学论》，但是最后也分道扬镳了。唯一的能够跟他在一起生活，还能够被接受的就是他的学生安斯康，她后来也是他的几本遗著的编者，因为安斯康从来不跟他计较，也从来不过分宣传他的哲学，所以他觉得安斯康比较靠谱。最后去世的时候也是在安斯康的家里，安斯康跟他的丈夫一块儿照顾他，并且请了自己的家庭医生来照顾维特根斯坦。 </p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  圣拉扎尔火车站（2）.jpg" alt="莫奈  圣拉扎尔火车站（2）"></p>
<p>维特根斯坦最后去世的时候那一句话让人匪夷所思，他说，请告诉他们，我度过了一个美好的人生。当他这句话出来以后大家很奇怪说，他还美好？为什么会美好？虽然他的哲学影响很大，但他个人生活是非常落魄的，但是这个事情对于他来说真的不重要。他在早年的时候把自己继承的遗产分给了自己的家人，并且还资助了比较落魄的诗人――青年诗人里尔克，到了晚年的时候，他基本上靠一点点退休金生活，因为他到1945年算是退出教职了，但是能够从剑桥拿到一些生活费，有时候朋友也接济他。所以他自己的生活是非常的简单，也没有特别高的追求，但他对于思想上的要求，对于哲学上的要求却是非常高的，他是一个完美的理想主义者，普通人真的达不到这一点。他给他的姐姐设计过房子，自己也做过一些小小的雕塑，设计过一些小小的工艺品，这些东西都是做到极致，精巧到极致，这是他追求的目标，所有的东西一定要做到最好，这是一般人做不到的。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  圣拉扎尔火车站（3）.jpg" alt="莫奈  圣拉扎尔火车站（3）"></p>
<p>他的这种思想观念，主要还是来自于早年家庭的教育和熏陶，因为他出生于一个很高贵的家庭，家庭成员具有很高的艺术修养和人文关怀，他的父亲很有钱，每天来的基本上都是有很高的思想跟学术的地位的社会名流，在这样的环境中被培养出来，他会要求自己也应当像那样。他在当小学教师的时候跟学生家长经常闹别扭，因为都是农村的家庭，家长基本上都是当地农民，他认为农民一般都比较纯朴简单，所以跟农民打交道没有问题，结果没想到当他去教孩子的时候并不是这样。他对孩子有极高的要求，如果完不成任务会打手板，家长不干了，认为你完全没有必要去惩罚，孩子做不好你好好跟孩子讲，维特根斯坦认为不惩罚他记不住，因为小学生刚开始上一二年级，你跟他讲道理他不会记住的，只有通过身体的惩罚他才能够记住。那些家长就把他告到法院，经过法庭开庭经过审理，最后双方和解。但是从此他也认识到了当地人并不是他想象的那么淳朴，那么可爱，所以他跟当地人一直搞不好关系，包括他周围的人。在很大程度上是他的那种清高和完美主义导致的结果，他一直认为别人做的没有那么好，所以他一直就觉得自己跟他们有距离，这样一种心态导致他在周围很难有一个真心的，可以长久交往的一个朋友，这完全归咎于他的这种完满的理想主义，这就是他的生活。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  圣拉扎尔火车站（4）.jpg" alt="莫奈  圣拉扎尔火车站（4）"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（18）：维特根斯坦论语言的限度（4）—— 语法与联系]]></title>
      <url>/2018/02/14/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8818%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E6%B3%95%E4%B8%8E%E8%81%94%E7%B3%BB/</url>
      <content type="html"><![CDATA[<p>所谓“语法”，就是看到联系。联系与结构息息相关，我们在结构当中才能为某一个具体的事物定位。我们看到任何一个游戏的时候，绝对不会想到这只是一个游戏，一定会想到这个游戏它相关的内容，因而，由这个相关的内容来理解游戏，而不是根据这个游戏来把握相关内容，不是由点到点，而是由面到点，这个面就是相互关系的面。</p>
<a id="more"></a>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  枫丹白露（1）.jpg" alt="莫奈  枫丹白露（1）"></p>
<p>维特根斯坦还有一个重要的概念――语法（grammer），并且他给出了关于语法（grammer）的说明。他在中期阶段曾经留下了一些笔记，去世以后他的学生们给他编撰了一本书，这本书被取名为《哲学语法》（Philosophical grammar），除了《哲学语法》以外，他还有一本书叫《哲学评论》，这两本书通常被看作是他中期思想的两本重要的代表作。</p>
<p>《哲学语法》这本书里面主要给我们提供的是如何以看见或者观察到我们所有语词，或者是我们的命题之间相互关系的这样一种方式来确定我们所玩的语言游戏，或者我们所使用任何一个概念它所具有的意义和内容。有一个很著名的例子――鸭兔图，如果我们往右面我们就会看到是一个兔子，如果我们往左面看是一只鸭子，两种动物同时体现一副画上。鸭兔图实际上反映了两个不同的视角，语法（grammer)就是当我们看到一个事物的时候，我们其实不仅仅是在了解这个事物的本身，也是在了解这个事物跟其他事物之间的相互联系，因为我们只有通过相互联系，才能够对所谈到的事物本身得以确切的了解，所以我们是在关系当中去认识一个对象的。同样，对于语言游戏也是这样，我们是在各种不同的语言游戏之间的相互联系中来了解某一个语言游戏是如何完成自己的工作的，这就是我们前面讲到的整体论的观念。维特根斯坦有一个词，叫可纵观性（surveyability），是说我们可以通过鸟瞰的方式把握对象，但是我们通常把握的方式不是点到点的方式，而是线到点的方式，通过联系的方式来看到每一个点所处的位置。所以通过联系来看待事物，这是维特根斯坦通过语法的概念来解释他对于语言游戏的理解。当然这个语法不是我们讲的自然语言，他仅仅是在讲这样的一种事物与事物之间相互联系的概念，这是维特根斯坦最重要的一个观点。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  枫丹白露（2）.jpg" alt="莫奈  枫丹白露（2）"></p>
<p>语法的概念就是看到联系，联系其实就是结构的概念，我们在结构当中来为某一个具体的事物定位。你看到任何一个游戏的时候，你绝对不会想到这只是一个游戏，一定会想到这个游戏它相关的内容，因而，由这个相关的内容来理解游戏，而不是根据这个游戏来把握相关内容，不是由点到点，而是由面到点，这个面就是相互关系的面。</p>
<p>现代拓扑学重要的观点是，当我们谈论一个点的时候，我们往往是通过这个点所触及的面或者触及的联系来认识它。当我们能够确定网络当中任何一个点，我们都不是通过这个点来认识面的，而是相反。这对我们考察人的神经元的构造是非常有帮助的，通过显微镜看我们神经元的变化，你会发现每一个神经元的突起点实际上是不固定的，它是不断的变化的。因为它随着我们思维活动，随着我们的大脑的活动它在发生变化，随着这种变化我们产生了一个节点，这个结点可能就是思维的节点，然后产生了概念。但我们说概念、观念，这是用语言来表达出来的，但大脑当中不是一个概念，它就是一个节点，就是我们讲的那种神经与神经之间交叉的地方，而这个交叉的形成是来自于这些不同的联系，这个思维方式跟原有的思维方式是不一样的，原有的思维方式是按照传统的形而上学提倡的实体观至上的观念，实体就是所有的事物的存在都是根据实体的性质来加以解释的，而实体本身是孤立的，就像莱布尼茨的单子一样，单子与单子之间是没有联系的，然后我们根据每一个单子来理解世界的变化。</p>
<p>过去认为，事物的存在是世界存在的最基础的内容。但是现在认为事物的存在是以这个事物相关的各种联系所造成的结果，那个点只是造成的一个结果，它本身并不是原因，所以我们要找原因的时候，找的是网络本身给我们的结果所造成的影响，所以考察变动的关系是考察这些网络与网络之间是如何导致这个结果的。现代拓扑学讲的点面关系，点的存在是取决于面，或者取决于关系，而关系本身它是可以变化的，所以这个点是不确定的，但是点一旦被确定，就说明关系被建立起来了，所以我们又是通过这个点来了解这个关系是如何被建立的，维特根斯坦说语言关系就是那个点，所以我们通过这个点，去了解什么东西构成了这个点，然后我们就通过这个点来知道了它相关的所有这些关系，看到的这些相互的关系，如果你看不到这些关系你就没有办法理解这些点。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  枫丹白露（3）.jpg" alt="莫奈  枫丹白露（3）"></p>
<p>在人类社会中其实也是这样，人类学考察的都不是孤立的对象，你看到一个原始人类出现的时候，你绝对不会想到这只是一个个体，你一定想到这个个体它背后有一个群体存在。我们的思维一定是往外扩散的，不是往内收缩的，因为只有通过这种扩张式的方法，我们才能够发现更多的点，然后我们才能够确立这个联系是如何被建立起来，这时候我们发现我们关心的不是这些点，我们关心的是网络本身，这是现代网络最重要的一个思路，如何通过网络的建立来寻找点或者建立点，而不是通过研究点来寻找网络。</p>
<p>有时候我能够想象，虽然我们不一定能够看见空间真实的状况，但可以想象到空间的状况，就像我们在一些现代的美术作品当中，他们所构想的一个你看不见的世界。我想，这个看不见的世界其实是存在的，不是因为它看不见而存在，而是因为它能够很好的通过另外一种方式向我们展示它的存在。就像地球的两极以及宇宙中的射线，这些东西是看不见的，但是我们能够通过仪器找到它们，能够发现它们，而这个仪器是间接的，但是能够想象的出来这种空间中所大量分布的这样一种结构性的安排，这要回到经典力学当中去讨论作用力跟反作用力的关系。一个物体的存在本身不是单向作用的结果，一定是双向作用，一个事物就像一个人能够在地球存在，不仅仅是我们有重量，重量其实是引力的结果，任何物体都是这样。就像我们知道一个星球它能够按照某种轨道运行，只是我们不知道为什么会这样运行，我们可以看到它运行的轨迹，其实它背后有很多力量在作用，所以导致我们现在新的物理学，天体物理学研究的时候会发现新的天体存在。发现新的天体好像是说过去它不在这里，我们重新又发现它了，不是这样，其实它一直在那里，只是我们过去检测的手段并没有那么先进。门捷列夫发现元素周期表，他觉得这个地方应该有一个元素，只是我们现在不知道这个元素是什么，因为如果没有这个元素你构成不了上下前后的元素关系，所以它中间一定有一个东西，这个方法其实就是拓扑学的方式，通过这样一种不同的联系来确定一个对象所存在的位置，这就是拓扑学的基本观念。然后通过这种方式来寻找这样对象的存在，它的根据是什么，这个根据恰恰是一种不变量所决定的。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  枫丹白露（4）.jpg" alt="莫奈  枫丹白露（4）"></p>
<p>维特根斯坦之所以用《逻辑哲学论》这种方式来表达他的思想，其实里面就包含很严密拓扑学论证。他在柏林读书期间借住在德国柏林的一个专门研究拓扑学的教授家，那是十九世纪末二十世纪初，拓扑学在今天来说还是个新的学科，上个世纪六十年代才正式确立拓扑学学科，在这之前基本上属于前历史阶段，那个教授就是早期从事几何拓扑研究的一个专家，他从中得到了很多启发，所以乃至他后来写文章，写所有的笔记都是按照这种方式来编的，不是论证的方式，因为论证的方式实际上是一个线性的方式，而是一个立体而且多元的方式。所以，他的这本《逻辑哲学论》里面你可以看到整个的篇章布局都是完全按照这样的方式构建出来的，而这个构建方式在他那个时代是少见的，我估计今天也少有这样的写作方法，这对于我们了解拓扑学是如何被贯彻到他的哲学描述当中，以及理解我们所生活的世界的各种结构关系的过程都是非常有意义的。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（17）：维特根斯坦论语言的限度（3）—— 语言背后的基础]]></title>
      <url>/2018/02/13/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8817%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E8%83%8C%E5%90%8E%E7%9A%84%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>维特根斯坦哲学的特殊性就在于，它不是以显形的方式让人们去接受的，维特根斯坦是一个隐士，他是隐身在我们的思想背后，然后去触发人们思考很多新的问题，而这些问题实际上在维特根斯坦都已经讨论过了，只是他并没有给我们提供一个现成的结论，而是让我们不断的去思考他提出的这些问题</p>
<a id="more"></a>
<p>维特根斯坦讨论语言的界限或者语言的边界的目的是什么？我们这样处理维特根斯坦前后期的关系，并且用语言的界限这样的说法来理解维特根斯坦的时候，我们到底在做什么？我想，当维特根斯坦讨论语言问题的时候，是把这个语言看作一个连贯的整体，虽然这样的语言前期对他来说是逻辑语言，后期是自然语言，虽然看上去各不相同，但它们都是在关心我们思想的表达方式，在这一点上两者是一致的，都是讨论思想的表达方式，只是前期用逻辑的方式给出了思想的定义，后期用显示的方式通过语言游戏来表达思想的内容，所以这两者都是对思想本身的关注，因而维特根斯坦表面上看是讨论语言，他实际上关心的是思想本身，这是我们第一个想要强调的一点。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈 勒阿弗尔（1）.jpg" alt="莫奈 勒阿弗尔（1）"></p>
<p>第二，我们更想强调一点是，维特根斯坦一直试图要追问：当我们谈论语言的时候，甚至在谈论思想的时候，我们背后的根据是什么？所谓背后的根据就是，我们能不能找到一个构成我们谈论语言和思想的基础的东西？他在前期哲学里讨论逻辑概念的时候是有的。因为所有的逻辑语言，其实都建立在我们对逻辑的一种自觉的先天的意识之上，这种先天的自觉意识是说，我们相信逻辑可以完成这个工作，因而逻辑就是它的基础。</p>
<p>到了后期哲学的时候，这个基础在哪里？这是维特根斯坦一直追问的问题。甚至不仅仅是维特根斯坦，是整个二十世纪后半叶的西方哲学家们一直追问的问题：当人类失去了一种能够追问的基础的时候，当人类失去我们共同的家园的时候，漂浮在大海上的我们，还能够做什么？</p>
<p>奥托・纽拉特经常说，人类其实就是在海上一艘永远不能够靠岸的船只，人类永远是生活在这样的船只之上，要修补这个船只我们只能在大海上修补，不能等到靠岸再修补，我们人类的认识活动和知识就像是这样一艘船只，所以当这个船只出了故障的时候，我们只能够通过在大海上的漂泊来修复或者替换它。知识论里面有可替换性原则，就是当我们不能够找到一个根据的时候，我们只能利用另外的东西想办法使得我们要补充修订的这一部分内容可以得到替换，但是要替换它的东西本身也是需要修补的，我们只能够取长补短。所以我们的认识活动是通过这种方式来相互协调而共同发展的，可是我们没有那个基础存在了，我们都找不到那样一个最终的可靠的根据了。</p>
<p>因为自从现代科学，特别是现代的物理学产生以后，人类知识的大厦就已经受到了严重的挑战。我们几乎很难相信还有一个所谓的最确定的东西，把它当作我们一切知识的根源。在人类的认识活动，包括我们对语言的讨论当中，我们找不到这样的东西。过去我们可以把上帝当作产生语言最终根据，甚至可以把人类的理性当作我们思想的根据，但是这在今天已经被大部分人放弃了，上帝的观念已经被放弃了，而对于理性绝对的盲目崇拜也被放弃了，这导致一个结果――我们失去了家园。现在我们老讲乡愁，我们人类好像似乎总是在往回走，总是要回顾我们过去所经历的那些事情，然后把经历的历史当作我们现在的起点。但事实上经历是回忆性，甚至是幻想性的，因为这些事物其实在我们的心目当中永远只是作为一个记忆中的存在。而如果人类的认识活动建立在这样一种幻想的、回忆的，甚至是虚构的背景之上，那人类的知识就面临很大的挑战。<br><img src="http://omu7tit09.bkt.clouddn.com/莫奈  勒阿弗尔（2）.jpg" alt="莫奈  勒阿弗尔（2）"><br>其实维特根斯坦整个的思考方式就是要追问有没有这个根据？能不能够找到这个根据，让我们人类的知识哪怕靠不了岸也可以就地抛锚？我们可以把我们的锚栽在河床之上，让这个船只能够稳定，而不是永远在漂浮当中。维特根斯坦试图找到这样一个河床，他把这个河床叫做思想的河床，这是维特根斯坦要做的一个重要工作。维特根斯坦在追问这样一个思想河床的时候，实际上是在为我们人类知识寻找一个根据，他试图想要找到这个东西并且他认为找到了。在《论确定性》当中，他认为这样的东西就叫做生活形式和世界图式。我自己将世界图式看成是类似于世界观的一种思想内容，跟我们通常理解斯宾格勒所说的世界观不太一样，我们通常在德国的语境当中讲世界观的时候，是指预先有一个对世界的理解，但是维特根斯坦理解的世界观是讲我们在已经具备对世界了解之后所形成的对世界的理解方式，而不是在理解世界之前，我们就预先有了某一个世界的概念，这个世界图式不是静态的，是动态的。因为在这个背景当中，有一些东西不断地会被替换掉，然后不断又有新的东西被增加进来，所以它是流动的。按照维特根斯坦的理解，这种变化的世界图景，总有一些东西是不变的，正因为这些不变的内容，使得我们把它看成是一副完整的图像，如果每一个东西都在变化，我们就没法把握这个图像。所以，在维特根斯坦心目当中，整个世界就处于这样的变动当中，而我们要把握就是那个变动当中的，某一些能够让我们把它看成完整图像的不变的内容，这是维特根斯坦要强调的一个重要的观念。</p>
<p>所以对他来说，他虽然反对以往传统本质主义的观念，认为人类可以有一个工作，可以把它当作一些知识的基础，但是他又试图要为我们能够去寻找这样的基础，这种心理的倾向找到一个说明，证明人类是有这个倾向的，只是找不到而已。就像我们每一个人都希望长寿，但是我们都知道这是不可能的，我们活多长时间不是由我们决定的。这个时候我们会发现，人们的这种主观愿望会直接影响到对事物的判断。在维特根斯坦看来这个倾向是实实在在的，因为实际上它确定了我们当下的这种存在方式，而当下的这种存在的方式恰恰是以这种以变化当中不变的因素来加以保障的，这是维特根斯坦要强调的一个重要的观念。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  勒阿弗尔（3）.jpg" alt="莫奈  勒阿弗尔（3）"></p>
<p>我们理解维特根斯坦的时候，会把他看作是一个比较极端的，激进的哲学家，看作是一个完全跟传统背道而驰的哲学家，他自己在《文化与价值》这本书中也多次表达过类似的观点，他反对现代科技，反对现代文化，反对整个现代主流的社会思潮，而我们认为维特根斯坦简直就是像嬉皮士一样是社会的叛逆者。其实仔细读一下他的书发现倒也未必，他思想里面有很多值得我们去进一步去追问的内容。这样就使得维特根斯坦整个的思想就变成大家思考所有当代问题的一个出发点，他不一定给出了答案，但是他的思考方式却引起大家去思索。在当代哲学当中有一个很有意思的现象，几乎每一个人言必称维特根斯坦，但是没有人承认自己是维特根斯坦的信徒，没有人承认自己是受维特根斯坦的影响，包括当代大的一些哲学家，他们在自己的书当中只是引述维特根斯坦的观点，但他们从来不认为维特根斯坦说法就是对的，或者说我认为我就是按照维特根斯坦的方式来做的。</p>
<p>这是维特根斯坦思想的特殊性，我曾经有文章专门讨论这个问题，维特根斯坦哲学的特殊性就在于，它不是以一个显形的方式让人们去接受的，维特根斯坦是一个隐士，他是隐身在我们的思想背后，然后去触发人们思考很多新的问题，而这些问题实际上在维特根斯坦都已经讨论过了，只是他并没有给我们提供一个现成的结论，而是让我们不断的去思考他提出的这些问题，他一辈子涉及的哲学问题实在太多了，哲学史上关注过的以及当代所面临的问题他都有所涉猎，而且几乎每一个人跟他接触的人都评价说维特根斯坦对所有事情有一个很好的感觉，他那种感觉是超人的，而且他说的话都不外行，但是他一说一表达对这个问题的看法，马上就可以表现出他自己那种天才的那一面，包括人类学、宗教、艺术、文学、诗歌、音乐，所有这些领域他都表达过自己的看法，我想恐怕在他那个时代，也少有这样的学者或者哲学家能够做到这一点，这也是维特根斯坦的过人之处。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  勒阿弗尔（4）.jpg" alt="莫奈  勒阿弗尔（4）"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（16）：维特根斯坦论语言的限度（2）—— 语言的边界]]></title>
      <url>/2018/02/12/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E7%9A%84%E8%BE%B9%E7%95%8C/</url>
      <content type="html"><![CDATA[<p>维特根斯坦说，当我们谈到边界的时候，我们不是刻意要追问这个边界在哪里，而是用语言的方式来说明，我们所有的人类活动只有按照语言的这种表达方式来理解，我们的思想才能够被加以定位，思想的活动是通过语言游戏这种方式来加以定位甚至加以理解的，不是纯粹用逻辑的方式去规定思想的目的或方式，而是把语言完全看作是我们人类自身具有的活动能力和活动内容。语言游戏既是我们能够讨论思想边界的一种方式，同时又是我们不断突破思想边界的方式。</p>
<a id="more"></a>
<p>维特根斯坦的哲学研究是用语言游戏的方式来显示思想的内容，不是通过语言来表达思想的内容，而是通过语言游戏来显示，由此，就跟逻辑的语言没有关系了，因为逻辑的语言在他看来并不是被代替了，而是说被搁置一边。有一部分是可以用逻辑的方式来表达的，但是还有一些不能够用逻辑的语言来表达得，那关于这个不能够用逻辑语言来表达那一部分，我们只能通过语言游戏的方式来显示它们。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈 查令十字桥（1）.jpg" alt="莫奈 查令十字桥（1）"></p>
<p>但是，维特根斯坦是如何用这样的语言游戏来显示他所说的那些不可能用逻辑的语言来表达的思想内容呢？大部分人都认为维特根斯坦后期对语言的批评和批判和前期不太一样，一种比较经典的观点认为，他的前期哲学是一种静态的对语言结构逻辑的分析，而到了后期是一种动态的对语言活动的一个游戏分析，一个是静态一个是动态，一个是对性质结构的了解，一个是对于活动、运动的了解，对于活动的了解，对于游戏的了解，这两个是完全不太一样，但就是这种不一样导致了维特根斯坦在后期所关心的侧重点不一样，但是并不意味着他完全背离了他前期的哲学，而只是说他关注的重点不同，所以，他只是转换了一个视角，他的研究对象并没有变化，仍旧是对于语言本身的考察。在他后期哲学里面，他对于语言的讨论就不再是界限、限度的问题，而是边界, the bounds of language or the boundary of language。维特根斯坦说，其实我们有时候很难对一个语言游戏划出一个界限，我们能够规定这个语言游戏跟其他的语言游戏是如何被加以区分的，因为我们知道，在《哲学研究》当中，维特根斯坦描述了大量的语言游戏，他并没有给出具体的哪一种语言游戏跟其他语言游戏之间有什么本质性的区别，但是他强调了语言游戏跟另外一个语言游戏之间的相似关系，他用了家族相似这个说法来说明所有的语言游戏都具有某种家族相似的特征，这个家族相似在他那里其实指的是所谓的遵守规则，无论是规则还是家族相似，在他那里都表征着一个特点――所有的语言游戏它们具有一些共同的东西，而这种共同的东西使得语言游戏可以完成它的功能，并且使得我们能够通过语言游戏去了解那些我们试图想表征的内容。</p>
<p>这里有一个很重要的节点，我们都认为维特根斯坦是在描述语言游戏，而并不认为语言游戏在维特根斯坦那里是被解释的，仅仅是被描述的，但是其实维特根斯坦在这里所描述的语言哲学隐含一种解释性功能，就像显示这个概念一样，显示并不是意味着这个行为本身它给我们显示出了某种特征，如果一个行为，一个活动，或者一个游戏，当它被说成是向我们显示了某一种生活形式的时候，是指这个活动本身它所显示生活形式能够被我们所理解，当我们看到一个我们完全没有见过的陌生游戏，我们没有办法通过这个游戏解读出这个游戏它所显示生活内容或者生活形式，而我们只有在知道有这样的生活形式之后，我们才能够了解这种生活形式是什么，如果完全不了解的话是没有办法解释的，所以表面上看它就只是描述性。他也反复强调说，我们的语言游戏不过就是一项跟平常的游戏一样，所以这个时候我们不需要任何解释。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  查令十字桥（2）.jpg" alt="莫奈  查令十字桥（2）"></p>
<p>按照维特根斯坦的观点，当我们要讨论语言游戏的时候，我们到底在做什么？它是以什么样的方式向我们显示它所谓的生活形式的？生活形式是一个很有意思的概念，我们需要仔细揣摩这个概念所包含的深层含义。因为我们讲的生活形式并不是大家公认的某一种生活方式，或者某一种习俗、传统、文化、历史等等，他讲的生活形式是指我们是按照这样一种方式来生活，它里面包含的内容都已经被隐含在我们的活动方式之中了，我们是通过这种活动方式来显示这样的生活内容。就像我们看到一幅绘画的时候，我们马上就能判断出这幅绘画它所包含一种历史的、文化的乃至民族的特征，这样的特征实际上并不是我们去赋予这个图画的，而是这副图画向我们显示出来的，因而我们在欣赏任何一幅作品的时候，这幅作品向我们显示出来的内涵和意义要远远多于我们赋予它解释的意义，因为解释都是依赖于它所显示的内容。所以在这个前提之下，维特根斯坦强调，我们的语言游戏是以这种显示的方式向我们呈现它的细节的，所有的语言游戏都是以其详细的活动方式和内容向我们显示它背后所隐含的生活方式和生活形式，这就是维特根斯坦一直强调的，当我们了解一个语言游戏的时候，一定要把握语言游戏它所具体的方式，而不是它一般性的内容。比如说下棋，当我们谈下棋的时候，我们一定要进一步追问什么棋、怎么下，什么棋、怎么下这里面背后追问的是：到底是什么样的规则决定了这个游戏的方式？</p>
<p>因而，你可以看得出维特根斯坦所讲的细节的概念就是讲我们遵守规则的具体活动内容，通过这种细节的描述，我们才能够真正把握维特根斯坦所说游戏的概念是以什么样的形式，向我们显示它背后所给我们提供的内容。我们每一个在日常生活当中生活的人每天都会接触大量的人和事，有一些东西是我们并不熟悉的，有一些东西是我们可能通过书本去了解的，或者通过图片去了解，当我们了解这些内容的时候，我们得有一个前提，我们要知道它向我们展现的这些思想内容背后的东西是什么？</p>
<p>如何去理解和把握这一点就很玄妙了，只有当我们能够真正理解这种文化所依赖根据的时候，我们才能够真正去理解这个文化向我们显示的内容，如果你不能够了解这个文化，就说明你自己没有办法把这个语言，或者这种活动看作是整个生活形式的一部分，而唯有你自己在整体上把握了这样的文化内容，你才能够真正地了解它所显示出来的文化意蕴。</p>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  查令十字桥（3）.jpg" alt="莫奈  查令十字桥（3）"></p>
<p>后来很多哲学家就把维特根斯坦这个观点发展成了整体论的语言观。所谓整体论的语言观是说，当我们理解一种语言的时候，或者说理解一个语言当中某一些命题的时候，就意味着我们能够完整地理解这个语言和命题所在的整个的语言。就像约翰・赛尔（Jhon Searle）所举的例子，一个英国士兵在二战期间被德军抓了，他大概会说一句德语，他就用这一句德语表示自己是一个德国人。比如他说的德语是鲜花很美，这句德语跟当时的场景完全无关，但他想用这句话来表示我也会德语，我是德国人，是友军。他通过这种方式来表征自己的身份，但事实上这句话本身在这里不表征任何他能够懂这个语言本身，但这是一种特例。在这种特殊的环境下，我们能够理解这句话并不是根据这句话本身它所表达的含义，而是根据它的整个场景，他用这句话来表征他其实是一个德国人。但是我们在理解任何一个语言中一个句子的时候，当我们能够理解一个语言中的一个句子的时候，也就意味着我们能够理解这个语言所承载的这样一种文化背景，其实也就意味着你可以理解这个语言，这就是所谓的整体论的观点。</p>
<p>这个整体论的观点的确是从维特根斯坦这儿引发出来的。按照维特根斯坦的说法，语言游戏的理解并不是一个单纯的、简单的步骤，也不是一次性完成的工作，它是需要大量的，无数的这样的语言游戏构成一个完整的整体，这个整体就意味着说，当我们理解某一个语言游戏活动的时候，我们能够知道这个活动它所附带的，或者说它所相关的其他的游戏活动。比如当我们玩扑克牌的时候，没有一种游戏叫做玩扑克游戏，因为扑克只是一个工具，可以玩桥牌游戏，打争上游，斗地主，我们可以做各种你可以想象出来的用扑克来玩儿的游戏。这样的游戏实际上它并不依照于纸质或者材料，而是游戏规则。因而，这个时候遵守规则是整个游戏活动当中最为至关重要的一个环节，正因为遵守规则而使得维特根斯坦发现，我们通过遵守规则才能够真正地定位我们所要做的事情，因为这些规则它给我们规定了我们这个游戏的性质，同时也规定了这个游戏它所反映出来的这样一种文化特征。因而通过这个小的游戏来看整个文化的概念，小的语言活动来看整个思想的构成方式，实际上就是维特根斯坦希望通过语言游戏所要达到的目的。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  查令十字桥（4）.jpg" alt="莫奈  查令十字桥（4）"></p>
<p>在这里，我要进一步解释“边界”的概念。因为我们前面说它不是一个界限，也不是一个范围，而是一个边界。就像孙悟空一样，说要给唐僧划一个界，划的这个边界是唐僧不能够超出的范围，超出了以后他就没有安全保障了。同样，对于语言来说，如果按照逻辑的方式划一个界，只有在逻辑的范围内能够表达清楚的思想，我们都可以用逻辑的方式来表达，这是早期的一个工作。到了后期的时候，他讲的边界是说，这个边界之内的事情都没问题了，但是我们现在要做的是边界之外的事情。这个边界有多大，我们在边界之外所能够完成的工作就有多大。所以维特根斯坦说，这样的边界是没有办法确定的，因为我们的语言活动是“佛法无边”的，语言相当于是我们人类所使用一个佛杖，怎么划它都会成为一个独立的场域。因而，在这个前提下，这个边界实际上是不可能完全被划定。所以维特根斯坦说，当我们谈到边界的时候，我们不是刻意要追问这个边界在哪里，而是用语言的方式来说明，我们所有的人类活动，只有按照语言的这种表达方式来理解的话，我们的思想才能够被加以定位，思想的活动是通过语言游戏这种方式来加以定位甚至加以理解的。这样它就摆脱了早期的纯粹用逻辑的方式去规定思想的目的或方式，而是把语言完全看作是我们人类自身具有的活动能力和活动内容。所以语言游戏对他来说，既是我们能够讨论思想边界的一种方式，同时又成为不断突破思想边界的一种方式。因为，当我们想到一种新的语言游戏的时候，我们实际上就想到一种新的思想的可能性，我们只要创造一种新的游戏规则，我们就又有一种新的思想的产生，我们现代社会的发展变化如此之快，其实也在不断创造新的语言游戏，而对于维特根斯坦来说，其实我们日常生活当中的很多思想观念是伴随着这些语言游戏的生成而形成的。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  查令十字桥（5）.jpg" alt="莫奈  查令十字桥（5）"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（15）：维特根斯坦论语言的限度（1）—— 语言的限度]]></title>
      <url>/2018/02/11/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8815%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6/</url>
      <content type="html"><![CDATA[<p>《逻辑哲学论》是维特根斯坦早期的一本代表作，也是他生前出版过的唯一的一本哲学著作。他在这本书里提出了关于哲学是一种对语言的批判的观点，即批判语言实际上是哲学的一个主要工作，甚至是哲学的全部工作。维特根斯坦认为，我们所有思想都是用语言表达的，因而我们即使是要考察思想，也只能通过语言来完成，批判语言就是在批判思想。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  日本桥（1）.jpg" alt="莫奈  日本桥（1）"></p>
<p>维特根斯坦是大家都喜欢的一个哲学家，无论是他自己的个人生平还是他的哲学思想，都引起了很多人的关注，尤其是关于他的哲学。通常会认为他有两种哲学，一个是前期的《逻辑哲学论》时期的哲学，另外一个是所谓的《哲学研究》时期的哲学，这两个时期的哲学思想，是有很大的区别的。维特根斯坦在一生当中提出两种具有深远影响的哲学思想，这个对于一般人来讲，的确很难做到，但维特根斯坦做到了，这就足以证明，维特根斯坦是一个非常具有天才性质的哲学家。</p>
<p>今天我其实要讲的是关于维特根斯坦整个哲学思想的理解，不仅仅是限于他的前期或后期，而是放在一个整体的发展脉络当中来加以把握。把握的方式就是通过他对语言限度的理解，来看待他的前后期之间的相互联系。谈两个哲学之间的差别比较容易，但是要谈他们两个之间的相互关系，确实需要我们去发现和寻找一些相关的线索。我们找的线索是通过对维特根斯坦语言的限度的说明，来看待作为一个整体的维特根斯坦思想的发展过程。限度的英文叫limitation，实际上讲的是在一个界限内我们所能够完成的工作，或者说我们不能够超出这个界限，只能在这个界限之内所做的工作，通常我会把这一部分的思想看作是他前期的哲学，即他在《逻辑哲学论》当中给我们提供的关于语言限度的思想。</p>
<p>《逻辑哲学论》是维特根斯坦早期的一本代表作，也是他生前出版过的唯一的一本哲学著作。这本书虽然篇幅不长但意义非常重大，他在这本书里首先提出了关于哲学是一种对语言的批判的观点，即批判语言实际上是哲学的一个主要工作，甚至是哲学的全部工作。维特根斯坦认为，我们所有思想都是用语言表达的，因而我们即使是要考察思想，也只能通过语言来完成，在这个意义上，批判语言就是在批判思想。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  日本桥（2）.jpg" alt="莫奈  日本桥（2）"></p>
<p>根据维特根斯坦自己的论述，我将他整个前期哲学里面关于语言界限的观点分成三个步骤来加以论述。</p>
<p>第一个步骤，它认为我们所有的思想不过是由有意义的命题构成的，这些有意义的命题形成了我们的思想，而所有这些命题又构成了我们的语言，我们通过了解有意义的命题来了解我们的思想，也就是说我们通过这些构成了有意义命题这样的一些语言来了解我们的思想。由此，语言本身就变成思想的全部，所以语言是思想的全部，我们只有通过考察语言才能了解思想本身。维特根斯坦相信，当我们把语言理解成能够去加以考察和思想的一种内容的时候，我们其实就是在对语言和思想本身做一种批判的工作。这种批判某种意义上意味着，当我们批判思想的时候，实际上是在批判语言，那么反过来说，当我们考察语言的时候其实是在考察思想。在维特根斯坦看来，整个哲学的工作就是要通过对有意义的命题所构成的语言的分析，来理解它所表达的思想内容，这是维特根斯坦一个非常重要的观点。对语言的考察和批判，也构成了维特根斯坦整个前期哲学主要内容。</p>
<p>维特根斯坦在第二步说，我们在考察语言的时候会发现，语言本身其实是我们人类机体的一个组成部分，由此，语言本身很可能会成为我们了解思想的一个障碍，因为如果我们真的可以通过语言来了解思想，语言就应该是一个透明的，毫无障碍的途径，我们可以直接通过语言去了解思想，但是现实却并非如此。我们的日常语言其实经常会出现模糊、混乱、误用等等这些情况。那么，我们在考察语言的时候如何通过语言的考察接近思想？这本身变成一个困难的事情，因而这里面是否有一个悖论——由于我们考察语言是在考察思想，而语言会妨碍思想，所以我们实际上考察语言是达不到思想的。维特根斯坦要解决这个矛盾，如果来看待我们对思想的考察是通过对语言的考察来完成的？</p>
<p>维特根斯坦是怎么解决这个问题的呢？他在第一步说，所有考察语言的过程，其实就是考察思想的过程。第二步又说语言本身又会妨碍我们对思想考察，两者之间会产生一些矛盾。第三步维特根斯坦提出，我们并不是完全只是通过考察语言的表达方式去理解思想，我们能够考察的语言是很有限的，因而对这样的语言的考察，满足的是符合逻辑句法要求的语言，只有这样的语言能够给我们清楚的展现它的句子结构并且通过这样的句子结构向我们显示思想的时候，我们才能够说这样的语言考察是对思想的研究。所以，可以看得出来，维特根斯坦这里所说的考察语言本身，考察的并不是自然语言，而是逻辑语言。</p>
<p>这样就解释了为什么维特根斯坦会提出，对语言的批判其实就是对思想的批判，甚至说整个哲学就是对语言的批判这个观点。这个批判有两个含义，一个是要考察语言所能够发挥的作用，第二就是要考察语言的限度。语言的作用是我们通过语言能了解思想，它的限度是指我们只能通过考察那些符合逻辑要求的语言，我们才能够去达到思想。换句话说，思想是通过符合逻辑要求的语言加以表达的，这是维特根斯坦的一个观点，至少是在他前期哲学当中他是这么认为的。</p>
<p>这里就有一个麻烦，如果语言是通过逻辑的方式所构造出来才能表达思想的话，那些非逻辑的语言怎么办？维特根斯坦在他的这本书的最后一个命题里说，对于那些不可说的东西，也就是我们不能用清楚的逻辑语言去表达的思想，我们只能保持沉默。最后一个命题维特根斯坦试图给我们表达了一种神秘主义的思想，因为发现我们不能理解和表达的东西，似乎都是不能说的东西，既然他不能够说，按照维特根斯坦的观点它甚至连思想都不能够进行，这是维特根斯坦一个神秘和玄妙之处。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  日本桥（3）.jpg" alt="莫奈  日本桥（3）"></p>
<p>很多人在讨论维特根斯坦的时候，认为维特根斯坦在这里给我们埋下了一个伏笔，这个伏笔是说一定有一些东西是不能够用逻辑的语言去表达的，而这些不能够用逻辑的语言来表达的东西，我们一定有别的方式可以得到。这个概念在维特根斯坦前期哲学中并没有明确的给出，它只提到一个词——显示，我们可以通过显示的方式给出那些不能够用语言所表达的思想内容。由于前期哲学他主要关心的是逻辑语言，他给出的关于世界的理解，基本上都是一个逻辑世界的概念，而不是一个我们现实的经验世界，因而就使得所有的逻辑语言只能够表达我们所构造出来那个逻辑世界的内容，而不能够去理解经验世界的内容。</p>
<p>如果退一步我们可以说维特根斯坦实际上隐含这样一个前提，凡是我们可以用逻辑的方式去表达的，我们在经验当中其实都是可以实现的。反过来说，凡是我们在经验中可以去感受和表达的东西，如果它有意义，也应当可以用逻辑的方式加以呈现，这是两个不同的含义。如果仔细体会一下，大概我们能够理解维特根斯坦这里说的一个深刻内涵——所有关于思想的内容，当我们不能够用逻辑的语言去表达的时候，我们到底在说什么？这是一个很重要的问题，维特根思言做到这一步的时候就停止了，他没有往下追问了，因为在他看来，他能够用逻辑语言表达的思想，基本上都在这本书里已经全部说的很清楚了，他认为没有必要说那些不能够用逻辑的语言去表达的思想，所有他说凡是我们不能用逻辑语言表达的思想我们只能够保持沉默。</p>
<p>维特根斯坦在这本书里面给我们提供了一个如何去理解哲学工作的方式。哲学工作的方式并不是在于我们能够用某一些哲学的概念和观念去提供给人们关于世界的理解，而是相反，所有这些概念跟观念其实都包含着一些能揭示我们对世界理解思想内容的一种陈述方式，这种陈述方式是逻辑的而非经验的，这是维特根斯坦特别强调的一点。所以他从罗素和弗雷格哪里得到了他们所谓的逻辑的观念，一切哲学只有按照这样的观念来做，我们才能真正在做哲学，除此之外都是无法在哲学中加以处理的，除此之外的东西是什么，就是那些不能够说只能被显示的东西。维特根斯坦1929年重返剑桥以后，实际上他的思想发生了一个很重要的变化，他开始重新思考他之前关于逻辑语言所表达的哲学观念这样一种思想是否恰当？大部分人都认为他放弃了之前的观念，而重新给出一种新的对哲学对语言的全新理解。但是，维特根斯坦其实并没有真的完全放弃他之前的那种观念，他只是转换了一个角度，他不再坚持原来的那种以逻辑的方式去观察世界，以逻辑的语言去表达我们思想这一部分的内容，这一部分的内容在他看来并没有错，它其实仍然可以保留在我们对逻辑语言的讨论当中，但是总有一些内容它是没有办法用逻辑的语言表达的，而对于那些不能够用逻辑语言表达的那些思想内容，我们还是不能用逻辑语言表达，但是他换了一种方式，他用语言游戏的方式来显示那些不能用逻辑的语言去加以表达的重要思想。他甚至认为这一部分的思想比用逻辑的语言表达的思想更为关键，在他的哲学体系中他认为这部分更为重要。</p>
<p>《逻辑哲学论》这本书他所表达的并不是他的全部思想，还有一部分是他这本书还有说出来的，而他认为没有说出来的那一部分还更加重要，这种说法隐含着他后期哲学里面所做工作的意义，为他后面的工作打下很好的基础。《逻辑哲学论》这本书讨论的是在逻辑的语言之内我们所能够从事的哲学工作和所能完成的事情，这一部分包括了他对思想概念和对于世界的理解。所以七个命题里面，前两个命题是关于语言和命题的讨论，最后一个命题关于不可说。<br><img src="http://omu7tit09.bkt.clouddn.com/莫奈  日本桥（4）.jpg" alt="莫奈  日本桥（4）"></p>
<p>这七个命题听上去都很简单，但是事实上理解起来却非常困难。因为一开始我们接触第一个命题的时候就不知所云。第一个命题说，世界就是所发生的一切。很多人都在问是什么意思？如果按照逻辑的语言，这句话就是重言式，世界就是他所发生的一切，就相当于A等于A，但事实上不是，那么这句话的重要意义在什么地方？只有把这七个命题颠倒过来理解我们才能真正把握这句话的含义，把它的第一个命题当做他最后一个命题来理解，这个时候我们才能真正体会到维特根斯坦到底在说什么。因为他在最后一个命题里说，一切不可说的东西我们只能保持沉默，他的第二句话应该说，以下所说的东西都是可以说的。关于命题，关于思想乃至于最后关于世界这些说法，在他看来是可以说的，但是这个可说仅仅限制在逻辑语言范围之内，只有在逻辑的概念当中我们才能够去表达我们关于命题、关于思想以及关于世界的观点。后面的这六个命题，我们往回倒着读的话，首先就是关于命题的讨论，所有的命题都是由一些形式展现它的意义的，这是可以说的。</p>
<p>第二个命题是关于思想。所有的思想都是那些关于事态的命题，因为思想就是关于世界和事实的逻辑图像，也就意味着我们可以通过逻辑的方式来展现这个事实本身，这个展现的图形就是命题，因而，命题就像是一幅图画，它向我们展现了世界的真实情况，这个展现的内容就是意义，或者叫做思想。所以这就是维特根斯坦告诉我们的观点。由这个思想本身再进入世界，因而世界就是我们通过思想展现命题的方式来给我们提供关于在这个命题当中向我们描述的事实的构成形式。</p>
<p>因为世界是由事实构成的，所有的事实都表现在命题当中，所以我们所了解的世界的概念就是所有这些事实的总和，由此，世界就是它所发生的一切，那发生的一切就是事实的总和。所以我们就可以理解，为什么维特根斯坦一开始就说，世界就是它所发生的一切，而它所发生的一切，就是事实的总和。这几句话实际上是他最后要推出的一个结果，而不是他论述的前提。通过这样的解读，我们就可以理解在维特根斯坦前期哲学当中，他所谈到的界限或者限度这个说法，实际上表征的是他在逻辑语言范围内，所能够表达的一些关于思想跟世界的理解的内容，这是他在前期哲学当中一个重要的观点。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 维特根斯坦 </tag>
            
            <tag> 语言哲学 </tag>
            
            <tag> 逻辑哲学论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（14）：康德《纯粹理性批判》句读（1）——  第一版序]]></title>
      <url>/2018/02/10/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%BA%B7%E5%BE%B7%E3%80%8A%E7%BA%AF%E7%B2%B9%E7%90%86%E6%80%A7%E6%89%B9%E5%88%A4%E3%80%8B%E5%8F%A5%E8%AF%BB%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%20%20%E7%AC%AC%E4%B8%80%E7%89%88%E5%BA%8F/</url>
      <content type="html"><![CDATA[<p>康德《纯粹理性批判》第一版的序宏观地展示了康德在写作这部著作时所面对的问题，许诺了他将要达到的结果，制定了它在整个体系方面所依据的方法论准绳，以及该著作在他整个哲学构思中的位置。</p>
<a id="more"></a>
<blockquote>
<p>人类理性在其知识的某个门类里有一种特殊的命运，就是它为一些无法摆脱的问题所困扰；因为这些问题是由理性自身的本性向自己提出来的，但它又不能回答它们；因为这些问题超越了人类理性的一切能力。</p>
</blockquote>
<p>“某个门类”显然是指形而上学，即纯粹哲学的门类；而“特殊的命运”是指理性的那些最高问题所带来的命运，或者说是一种厄运。理性只有在形而上学中才给自己提出一些他自己不能解答的问题，这是理性的不幸，当然，也是理性的能耐，只有理性才能在他的最高形态中给自己提出超出自己能力的问题。理性的这种自然倾向或者说“本性”自有他的积极意义。不过康德在这里一开始强调的是这种命运的消极面，这也正是他进入问题的入口，即要寻找一条解决理性的困境之道。下面他就来阐明理性的这种困境和引起这种困境的原因。</p>
<blockquote>
<p>人类理性陷入这种困境并不是它的罪过。它是从在经验的进程中不可避免地要运用、同时又通过经验而证明其运用的有效性的那些基本原理出发的。</p>
</blockquote>
<p>理性的困境之所以是它的“命运”，就在于它并不是有意要陷入进去的，而是从一种正当的要求中不知不觉地陷入的，这种正当要求就是在那些在经验运用中已经证明了其必然性和有效性的理性原理的要求。理性的基本原理在经验中是有效的，这个谁也否认不了，所以这些原理也是不可取消的；但也正是这些原理，当我们立足于它们来追求更高的目标时，就必然把我们带入歧途。</p>
<blockquote>
<p>借助这些原理，它们（正如它的本性所将导致的那样）步步高升而达到更遥远的条件。</p>
</blockquote>
<p>理性的基本原理具有一种“本性”（或自然），就是要从有条件者去追溯他的条件，也即不但要知其然“而且要“知其所以然”。而这种追溯按照理性的本性来说是无穷尽的，他总是要从近到远，从已知的东西到未知的东西，从经验知识到使这种经验知识得以可能的条件，以及条件的条件。</p>
<blockquote>
<p>但由于它发现，以这种方式他的工作必将永远停留在未完成状态，因为这些问题永远无法解决，这样，他就看到自己不得不求助于一些原理，这些原理超越一切可能的经验运用，却仍然显得是那么不容怀疑，以至于就连普通的人类理性也对此表示同意。</p>
</blockquote>
<p>理性的这种步步高升的工作永无止境，因而将“永远停留在未完成状态”；但完全停下来又不符合理性的本性，所以理性就“不得不求助于一些原理”，这就是那些超越一切可能经验范围之上的形而上学原理。这里用“一切可能经验”，不只是现在的经验，而且包括一切过去、未来可能有的经验，就是不但从现实的范围上，而且从性质上、本质上是超经验的，也就是超时空的。形而上学原理离开了经验的领域，他们不是要运用于任何经验之上，而是仅仅按照纯粹理性本身的逻辑法则来运行；但由于这些逻辑法则是普通的人类理性中固有的法则，所以虽然没有经验的内容来充实它们，这些抽象的理性原理和法则却仍然能够得到一般人的同意，甚至在他们看来是无可置疑的。</p>
<blockquote>
<p>但这样一来，人类理性也就跌入到黑暗和矛盾冲突之中，他虽然由此可以得悉，必定在某个地方隐藏着某些根本性的错误，但它无法把它们揭示出来，因为它的使用的那些原理当超出了一切经验的界限时，就不再承认什么经验的试金石了。这些无休止的争吵的战场，就叫做形而上学。</p>
</blockquote>
<p>这就是人类理性的厄运的来由。也就是说，人类理性出于它的本性要求寻找一个超出一切经验之上的最高的无条件者，以便使自己不再停留于未完成状态；但一旦它超出了一切经验，它就失去了用经验来检验自己原理的真理性的试金石，从而变成了公说公有理，婆说婆有理，陷入到“无休止的争吵”和无法解决的“矛盾冲突”之中。这就使形而上学成为了一个这类矛盾冲突的战场。而且，虽然形而上学充斥着这类矛盾，人们却既没有办法解决矛盾，也不知道这类矛盾从何而来，因为他们原先正是为了避免矛盾、特别是为了避免经验的有限性和理性的无限追求之间的矛盾，才跳出一切经验的范围而寻求理性的逻辑一贯性的。但现在矛盾并不是出现在经验和理性之间，而是出现在通常认为不可能有矛盾的理想本身内部。康德在后面关于纯粹理性的“二律背反”的讨论中所揭示的正是这种情况，即两个完全相互矛盾的命题似乎都各自有自己充分的逻辑根据，谁也消灭不了谁。当然康德在这里已经预先暗示，所有这一切矛盾都是由于这些原理超越了一切经验的界限，从而抛弃了经验这个“试金石”。因为关于知识和真理的问题，虽然我们必须借助于理性的先天原理，但这些先天原理是否运用的正当，还是要凭借经验来检验，离开经验的领地来谈知识和真理是不可能的，无论我们把理性的功能发挥到何等地步，如果没有经验的内容，都免不了空谈，甚至免不了陷入自相矛盾的尴尬。由此我们可以看出，康德本身虽然出自大陆理性派传统，但却体现出力图调和经验论和唯理论的主导倾向。不过在康德的时代，还没有人能够清楚地认识到康德所指出的问题，人们已经意识到“必定在某个地方隐藏着某些根本性的错误”，因为那些矛盾冲突已经表面化和白热化了，但他们却“无法把它们揭示出来”，而是仍然固执着自己的片面观点，不是局限于经验论，并由此走向怀疑论，就是执着于唯理论的独断轮。这种分裂局面就使形而上学的名誉遭到了极大的损害。而这就是康德当时所面临的绝望处境。</p>
<blockquote>
<p>曾经有一个时候，形而上学称为一切科学的女王，并且，如果把愿望当做实际的话，那么她由于其对象的突出的重要性，倒是值得这一称号。</p>
</blockquote>
<p>在历史上，形而上学自亚里士多德倚赖就被称为“第一哲学”，它是“物理学之后”或物理学之上，是一切自然科学和数学的指导科学，所以它是“科学之科学”。把形而上学视为”一切科学的女王“，这是西方两千年来的传统。康德对这个传统是有保留地认同的，就是说，”如果把愿望当做实际的话“，康德认为形而上学的主观愿望和意图确实是指向理性最重要的事业的，只不过她实际上并没有能够做到她想要做的事情罢了。但康德并不因为形而上学未能达到其目的而完全抛弃她，而是致力于用新的途径恢复形而上学的权威地位，重建形而上学。所以他对形而上学今天的这种不景气的状况深表同情。</p>
<blockquote>
<p>今天，时代的时髦风气导致她明显地遭到完全的鄙视，这位受到驱赶和遗弃的老妇像赫卡柏一样抱怨：”不久前我还是万人之上，以我众多的女婿和孩子而当上女王————到如今我失去了祖国，孤苦伶仃被流放他乡。“</p>
</blockquote>
<p>赫卡柏是特洛伊城最繁荣时期的王后，特洛伊城被希腊人攻陷后，她失去了丈夫和所有的子女，本人变成了俘虏，传说最后被人们用石头砸死，又说她被众神变成了一条狗。她悲惨的遭遇引起了很多诗人的同情，包括欧里彼得斯、但丁和莎士比亚。康德在这里也是以同情的笔调引用她的命运来比喻形而上学的遭遇的。有种常见的说法，说康德摧毁了形而上学，抛弃了形而上学，这是不准确的。应当说他摧毁的是传统的形而上学，但却建立起了新的形而上学，使古老的形而上学焕发了青春。</p>
<blockquote>
<p>最初，形而上学的统治在独断论的管辖下是专制的。</p>
</blockquote>
<p>哲学上的”独断论“相当于政治上的”专制主义“，它不由分说，也不说明理由，而是以既定的意见和信念作为不言而喻的前提。在形而上学的王国利，长期以来都是独断论站统治地位，但由于这种专制统治必然要引起内部的分裂和外部的抗议，所以在近代它的地位就遭到了不可避免的动摇。</p>
<blockquote>
<p>不过，由于这种立法还带有古代野蛮的痕迹，所以它就因为内战而一步步沦为了完全的无政府状态，而怀疑论者类似于游牧民族，他们憎恨一切地面的牢固建筑，便时时来拆散市民的联盟。</p>
</blockquote>
<p>“古代野蛮的痕迹”指传统的非批判的粗糙信念；”内战“指独断论的内部分裂，如经验论和唯理论、唯物主义和唯心主义的分裂，其实各方都是独断的，都想实现自己的霸权。这就导致了”完全的无政府状态“，即公说公有理，婆说婆有理，没有一个公正的法庭来对各方的主张进行裁决，从而使各方都在打一场毫无结果的消耗战。但尽管如此，这些独断论毕竟在对真理的信念还是一致的，所以尽管处于无政府状态中，却还是处于共同的利益而建立起某种”市民的联盟“，以便个人经营自己的小地盘，井水不犯河水。但怀疑论者对于这种联盟则是一种更大的威胁，他们”类似于游牧民族“，扫荡一切地面建筑，解构一切固有的联系，如休谟的怀疑论就是无论经验论还是唯理论都不能接受的，它使一切认识论的形而上学探讨都失去了意义。</p>
<blockquote>
<p>但幸好他们只是少数人，所以他们不能阻止独断论者一再地试图把这种联盟重新建立起来，哪怕并不根据任何在他们中一致同意的计划。</p>
</blockquote>
<p>真正像休谟一样主张彻底的怀疑论的人毕竟只是少数，一般人认为他们只是走极端而已，并不认真对待他们提出的挑战。当时盛行的所谓”健全知性“的观点就是大多数哲学家所依赖的一种权宜之计，他们借此很简单地把休谟式的怀疑主义扫到一边，认为这是一种知性的病态。但是”健全知性“本身是一个模糊概念，究竟怎样才算健全，健全知性应该包含哪些要素，人们并没有明确的规定。所以通常人们只是利用这一概念的模糊性而建立起一种认识论上的”新的联盟“，也就是”可知论“的联盟，来共同对付休谟的不可知论。但其实在他们中并没有”一致同意的计划“，同样是主张健全知性，经验派的健全知性和理性派的健全知性却大不相同，他们的共同之处仅仅在于不走极端，以及为了捍卫科学的尊严和共同抵御怀疑论的进攻。</p>
<blockquote>
<p>在近代，虽然一度看来这一些争论似乎应当通过（由著名的洛克所提出的）人类知性的某种自然之学（Physiologie）来做一个了结，并对那些要求的合法性进行完全的裁决；</p>
</blockquote>
<p>洛克在《人类理解论》中对人类知性的方方面面进行了详尽的分析，这些分析都是把人类的知性当做一个自然对象来看待，因此康德把它不称之为形而上学或哲学，而称之为“自然之学”，也就是用物理学或自然科学的方式来研究人的认识的结构。在这方面，洛克可以说是做大了最大可能的完备和系统化，人们似乎可以认为他已经对人类认识能力的各种要素的作用的合法性做出了“完全的裁决”。</p>
<blockquote>
<p>但结果却是，尽管那位所谓的女王的出身是来自普通经验的贱民，然而，由于这一世系事实上是虚假地为她捏造出来的，而她还一味地坚持她的要求，这就使得一切又重新堕入那旧的、千疮百孔的独断论中去，并由此而陷入到人们想要使科学拜托出来的那种被蔑视的境地。</p>
</blockquote>
<p>洛克的哲学实际上还是经验论哲学，康德称之为“出身”于“普通经验的贱民”，并没有任何先天的高贵之处，按照康德的看法经验派必然要称为怀疑论；但是洛克依然主张这种经验知识具有形而上学的认识论意义，即能够认识事物本身，但这种要求实际上是经验派所不能提出来的，所以康德说这种形而上学的世系“是虚假地为他捏造出来的”。但洛克的这位女王仍然一味地提出这种认识事物本身的要求，这就使得洛克的哲学又重新堕入独断论中去了，也就是没有根据地认定自己的知识就是对自在之物本身的认识。而这种无根据也正是人们蔑视形而上学的根本症结，它使形而上学名誉扫地，得到了“伪科学”的恶名。</p>
<p>不过，康德如此批评洛克是有些不太公平的，因为在洛克的哲学中已经包含有后来由休谟发挥出来的不可知论因素了，他首先把我们从感官直接获得的感觉，如色、声、香、味等等，称之为事物的“第二性的质”，是由我们感官的性质所决定的，因而是主观的，只有像体积、形相、运动、数量等等才属于“第一性的质”，课件他并不完全相信我们的感觉经验，其次他认为即使第一性的质，也只是我们对于客观实在所知道的“名义本质”，而不是它们的“实在本质”，实体的实在本质我们是永远也不可能知道的。这就已经有不可知论的色彩了。这正是休谟后来提出怀疑论和不可知论的最初的思想来源。当然洛克的体系中是包含尖锐的矛盾的，他甚至不太关心使自己的这种矛盾调和起来，所以康德说他是独断论也没有错，只是不全面。</p>
<blockquote>
<p>今天，当一切道路都白费力气地尝试过了之后，在科学中占统治地位的是厌倦和彻底的冷淡态度，是混沌和黑夜之母，但毕竟也有这些科学临近改造和澄清的苗头，至少器=是其序幕，他们是用力用得完全不是地方而变得迷糊、混乱和不适用的。</p>
</blockquote>
<p>康德的时代哲学界就是这样一片混乱的状态，但也不是毫无希望，而是在这种混乱中包含着“这些科学临近改造和澄清的苗头”，这是康德独具慧眼看出来的。“这些科学指形而上学的各种形态，唯理论和经验论，独断论和怀疑论，康德自认为负有历史使命来改造和澄清它们。但他并不想把它们全盘抛弃，而只是想把它们做一个调和，吸取它们各自的长处，批判它们的缺点。他承认，这些学说都有其合理之处，只是“用力用得完全不是地方”，从而把问题搞乱了。但只要对他们所展示出来的问题进行一番彻底的清理，康德认为将拉开真正科学的形而上学的序幕。</p>
<p>因此康德对当代哲学中的无所作为的态度是很瞧不起的，他说：</p>
<blockquote>
<p>因此，想要对这样一些研究故意装作无所谓的态度是徒劳的，这种研究的对象对于人类的本性来说是不可能是无所谓的。</p>
</blockquote>
<p>形而上学在当代出现了问题，但是这些问题是非妥善解决不可的，因为这是涉及人类理性的生死存亡的大问题。但当时哲学界人们都在装聋作哑，用一些模糊概念来回避和打发那些棘手的问题，特别是对于休谟所提出的挑战装作视而不见，从而使科学的基础面临严重的危机。</p>
<blockquote>
<p>上述那些冷淡主义者也是这样，不论他们如何想通过改换学院语言而以大众化的口吻来伪装自己，只要他们在任何地方想到某物，他们就不可避免地退回到他们曾装作极为鄙视的那些形而上学主张上去。</p>
</blockquote>
<p>“上述那些冷淡主义者”，指上一句话中提到的在科学中“占统治的”冷淡态度，也就是所谓“拒斥形而上学”的态度，持有这种态度的人干脆放弃一切形而上学的努力，而对任何形而上学加以拒斥，甚至在语言上也加以改换，不用形而上学的学院语言，而尽量采取大众化的语言。这种态度在康德的时代还是刚刚萌芽，但在20世纪以来则蔚然成风，一切实用主义和实证主义以及语言分析哲学都在标榜这种态度。但康德指出，这些人拒斥形而上学是虚伪的，实际上他们暗中所遵循的是最坏的形而上学，这与恩格斯的见解如出一辙。形而上学，或者说哲学，是人类逃脱不掉的命运。</p>
<blockquote>
<p>然而，这种在一切科学繁盛的中心发生并恰好针对这这些科学——这些科学的知识一当它能够被拥有，人们就无论如何也不会作出丝毫放弃——的无所谓态度，毕竟是一种值得注意和深思的现象。这种态度显然不是思想轻浮的产物，而是这个时代的成熟的判断力的结果。</p>
</blockquote>
<p>康德尽管对当时哲学家们“装作无所谓的态度”不满，但仍然从这种态度中看出现代哲学思想的“成熟”因为这种态度只不过是当时人们为了维护科学的尊严而临时借用的一块挡箭牌，他们显然并不为休谟所提出的怪论而受到惊扰，而是对于已经到手的科学知识怀有一种不可动摇的信念。他们之所以装出无所谓的态度，只不过是他们还没有找到可以有力地反驳对方的方法而已，但他们绝不轻易为对方的能言善辩所迷惑、所打动，这一点确实康德十分赞赏的，也是他有信心让自己的批判获得广大科学界人士赞同的理由。他相信，在“一切科学繁盛的中心”所发生的这种“无所谓态度”，其实是一种谨慎的态度，他不是“思想轻浮的产物”，而是人类理性已经成熟到可以面对任何针对科学的挑战的表现，在这样一个基础上，康德批判才能够奏效。</p>
<p>在这里，康德有一个注释：</p>
<blockquote>
<p>人们时常听到抱怨当代思维方式的肤浅和彻底科学研究的沦落。但我看不出那些根基牢固的科学如数学和自然学说等等有丝毫值得如此责备的地方，相反，他们维护了彻底性的这种古老的荣誉，而在物理中甚至超过以往。</p>
</blockquote>
<p>显然，康德心目中的数学和自然科学（物理学）的地位是神圣不可侵犯的，他熟悉当时的各门科学，并且自己就是一位杰出的科学家。数学和自然科学是他的批判哲学一切理论的基础，他对科学的信念从未动摇过。所以，他的批判哲学并不是批判这些科学，而是对这些科学的哲学解释。当休谟对科学的普遍必然性提出怀疑的时候，康德的反驳并不在于科学的普遍必然性，在它看来科学的普遍必然性是一个不容否认的事实。他要证明的是这种普遍必然性是从哪里来的，它的根据和条件是什么。所以在某种意义上说，康德其实并没有正面反驳休谟，或者说，一个彻底的休谟主义者、怀疑论者是“驳不倒的”。一个人当他连科学都不相信了，你还能向他证明什么呢？你的一切证明不都是科学吗？所以康德只是在大家公认的科学事实的基础上提出自己的一整套原理，使人们看到这些科学事实是有自己牢固的根基的，这样来维护科学的权威。在它看来，科学精神最可宝贵的就是科学中的彻底性，自从古希腊欧几里得几何学产生依赖，这种科学精神就有了自己的典范。当然，这种彻底性一直贯穿到今天，他的集中体现者是大陆理性派哲学，康德出身于理性派，他对这彻底精神比任何人都更执着。它在牛顿物理学中看到了这种彻底性的当代楷模，这种物理学以极少数的原理贯通天上地下，万事万物，其规模和深刻性显然不是任何古代的科学思想所可以比拟的，康德受这种彻底精神的熏陶，力图将它引进到形而上学中来：</p>
<blockquote>
<p>而现在，正是同一个彻底精神也将在另一些知识类型中表明其作用，只要我们首先留意对他们的原则加以校正。在缺乏这种校正的情况下，冷淡、怀疑，最后是严格的批判，反倒是彻底的思维方式的证据。</p>
</blockquote>
<p> “另一些知识类型”也就是指哲学、形而上学的知识类型，康德的批判哲学正是要使形而上学本身具有如同牛顿物理学那样的真正的彻底性。但前提是，必须“首先留意对他们的原则加以校正”。以往形而上学的原则本身就是建立在一个错误的基础之上的，如果这个错误基础得不到校正，那么越是彻底，这种体系就越是错的远，正所谓差之毫厘谬以千里。例如休谟的哲学够彻底了，它彻底到几乎没人能够挑出它的逻辑上的毛病，但正因为如此，他的哲学就成了一种无人能够接受的怪论。这种校正显然是康德准备留给自己来做的一件重要的工作。不过，即使这种校正还没有完成，康德已经在上述的冷淡态度和怀疑中，以及在他自己首先提出的“批判”中，看到了“彻底思维方式的证据”。冷淡态度其实也是一种不信任的态度，就是说，当人们还没有被彻底说服的时候，正是出于对彻底性的要求，反而使他们保持一种冷静，对一切奇思怪想姑妄听之。怀疑态度也是这样，就是说，你如果拿不出确凿的证据使我彻头彻尾的信服，那我就宁可停留在怀疑之中。康德的批判则是更进一步，力图对那些貌似有理的理论体系之所以可能的前提进行考察，对凡事不能彻底自圆其说的理论呢都加以清除，以便在真正经得起批判的基地上重建形而上学。其实批判本身已经是校正工作了，它是校正的第一步，但校正的完成则有待于康德自己的形而上学体系的建立。无论如何，所有这些倾向在康德看来都表明一种时代风气，即对彻底的思维方式的严格要求，他自己的批判哲学在这样一种氛围中正是基础体现了这个时代的时代精神的需要。</p>
<p>这就是康德的下面这句名言的意义：</p>
<blockquote>
<p>我们的时代是真正批判的时代，一切都必须经受批判。</p>
</blockquote>
<p>康德在他的如此晦涩艰深的哲学研究中竟然体会到了时代精神的内在的脉搏，或者说，真正深刻的哲学都是时代精神的反映，康德是有这种自觉意识的，这也可以说他殚精竭虑、穷其一生来进行哲学批判的内部热情和动力。</p>
<p>下面他点名了两个在他看来最需要批判的现实邻域：</p>
<blockquote>
<p>通常，宗教凭借其神圣性，而立法凭借其权威，想要逃脱批判。但这样一来，他们就激起了对自身的正当的怀疑，并无法要求别人不加伪饰的敬重，理性只会把这种敬重给予那经受得住他的自由而公开的检验的事物。</p>
</blockquote>
<p>一般人很难由康德的《纯粹理性批判》联想到宗教和立法这样一些现实生活的内容，虽然他在书中又对上帝存在的证明的各种批判，也经常引用法律方面的例子来说明概念的关系，但是这部著作的主题只是为科学知识和认识论提供先天根据，而并没有明确表示出某种激进的政治倾向。然而，正是这样的理性批判，对于当时统治整个社会的保守势力具有最强烈的摧毁作用。因为通过批判，康德所建立起来的是一个至高无上的理性法庭，凡是不符合理性的要求的事物都将被扫进历史地垃圾箱。从这里我们可以看到康的哲学的启蒙意义，以及对当时德国现实生活的冲击的力度。康德之所以把这样一段重要的话放在不起眼的注释中，大约连他自己都感到了他的批判的锋芒和威力，为了稳妥起见，有必要藏起来一些。</p>
<p>下面的正文恰好也表达了类似的意思：</p>
<blockquote>
<p>这个时代不能够再被虚假的知识拖后腿了，它是对理性的吁求，要求他重新接过他的一切任务中最困难的那件任务，即自我认识的任务，</p>
</blockquote>
<p>时代要求理性作出它的“自我认识”，即对理性自身进行再次的考查。自文艺复兴以来，人们已经意识到自己的理性了，莱布尼茨—沃尔夫派的启蒙理性也已经把理性摆到了一个“法庭”的位置上；但康德提出的是一个更高的要求，即理性应该对自己起诉，对自己以往所获取的那些“虚假的知识”起诉，以便对自己重新加以检查。所以康德的纯粹理性批判其实是理性的自我批判，是更高层次的理性，而这也是理性的“一切任务中最困难的那件任务”。所以康德的理性法庭是为理性自己而开的。</p>
<p>这就是：</p>
<blockquote>
<p>委任一个法庭，这个法庭能够受理理性的合法性保障的请示，相反，对于一切无根据的非分要求，不是通过强制命令，而是能够按照理性的永恒不变的法则来处理，而这个法庭不是别的，正是纯粹理性批判。</p>
</blockquote>
<p>理性要在这个法庭面前为自己的合法性取得保障，也就是排除一切由于理性的滥用而提出的“非分要求”，如何排除？仍然是“按照理性的永恒不变的法则”来排除。而这个法庭就是他的纯粹理性批判。这就开始把话题引入到正题里面来了。</p>
<p>以上就是康德对于他在他的时代所面临的问题的一个交代和概括，主要是当时的哲学所遇到的难题以及由此反映出来的时代精神。下面开始介绍这本书的主题内容，也就是他将要在书中所阐明和解决的问题。</p>
<blockquote>
<p>但我所理解的纯粹理性批判，不是对这些书或体系的批判，而是对一般理性能力的批判，是对纯粹理性可以独立于任何经验而追求的一切知识来说的，因而是对一般形而上学的可能性或不可能性进行裁决，对他的根源、范围和界限加以规定，但这一切都是出自原则。</p>
</blockquote>
<p>康德的纯粹理性批判并不是着眼于对上面所触及到的现有的那些哲学著作或哲学流派的批判，他当然要涉及这些哲学流派，而且甚至处处都在和这些哲学家对话，但它的目的并不是具体针对某个人、某本书，而是更高也更加一般的主题。康德认为他是要对“一般理性能力”进行批判，“是就纯粹理性可以独立于任何经验而追求的一切只是来说的”。通常的批判都是针对具体某本书、某个人、某种思想或某个被宣称的知识；单抗的目标超越于这一切之上，他是要一劳永逸地彻底解决问题，即针对理性能力本身，看它独立于经验而有可能追求到一些什么样的知识。通过这种批判，我们就能够对那个最高的形而上学问题，即“一般形而上学的可能性或不可能性”的问题作出裁决。这就比通常形而上学所探讨的那些内容，如形而上学的基本概念是什么，他们的关系和结构如何，他们与现实的各门科学怎样发生联系等等，要更高一层，因为它涉及任何形而上学是否以及如何可能的问题。如果形而上学根本就不可能，那么所有关于形而上学的理论再好也都是虚假的。而如果形而上学是可能的，那么它又是如何可能的？它的“根源、范围和界限”如何？这同样也是先于一切其他形而上学问题的元问题。总之，康德就是要在进行形而上学的探讨和具体规定之前，先将我们人类理性建立形而上学的能力和可能性搞清楚，或者说，要在认识之前预先清点一下我们用来认识的工具，检查一下这些工具的作用和效能，以免到头来力不从心。但康德强调“这一切都是出自原则”，也就是出自理性本身。康德在后面多次提到，理性归根到底是一种“原则的能力”。通常“原则”比“原理”和“法则”要高，比“规则”更高，是康德认为最高层次的规律。当然这些术语有时也不太严格，康德经常违反自己的规定，但大致上可以这样区分。总而言之，康德在这里想表达的是他的理性法庭是理性对理性本身的审判，最终还是要遵守理性的原则。后来黑格尔批判康德说，他就像一个游泳教练，告诫自己的学生说“未学会游泳之前切勿下水”。康德要在认识之前先对认识的工具进行检验，也就是进行认识，这本身是一个悖论。因为当你检查理性工具时，你已经运用了理性，而你所运用的这个理性必然又是没有经过检查的，这就会导致无穷后退，而一步也迈不开。不过，康德把理性提升到自我意识，这毕竟是他的一大功劳，从此理性就在通常的逻辑理性之上，又增加了一个“批判理性”，即理性的自我批判，这就为后来黑格尔的理性的自我否定运动、即“否定性的辨证论”提供了启示。黑格尔的自我否定就不再是一次性的从一个不可否定的前提出发，而是表现为一连串的否定运动，从中生发出一系列的范畴。在康德这里还未达到这种理解，理性的自我否定有一个上限，这就是理性的最高“原则”，其他的都是从那个原则降下来的。但这在当时至少也是一个很大的推进，是从来没有人做过的工作。</p>
<p>所以康德接下来说：</p>
<blockquote>
<p>现在我走上了这条唯一留下来尚未勘察的道路，我自认为在这条道路上，我找到了迄今使理性在摆脱经验的运用中与自身想分裂的一切谬误得以消除的办法。</p>
</blockquote>
<p>经验论和唯理论都不可能摆脱理性的纯粹运用时产生的自身分裂，例如二律背反的分裂，因为在涉及理性脱离经验的运用时，他们都从不同的方向不由自主地陷入了独断论；而怀疑论则只不过是发现了这种分裂，但却毫无批判地认可了这种分裂，从而停留于怀疑。迄今为止，还没有人从批判角度来对待理性的这种自我分裂，即从理性本身去寻找分裂的原因，检查理性运用的范围，它的那些原则的根源，以及他的限度。在康德看来，我们只要找到了理性在理论的运用中的范围和界限，并将它的最高原则作出定位，分清理论的理性和实践的理性的界限，分清可以认识的现象和不可认识的自在之物，理性的这些分裂自然就会烟消云散。</p>
<blockquote>
<p>对于理性的这些问题，我不是例如通过借口人类理性的无能而加以回避，而是根据原则将它们完备地详细开列出来，并在把理性对它自己的误解之点揭示出来之后，对这些问题进行使理性完全满意的解决。</p>
</blockquote>
<p>在康德看来，休谟式的怀疑本质上是对人类理性的无能的承认，他提出了问题却不能解决问题，而只是回避了问题。康德却试图“根据原则将他们完备地详细开列出来”搞清他们的来龙去脉，然后做出“使理性完全满意的解决”。显然，在他看来休谟之所以不能解决这些问题是由于他的方法不对，不是理性的系统的方法，只是凭借机智而纯粹经验性地发现问题，所以他不能看出问题的症结所在。康德却根据这些问题的性质而在逻辑上对他们进行了条分缕析的清理，只有这样，理性的自相冲突的根源才会显露出来，而抓住这个最终的源头，这些矛盾才有可能迎刃而解。也只有这样，这种解决问题的方式才是理性的，才是使人的理性完全信服的。</p>
<blockquote>
<p>虽然对那些问题得出的回答根本不是像独断论的狂人的追求者们所可能期望的那样；因为这些人除了我所不在行的魔法的力量之外，没有什么能够使他们满足。</p>
</blockquote>
<p>独断论者在这里被康德等同于魔法，是因为他们试图不根据任何确凿的出自原则的理由就断言事物出身的状况，并对理性所产生的各种幻想信以为真。在他们看来，理性的冲突如果要得到满意的解决，就必须提供出某种非理性的理由，不论是莱布尼茨的“前定和谐”，还是贝克莱德“上帝的知觉”都可以凭空想象出来，而康德则认为自己在这方面不在行，实际上带有讽刺意味。</p>
<blockquote>
<p>然而，这倒也并非我们理性的自然使命原来的意图；哲学的职责曾经是：消除由误解而产生的幻觉，哪怕与此同时还要去掉很多倍高度评价和爱好的妄想。</p>
</blockquote>
<p>就是说，独断论的那种狂热追求并不符合理性的“自然使命”，因为“哲学的职责”并不是去猜测和强求那种不可能获得的知识，而是要消除“由误解而产生的幻觉”，也就是一种否定性的职责。知识近代哲学在他的发端出一开始就显示出来的理性的作用，例如培根的“四假象”说，就是要把充斥在人们思想中的各种假象清除掉；笛卡尔的“怀疑一切”原则也是如此，凡是不符合理性或与理性的原则有出入的东西都一概被置于怀疑之中。在这样做时，要坚持一种彻底性，“哪怕与此同时还要去掉很多被高度评价和爱好的妄想”，这一点前人已经做出了榜样。在康德看来，理性是一切真理的标准，它的作用首先是批判，是否定那些本该否定的东西，不论他们戴着怎样的权威的面具，或是被众多的追求者所看重。单抗的自认为他在这方面比他的先驱者更上一层楼。</p>
<blockquote>
<p>在这件工作中我把很大的关注放在了详尽性方面，我敢说，没有一个形而上学的问题在这里没有得到解决，或至少为其解决提供了钥匙。</p>
</blockquote>
<p>也就是说，康德在清除哲学中流行的误解和幻觉的时候，首先在“详尽性”方面超过了前人。这种详尽性，康德理解为系统性，而不是细节上的繁琐性。所谓“没有一个形而上学的问题在这里没有得到解决”，就是说无一遗漏地把所有的形而上学问题都系统考察了一遍并做出了妥善的解决，”或至少为其解决提供了钥匙“。例如传统形而上学的三大问题，灵魂宇宙和上帝的问题，康德在这里都一一作了处理，揭示了它们的谬误和幻相，排除了由此形成的一系列伪科学；但同时又为它们的那些命题的真实含义留下了地盘，即认为它们作为实践理性的一些悬设还是可以接受的。当然在《纯粹理性批判》这部著作里作者讲的形而上学主要还是自然形而上学，或者说理论理性中的形而上学，而不是道德形而上学，道德形而上学是有待于康德自己去建立的，它有关实践理性。所以，，在理论理性中没有得到最终解决的形而上学问题，康德是把它们放到实践理性中去解决的，而在《纯粹理性批判》的结尾部分则对此作了明确的提示。所以康德可以大胆地说，所有的形而上学问题在他这里都得到了解决，因为他不是凭借一些偶然的经验考察或机智的联想，而是按照严格理性的原则来梳理和安排这些问题，使他们处在一个必然的关联之中，所以才能做到无一遗漏。</p>
<blockquote>
<p>事实上，就连纯粹理性也是一个如此完善的统一体：只要他的原则哪怕在它凭自己的本性所提出的一切问题中的一个问题上是不充分的，人们就只好将这个原则抛弃，因为这样一来它也就无法胜任以完全的可靠性来处理任何其他问题了。</p>
</blockquote>
<p>这表明康德的方法是一种严格系统化的方法，它不是到处去搜集问题，而是凭借纯粹理性的原则去推导问题，他相信由此所推导出来的问题系统不可能是不完备的，而真正的理性原则应该是放之四海而皆准的，只要有一个例证不能为这种原则所证明，这个原则就应当弃置不用。这正是理性派哲学典型的方法，罗素曾在他的《西方哲学史》中称之为”倒金字塔“的体系，这种体系只要抽掉了底下的一块砖，整座建筑就会垮下来。当然罗素所推崇的经验派的”金字塔“形的体系，即如果你从它底下抽掉一块甚至多块砖，对于整个体系并没有多大妨碍。但这种”金字塔“形的体系在康德看来根本算不上什么体系，而只是一些临时应付的偶然观点的堆积，他的解释力是有限的、就事论事的，揭示不了任何普遍的法则，当然也就不存在什么“完全的可靠性”了。康德所追求的则是一种规律性的原则，它能够以一统多，没有例外，只有这样的原则才能获得必然的可靠的知识，使问题得到彻底的解决。可见，他对他的方法的这种自信是建立在一个理性主义者对理性原则的普遍性的信念之上的，他认为它的优势就在于在形而上学这个复杂的领域内他的方法具有完全的彻底性，能够全面颠覆以往一切形而上学，而从一个崭新的基地上重建新的大厦。他对自己体系的这种划时代的重大意义是有充分的自觉和自豪的。这一段后面的这两句话实际上已经从《纯粹理性批判》一书的主题转入到写这本书的方法了。但在对他的方法展开全面阐述之前，他马上想到他对自己的方法的这种自信所可能引来的异议，于是在下面一段中先作了一番澄清。</p>
<blockquote>
<p>说到这里，我相信可以在读者脸上看出对于表面上似乎如此大言不惭和却不谦虚的要求报以含有轻蔑的不满神态，然而，这些要求比起那些伪称要在其最普通的纲领中，证明例如灵魂的单纯本质或最初的世界开端的必然性的任何一个作者的要求来，还算温和无比的。</p>
</blockquote>
<p>康德似乎担心自己的这种自信是否会给人带来反感，但他马上找到了辩护的理由，即他的这种自信比那些说大话的独断论者其实要谦虚得多。那些人声称他们提出的纲领是“最普通”的，因而看起来“很谦虚”，但却许诺要在关于灵魂和世界整体的方面提供出纯粹理性的知识，实际上狂妄的很。而康德的目标却只不过是指出和严格遵守人类理性的限度，对于不可知的东西就应该保持谦虚的态度。所以他说：</p>
<p>所以他说：</p>
<blockquote>
<p>因为这种作者自告奋勇地想要把人类知识扩展到可能经验的一切界限之外，对此我谦卑地承认：这种事完全超出了我的能力。</p>
</blockquote>
<p>康德自认为自己的方法是彻底的、无所不包的，只是由于他对人类的认识能力作出了严格的限制，他只是在认识能力本身的合法范围内，即“可能经验范围内”使理性的运用具有了彻底性，所以看起来好像很骄傲，其实是谦虚的，而独断论者却凭借一般常识就把理性的能力提高到与之不相称的地步。这里的“可能经验”一语是一个很有用的概念，后面还要经常遇到的，说明康德看重经验，反对理性脱离经验，但并不是狭隘的经验论，他指的经验是在先天条件下可以预见到的经验，不仅是眼前所见的经验，而且包括一切以往和将来的经验。不过即使有先天的条件，可能经验毕竟是经验，它对知识的范围做了确切的限定，康德承认，到可能经验的一切界限之外去寻求知识这件事“完全超出了我的能力”。</p>
<blockquote>
<p>相反，我只想和理性本身及其纯粹思维打交道，对他的详尽的知识我不可以远离我自己去寻找，因为在我我自身中发现了它们，</p>
</blockquote>
<p>康德反对独断论者把知识扩展到超出可能经验之外的理由正在于它们没有检查理性本身的能力，而是借助于某种高于人类理性的力量，相反，康德却“只想和理性本身及其纯粹思维打交道”，首先把我自己的思维结构搞清楚，在我自身内部探求纯粹理性本身的“详尽的知识”。只有这样，我们才能明智地恪守自己的本分，不去强求那些明知超出了自身能力的所谓知识。</p>
<blockquote>
<p>在这方面我甚至已经有普通逻辑作为例子，即逻辑的一切简单活动都可以完备而系统地列举出来；</p>
</blockquote>
<p>纯粹理性的内部结构首先体现在普通逻辑也就是传统的形式逻辑中，形式逻辑的知识有一个最突出的特点，就是它的那些基本原则自从亚里士多德以来就已经完备的形成了系统，两千年间几乎没有大的变化。在康德心目中，正如一切理性派哲学家一样，形式逻辑是任何严格科学的楷模，因为它最先全面地体现出人类理性思维的“详尽性”，也就是完备性、系统性，这种系统性是不必超出理性能力的界限之外而在自身之内就可以完成的。当然形式逻辑在康德看来也并不代表一切，而只是人类理性思维的形式规律，它的详尽性和系统性也只限于这个层面，康德只是“作为例子”而将它提出来，所以康德接下来补充说：</p>
<blockquote>
<p>只是这里有一个问题，即如果我抽掉经验的一切素材和成分，我凭借逻辑可以大致希望有多大的收获。</p>
</blockquote>
<p>也就是说，形式逻辑不管经验的内容，而只着眼于思维的形式，这些形式尽管可以详尽而系统地列举出来，但它们毕竟只是形式的知识，还不是我们真正想要达到的现实的知识。以往的逻辑学家们一个最大的误解就在于，他们以为单凭逻辑上的形式推理就可以获得一些现实的客观知识，这就导致了理性派的独断论的狂妄。所以，形式逻辑固然值得推崇，但是如果不考虑经验直观的内容，这些逻辑形式并不能给我们带来更大的收获，更不用说给我们带来真理性的知识了。所谓真理不仅是一种正确的思维方式，而且是思维和对象的符合，这就需要考虑经验的东西，所以在人类理性本身的系统知识中，除了形式逻辑之外应该有一种指导我们如何去获取经验知识的纯粹理性原理，这就是康德在本书中提出的所谓“先验逻辑”。当然先验逻辑也是以形式逻辑为楷模并从形式逻辑中引导出来的，这是后话了。</p>
<p>上面一段是插入进来的，下面才开始全面归纳他自己在本书中所采用的方法。 </p>
<blockquote>
<p>在达到每个目的方面注重完整性的同时，也注重在达到一切目的方面的详尽性，这些并非任意采取的决心，而是知识本身作为我们批判研究的质料的本性向我们提出的任务。</p>
</blockquote>
<p>注意，这里提出了两个概念，一个是“完整性”，一个是上面说的“详尽性”。</p>
<p>完整性是针对“每个目的”本身而言的，详尽性是针对“一切目的”而言的。其实，就研究的对象或“质料”而言，这里的完整性和详尽性都是一个意思，就是要考虑得周全和完备。它们都属于“量”的范畴，一个是内包的量，一个是外延的量。而“内包的量”在康德后面的知性原理体系中有时候又被归于质的原理，即“知觉的预测”之下。所以这两个要求实际上分别属于质的方面和量的方面的要求，它是由康德后面范畴表上的质的范畴和量的范畴所决定的。这就可以理解，为什么康德在这里说这两个要求并不是“任意采取的决心，而是知识本身作为我们批判研究的质料的本性向我们提出的任务”，他们实际上是康德自认为放之四海而皆准的“范畴表”对于任何一个研究对象在质料上提出的要求。康德的范畴表有四大类范畴，即量、质、关系和模态，他把量和质称之为“数学性的原理”，把关系和模态称之为“力学性的原理”，有时又把前者称之为一个对象的“构成性原理”，而把关系和模态称之为“调节性原理”。而在这里，他把量和质的要求归于研究对象的质料，下面则把模态的要求归于对研究对象的形式的要求。总之，康德对于研究方法的选择不是随意的，而是根据研究对象的情况而定的；而研究对象又被纳入到他自己的那一套逻辑框架之中。所以他的方法本身是有方法的、系统化的，这是与以前一切哲学家都不大相同的地方。</p>
<p>下面是他对于形式方面所提出的要求</p>
<blockquote>
<p>再就是确定性和明晰性这两项，这涉及这门研究的形式，它们必须被看做人们对一个敢于做这样一种难以把握的工作的作者可以正当提出的基本要求。</p>
</blockquote>
<p>根据康德下面的具体解释，“确定性”属于模态范畴是没有问题的，但是“明晰性”属于什么范畴，这个下面再谈。前面两个要求都是涉及研究对象本身的构成的，即加入你连对研究对象都没有一个周全的概览，或者有些问题和目标在你的视野之外的话，那当然就不可能得出经得起推敲和质疑的正确结论了；但是光有了全景式的视野，还不足以使你的结论达到确定性和清晰性。所以后面这两种方法更具有方法论的形式法则的意义，康德指出他们“涉及到这门研究的形式”，以与上面讲的这门研究的“质料”相区别。应该说，确定性和明晰性是对作者提出了更高的要求，康德说它们“必须别看做人们对一个敢于做这样一种难以把握的工作的作者可以正当提出的基本要求”，虽然是基本要求，但是针对这样一种难以把握的工作所提出来的，而不是对一般研究提出来的；而从这两个要求的性质来看，它们不仅牵涉到研究对象的完备性，而且已经牵涉到研究对象本身的真理性了。所以康德对他们特别重视，花了更大的篇幅来谈他们。先谈确定性。</p>
<blockquote>
<p>谈到确定性，那么我们曾经对我自己作过一项决定：在这类的考察中不允许任何方式的意见，一切在其中只是被视为类似于假设的东西都将是禁品，即使以最低的价格也不得出售，而必须一经发现便予以封存。</p>
</blockquote>
<p>这里和“确定性”相对立的是“意见”，而在西方传统哲学中，与“意见”相对的正好是“真理”，所以他这里的“确定性”也相当于“真理性”。意见总是动摇的，真理则是确定的，这是柏拉图以来的理性派的观点。康德的这项“决定”显然是由他所出身的大陆理性派哲学所带来的，在他看来意见只是一种“被视为类似于假设的东西”，在这样的一种严格的学术研究中是必须禁止的。当然，康德自己在本书中也作了一些假设，包括纯粹实践理性的“悬设”；但他强调他的这些假设完全是建立在确定性的原理上的。他在后面的“先验方法论”部分甚至专门辟出一节来谈“纯粹理性在假设上的训练”，他在那里说：“如果想象力不应当是狂热，而应当是在理性的严格监视下的构想的话，那么就总是必须预先有某种东西是完全确定的，而不是虚构出来的或是单纯的意见，这种东西就是对象本身的可能性。这样一来就可以允许人们为了对象本身的现实性而最后求助于意见，但这种意见为了不至于是无根据的，就必须与作为解释根据的现实地给予的、因而是确定了的东西连结起来，于是这种意见就叫做假设。”并且他把这种假设的作用限定为“在纯粹理性领域内只容许作为作战武器，不是为了在这上面建立一种权利，而只是为了捍卫这种权利”。纯粹理性的权利已经由确定性建立起来了，才有可能用假设来捍卫它，来对付那些同样只是一些假设的对方观点的攻击。所以作为这项研究本身的任务只能是寻求确定性。而这种确定性在康德看来只能够通过先天性来保证：</p>
<blockquote>
<p>因为每一种据认为先天地确定的知识本身都预示着它要被看做是绝对必然的，而一切纯粹先天知识的规定则更进一步，它应该是一切无可争辩（哲学上）确定性的准绳、因而甚至是范本。</p>
</blockquote>
<p>先天确定的知识的性质和意见是根本不同的，它应当是“绝对必然的”，而不是偶然的、可以这样也可以那样的。为什么这里说“据认为”先天确定的知识？这里面包含着没有说出来的意思，即你认为是先天确定的，而实际上是不是这样还不一定。康德在后面曾提到一种先天必然性，例如一个人挖一栋房子的基脚，他完全可以预见到这样挖下去房子一定会倒塌，这种知识就被认为是先天确定的，而实际上它是由以往的其他经验所证实了的，所以归根结底还是后天经验的。但无论如何，凡是被认为先天确定的知识都是“预示着它要被看做是绝对必然的”，这一点却是无可怀疑的，至少你心目中是这样预计的。所以接下里康德就讲：“而一切纯粹先天知识的规定则更进一步”，所谓“纯粹先天知识”和“据认为先天地确定的知识”就不同了，后者是可以争辩和讨论的，即这种知识究竟是不是先天的还未定；它也许在这个场合下是先天的，因为房子还未倒，你已经预见到了；但它掺杂任何经验成分的知识，就比那种不纯粹的先天知识更进一步了。在什么方面进一步了呢？“它应当是一切无可争辩的（哲学上）确定的准绳，因而甚至是范本”。就是说，在这里这种确定性在没有什么可争论的了，而且不但是无可争辩的确定性，还是这种确定性的“准绳”和“范本”，也就是最高确定性，衡量一切确定性的确定性。所以这种最高确定性就是“哲学上的”确定性，它是康德在这里所努力追求的。</p>
<blockquote>
<p>我在这里自告奋勇做的这件事在这一点上是否做到了，这完全要留给读者来判断，因为对于作者来说应做的只是提供根据，却不是判断这些根据在法官那里得出的结果。</p>
</blockquote>
<p>这里康德表示了一点谦虚，当然实际上他是很自信的，他要由读者来判断自己的成绩正表明他的自信，即他认为任何一个有理性的人都能够通过自己的理性判断而对他所达到的确定性的知识深信不疑。</p>
<blockquote>
<p>但为了不至于有什么东西不负责任地削弱了这些根据，所以倒是可以容许作者自己对那些容易引起误解的地方，即使它们知识涉及附带的目的，也加以注解，以便及时地防止在主要目的方面读者在其判断的这一点上哪怕只有丝毫的怀疑所可能产生的影响。</p>
</blockquote>
<p>这里的“但”说明，虽然康德表示了自己的自信，但是他仍然担心读者会在某些地方产生误解，认为他并没有做到真正的确定性，而是自己引入了某些“意见”或“类似于假设的东西”。为此他需要对那些容易引起误会的地方加以解释，这些地方主要是那些“涉及附带目的”的地方，但在这些地方所引起的误解很可能会影响到那些“主要目的”，这是康德所要极力防止的。所以下面一大段就是专门谈“容易引起误解的地方”的，实际上只谈了一处地方，在康德看来也是最重要的地方，就是关于纯粹知性范畴的“先验演绎”的讨论。这个部分也是康德在第二版中作了大量修改的部分之一，另一个修改得更多的地方是关于理性心理学的批判。这两处修改都是为了解决同一个误解，即当时有人对《纯粹理性批判》第一版妄加解释，认为康德的体系不过是贝克莱主观唯心主义哲学的翻版。其实在第一版中，康德已经预见到了这一误解了，所以他在这个序言中提出要对书中容易引起误解之处进行注释。当然他此时所关注的还只是“先验演绎”，但他认为这是最重要的关键，只有在这里把误解澄清了，后面也就不会发生误解了。</p>
<p>看下面这一段。</p>
<blockquote>
<p>我不知道在对我们所谓知识的能力加以探索并对其运用的规则和界限进行规定的研究中，有什么比我在题为纯粹知性概念的演绎的先验分析论第二章中所从事的研究更重要了；这些研究也是我花费了最多的、但我希望不是没有回报的精力的地方。</p>
</blockquote>
<p>这就是说，在他看来他的纯粹理性批判最重要的部分就是“范畴的先验演绎”部分。要对我们的认识能力作批判的考察，要确定他运用的规则和范围，最为关键的就是要对知性范畴如何能够运用于经验性的材料之上做出说明，而这种说明主要就是在“先验演绎”部分进行的。这一部分也是康德整个《纯粹理性批判》中最为艰深难读的部分，是康德“花费了最多的、但我希望不是没有回报的精力的地方”。实际上先验演绎所涉及的是一个康德认识论的根本问题，即他的“哥白尼式的革命”何以可能的问题。康德认识论对传统认识论作了一个颠倒，即把“观念符合于对象”倒转为“对象符合于观念”，把主观符合客观变成了客观符合主观。这样一来，主观观念如何能够必然具有客观效力就是非解决不可的问题，而这正是先验演绎所要解决的问题。先验演绎的任务就是要证明主观的先验范畴所建立起来的知识不是单纯主观中的观念，而且也是有关客观经验对象的知识，因为所谓认识的对象不过是主观范畴能动地建立起来的。但正是在这一论证过程中，康德预计到有可能发生严重的误解，即把它误解成一个贝克莱式的主观唯心主义者。</p>
<p>他下面就对这种可能的误解加以预防。</p>
<blockquote>
<p>但这一颇为深入的考察有两个方面。一方面涉及到纯粹知性的那些对象，应当对知性的先天概念的客观有效性作出阐明和把握；正因为这也是属于我的目的中本质的方面。</p>
</blockquote>
<p>他首先强调他的演绎中“本质的方面”就是对知性范畴的“客观有效性作出阐明和把握”，也即是他所谓的“客观演绎”方面。这方面涉及到知性认识的对象，它解决的是“关于经验对象的知识何以可能”的问题，这个问题也等于“经验性的东西作为对象何以可能”的问题。康德对此的解答是，经验性的东西如果没有先验的范畴来规范，它们根本就是一团虚幻的过眼云烟，是完全主观的表象，哪里会有什么客观性呢？这就会堕入到贝克莱和休谟的主观唯心主义和怀疑论中去。但如果经过先验范畴的整理和规范，它们就会凝聚成一个“对象”，且只有这样它们才具有客观性，才能成为有关客观对象的知识。所以从经验的方面看，要么就没有客观对象，要有客观对象就离不开先验范畴的作用，这就是鲜艳范畴必然具有客观效力的证明。所以认识的客观性归根到底是由知性范畴所先天地带来的，只有范畴才能给经验赋予客观性，而靠经验后天地接受只能获得主观性。说明这一点就是康德先验演绎的主要目的。不过，除了这一主要目的之外，他还有一个次要目的，这就是：</p>
<blockquote>
<p>另方面则是着眼于纯粹知性本身，探讨它的可能性和它自身立足其上的认识能力，因而是在主观的关系中来考察它，但即使这种讨论对我的主要目的极其重要，但毕竟不是属于主要目的的本质部分；因为主要问题仍然是：知性与理性脱离一切经验能够认识什么、认识多少？而不是：思维的能力自身是如何可能的？</p>
</blockquote>
<p>这就是他所谓的“主观演绎”的方面。简单地说，前面的客观演绎主要是探讨知性的对象何以可能，这里的主观演绎则是探讨知性本身何以可能。当然这两方面是有联系的，知性对象何以可能，这里的主观演绎则是探讨知性本身何以可能，当然这两个方面是有联系的，知性对象何以可能，康德是把它归结为知性的能动活动，那么知性到底是如何活动的，这个问题也就是必须加以探讨的了。所以后面这方面的探讨对于前一方面的目的而是“极其重要的”，但它的重要性毕竟不能和前一方面相比较。“因为主要问题仍然是：知性和理性脱离一切经验能够认识什么、认识多少？而不是：思维能力自身是如何可能的？”这里“脱离一切经验”是说，先于一切经验，即先天地能够“认识什么”。当然知性脱离一切经验实际上什么也不能认识，它只能用于经验；但康德在这里所关注的是，在经验知识中有哪些成分是知性先天赋予的，而不是从经验中来的。在这个意义上这些成分是知性“脱离一切经验”而认识到的，这就是诸范畴。至于“认识多少”，这个问题主要是针对理性的狂妄而提出来的，理性总是不满足于知性所获得的那一点点关于现象的知识，而时刻想要把知识的范围扩展到自在之物身上去，所以必须对它的这种狂妄进行批判，加以限制，树立一个界碑。这就是康德在“先验逻辑”的两个主要部分所做的工作：在先验分析论中讨论知性，他主要解决了“认识什么”的问题；在先验辩证论中讨论理性，他主要解决了“认识多少”的问题。当然实际上在每个部分中都涉及到这两个问题，但具体的解决是分两步走的。所有这些都是客观演绎所关心的主题，而这是主观演绎所不关心的，后者关心的是“思维能力自身是如何可能的？”即知性在建立客观知识的过程中，它的内部使如何运作的。这个知性内部的运作过程，康德在第一版的主观演绎中把它分为三个阶段或层次，即“直观中领会的综合”、“想象力中再生的综合”和“概念中认定的综合”，它们最终都依赖于并归结到先验自我意识的统觉的综合统一。当然这种分析就很有一些心理学色彩了，如果孤立起来看，确实也容易与贝克莱的主观唯心主义混为一谈。</p>
<p>对此康德解释说：</p>
<blockquote>
<p>由于后一个问题仿佛是在寻找某个已给予的结果的原因，因而看起来在这里的情况似乎是，由于我允许自己发表这种意见，我也就不得不听凭读者发表另一种意见。</p>
</blockquote>
<p>“后一个问题”，亦即上一句中的“思维的能力自身是如何可能的？”这一问题，看起来好像是要寻求一个假设，也就是通过假设一个先验自我意识的统觉来解释我们在认识活动中的这种构造经验对象的过程，而这种假设只不过是我对于这一过程的一种主观的意见，既然只是“意见”，则别人也完全有权采取和发表另外一种不同的意见。但康德说他“在另一个地方将要指出”，其实情况并不是如此。这里说的“另一个地方”，我们可以参看第一版演绎的最后一段，标题是“概述这个纯粹知性概念演绎的正确性和唯一可能性。”所谓“正确性和唯一可能性”，也就相当于这里所说的“确定性”的意思。康德在那段话里说道：“现在，说我们所研究的所有这一切现象、因而所有的对象全都在我们里面，亦即全都是我的同一的自身的诸规定，这种说法本身即把同一个统觉中诸现象的无例外的统一性表达为必然的了。”“通过纯粹想象力而对感性表象的综合，以及一切表象在与本源的统觉的关系的同一，是先行于一切经验性的知识的。所以，纯粹知性概念之所以是先天可能的，甚至在与经验的关系中是必然的，只是由于我们的知识仅仅与现象打交道，这些现象的可能性存在于我们自身中，它们的结合和（在一个对象中的）统一只是在我们里面才被找到，因而是必须先行于一切经验并使一切经验按其形式首次成为可能的。而从这个一切理由中唯一可能的理由中，也才引出了我们的范畴演绎。”这两段话的一个共同的意思就是，先验的演绎虽然是在主观中按照认识能力的层次（感官、想象力和知性）而展开，但实际上并不是一种心理学上的假设，而是对认识对象（现象）的可能性的一种先天必然的客观确定，所以并没有给贝克莱式的主观唯心主义留下任何可钻的空子。康德并不是从经验中“寻找某个已给予的结果的原因”，似乎这个原因在经验本身之外，我们虽然没有经验到它但可以猜测它。如果这样，这个“原因”就会仅仅是一种意见了，如同贝克莱对灵魂的一种假定一样，经不起休谟的怀疑论的攻击。相反，康德正是从经验的结构中分析出了它本身的先天形式（范畴），这个形式就体现在经验中，没有这种形式，经验本身就不可能，甚至不可想象。而一旦有了它，这个经验就借此而成了关于对象的客观知识，而不只是关于我的认识能力的主观知识。所以康德的主观演绎绝不是什么个人意见，而是任何可能的知识的先天结构的展示，当然这个要联系到客观演绎才能完成这层意思，主观演绎本来就是为客观演绎作准备的。所以他最后说：</p>
<blockquote>
<p>在这种考察中我必须预先提醒读者：即使我的主观演绎不能对读者产生我所期望的全部说服力，但我在这里给予优先关注的客观演绎却会获得其全部力量，必要时单凭第92-93页所说的东西就足可以应付了。</p>
</blockquote>
<p>可将康德是寄希望于它的客观演绎这一”主要目的的本质部分“能够把他的意思说明白，主观演绎只不过是一个引线，一个入口。这种情况有点像胡塞尔现象学的情况，胡塞尔也认为他虽然批判“心理主义”，但他的线性学还得要借助于心理学来进入，即从所谓“描述的心理学”、“纯粹心理学”入手，认为只要不仅仅局限于心理学的理解，就可以从中引出它的先验现象学的诸多原理。康德本人的意思也绝不是要讨论心理学问题，即人的认识能力本身的构造问题，而是要讨论一般可观知识的构造问题；所以他注重的不是思维活动的经验事实的描述，而是这种活动所构成的只是的先天必然性条件。</p>
<blockquote>
<p>最后，谈到明晰性，那么读者有权首先要求有凭借概念的那种推论的（逻辑的）明晰性，淡然和也可以要求有凭借直观的直觉的（感性的）明晰性，即凭借实例或其他具体说明的明晰性。</p>
</blockquote>
<p>那么，明晰性在康德这里是属于什么范畴的呢？按照康德在《逻辑学讲义》中的划分，它应该归于“质”的范畴。在《逻辑学讲义》中的“知识的特殊的逻辑完备性”这一标题之下，康德分别探讨了：“量”方面的“广泛性和彻底性或重要性和丰富性”，这相当于我们上面讲到的“外延的量和内包的量”（即上面归入“量”和“质”的要求）；“关系”方面的“真理性”，包括形式逻辑的矛盾律和同一律、充足理由律和排中律，相当于我们上面讲的由确定性引出的真理性，只不过是从形式逻辑上讲的，而不是像这个“序言”中是从先验逻辑的角度谈的；“质”方面的“明晰性”，正好相当于我们这里接触到的明晰性要求；最后是模态（中译者许景行翻译为“样式”）方面的“确定性”，它与“意见”相对立，也恰好与这里的前述对研究对象的“确定性”要求相吻合。所以这里的划分与《逻辑学讲义》中的划分大致一致，区别仅仅在于，后者把前者中的“外延的量和内包的量”全部都划归于“量”了，而把“质”的位置留给了“明晰性”；此外，这里没有特别提出“真理性”作为“关系”方面的要求，而是把它合并到“确定性”这一“模态”中，作为从中引出来的一个要求。再就是《逻辑学讲义》中主要是从形式逻辑的层次来谈方法，而这里则既有形式逻辑的要求，也有先验逻辑的要求。现在，在当前这句话中，康德又从“明晰性”中区分出了两种不同的明晰性，即一种是“逻辑的”明晰性，另一种是“感性的”明晰性。这种区分也见于《逻辑学讲义》。康德在那里说：“首先我们必须把一般逻辑的明晰性同感性的明晰区别开来。逻辑的明显以诸多特征的客观的清除为基础，感性的明晰以诸多特征的主观的清除为基础。前者是由概念而来的清楚，后者是由直观而来的清楚。”而这两种明晰是相互冲突的：“客观的明显常常引起主观的模糊，反之亦然。因此，逻辑的明晰往往只能有害于感性的明晰；相反地，借助于例证和比喻（它们并非严格地适宜，而是仅仅按照类推被采用）的感性明晰，则长城那个对于逻辑的明晰是有害的。”由此来理解下面的话就很容易了：</p>
<blockquote>
<p>对于前者我已给予了充分的注意。这涉及到我的意图的本质，但它也是种偶然的原因，使得我未能考虑这第二个虽然不是那么严格但毕竟是合理的要求。</p>
</blockquote>
<p>对这俩个相互冲突的要求，康德显然偏重于“前者”，即“逻辑的明晰性”，因为它“涉及到我的意图的本质”。但这样一来它也就不得不牺牲感性的明晰性了，后者本来也是一种“合理的要求”，康德对他的放弃不是有意的，而是“偶然的”，即是由于课题本身的性质所决定的，因为这个课题本身只是要搞清只是的逻辑关系。对于这种牺牲，康德自己也感到很遗憾，所以他说：</p>
<blockquote>
<p>我在自己的工作进程中对于应如何处理这个问题几乎一直都是犹豫不决的。实例和说明在我看来总是必要的，因而实际上在最初构思时也附带给予了它们以适当的地位。</p>
</blockquote>
<p>康德在写作方面并不缺乏感性的明晰生动的才能，这从他多年讲授并在晚年出版的《实用人类学》中可以看出来。一些康德传记也表明，康德在日常生活中经常是谈吐风趣、思想活泼的，并不是一个使周围的人感到沉闷的人。他知识丰富，博闻强识，各种逸闻趣事信手拈来，打比方生动贴切，甚至很懂得讨女人喜欢。应当说这样一个学者在构思自己的主要著作时，不可能不考虑到表达的生动和平易。但他承认，他在写作的一开始就在犹豫不决，究竟是照顾逻辑上的明晰呢，还是兼顾感性直观的明晰？</p>
<blockquote>
<p>但我马上看出我将要处理的那些课题之巨大和对象之繁多，并觉得这一切单是以枯燥的、纯粹经院的方式来陈述就已经会使这本书够庞大了，所以我感到用那些仅仅是为了通俗化的目的而必要的实例和说明来使这本书变得更加膨胀是不可取的</p>
</blockquote>
<p>就是说，随着构思的深入，他马上看出要兼顾两方面几乎是不可能的。因为他所面对的课题太庞大、太复杂了，单是想要把里面的关系理清楚就已经足够繁琐的了，如果再加上一些说明性的例证和比喻，就会使这件工作超出一般人头脑的负荷。我们经常听到一些人抱怨康德这部著作的艰深难读，完全是概念到概念的抽象思辨，感受不到任何思维的乐趣。但这也正是扛得自己深感苦恼的，他不得不以这种“枯燥的、纯粹经院的方式”来写作，并不是她有意要使人读不懂，而是对象本身的性质所决定的。它以这种纯粹学院化的语言尽量简明地表达思想的内在线索，这本来是一种最节约的表达方式，但就这样也已经使这本书拥有巨大的篇幅了。如果再加上一些通俗化的例子，而为了这些例子不被误会，又必须对之加以说明，这就会使书的篇幅过于膨胀，同时也无助于逻辑的明晰。所以他忍痛牺牲掉感性的明晰性实在是不得已。</p>
<blockquote>
<p>尤其是，这本书绝不会适合于大众的使用，而真正的科学内行又并不是那么迫切需要这样一种方便，尽管这种方便总是令人舒服的，但在这里甚至可能引出某种与目的相违的结果来。</p>
</blockquote>
<p>也就是说，这本书的目的并不是给一般大众看的，而是一部纯学术著作，而且到了这样一种高深的层次，几乎不可能考虑“雅俗共赏”的问题。相反，为了照顾通俗化而增加一些阅读的“方便”，这往往并不能达到目的，反而会两败俱伤，既没有做到真正的通俗，有打乱了思维的逻辑线索，所以他担心“可能引出某种与目的相违背的结果来”。他把对这本书的理解寄托于“真正的科学内行”，也就是那些纯专业人士，甚至可以说，归根到底，他相信只要他严格按照逻辑的明晰性写作，就会有人理解一种深刻的思想。对学术的真诚和对纯粹真理的追求压倒了媚俗的期待。下面他引用了特拉松院长的一句话：</p>
<blockquote>
<p>虽然修道院院长特拉松尝云：如果对一本书的篇幅不是按页数、而是按人们理解他所需要的时间来衡量的话，那么对有些书我们就可以说，如果它不是这么短的话，它将会短得多。</p>
</blockquote>
<p>这话的意思是说，如果一本书增加一些生动有趣的例子，就便于人们很快地理解，读者就甚至会缩短阅读的时间，所以虽然书的篇幅增加了，按阅读速度算却相当于读一本篇幅更短的书。康德并不反对这种说法，但他根据自己的情况对之做了引申：</p>
<blockquote>
<p>但另一方面，如果我们把目的放在对宽泛但却结合于一条原则中的那个思辨知识整体的可理解之上，那么我们就会有同样的正当理由说：有些书，如果它并不想说地如此明晰的话，它就会更加明晰得多。</p>
</blockquote>
<p>这种说法与特拉松的说法实际上是对着干的，就是说，如果不增加那些说明性的生动例子，这本书反而会更加明晰，因为它的逻辑线索没有受到那些感性例证的干扰。当然这是针对着“宽泛但却结合于一条原则中的那个思辨知识整体的可理解性”而言的，即不是一般的可理解性，而是对于按照一条逻辑原则组织起来的思辨体系的可理解性，这就是康德这本书的情况。</p>
<blockquote>
<p>这是因为明晰性的辅助手段虽然在部分中有效，但在整体中往往分散了，这样它们就不能足够快地让读者达到对整体的概观，倒是用它们所有那些明亮的色彩贴在体系的结合部或骨架上，使它们面目全非了，而为了能对这个体系的统一性和杰出之处下判断，最关键的却是这种骨架。</p>
</blockquote>
<p>这也就是上面所说的，细节的感性的明晰性和整体的逻辑明晰性相互之间有一种冲突关系，感性的明晰性只能作为辅助手段而使部分细节突显出来，但却喧宾夺主，不仅不能达到整体的清晰，反而把整体的概观弄模糊了。康德在本书中所追求的不是这种部分的明晰性，这种表面的通俗和华丽，他不想为了讨好一般读者而使思想的逻辑骨架受到损失。他很清楚自己著作的价值和“杰出之处”在什么地方，一种对真理本身的真诚使他宁可被人抱怨，甚至由于人们无法把握而产生种种误解，而不愿放弃体系的严谨一惯性。所以我们在阅读康德的书时必须要丢掉一切幻想，不要以为他会对我们的思维能力心存怜悯，而要把这本书看作磨砺我们哲学思维最好的磨刀石。</p>
<p>下面：</p>
<blockquote>
<p>我认为，对读者可以构成不小的诱惑的是，将他的努力和作者的努力结合起来，如果作者有希望按照所提出的构想完整地并且持之以恒地完成一部巨大而重要的著作的话。</p>
</blockquote>
<p>康德相信，他的《纯粹理性批判》如果有人读懂了的话，就会产生一种“接着讲”的冲动，即以合作者的身份配合康德去共同完成一部完整的形而上学著作，或者是在康德以后继续完成康德未竟的重建形而上学的事业。显然，康德并不认为他的《纯粹理性批判》就是一个完整的体系了，虽然就其本身的任务来说他无疑是完整的，但是它的任务只是“批判”，而不是建设。批判是为建设开道的。不过批判一旦完成，地基一旦清扫干净，对人们就会形成一种诱惑，既要在这一片全新的基地上建设起一座宏伟的大厦来，这座宏伟的大厦就是未来的形而上学。</p>
<blockquote>
<p>现在，形而上学，按照我们再次将给出的它的概念，是一切科学在唯一的一门这样的科学，它可以许诺这样一种完成，即在较短的时间内，只花较少的、但却是联合的力气来完成它，以至于不再给后世留下什么工作，只除了以教学法的风格按照自己的意图把一切加以编排，而并不因此就会对内容有丝毫增加。</p>
</blockquote>
<p>形而上学按照康德的设想可以一劳永逸地建立起来，这种想法在今天看来十分可笑，但康德的确实认真的。他认为其他的科学都有一些无限的发展过程，比如物理学、化学、电磁学、光学等等，都总是会有新的规律发现出来，没有人敢于说他们中任何一门今天已经完成了，不需要再发展了。因为他们除了先天的认识结构之外，还需要不断涌现的偶然的经验材料，它们的那些规律而是作为偶然被我们发现的经验规律而出现在科学中的，因此总是可以不断增加的。形而上学却不同，他的那些规律和法则只是先天地存在于我们人类的纯粹理性中，因此我们可以不需要顾及到后天经验中又出现了一些什么新的情况，而是单凭反思自己先天固有的各种能力就可以找到它们的那些必然性法则，就像逻辑学自从亚里士多德以来就已经基本奠定了，不需要再作很大的修改。所以他设想，只要人们接受了他的批判哲学的原理，就有可能发挥联合的力量来完成一种最终的形而上学，来给一切科学知识提供出完整的一套形而上学原则。那样的工作不会很困难，因为最艰难的工作已经由他自己完成了，这就是纯粹理性本身进入深入的批判。地基已经打好，材料已经备齐，甚至蓝图也已经设计出来了，一切都经过精密的勘察和敲定，现在只要大家齐心协力，就可以“在较短的时间内，只花较少的、但却是联合的力气”来完成整个形而上学了。他所奠定的这份家业，后人将享用不尽，而不再有什么工作要做了。唯一可以做的只剩下“以教学法的风格按照自己的意图把一切加以编排”而已。所谓“教学法”，又译作“教授法”，德文为Didaktik，指一种通过举例说明的方式通俗地讲解一种学说的原理的方法。例如《实用人类学》的主体部分就是所谓“人类学教授法”，其中按照知情意的次序通俗地描述了人类学的各种实用的原理，这些原理的根基是奠定于他的三大批判之上的，所以有人说读康德的书最好是从《实用人类学》读起。另外在《道德形而上学》最后的“伦理学的方法论”中也有“伦理学教授法”一章，谈到教义问答和榜样的作用。可见所谓“教学法”并不给原理增加任何内容，而只是为了达到通俗易懂的效果而设计的一种策略，是对诸多原理所做的一种合乎目的的安排。所以康德接下来说：</p>
<blockquote>
<p>因为这无非是对我们所拥有的一切财产的清单通过纯粹理性而加以系统地整理而已。我们在这里没有忽略任何东西，因为凡是理性完全从自身中带来的东西，都不会隐藏起来，而是只要我们揭示了它的共同原则，本身就会由理性带到光天化日之下。</p>
</blockquote>
<p>理性的财产必须带上理性的形式，只有这样才能很容易地被有理性的人所理解和把握；但这种理性的形式本身就包含在理性的共同原则本身中，所以不必从外面拿来，而只需从理性的院里里面发挥出来呢就是。所以这是一种比较轻松的工作，可以由一些智力也许不如康德、但精力比康德要好的人去做。因为纯粹理性的原理经过批判已经得到了最终的确立，它们不会隐藏起来，而是必然由自身而“带到光天化日之下”，昭然于世。教学法的这种作用是一种普及作用，而有了纯粹理性作为它的原则，这种普及不必担心偏离理性的轨道。</p>
<blockquote>
<p>对于出自真正纯粹概念的知识，任何经验的东西或哪怕只是应当导致确定几百个样的特殊直观都不能对之产生丝毫影响而使之扩展和增加，这类知识的完全的统一性，将会使这种无条件的完整性称为不仅是可行的，而且是必然的。”看看你自己的住所周围，你将知道你的财产是多么的简单。——波修斯”。</p>
</blockquote>
<p>教学法当然要引入经验和直观，其他的各门具体的科学的知识也必须借助于经验直观，但所有这些都不会使一门“出自真正纯粹概念的知识”、即形而上学的知识有丝毫的影响和扩展，所以形而上学有望达成一种“完全的统一性”，并且必然实现体系上的“无条件的完整性”。因为形而上学的原理并不是无限增多的，它们埋藏在人类理性的深处，并不会由于经验材料的加入而增加，而一旦被人类的批判和反思精神所挖掘出来，它们就一劳永逸地摆在那里了。现在康德已经把人类理性的那些原理的“清单”开列出来了，这就是我们所拥有的一切纯粹理性的“财产”，不会再增加，也不会再减少，所以他有充分的信心在不久的将来最终完成形而上学。</p>
<p>但这里所指的形而上学还只是指“自然地形而上学”。</p>
<blockquote>
<p>我希望这样一种纯粹的（思辨的）理性的体系在自然的形而上学这个标题下被提供出来，这个体系比起这里的批判来虽然篇幅还不及一半，但却具有无可比拟地更为丰富的内容。</p>
</blockquote>
<p>按照康德的设想，《纯粹理性批判》直接为之奠基的是一门自然科学的形而上学体系，又叫做“自然的形而上学”，属于他在《任何一种能够作为科学出现的未来形而上学导论》（简称《未来形而上学导论》）中所想要建立的那种形而上学。不过这种形而上学康德自己最终并没有建立起来，而只是谢了一本《自然科学的形而上学基础》的小册子。康德《纯粹理性批判》的直接目标就是为这样一种未来的形而上学扫清地盘，但与此同时，一个间接性的、但层次更高的目标则是要建立一门《道德形而上学》。这个任务康德倒是完成了，他不但写了一本《道德形而上学基础》的小册子，而且写出了正式的《道德形而上学》。所以它的未来形而上学体系其实有两部分，一个是自然形而上学，一个是道德形而上学，而这两部分都是依次奠基在《纯粹理性批判》之上的，用康德在第二版序言中的说法，他的《纯粹理性批判》是要通过“悬置知识，以便给信仰留下位置”，具有一箭双雕的作用。不过在这段话里他并没有涉及道德形而上学，而只提到自然形而上学。他为什么没有写出《自然形而上学》来，很可能是由于他认为这种工作比较容易，属于一种事务性的工作，而不是一种创造性、开拓性的工作。最困难的工作已经由他自己做了，剩下来的事情就是按照他所提供出来的基本概念和原则而把那些派生的概念填充到框架里面去就行了。所以康德预计，“这个体系比起这里的批判来虽然篇幅不及一半，但却具有无可比拟地更为丰富的内容”。篇幅短的原因是省去了《纯粹理性批判》中所做的那些大量的繁琐论证，只需要做一些概念的组织和安排；而内容丰富则是由于它是《纯粹理性批判》中已提出的那个基本框架的扩展和充实，每个概念都可以扩展出一系列的派生概念，每个原则也可以推演出一系列的派生原则。例如，康德在后面第十节解释他的“范畴表”的时候说：“范畴作为纯粹知性的真正的主干概念，也有自己的同样纯粹的派生概念，它们在先验哲学的一个完备的体系中是绝不可以忽略的，但我在一个单纯批判性的研究中刻印满足于只要提到它们就行了。”接下来他还做了一个初步的示范：“例如把力、行动、承受的宾位词从属于因果性范畴之下，把当下、阻抗的宾位词”从属于协同性范畴之下，把产生、消失、变化的宾位词从属于模态的云谓关系之下，如此等等。把范畴和纯粹感性的样态相结合，或者也使这些范畴相互结合，就会提供大量先天的派生概念，注意到这些概念，并在可能时把它们记载下来直到完备无遗，这将是一项有用的、不无兴致的劳作，但在这里尚无必要。”显然，有了康德的范畴表，你所能够想到的任何其他的派生的概念都可以各归其位，并由此显示出它们与别的概念之间的逻辑关系。所以《纯粹理性批判》和“未来形而上学”的任务是很不相同的，康德说：</p>
<blockquote>
<p>这个批判必须首先阐明形而上学之可能性的来源和条件，并清理和平整全部杂草丛生的基地。在这里我期待读者的是一个法官的耐心和不偏不倚，但在那里则是以为帮手的襄助和支持；</p>
</blockquote>
<p>《纯粹理性批判》的任务是平整地基，包括批判和清算旧形而上学的杂草，顶多是在已经平整好的地基上策划未来形而上学的蓝图。这个工作是高度抽象二富有对抗性的，充满着繁琐而细致的推理和论证，必须要有法官的耐心和公正才能理解和掌握。而在“那里”，也就是在未来形而上学中，康德期待的是一位“帮手”，所做的是一种锦上添花的工作。相比较而言，后一种工作是更轻松一些，但也是必不可少的。</p>
<blockquote>
<p>因为，即使把该体系的所有原则都完全在批判中陈述出来，属于该体系本身的详尽性的毕竟还有：不要缺乏任何派生出来的概念，这些概念不能先天地凭跳跃产生出来，而必须逐步逐步地去探寻，</p>
</blockquote>
<p>未来形而上学的所有基本原则在《纯粹理性批判》中都已经陈述出来了，但这对于该体系的详尽性来说仍然还是不够的，这只是一个初步的蓝图，具体如何建设形而上学的大厦，用什么材料，如何作细部的加工，这些都有待于逐步地完善。一个完整的形而上学体系必须把一切基本概念和由基本概念所派生出来的概念都完全包括在自身之中，这才能达到体系的详尽性。这些派生概念当然也是纯粹的，但在层次上不如基本概念那么高，所以不能够一下子“凭跳跃”产生出来，而必须在基本概念已经提供出来的前提下逐步推演出来，以便在自然形而上学和纯粹自然科学之间形成一种更加对应的结合。例如康德前面举的例子：“力、行动和承受”的概念是从“因果性”范畴中引申出来的，它们与物理学的基本概念就处于更加直接的关系中，如此等等。由此构成的一个完备体系就能够符合形而上学所要求的详尽性，而没有任何遗漏和缺口。</p>
<blockquote>
<p>同样，由于在那里概念的全部综合已被穷尽了，所以在这里就额外要求在分析方面也做到这样，这一切将是轻松的，与其说是工作，还不如说是消遣。</p>
</blockquote>
<p>《纯粹理性批判》的总问题是“先天综合判断如何可能”，要解决的主要是综合的问题；而在此基础上所建立的未来形而上学则是把已经综合起来的原理加以发挥，从中分析出所有可能的原理和概念，这是一种顺水推舟的工作。综合是难的，特别是先天综合，要求人发挥自己全部的能动性努力超越，才能够找出综合之所以可能的最高条件；分析则是在已有的条件和基础上进行分解，看看从一个前提可以推出什么样的一些概念和原理。所以未来形而上学的完成不再需要像康德在《纯粹理性批判》中所做的那种自下而上的艰苦的努力，而只需要自上而下地收获那些顺理成章的理论成果就行了，所以说它是“轻松的”，甚至是一种“消遣”。这或许正是康德没有自己去建立一门“自然形而上学”的原因，他认定自己是专门对付那些困难问题的。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 康德 </tag>
            
            <tag> 纯粹理性批判 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（13）：《行为经济学讲义：演化论视角》 第二讲]]></title>
      <url>/2018/02/05/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8813%EF%BC%89%EF%BC%9A%E3%80%8A%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AE%B2%E4%B9%89%EF%BC%9A%E6%BC%94%E5%8C%96%E8%AE%BA%E8%A7%86%E8%A7%92%E3%80%8B%20%E7%AC%AC%E4%BA%8C%E8%AE%B2/</url>
      <content type="html"><![CDATA[<p>囚徒困境：<br>故事：两位印第安人在一条狭长河谷里捉兔子，这时它们面临囚徒困境。兔子可以沿着河谷向两端跑，所以必须各有一个印第安人把守。这样两人向中间推进，可使兔子无路可逃。问题是，如果其中一个人先捉到兔子，那么他可以拒绝与另一个人瓜分这只兔子，这是不合作的策略。他也可以与对方平分这只兔子，这是合作的策略。</p>
<a id="more"></a>
<p>囚徒困境发生的前提是：</p>
<ul>
<li>1）如果我和你同时采取合作策略，那么我们各自的福利就都会比我们同时采取不合作策略时更好。</li>
<li>2）如果我合作而你不合作，那么我的福利会比我们同时采取不合作策略时更差并且你的福利会比我们同时采取合作策略时更好。对称地，如果你合作而我不合作，那么你的福利会比我们同时采取不合作策略时更差并且我的福利会比我们同时采取合作策略时更好。</li>
</ul>
<p>囚徒困境博弈的最弱版本：滚雪球博弈</p>
<p>你和你的邻居早晨起来发现都被昨晚的大雪困在家里，如果你和你的邻居合力铲雪，可以比你或你的邻居单独铲雪更早走出困境。当然，你或你的邻居愿意单独出来铲雪，哪怕知道对方是搭便车的人，因为，单独铲雪总比饿死在家里好得多。</p>
<p>介于滚雪球和囚徒困境之间的，是公共品博弈：拟合你的邻居上了修一条路，自愿出资。双方知道对方可能“免费搭车”，但最优的选择仍是修路，哪怕是独立出资。不过，有时候公共品很贵，必须有足够多的人分摊它的成本，否则公共品就不能存在。这时，公共品博弈就成为多人的囚徒困境博弈。</p>
<p>囚徒困境意味着社会科学基本问题，即对于一群理性人而言，社会何以可能存在并延续了许多年呢？</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（12）：《行为经济学讲义：演化论视角》 第一讲]]></title>
      <url>/2018/01/28/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8812%EF%BC%89%EF%BC%9A%E3%80%8A%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AE%B2%E4%B9%89%EF%BC%9A%E6%BC%94%E5%8C%96%E8%AE%BA%E8%A7%86%E8%A7%92%E3%80%8B%20%E7%AC%AC%E4%B8%80%E8%AE%B2/</url>
      <content type="html"><![CDATA[<p>本书是北大ccer教授汪丁丁2010年在北京大学讲授“行为经济学”课程的课堂讲义，书内汪教授内容旁征博引，涉及心理学、社会学、经济学、博弈论、神经脑科学等，各种各样的知识模块扑面而来，整理至此。</p>
<a id="more"></a>
<p>心智地图：可以表达网状的多因多果联系，是PPT线性表达方式的必要补充。Buzan’s iMindMap 4.0/微软Concept Draw/Mindjet Mind-manger 8.0</p>
<p>手写软件：“黑板” Chalk Board 1.5</p>
<p>熊十力倡导的读书方法：沉潜往复，从容含玩</p>
<p>行为经济学参考书：《行为经济学新进展》 凯莫罗（Colin Camerer）/罗文斯坦（George Loewenstein）/拉宾（Mattew Rabin）</p>
<p>凯莫罗（Colin Camerer）:加州理工教授，研究兴趣为实验经济学和脑科学（认知心理学），专著《行为博弈论》。1980年代芝加哥大学培养的博士，方向为“行为决策理论”。</p>
<p>罗文斯坦（George Loewenstein）：卡耐基梅隆大学的心理学与经济学讲座教授，CMU可谓是行为经济学的发源地，西蒙（Herbert Simon，1978年因决策过程的行为学研究获得诺贝尔经济学奖，2001年去世）在那里建立的学术传统承前启后，将1940年代的行为经济学研究传统容易CMU深厚的管理学和行为学传统，培养了诸如卢卡斯（Robert Lucas，理性预期学派领袖，获得1995年诺贝尔经济学奖）和威廉姆斯（Oliver Williamson，新制度经济学领袖之一，2009年诺贝尔经济学奖）这样的诺贝尔经济学奖。在行为经济学领域，罗文斯坦比拉宾资深，他在耶鲁的博士论文题目为“预期与跨期选择”，据拉宾介绍，他“无所不知”。</p>
<p>拉宾（Mattew Rabin）：伯克利经济学助教，在《美国经济评论》发表论文，研究博弈双方公平感对博弈行为的影响。2001年因行为经济学研究获得克拉克奖（两年一次，40岁以下），关注道德问题，本科数学，博士MIT经济系，伯克利任教至今，讲授“心理学与经济学的基础”</p>
<p>西方文献发表三年周期：</p>
<p>2009年：“社会仿真”、“诺瓦克”（Martin A. Nowak，哈佛教授，生物学与演化社会仿真学派的领袖人物）</p>
<p>2006年：“脑科学”、“费尔”（Ernst Fehr，苏黎世学派十堰经济学和神经经济学领袖人物，被认为有等于或高于凯莫罗的诺贝尔奖获奖概率）</p>
<p>2003年：“桑塔费学派”及其“合作的演化”研究</p>
<h3 id="一、行为经济学的定义"><a href="#一、行为经济学的定义" class="headerlink" title="一、行为经济学的定义"></a>一、行为经济学的定义</h3><p><img src="http://omu7tit09.bkt.clouddn.com/15178940291137.png" alt=""></p>
<blockquote>
<p>行为:</p>
</blockquote>
<p>是生命的表征。从原核生物到微生物及至人类，只要有生命，就可以有行为。个体、群体     经济学家的问题意识是个体主义的，也叫作“方法论个人主义”，优先研究个体行为，然后才考虑集结一群个体行为去试图解释群体行为。</p>
<p>价值、判断：行为的研究者有一个共同的假设，就是行为的主体，只要它有生命，它的行为便预设了一套价值和价值判断的评价系统。在这一体系的指引下，有“选择”行为。</p>
<blockquote>
<p>选择：</p>
</blockquote>
<p>（1）将备选的事物，依照基于过往经验形成的某种既存标准，分别装进由于过往经验而事先存在的一套格子里的这样一种过程。</p>
<p>（2）可选方案相当于各种可能手段的集合，选出来的结果相当于各种可能目标的集合（希望满足的全部欲望的集合），在这两个集合之间有一种映射（选择算子，在手段的集合里确定一个子集，在这一子集上，目标集的某些目标更容易实现），称为“选择”映射。</p>
<p>新古典经济学派要求这样的映射必须是“最优的”，并且是“全局最优”而不是“局部最优”。根据价值来评判最优。价值是行为主体长期演化的结果（演化论），若不涉及公共选择，也可称为偏好。贝克尔（Gary Becker，芝加哥学派领袖人物，成功拓展经济学理性选择原理于广泛领域，1992年诺贝尔经济学奖）和他的一名助手在《政治经济杂志》（Journal of Political Economy，芝加哥大学经济系的机关刊物）发表了一篇论文（“Evolutionary Efficiency and Happiness”），旨在解释人类演化过程如何使我们的偏好达到均衡从而稳定地表现出今天可观测到的一些重要性质，即性状。</p>
<p>哈耶克论证，我们的偏好来源于三重历史：（1）种群演化的历史（2）社会与文化的历史（3）个人史</p>
<blockquote>
<p>判断：</p>
</blockquote>
<p>两难情境内的选择，因为有至少两种相反且相等强有力的原则将我夹在中间，我必须抉择，否则就毁灭。这就是判断不同于选择的本质之处。判断意味着创造，因为如果没有创造，选择通常不会是“两难的”。当你知道如何选择的时候，你的选择通常不是创新行为，它只是遵循以前发生过的案例而已。当你不知道怎样选择时，你有了创造的机会。</p>
<p>经济学家在研究任何问题时务求有可观测的数据，从而有统计关系和由统计关系推测建构因果链条的可能性。否则，经济学解释就不再是科学的，而变成一种文学解释了。文学的特征是：因刻画了不可重复的人类经验而无法接受任何科学方法的检验。</p>
<blockquote>
<p>约束：</p>
</blockquote>
<p>经济学千招万式最终化为一招一式，其中的那“一式”，就是“约束条件”，最优选择是那“一招”，合起来就是“约束下的最优选择”</p>
<blockquote>
<p>成本：</p>
</blockquote>
<p>Cost is the highest alternative value，在可选方案集合里被你的选择放弃了的那些方案当中可能为你带来最高价值的那些方案的价值。最高价值的定义也就是追求最大幸福，这是经济学假设一切行为的目标函数。贝克尔强调，最大的幸福一定是行为主体想象中的最大幸福（未必是真是的），英文是“perceived maximization”（是基于真实感觉的“想象”，而不应简单译作“统觉”），为什么不是贝克尔当初使用的“imagined maximization”，因为后者有胡思乱想的意思，太不真实。例如你可以在梦境里最大化你的幸福感，但醒后就幻灭了。</p>
<blockquote>
<p>道德成本：</p>
</blockquote>
<p>就是当你违背了道德自律时你感受到的幸福感下降的程度。这是一种心理成本，为了补偿幸福感的下降，你就限制自己不去做违背道德自律的事情。为了界定道德成本，我们必须界定包含道德行为的可选方案集合，以及包含道德行为的目标函数最大化问题。但你诸如信仰、灵魂和道德这样的事情，通常具有“全有或全无”性质，也就是说，要么你是道德的，要么你是不道德的。59%道德和41%道德这样的状态时很可疑的。关于比道德要求更高的信仰，早如克尔凯郭尔在《或此或彼》中指出的，你要么有信仰，要么无信仰。这两种状态之间是一道难以逾越的深渊。要获得信仰，就必须有纵身一跃的勇气，这里不存在“边际”量，因此渐变是不可能的。</p>
<h3 id="二、价值、认知与判断"><a href="#二、价值、认知与判断" class="headerlink" title="二、价值、认知与判断"></a>二、价值、认知与判断</h3><p><img src="http://omu7tit09.bkt.clouddn.com/15178977545331.png" alt=""></p>
<blockquote>
<p>价值：</p>
</blockquote>
<p>小密尔（John Stuart Mill，1806—1873）的理解，价值就是“importance felt”（被感受到的重要性）。则新古典经济学的偏好可表达成”所有被感受到的重要性的一个排序”。“什么是重要性？”怀特海三段论——在任何理解之前先有表达，在任何表达之前先有关于重要性的感受。这里，按照怀特海的表达，关于重要性的感受总是面向实事的。金岳霖认为重要性是一种“真实感”——“真”（不假）而且“正”（不邪）。前面说过，判断必须是两难的抉择，我们之所以有判断的冲动，是因为要追求价值。故而有从“价值”到“判断”的关系箭头</p>
<blockquote>
<p>重要性：</p>
</blockquote>
<p>在漫长的演化过程中，我们或任何动物脑内的神经元网络形成一些专为外界刺激信号分类的“格子”。反复遇到对我们的生存具有重要性的刺激信号，就诱致相应类型的格子的形成。在这一基础上还可形成格子之间的关系，进而有更复杂的关系。一层一层地从较低级的脑区涌现到较高级的脑区，最终映射到“自我意识”脑区，被我们意识到。这就是外部世界的各种重要性在我们脑内的表达过程。《感觉的秩序》（哈耶克毕生唯一重要的学术作品）</p>
<blockquote>
<p>认知：</p>
<p>海纳模型：</p>
</blockquote>
<p>越是理性能力完备的行为主体，它的最优选择行为就越是不可预期。</p>
<p>设想有人对股票市场走势的洞察如上帝那样，全知全能，那么他的最优选择在我们这些智能较低的人的观察中，必是如”白噪声“那样的完全随机行为。</p>
<p>在智能的另一极端，是所谓的“零智能”假设，我们假设一些动物，智能远比例如蚂蚁更弱，那么这些动物的行为，在对他们而言“变幻莫测”的环境里，只好是循规蹈矩的，于是在我们这些智能较高的观察者来看，这些动物的行为具有很高的可预测性。</p>
<p>在得到上述结论后，海纳开始广泛运用他的结论。他试图用这一模型解释极其广泛的现象，从科学范式和技术进步路径依赖，到经济行为与文化传统。这就引起广泛的反感</p>
<blockquote>
<p>“认知”依赖于“能力”</p>
</blockquote>
<p>关于“判断”，真正重要的只有两大要素：价值和认知。认知能力让你能知道多少信息，并且你能处理多少信息？这样一个认知能力的问题，它导致了“海纳模型”及其结论。</p>
<p>从“能力”到“认知”，虽然只是一个关系箭头，却包含了例如劳动经济学家的主要努力。劳动经济学的努力表明，我们很难测定认知能力，究竟在多大程度上由先天禀赋决定？也就是在多大程度上由后天教养决定？这就是所谓“nature VS nurture”（自然对教养）的争论。</p>
<h3 id="三、信息、外部环境与认知"><a href="#三、信息、外部环境与认知" class="headerlink" title="三、信息、外部环境与认知"></a>三、信息、外部环境与认知</h3><p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-02-06 下午4.16.56.png" alt="屏幕快照 2018-02-06 下午4.16.56"></p>
<blockquote>
<p>信息：</p>
</blockquote>
<p>亚当·斯密假设决策者要充当知情且公正无偏的旁观者（fully informed and impartial spectator），这一假设在他的《道德情操论》里占据核心位置，也就是正确的决策，要求决策者掌握足够的信息。哈耶克对信息的定义为生物在长期经验中感受到外界刺激信号并形成一套专用于分类的神经元网络系统。知识或分类系统可以存储在个体脑内，也可以存储在群体里面。而群体常常以社会网络的方式存在。</p>
<blockquote>
<p>信息来源依赖于社会网络结构：</p>
</blockquote>
<p>信息的来源，有个体也有群体。一个社会的创造性，一方面取决于这个社会里每一个体的认知能力，另一方面取决于这个社会的结构。社会结构可以用拓扑学的语言描述：社会是一个网络，网络有自己的“中心”，有“科层”，还有其他的网络拓扑结构。</p>
<p>社会网络方法，好处是将个体行为嵌入到群体之内。</p>
<p>一个群体，不是完全没有结构就可存储信息和知识的。不同的社会结构，费孝通早已论证过（皇权与绅权），一个社会有什么样的结构，决定了它可能积累什么样的知识。中国两千多年来的社会结构决定了它不可能积累下西方社会所积累的那些知识。因为，许多知识，刚开始是以信息的形式存在的，在特定社会结构里，可能不允许被传授。</p>
<blockquote>
<p>信息的另一来源：认知的“外部环境”及其不确定性</p>
</blockquote>
<p>当外部环境高度不确定时，最好减少动作。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-02-06 下午4.45.47.png" alt="屏幕快照 2018-02-06 下午4.45.47"></p>
<p>随着决策环境的不确定性从小到大，根据海纳模型，个体行为表现出来的理性程度就应当是从高到底变动。当不确定性极低时，我们观察到个体理性（reason）。如果环境不确定性继续增加，那么就要有行为灵活性的大幅下降。这是出现了行为规范，尤其是存储在社会里的那些规范（群体规范）。许多社会规范，个体虽然很难理解，但因为违反规范可能遭受社会制裁，也就不得不遵守，久而久之，群体生存得越久远，对个体而言这些规范的“合理性”就变得越显明。哈耶克倾向于将这样的理性称为“演化理性”，与完全基于个体经验的“建构理性”相对峙。</p>
<p>社会影响通过三种途径施于个人：其一是模仿，尤其是对成功者的模仿。其二是教化，学校的教育，家庭的教育，还有在职培训和自学等方式。其三是遗传，许多行为是遗传决定的，你很难改变这些行为，例如你的人格。晚近发表的对大批受试者的长时期跟踪调查表明，一个人的性格，如果用人格五要素模型来测度，在25岁至83岁这段时间里，他在各维度上的得分，很少发生改变。</p>
<p>社会结构里存储了大量的决策知识，让个体能够应付极不确定的决策环节。根据海纳模型，在最高不确定的决策环境里，我们只能依靠文化传统。我们的传统延续了数千年，在这样漫长的时间里，必定会遭遇比目前严重数倍的危机。但依旧延续到今天，这就意味着它“管用”，虽然可能令人迷惑。哈耶克说，我们是我们 传统的选择，而不是我们选择了我们的传统。传统越久远，它的合理性就越不能根据一时一事是否合理来评价。我们适应了自己的传统，在这一意义上我们是被传统选择的，并因此而更可能生存下去。</p>
<blockquote>
<p>传统：</p>
</blockquote>
<p>希尔斯的《论传统》及其序言对传统的定义如下：“传统一词的拉丁文为traditum，意即从过去延传到现在的事物······延传三代以上的、被人类赋予价值和意义的事物都可以看作是传统。······可以看出，这种意义上的传统概念与文化人类学家所使用的“大文化”概念是一致的······不过“传统”一词还有一种更特殊的内涵，即指一条世代相传的事物之变体链，也就是说，围绕一个或几个被接受和延传的主题而形成的不同变体的事物之变体链，也就是说，围绕一个或几个被接受和延传的主题而形成的不同变体的一条时间链。这样，一种宗教信仰、一种哲学思想、一种艺术风格、一种社会制度，在其代代相传的过程中既发生了种种变异，又保持了某种共同的主题······传统是一种社会的文化遗产，是人类过去所创造的种种制度、信仰、价值观念和行为方式等构成的表意象征；它使代与代之间、一个历史阶段与另一个历史阶段之间保持了某种连续性和同一性，构成了一个社会创造与再创造自己的文化密码，并且给人类生存带来了秩序和意义。”</p>
<p>韦伯的“克里斯玛”学说：马克思·韦伯指出，克里斯玛式人格是历史上尤其富有创造性的革命力量。由于克里斯玛统治下的法规来自于高度个人化的对神思的体验，以及神一般的英雄力量，并且为了体现先知式的对神思的体验，以及神一般的英雄力量，并且为了体现先知式的精神气质而排除了所有外在秩序，所以克里斯玛统治以一种革命的方式转变了所有价值观，破除了所有传统的和理性的规范。这就是说，不仅创建一种传统需要非凡的克里斯玛式的想象力（当然也需要魄力，记忆力和推理能力），而且破除一种传统同样离不开克里斯玛特质，甚至需要有双倍的克里斯玛特质。因为破除一种传统必须同时创建一种更适合时宜和环境的、也更富于想象力的新传统；只有在新传统的克里斯玛力量压倒了旧传统的习惯势力之后，旧传统才会逐渐退出历史舞台。</p>
<p>学术界公认，希尔斯的重要贡献是论证了“传统”具有某种克里斯玛特性。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-02-06 下午6.28.06.png" alt="屏幕快照 2018-02-06 下午6.28.06"></p>
<p>文化传统的演变，可纳入上图的框架。</p>
<p>物质生活：人类社会从游猎到农耕到工业社会和后工业社会的技术进步，都可置于这一维度上；</p>
<p>社会情感：随着社会结构的演变，例如，从皇权的社会演变为民主或现代的社会，人们的情感也发生改变。在人类情感当中，几乎没有不是社会的情感。可能有例外，就是“宗教感”。在今天，学术界达成共识：信仰是纯粹的私人事件。除了信仰之外，其他几乎可以列举出来的情感，都是社会情感。</p>
<p>精神生活：这里可以发生导致社会变迁的重要力量。例如在欧洲社会变迁过程中，如韦伯论证的那样，宗教和信仰的改变，有决定性的意义。</p>
<p>文化表达过程中社会网络和文化资本对文化表达几乎处处存在重要的影响，这就引导我们到行为经济学和社会学交叉的一个界面——所谓“社会资本”问题。</p>
<blockquote>
<p>社会资本：</p>
</blockquote>
<p>社会资本就是一个社会网络内存在的全部有利于囚徒困境合作解的那些因素。</p>
<p>全部社会科学的基本问题，在过去的一个世纪里，是要解释“社会何以可能”。全部行为经济学的基本问题，至今为止，是要解释“合作何以可能”。社会网络的社会资本定义是内生演化的社会资本，注意，这里的社会资本不再是给定的，它是内生的，是演化的。如果合作的人群规模太小，那么它会完全消失。如果合作规模超过了例如三分之一定律所要求的范围，那么它会扩展到全部网络。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 行为 </tag>
            
            <tag> 价值 </tag>
            
            <tag> 判断 </tag>
            
            <tag> 传统 </tag>
            
            <tag> 海纳模型 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（11）：存在主义书单]]></title>
      <url>/2018/01/25/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8811%EF%BC%89%EF%BC%9A%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E4%B9%A6%E5%8D%95/</url>
      <content type="html"><![CDATA[<p>存在主义相关书籍整理。</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1463876/" target="_blank" rel="noopener">存在主义</a></td>
<td>[美] W. 考夫曼 编著</td>
<td>商务印书馆</td>
<td>8.3 (163人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10575149/" target="_blank" rel="noopener">存在主义</a></td>
<td>[美]科珀</td>
<td>复旦大学出版社</td>
<td>8.4 (41人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/27170538/" target="_blank" rel="noopener">存在主义咖啡馆</a></td>
<td>[英]  莎拉·贝克韦尔</td>
<td>北京联合出版公司</td>
<td>9.0 (567人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2222359/" target="_blank" rel="noopener">非理性的人</a></td>
<td>[美] 威廉·巴雷特</td>
<td>上海译文出版社</td>
<td>9.0(463人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3034229/" target="_blank" rel="noopener">存在主义简论</a></td>
<td>[美]托马斯·R，弗林</td>
<td>外语教学与研究出版社</td>
<td>8.2 (284人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2051548/" target="_blank" rel="noopener">人的奴役与自由</a></td>
<td>[俄]尼古拉·别尔嘉耶夫</td>
<td>贵州人民出版社</td>
<td>8.9 (105人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1038306/" target="_blank" rel="noopener">思·史·诗</a></td>
<td>[中]叶秀山</td>
<td>人民出版社</td>
<td>8.6 (45人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1127271/" target="_blank" rel="noopener">另类人</a></td>
<td>[美]科林·威尔逊</td>
<td>经济日报出版社</td>
<td>8.3 (100人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1907590/" target="_blank" rel="noopener">从存在到存在者</a></td>
<td>[法]埃马纽埃尔·列维纳斯</td>
<td>江苏教育出版社</td>
<td>8.8 (141人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4314074/" target="_blank" rel="noopener">在生命最深处与人相遇</a></td>
<td>[美]朱瑟琳·乔塞尔森</td>
<td>机械工业</td>
<td>7.8 (186人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1051659/" target="_blank" rel="noopener">我与你</a></td>
<td>[德] 马丁·布伯</td>
<td>生活·读书·新知三联书店</td>
<td>8.9 (489人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2184710/" target="_blank" rel="noopener">生命的悲剧意识</a></td>
<td>[西]乌纳穆诺</td>
<td>花城出版社</td>
<td>8.2 (258人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="陀思妥耶夫斯基"><a href="#陀思妥耶夫斯基" class="headerlink" title="陀思妥耶夫斯基"></a>陀思妥耶夫斯基</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1856494/" target="_blank" rel="noopener">卡拉马佐夫兄弟</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>9.4(7818人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1022632/" target="_blank" rel="noopener">罪与罚</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>9.1(5075人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1859108/" target="_blank" rel="noopener">白痴</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>8.9(3775人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/21993104/" target="_blank" rel="noopener">白夜</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>8.1(2604人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10759466/" target="_blank" rel="noopener">地下室手记</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>漓江出版社</td>
<td>9.1(1927人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1088418/" target="_blank" rel="noopener">死屋手记</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>人民文学出版社</td>
<td>9.1(1356人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1027002/" target="_blank" rel="noopener">被欺凌与被侮辱的</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>人民文学出版社</td>
<td>8.6(1075人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1437488/" target="_blank" rel="noopener">群魔</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>译林出版社</td>
<td>9.3(691人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5391978/" target="_blank" rel="noopener">作家日记（上下）</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>河北教育出版社</td>
<td>8.4(40人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2140416/" target="_blank" rel="noopener">穷人的美德 : 陀思妥耶夫斯基天才犯罪论集</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>天津人民出版社</td>
<td>7.3(270人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2055794/" target="_blank" rel="noopener">赌徒</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>9.0(217人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3752758/" target="_blank" rel="noopener">少年</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>8.7(165人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3191428/" target="_blank" rel="noopener">舅舅的梦</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>山西人民出版社</td>
<td>8.0(45人评价)</td>
</tr>
<tr>
<td><a href="">人不单靠面包活着 : 陀思妥耶夫斯基书信选</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>8.5(121人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3723571/" target="_blank" rel="noopener">陀思妥耶夫斯基论艺术</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海书店出版社</td>
<td>8.5(89人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6878082/" target="_blank" rel="noopener">托尔斯泰或陀思妥耶夫斯基</a></td>
<td>[美]乔治·斯坦纳</td>
<td>浙江大学出版社</td>
<td>8.8(215人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4149229/" target="_blank" rel="noopener">托尔斯泰与陀思妥耶夫斯基（上下）</a></td>
<td>[俄]梅列日科夫斯基</td>
<td>华夏出版社</td>
<td>9.3(193人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1031244/" target="_blank" rel="noopener">鬼</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>上海译文出版社</td>
<td>9.3(252人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1083030/" target="_blank" rel="noopener">双重人格 地下室手记</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>译林出版社</td>
<td>9.0(687人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3127208/" target="_blank" rel="noopener">女房东</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>文光书店</td>
<td>8.6(11人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5391972/" target="_blank" rel="noopener">穷人</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>河北教育出版社</td>
<td>8.3(82人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3069267/" target="_blank" rel="noopener">陀思妥耶夫斯基的世界观</a></td>
<td>[俄]尼·别尔嘉耶夫</td>
<td>广西师范大学出版社</td>
<td>9.0(187人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2219824/" target="_blank" rel="noopener">文化的哲学</a></td>
<td>[俄]尼·别尔嘉耶夫</td>
<td>上海人民出版社</td>
<td>8.3(44人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1916659/" target="_blank" rel="noopener">关于陀思妥耶夫斯基的六次讲座</a></td>
<td>[俄] 陀思妥耶夫斯基</td>
<td>广西师范大学出版社</td>
<td>8.6(372人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1937449/" target="_blank" rel="noopener">陀思妥耶夫斯基诗学问题 : 复调小说理论</a></td>
<td>[俄]巴赫金</td>
<td>生活·读书·新知三联书店</td>
<td>9.2(236人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/11442818/" target="_blank" rel="noopener">读书与识字 : 陀思妥耶夫斯基读书随笔</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>金城出版社</td>
<td>7.3(32人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/21778431/" target="_blank" rel="noopener">同时代人回忆陀思妥耶夫斯基</a></td>
<td>[俄罗斯]多利宁</td>
<td>广西师范大学出版社</td>
<td>8.7(30人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1023893/" target="_blank" rel="noopener">陀思妥耶夫斯基的三次爱情</a></td>
<td>[美]马克・斯洛尼姆</td>
<td>广西师范大学出版社</td>
<td>7.7(88人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26370027/" target="_blank" rel="noopener">尼采与陀思妥耶夫斯基 : 关于悲剧哲学的随笔</a></td>
<td>[俄]列夫·舍斯托夫</td>
<td>华东师范大学出版社</td>
<td>8.4(55人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1007492/" target="_blank" rel="noopener">在约伯的天平上</a></td>
<td>[俄]舍斯托夫</td>
<td>上海人民出版社</td>
<td>9.0 (163人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1312056/" target="_blank" rel="noopener">陀思妥耶夫斯基哲学 : 系统论述</a></td>
<td>[德] 赖因哈德·劳特</td>
<td>广西师范大学出版社</td>
<td>8.3(64人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2266148/" target="_blank" rel="noopener">伪君子及其崇拜者 : 摘自一个无名氏的回忆录</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>云南人民出版社</td>
<td>8.1(30人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25843679/" target="_blank" rel="noopener">陀思妥耶夫斯基（第1卷）</a></td>
<td>[美] 约瑟夫·弗兰克</td>
<td>广西师范大学出版社</td>
<td>8.8(87人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4838110/" target="_blank" rel="noopener">陀思妥耶夫斯基诗学问题</a></td>
<td>[中]巴赫金</td>
<td>中央编译出版社</td>
<td>8.3(73人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1040092/" target="_blank" rel="noopener">忧郁的先知:陀思妥耶夫斯基</a></td>
<td>[中]冯川</td>
<td>四川人民出版社</td>
<td>8.4(32人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1910461/" target="_blank" rel="noopener">陀思妥耶夫斯基小说艺术研究</a></td>
<td>[中]彭克巽</td>
<td>北京大学出版社</td>
<td>7.7(32人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2350321/" target="_blank" rel="noopener">冬天记的夏天印象</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>人民文学出版社</td>
<td>8.1(11人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26670936/" target="_blank" rel="noopener">诚实的贼</a></td>
<td>[俄]陀思妥耶夫斯基</td>
<td>新星出版社</td>
<td>8.6(20人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1833520/" target="_blank" rel="noopener">宗教文化语境下的陀思妥耶夫斯基诗学</a></td>
<td>[中]王志耕</td>
<td>北京师范大学出版社</td>
<td>8.4(29人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2297973/" target="_blank" rel="noopener">陀思妥耶夫斯基与世界文学</a></td>
<td>[俄]弗里德连杰尔</td>
<td>上海译文出版社</td>
<td>8.3(11人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10540819/" target="_blank" rel="noopener">巴登夏日 : 关于陀思妥耶夫斯基的一切</a></td>
<td>[俄]列昂尼德·茨普金</td>
<td>南海出版公司</td>
<td>8.1(191人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3546274/" target="_blank" rel="noopener">精神领袖 : 俄罗斯思想家论陀思妥耶夫斯基</a></td>
<td>[俄]索洛维约夫 等著</td>
<td>上海译文出版社</td>
<td>9.4(99人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5269156/" target="_blank" rel="noopener">道德·上帝与人 : 陀思妥耶夫斯基的问题</a></td>
<td>[中]何怀宏</td>
<td>北京大学出版社</td>
<td>8.8(60人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1287628/" target="_blank" rel="noopener">漂泊的灵魂 : 陀思妥耶夫斯基与俄罗斯传统文化</a></td>
<td>[中]赵桂莲</td>
<td>北京大学出版社</td>
<td>8.0(31人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2101439/" target="_blank" rel="noopener">三大师 : 巴尔扎克 狄更斯 陀思妥耶夫斯基</a></td>
<td>[奥地利]茨威格</td>
<td>安徽文艺出版社</td>
<td>8.8(24人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/21778487/" target="_blank" rel="noopener">安娜·陀思妥耶夫斯卡娅回忆录</a></td>
<td>[俄]安娜·陀思妥耶夫斯卡娅</td>
<td>广西师范大学出版社</td>
<td>8.6(80人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/21778060/" target="_blank" rel="noopener">一八六七年日记</a></td>
<td>[俄]安娜·陀思妥耶夫斯卡娅</td>
<td>广西师范大学出版社</td>
<td>7.8(29人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="列夫·托尔斯泰"><a href="#列夫·托尔斯泰" class="headerlink" title="列夫·托尔斯泰"></a>列夫·托尔斯泰</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1255288/" target="_blank" rel="noopener">复活</a></td>
<td>[俄]列夫·托尔斯泰</td>
<td>人民文学出版社</td>
<td>8.0(15307人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1013469/" target="_blank" rel="noopener">战争与和平</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>人民文学出版社</td>
<td>8.7(4791人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2253380/" target="_blank" rel="noopener">安娜·卡列尼娜</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>9.2(2708人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2266567/" target="_blank" rel="noopener">伊凡·伊里奇的死</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>山西人民出版社</td>
<td>9.2(513人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1959124/" target="_blank" rel="noopener">生活之路</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>中国人民大学出版社</td>
<td>8.9(274人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4841506/" target="_blank" rel="noopener">忏悔录</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>中国对外翻译</td>
<td>8.4(332人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1445012/" target="_blank" rel="noopener">艺术论</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>中国人民大学出版社</td>
<td>8.4(80人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1833518/" target="_blank" rel="noopener">生活值得过吗</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>中国发展出版社</td>
<td>8.2(107人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6863526/" target="_blank" rel="noopener">克莱采奏鸣曲</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>译林出版社</td>
<td>8.7(114人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3076983/" target="_blank" rel="noopener">哈吉穆拉特</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.7(102人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3076990/" target="_blank" rel="noopener">哥萨克</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.8(90人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10742716/" target="_blank" rel="noopener">忏悔录</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>译林出版社</td>
<td>8.9(133人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3076988/" target="_blank" rel="noopener">童年·少年·青年</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.4(92人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3076987/" target="_blank" rel="noopener">一个地主的早晨</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.5(59人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1232992/" target="_blank" rel="noopener">托尔斯泰传</a></td>
<td>罗曼·罗兰</td>
<td>商务印书馆</td>
<td>7.7(93人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25894989/" target="_blank" rel="noopener">生活之路</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>商务印书馆</td>
<td>8.0(25人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1186367/" target="_blank" rel="noopener">托尔斯泰中短篇小说选</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>人民文学出版社</td>
<td>9.2(57人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3741314/" target="_blank" rel="noopener">生活即幸福</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>长江文艺出版社</td>
<td>8.3(33人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1896422/" target="_blank" rel="noopener">论科学和艺术的价值</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>江苏教育出版社</td>
<td>7.7(18人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3626317/" target="_blank" rel="noopener">家庭的幸福</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>浙江人民出版社</td>
<td>9.3(18人评价)</td>
</tr>
<tr>
<td><a href="">谢尔基神父</a></td>
<td></td>
<td>四川人民出版社</td>
<td>8.1(26人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1079617/" target="_blank" rel="noopener">哥萨克</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.8(33人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1327345/" target="_blank" rel="noopener">自画像 : 卡萨诺瓦、司汤达、托尔斯泰</a></td>
<td>[奥]斯蒂芬 茨威格</td>
<td>西苑出版社</td>
<td>8.5(52人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1062826/" target="_blank" rel="noopener">一个地主的早晨</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>上海文艺出版社</td>
<td>8.6(25人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1125262/" target="_blank" rel="noopener">托尔斯泰忏悔录</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>华文出版社</td>
<td>8.6(203人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/11636793/" target="_blank" rel="noopener">歌德与托尔斯泰</a></td>
<td>[德]托马斯·曼</td>
<td>浙江大学出版社</td>
<td>8.1(51人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/19980346/" target="_blank" rel="noopener">伊万·伊里奇之死</a></td>
<td>[俄] 列夫·托尔斯泰</td>
<td>江苏文艺出版社</td>
<td>9.0(104人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="索伦-克尔凯郭尔"><a href="#索伦-克尔凯郭尔" class="headerlink" title="索伦.克尔凯郭尔"></a>索伦.克尔凯郭尔</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/3987027/" target="_blank" rel="noopener">非此即彼(上卷)</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>8.6 (306人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3987029/" target="_blank" rel="noopener">非此即彼(下卷)</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>9.3 (91人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1506334/" target="_blank" rel="noopener">论反讽概念 : 以苏格拉底为主线</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>9.2 (158人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/24697397/" target="_blank" rel="noopener">畏惧与颤栗 恐惧的概念 致死的疾病 </a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>9.5 (98人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26285475/" target="_blank" rel="noopener">爱的作为 : 克尔凯郭尔文集7</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>9.3 (29人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/24697574/" target="_blank" rel="noopener">哲学片断</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国社会科学出版社</td>
<td>8.7 (48人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1015145/" target="_blank" rel="noopener">十八训导书</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国工人出版社</td>
<td>7.9 (70人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1019043/" target="_blank" rel="noopener">哲学寓言集</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>商务印书馆</td>
<td>8.3 (117人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3737103/" target="_blank" rel="noopener">克尔凯郭尔</a></td>
<td>[英]加迪纳</td>
<td>译林出版社</td>
<td>7.8 (200人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1044502/" target="_blank" rel="noopener">颤栗与不安</a></td>
<td>[丹麦] 克尔凯郭尔</td>
<td>陕西师范大学出版社</td>
<td>8.0 (275人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1462846/" target="_blank" rel="noopener">克尔凯戈尔日记选</a></td>
<td>[丹] 彼德·P. 罗德 编</td>
<td>上海社会科学院出版社</td>
<td>8.8 (265人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1008757/" target="_blank" rel="noopener">致死的疾病</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中国工人出版社</td>
<td>9.0 (255人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1020296/" target="_blank" rel="noopener">恐惧与颤栗</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>华夏出版社</td>
<td>8.6 (408人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1776823/" target="_blank" rel="noopener">勾引家日记</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>作家出版社</td>
<td>8.3(128人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1011746/" target="_blank" rel="noopener">基督徒的激情</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>中央编译出版社</td>
<td>7.9(203人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1219658/" target="_blank" rel="noopener">百合·飞鸟·女演员</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>华夏出版社</td>
<td>7.8(151人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/27052079/" target="_blank" rel="noopener">人生道路诸阶段</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>商务印书馆</td>
<td>9.2(26人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1848182/" target="_blank" rel="noopener">论怀疑者</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>上海人民出版社</td>
<td>8.3(94人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25982699/" target="_blank" rel="noopener">克尔凯郭尔日记选</a></td>
<td>[丹]彼得•P.罗德</td>
<td>商务印书馆</td>
<td>8.3(86人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2974191/" target="_blank" rel="noopener">克尔凯郭尔：审美对象的建构</a></td>
<td>[德]T.W.阿多诺</td>
<td>人民出版社</td>
<td>7.6(30人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1784973/" target="_blank" rel="noopener">重复</a></td>
<td>[丹麦] 索伦.克尔凯郭尔</td>
<td>百花文艺出版社</td>
<td>8.1(56人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3243814/" target="_blank" rel="noopener">绝望的一跃 : 孤独天才克尔恺郭尔</a></td>
<td>[中]林和生</td>
<td>华文出版社</td>
<td>7.0(73人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1037836/" target="_blank" rel="noopener">克尔凯戈尔入门</a></td>
<td>[美]唐纳德・帕尔默</td>
<td>东方出版社</td>
<td>8.7(45人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1080005/" target="_blank" rel="noopener">旷野呼告</a></td>
<td>[俄] 列夫·舍斯托夫</td>
<td>华夏出版社</td>
<td>8.5(60人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1199056/" target="_blank" rel="noopener">克尔恺廓尔</a></td>
<td>[美]苏珊·李·安德森</td>
<td>中华书局</td>
<td>8.6(65人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1219218/" target="_blank" rel="noopener">不幸与幸福</a></td>
<td>[美]尼尔斯・托马森</td>
<td>华夏出版社</td>
<td>9.1(23人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="卡尔·雅思贝尔斯"><a href="#卡尔·雅思贝尔斯" class="headerlink" title="卡尔·雅思贝尔斯"></a>卡尔·雅思贝尔斯</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1482446/" target="_blank" rel="noopener">生存哲学</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>上海译文出版社</td>
<td>7.6 (88人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1315260/" target="_blank" rel="noopener">时代的精神状况</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>上海译文出版社</td>
<td>8.7 (230人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2082154/" target="_blank" rel="noopener">历史的起源与目标</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>华夏出版社</td>
<td>8.7(129人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1451698/" target="_blank" rel="noopener">什么是教育</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>生活·读书·新知三联书店</td>
<td>8.9(268人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2068955/" target="_blank" rel="noopener">大学之理念</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>上海人民出版社</td>
<td>8.9(228人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3878639/" target="_blank" rel="noopener">大哲学家</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>社会科学文献出版社</td>
<td>9.0(30人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3222032/" target="_blank" rel="noopener">苏格拉底、佛陀、孔子和耶稣</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>安徽文艺出版社</td>
<td>8.2(28人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1078603/" target="_blank" rel="noopener">尼采 : 其人其说</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>社会科学文献出版社</td>
<td>8.4(48人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/20387640/" target="_blank" rel="noopener">海德格尔与雅斯贝尔斯往复书简</a></td>
<td>[德]瓦尔特·比默尔 等编</td>
<td>上海人民出版社</td>
<td>8.2(37人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2026930/" target="_blank" rel="noopener">悲剧的超越</a></td>
<td>[德]卡尔·雅斯贝尔斯</td>
<td>工人出版社</td>
<td>8.8(71人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3227944/" target="_blank" rel="noopener">雅斯贝尔斯 : 大哲学家的生活与思想</a></td>
<td>[德]叔斯勒</td>
<td>中国人民大学出版社</td>
<td>7.5(18人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="尼采"><a href="#尼采" class="headerlink" title="尼采"></a>尼采</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1949061/" target="_blank" rel="noopener">不合时宜的沉思</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>8.5 (426人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1063852/" target="_blank" rel="noopener">悲剧的诞生 : 尼采美学文选</a></td>
<td>[德]尼采</td>
<td>生活·读书·新知三联书店</td>
<td>8.8(4002人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2359052/" target="_blank" rel="noopener">查拉图斯特拉如是说</a></td>
<td>[德]尼采</td>
<td>生活·读书·新知三联书店</td>
<td>8.9(4885人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1021702/" target="_blank" rel="noopener">偶像的黄昏</a></td>
<td>[德]尼采</td>
<td>光明日报出版社</td>
<td>8.6(1146人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1048672/" target="_blank" rel="noopener">尼采生存哲学</a></td>
<td>[德]尼采</td>
<td>九州出版社</td>
<td>8.0(1476人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2045704/" target="_blank" rel="noopener">权力意志 : 1885-1889年遗稿</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>8.9(522人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3016983/" target="_blank" rel="noopener">人性的，太人性的</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>9.1(441人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1949782/" target="_blank" rel="noopener">快乐的科学</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>8.6(667人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1245537/" target="_blank" rel="noopener">苏鲁支语录</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>9.2(658人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26264205/" target="_blank" rel="noopener">道德的谱系</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>9.3(148人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1057852/" target="_blank" rel="noopener">疯狂的意义 : 尼采超人哲学集</a></td>
<td>[德]尼采</td>
<td>陕西师范大学出版社</td>
<td>8.4(443人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1958791/" target="_blank" rel="noopener">朝霞</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>9.1(366人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1888070/" target="_blank" rel="noopener">尼采诗集</a></td>
<td>[德]尼采</td>
<td>中国文联出版社</td>
<td>8.3(318人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2008074/" target="_blank" rel="noopener">希腊悲剧时代的哲学</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>8.6(261人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10485841/" target="_blank" rel="noopener">作为教育家的叔本华</a></td>
<td>[德]尼采</td>
<td>译林出版社</td>
<td>8.8(267人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1048147/" target="_blank" rel="noopener">反基督 : 尼采论宗教文选</a></td>
<td>[德]尼采</td>
<td>河北教育出版社</td>
<td>8.2(140人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/23020036/" target="_blank" rel="noopener">重估一切价值</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>9.1(81人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/7056695/" target="_blank" rel="noopener">论我们教育机构的未来</a></td>
<td>[德]尼采</td>
<td>译林出版社</td>
<td>9.0(140人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6838057/" target="_blank" rel="noopener">瓦格纳事件 尼采反瓦格纳 : 尼采反瓦格纳</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>8.7(101人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26663535/" target="_blank" rel="noopener">善恶的彼岸</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>9.3(81人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10862503/" target="_blank" rel="noopener">狄俄尼索斯颂歌 : 经典与解释. 尼采注疏集</a></td>
<td>[德]尼采</td>
<td>华东师范大学出版社</td>
<td>9.0(50人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2134760/" target="_blank" rel="noopener">历史对于人生的利弊</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>8.8(64人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1851383/" target="_blank" rel="noopener">哲学与真理 : 尼采1872-1876年笔记选</a></td>
<td>[德]尼采</td>
<td>上海社会科学院出版社</td>
<td>9.2(60人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26782124/" target="_blank" rel="noopener">瞧！这个人</a></td>
<td>[德]尼采</td>
<td>商务印书馆</td>
<td>8.8(98人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4162698/" target="_blank" rel="noopener">生命的意志</a></td>
<td>[德]尼采</td>
<td>长江文艺</td>
<td>8.3(78人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10566744/" target="_blank" rel="noopener">尼采读本</a></td>
<td>[德]尼采</td>
<td>作家出版社</td>
<td>8.0(77人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1079104/" target="_blank" rel="noopener">尼采 : 在世纪的转折点上</a></td>
<td>周国平</td>
<td>上海人民出版社</td>
<td>8.6(3452人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2131382/" target="_blank" rel="noopener">尼采</a></td>
<td>[丹]乔治·勃兰兑斯</td>
<td>中国社会科学出版社</td>
<td>8.7(177人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1027608/" target="_blank" rel="noopener">我妹妹与我 : 尼采佚失的最后告白</a></td>
<td>[德]尼采</td>
<td>文化艺术出版社</td>
<td>8.0(448人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1448961/" target="_blank" rel="noopener">艺术与归家 : 尼采、海德格尔、福柯</a></td>
<td>余虹</td>
<td>中国人民大学出版社</td>
<td>8.9(187人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/19963460/" target="_blank" rel="noopener">与魔鬼作斗争 : 荷尔德林、克莱斯特、尼采</a></td>
<td>[奥地利] 斯蒂芬·茨威格</td>
<td>译林出版社</td>
<td>8.8(150人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1426732/" target="_blank" rel="noopener">尼采反卢梭 : 尼采的道德政治思想研究</a></td>
<td>凯斯.安塞尔-皮尔逊</td>
<td>华夏出版社</td>
<td>8.5(80人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5501445/" target="_blank" rel="noopener">幻觉的哲学 : 尼采八十年代手稿研究</a></td>
<td>[丹麦]哈斯</td>
<td>东方出版社</td>
<td>8.1(34人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1084691/" target="_blank" rel="noopener">墙上的书写 : 尼采与基督教</a></td>
<td>洛维特</td>
<td>华夏出版社</td>
<td>8.1(75人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1315167/" target="_blank" rel="noopener">历史的用途与滥用</a></td>
<td>[德]尼采</td>
<td>上海人民出版社</td>
<td>9.0(449人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1048524/" target="_blank" rel="noopener">尼采传 : 一个特立独行者的一生</a></td>
<td>丹尼尔・哈列维</td>
<td>贵州人民出版社</td>
<td>8.1(581人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1768046/" target="_blank" rel="noopener">从黑格尔到尼采 : 19世纪思维中的革命性决裂</a></td>
<td>卡尔·洛维特</td>
<td>生活·读书·新知三联书店</td>
<td>8.6(233人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1028095/" target="_blank" rel="noopener">缪斯的痛苦与激情 : 尼采、里尔克与萨乐美</a></td>
<td>周濂</td>
<td>社会科学文献出版社</td>
<td>7.3(22人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1531707/" target="_blank" rel="noopener">解读尼采 : 尼采哲学导读图</a></td>
<td>[法]吉尔·都鲁兹</td>
<td>百花文艺出版社</td>
<td>8.6(55人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1020182/" target="_blank" rel="noopener">尼采与柏拉图主义 : 思想与社会丛书</a></td>
<td>吴增定</td>
<td>上海人民出版社</td>
<td>8.5(335人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3097364/" target="_blank" rel="noopener">尼采与形而上学</a></td>
<td>周国平</td>
<td>新世界出版社</td>
<td>8.4(370人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1026171/" target="_blank" rel="noopener">尼采与哲学</a></td>
<td>[法] 吉尔·德勒兹</td>
<td>社会科学文献出版社</td>
<td>8.7(196人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25906249/" target="_blank" rel="noopener">导读尼采</a></td>
<td>李·斯平克斯</td>
<td>重庆大学出版社</td>
<td>8.6(131人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1940994/" target="_blank" rel="noopener">叔本华与尼采 : 一组演讲</a></td>
<td>格奥尔格·西美尔</td>
<td>上海译文出版社</td>
<td>8.8(96人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1958794/" target="_blank" rel="noopener">尼采思想传记</a></td>
<td>萨弗兰斯基</td>
<td>华东师范大学出版社</td>
<td>8.3(178人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1456201/" target="_blank" rel="noopener">施特劳斯与尼采</a></td>
<td>朗佩特</td>
<td>上海三联书店</td>
<td>8.5(91人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2568214/" target="_blank" rel="noopener">尼采与身体</a></td>
<td>汪民安</td>
<td>北京大学出版社</td>
<td>7.5(107人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3344643/" target="_blank" rel="noopener">尼采的使命 : 《善恶的彼岸》绎读</a></td>
<td>[美]朗佩特</td>
<td>华夏出版社</td>
<td>8.6(50人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/21330707/" target="_blank" rel="noopener">尼采传</a></td>
<td>玛克西米利安</td>
<td>云南美术出版社</td>
<td>9.0(42人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1795465/" target="_blank" rel="noopener">悲剧哲学家尼采</a></td>
<td>陈鼓应</td>
<td>上海人民出版社</td>
<td>7.7(169人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26911505/" target="_blank" rel="noopener">尼采：生命之为文学</a></td>
<td>[美] 亚历山大•内哈马斯</td>
<td>浙江大学出版社</td>
<td>9.5(15人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1795466/" target="_blank" rel="noopener">尼采新论</a></td>
<td>陈鼓应</td>
<td>上海人民出版社</td>
<td>7.6(90人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1313148/" target="_blank" rel="noopener">审美主义 : 从尼采到福柯</a></td>
<td>李晓林</td>
<td>社会科学文献出版社</td>
<td>8.0(35人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="海德格尔"><a href="#海德格尔" class="headerlink" title="海德格尔"></a>海德格尔</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/20508943/" target="_blank" rel="noopener">海德格尔</a></td>
<td>乔治·斯坦纳</td>
<td>浙江大学出版社</td>
<td>8.4(66人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1054907/" target="_blank" rel="noopener">海德格尔传</a></td>
<td>吕迪格尔·萨弗兰斯基</td>
<td>商务印书馆</td>
<td>8.5(93人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/22690655/" target="_blank" rel="noopener">海德格尔</a></td>
<td>[英国] 迈克尔·英伍德</td>
<td>译林出版社</td>
<td>7.7(50人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25894124/" target="_blank" rel="noopener">海德格尔</a></td>
<td>帕特里夏·奥坦伯德·约翰逊</td>
<td>中华书局</td>
<td>8.2(33人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1783111/" target="_blank" rel="noopener">存在与时间</a></td>
<td>[德] 马丁·海德格尔</td>
<td>生活·读书·新知三联书店</td>
<td>8.7  /  3305人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25865394/" target="_blank" rel="noopener">《存在与时间》释义</a></td>
<td>张汝伦</td>
<td>上海人民出版社</td>
<td></td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1030294/" target="_blank" rel="noopener">林中路</a></td>
<td>[德] 马丁·海德格尔</td>
<td>上海译文出版社</td>
<td>8.8  /  913人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1439792/" target="_blank" rel="noopener">形而上学导论</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>8.6  /  565人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1185730/" target="_blank" rel="noopener">海德格尔存在哲学</a></td>
<td>[德] 马丁·海德格尔</td>
<td>九州出版社</td>
<td>8.2 (136人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1052276/" target="_blank" rel="noopener">路标</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>8.9 (352人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1034534/" target="_blank" rel="noopener">人，诗意地安居</a></td>
<td>[德] 马丁·海德格尔</td>
<td>广西师范大学出版社</td>
<td>8.1 (539人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1074329/" target="_blank" rel="noopener">尼采（上下）</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>8.8  /  423人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1228316/" target="_blank" rel="noopener">荷尔德林诗的阐释</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>8.7  /  392人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1005360/" target="_blank" rel="noopener">面向思的事情</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>8.7  /  350人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1444141/" target="_blank" rel="noopener">演讲与论文集</a></td>
<td>[德] 马丁·海德格尔</td>
<td>生活·读书·新知三联书店</td>
<td>9.1  /  310人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3226085/" target="_blank" rel="noopener">论真理的本质  </a></td>
<td>[德] 马丁·海德格尔</td>
<td>华夏出版社</td>
<td>9.3  /  129人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2969222/" target="_blank" rel="noopener">现象学之基本问题</a></td>
<td>[德] 马丁·海德格尔</td>
<td>上海译文出版社</td>
<td>9.0  /  128人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1054203/" target="_blank" rel="noopener">尼采十讲 </a></td>
<td>[德] 马丁·海德格尔</td>
<td>中国言实出版社</td>
<td>7.6  /  108人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4814377/" target="_blank" rel="noopener">同一与差异</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.1  /  96人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10748231/" target="_blank" rel="noopener">哲学论稿</a></td>
<td>[德]马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.3  /  90人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4262651/" target="_blank" rel="noopener">物的追问</a></td>
<td>[德]马丁·海德格尔</td>
<td>上海译文出版社</td>
<td>9.1  /  85人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3277048/" target="_blank" rel="noopener">思的经验</a></td>
<td>[德]马丁·海德格尔</td>
<td>人民出版社</td>
<td>8.0  /  75人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4908875/" target="_blank" rel="noopener">康德与形而上学疑难</a></td>
<td>[德]马丁·海德格尔</td>
<td>上海译文出版社</td>
<td>9.4  /  75人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3587689/" target="_blank" rel="noopener">时间概念史导论</a></td>
<td>[德]马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.1  /  71人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3507033/" target="_blank" rel="noopener">系于孤独之途</a></td>
<td>[德]马丁·海德格尔</td>
<td>天津人民出版社</td>
<td>8.2  /  62人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1184348/" target="_blank" rel="noopener">荷尔德林的新神话</a></td>
<td>[德] 马丁·海德格尔</td>
<td>华夏出版社</td>
<td>8.2  /  37人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3773128/" target="_blank" rel="noopener">存在论 </a></td>
<td>[德]马丁·海德格尔</td>
<td>人民出版社</td>
<td>8.3  /  37人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26831802/" target="_blank" rel="noopener">根据律</a></td>
<td>[德] 马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.6  /  16人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26687672/" target="_blank" rel="noopener">在通向语言的途中 </a></td>
<td>[德]马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.3  /  10人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26641578/" target="_blank" rel="noopener">柏拉图的《智者》</a></td>
<td>[德]马丁·海德格尔</td>
<td>商务印书馆</td>
<td>9.4 (19人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25908551/" target="_blank" rel="noopener">亚里士多德哲学的基本概念</a></td>
<td>[德] 马丁·海德格尔</td>
<td>华夏出版社</td>
<td>9.1(22人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/series/24683" target="_blank" rel="noopener">海德格尔文集</a></td>
<td>[德] 马丁·海德格尔</td>
<td>华夏出版社</td>
<td>9.1  /  22人评价</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4011409/" target="_blank" rel="noopener">阿伦特与海德格尔 : 爱和思的故事</a></td>
<td>安东尼娅·格鲁嫩贝格</td>
<td>商务印书馆</td>
<td>7.6(175人评价)</td>
</tr>
<tr>
<td><a href="http://book.douban.com/subject/1084250/" target="_blank" rel="noopener">《存在与时间》读本</a></td>
<td>陈嘉映</td>
<td>三联书店</td>
<td>8.5(114人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2281398/" target="_blank" rel="noopener">海德格尔思想与中国天道</a></td>
<td>张祥龙</td>
<td>生活·读书·新知三联书店</td>
<td>8.6(153人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4062211/" target="_blank" rel="noopener">还原与给予 : 胡塞尔、海德格尔与现象学研究</a></td>
<td>[法] 让-吕克·马里翁</td>
<td>上海译文出版社</td>
<td>9.5(52人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4230644/" target="_blank" rel="noopener">分道而行 : 卡尔纳普、卡西尔和海德格尔</a></td>
<td>[美] 迈克尔·弗里德曼 / 张卜天</td>
<td>北京大学出版社</td>
<td>8.9(112人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1685331/" target="_blank" rel="noopener">海德格尔哲学概论</a></td>
<td>陈嘉映</td>
<td>北京三联书店</td>
<td>8.5(144人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4891166/" target="_blank" rel="noopener">策兰与海德格尔 : 一场悬而未决的对话：1951－1970</a></td>
<td>[美] 詹姆斯·K. 林恩</td>
<td>北京大学出版社</td>
<td>7.8(92人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3618179/" target="_blank" rel="noopener">海德格尔与其思想的开端 : 海德格尔年鉴 第一卷</a></td>
<td>[法]阿尔弗雷德·登克尔</td>
<td>商务印书馆</td>
<td>9.0(37人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2080776/" target="_blank" rel="noopener">海德格尔的根 : 尼采，国家社会主义和希腊人</a></td>
<td>[美]查尔斯·巴姆巴赫</td>
<td>上海书店出版社</td>
<td>8.0(56人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2117001/" target="_blank" rel="noopener">海德格尔与伦理学问题</a></td>
<td>韩潮</td>
<td>同济大学出版社</td>
<td>8.6(65人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4038589/" target="_blank" rel="noopener">存在的一代 : 海德格尔哲学在法国1927-1961</a></td>
<td>伊森•克莱因伯格</td>
<td>新星出版社</td>
<td>8.5(41人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26334505/" target="_blank" rel="noopener">海德格尔与哲学的开端</a></td>
<td>王庆节</td>
<td>生活·读书·新知三联书店</td>
<td>7.9(77人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1043194/" target="_blank" rel="noopener">说不可说之神秘 : 海德格尔后期思想研究</a></td>
<td>孙周兴</td>
<td>生活·读书·新知上海三联出版社</td>
<td>8.7(31人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25891896/" target="_blank" rel="noopener">论精神 : 海德格尔与问题</a></td>
<td>[法]雅克·德里达</td>
<td>上海译文出版社</td>
<td>9.1(21人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3107666/" target="_blank" rel="noopener">时间性：自身与他者 : 从胡塞尔、海德格尔到列维纳斯</a></td>
<td>王恒</td>
<td>江苏人民出版社</td>
<td>7.7(22人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10523191/" target="_blank" rel="noopener">时间与永恒 : 论海德格尔哲学中的时间问题</a></td>
<td>黄裕生</td>
<td>江苏人民出版社</td>
<td>9.3(24人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/25866227/" target="_blank" rel="noopener">时间与存在 : 胡塞尔与海德格尔现象学的基本问题</a></td>
<td>方向红</td>
<td>商务印书馆</td>
<td>8.4(33人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="加缪"><a href="#加缪" class="headerlink" title="加缪"></a>加缪</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1082469/" target="_blank" rel="noopener">加缪全集（全四册）</a></td>
<td>柳鸣九 / 沈志明 主编</td>
<td>河北教育出版社</td>
<td>9.3 (410人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1070246/" target="_blank" rel="noopener">置身于苦难与阳光之间</a></td>
<td>[法]加缪</td>
<td>上海三联书店</td>
<td>8.6 (1147人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2143761/" target="_blank" rel="noopener">西西弗的神话</a></td>
<td>[法]加缪</td>
<td>天津人民出版社</td>
<td>8.6 (770人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2255873/" target="_blank" rel="noopener">局外人 鼠疫</a></td>
<td>[法]加缪</td>
<td>译林出版社</td>
<td>9.0 (1591人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1266588/" target="_blank" rel="noopener">加缪</a></td>
<td>理查德·坎伯</td>
<td>中华书局</td>
<td>7.4 (129人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5951356/" target="_blank" rel="noopener">荒谬的自由</a></td>
<td>[法]加缪</td>
<td>江苏文艺出版社</td>
<td>8.2 (73人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1785371/" target="_blank" rel="noopener">正义者</a></td>
<td>[法]加缪</td>
<td>漓江出版社</td>
<td>9.2 (121人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1032706/" target="_blank" rel="noopener">第一个人</a></td>
<td>[法]阿尔贝・加缪</td>
<td>译林出版社</td>
<td>8.8 (172人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1951131/" target="_blank" rel="noopener">卡里古拉</a></td>
<td>[法]阿尔贝・加缪</td>
<td>桂冠</td>
<td>9.4 (171人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4317958/" target="_blank" rel="noopener">加缪传</a></td>
<td>[法] 奥利维·托德</td>
<td>商务印书馆</td>
<td>8.5 (76人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6021091/" target="_blank" rel="noopener">反与正·婚礼集·夏天集</a></td>
<td>[法] 阿贝尔·加缪</td>
<td>译林出版社</td>
<td>8.8 (451人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1034066/" target="_blank" rel="noopener">加缪文集</a></td>
<td>[法] 阿尔贝·加缪</td>
<td>译林出版社</td>
<td>9.1 (1882人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1320062/" target="_blank" rel="noopener">加缪和萨特</a></td>
<td>[美] 罗纳德·阿隆森</td>
<td>华东师范大学出版社</td>
<td>7.8 (217人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="萨特"><a href="#萨特" class="headerlink" title="萨特"></a>萨特</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/1398463/" target="_blank" rel="noopener">萨特文集（全八卷）</a></td>
<td>[法] 让·保尔·萨特</td>
<td>人民文学出版社</td>
<td>9.0 (528人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2305738/" target="_blank" rel="noopener">存在与虚无</a></td>
<td>[法] 让·保尔·萨特</td>
<td>生活·读书·新知三联书店</td>
<td></td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3040509/" target="_blank" rel="noopener">想象</a></td>
<td>[法] 让·保尔·萨特</td>
<td>上海译文出版社</td>
<td>7.3 (82人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1266117/" target="_blank" rel="noopener">自我的超越性</a></td>
<td>[法] 让·保尔·萨特</td>
<td>商务印书馆</td>
<td>7.8 (42人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1444025/" target="_blank" rel="noopener">存在主义是一种人道主义</a></td>
<td>[法] 让·保尔·萨特</td>
<td>上海译文出版社</td>
<td>8.5 (439人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2139903/" target="_blank" rel="noopener">魔鬼与上帝</a></td>
<td>[法] 让·保尔·萨特</td>
<td>漓江出版社</td>
<td>9.1 (57人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2213965/" target="_blank" rel="noopener">他人就是地狱</a></td>
<td>[法] 让·保尔·萨特</td>
<td>天津人民出版社</td>
<td>8.2 (1476人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1316536/" target="_blank" rel="noopener">萨特读本</a></td>
<td>[法] 让·保尔·萨特</td>
<td>人民文学出版社</td>
<td>8.5 (760人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1019171/" target="_blank" rel="noopener">词语</a></td>
<td>[法] 让·保尔·萨特</td>
<td>三联书店</td>
<td>8.5 (232人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3106705/" target="_blank" rel="noopener">萨特自述</a></td>
<td>[法] 让·保尔·萨特</td>
<td>天津人民出版社</td>
<td>7.3 (216人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1317885/" target="_blank" rel="noopener">寄语海狸</a></td>
<td>[法] 让·保尔·萨特</td>
<td>人民文学出版社</td>
<td>8.0 (201人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1433569/" target="_blank" rel="noopener">萨特的世纪</a></td>
<td>[法]贝尔纳·亨利·列维</td>
<td>商务印书馆</td>
<td>8.5 (85人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1459717/" target="_blank" rel="noopener">萨特研究</a></td>
<td>柳鸣九主编</td>
<td>中国社会科学出版社</td>
<td>7.7 (45人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1536639/" target="_blank" rel="noopener">萨特</a></td>
<td>[美] 理查德·坎伯</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1423369/" target="_blank" rel="noopener">百年萨特</a></td>
<td>黄忠晶</td>
<td>中央编译出版社</td>
<td>7.7 (342人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1665326/" target="_blank" rel="noopener">萨特精选集</a></td>
<td>[法] 让·保尔·萨特</td>
<td>北京燕山出版社</td>
<td>8.9 (186人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1016160/" target="_blank" rel="noopener">不惑之年（自由之路第一部）</a></td>
<td>[法] 让·保尔·萨特</td>
<td>中国文学出版社等</td>
<td>8.5 (74人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1040394/" target="_blank" rel="noopener">萨特论艺术</a></td>
<td>[法]萨特/ [美]韦德·巴斯金 编</td>
<td>中国人民大学出版社</td>
<td>7.6 (216人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1487995/" target="_blank" rel="noopener">文字生涯</a></td>
<td>[法] 让·保尔·萨特</td>
<td>人民文学出版社</td>
<td>8.5 (936人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1874508/" target="_blank" rel="noopener">厌恶及其他</a></td>
<td>[法] 让·保尔·萨特</td>
<td>上海译文出版社</td>
<td>9.1 (122人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4162701/" target="_blank" rel="noopener">超越生命的选择</a></td>
<td>[法] 让·保尔·萨特</td>
<td>长江文艺</td>
<td>7.9 (78人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1316536/" target="_blank" rel="noopener">萨特读本</a></td>
<td>[法] 让·保尔·萨特</td>
<td>人民文学出版社</td>
<td>8.5 (760人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1023557/" target="_blank" rel="noopener">萨特戏剧集(上下)</a></td>
<td>[法]让·保罗·萨特</td>
<td>安徽文艺出版社</td>
<td>9.1 (463人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2149157/" target="_blank" rel="noopener">墙</a></td>
<td>[法]让·保罗·萨特</td>
<td>安徽文艺出版社</td>
<td>8.6 (489人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1030554/" target="_blank" rel="noopener">恶心</a></td>
<td>[法]让·保罗·萨特</td>
<td>中国友谊出版公司</td>
<td>8.6 (1987人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1007824/" target="_blank" rel="noopener">萨特传</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>百花洲文艺出版社</td>
<td>8.1(169人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="西蒙·波伏瓦"><a href="#西蒙·波伏瓦" class="headerlink" title="西蒙·波伏瓦"></a>西蒙·波伏瓦</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/4745949/" target="_blank" rel="noopener">第二性</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td>8.7(3568人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1079208/" target="_blank" rel="noopener">名士风流（全二册）</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>中国书籍出版社</td>
<td>8.2 (78人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6310320/" target="_blank" rel="noopener">人都是要死的</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td>8.7(1209人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3081371/" target="_blank" rel="noopener">他人的血</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>外国文学出版社</td>
<td>8.0 (13人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1088395/" target="_blank" rel="noopener">女宾</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td></td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3722970/" target="_blank" rel="noopener">波伏娃：激荡的一生</a></td>
<td>[法] 弗朗西斯 / [法] 贡蒂埃</td>
<td>广西师范大学出版社</td>
<td>7.9 (453人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1211533/" target="_blank" rel="noopener">波伏瓦</a></td>
<td>萨莉·J·肖尔茨</td>
<td>中华书局</td>
<td>7.7 (41人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6795314/" target="_blank" rel="noopener">波伏瓦回忆录</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>作家出版社</td>
<td>7.9(80人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/20375518/" target="_blank" rel="noopener">模糊性的道德</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td><a href="https://book.douban.com/subject/20375518/" target="_blank" rel="noopener">https://book.douban.com/subject/20375518/</a></td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10563558/" target="_blank" rel="noopener">长征 : 中国纪行</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>作家出版社</td>
<td>7.7(59人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1861319/" target="_blank" rel="noopener">女人是什么</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>中国文联出版公司</td>
<td>8.2(52人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2074937/" target="_blank" rel="noopener">一个与他人相当的人</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>光明日报出版</td>
<td>7.3(94人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1796305/" target="_blank" rel="noopener">面对面 : 让-保罗·萨特与西蒙娜·德·波伏瓦</a></td>
<td>[美]黑兹尔·罗利</td>
<td>中信出版社</td>
<td>8.0(85人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10529773/" target="_blank" rel="noopener">独白</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td>7.9(521人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6310321/" target="_blank" rel="noopener">要焚毁萨德吗</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>上海译文出版社</td>
<td>8.1(228人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1021037/" target="_blank" rel="noopener">波伏瓦 : 一位追求自由的女性</a></td>
<td>李亚凡</td>
<td>人民文学出版社</td>
<td>7.0(277人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1068246/" target="_blank" rel="noopener">越洋情书（上下卷）</a></td>
<td>[法] 西蒙·波伏娃</td>
<td>中国书籍出版社</td>
<td>7.8(165人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="梅洛·庞蒂"><a href="#梅洛·庞蒂" class="headerlink" title="梅洛·庞蒂"></a>梅洛·庞蒂</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/3299026/" target="_blank" rel="noopener">辩证法的历险</a></td>
<td>[法]梅洛·庞蒂</td>
<td>上海译文出版社</td>
<td>7.8 (55人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1046468/" target="_blank" rel="noopener">知觉的首要地位及其哲学结论</a></td>
<td>[法]梅洛·庞蒂</td>
<td>三联书店</td>
<td>7.6 (44人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1458716/" target="_blank" rel="noopener">知觉现象学</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>7.5 (183人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2141589/" target="_blank" rel="noopener">眼与心</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>8.7 (148人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1135846/" target="_blank" rel="noopener">梅洛·庞蒂</a></td>
<td>丹尼尔.托马斯.普里莫兹克</td>
<td>中华书局</td>
<td>7.6 (33人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1253311/" target="_blank" rel="noopener">符号</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>6.8(52人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3069521/" target="_blank" rel="noopener">可见的与不可见的</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>7.9(43人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/6013775/" target="_blank" rel="noopener">理解梅洛·庞蒂 : 梅洛·庞蒂在当代</a></td>
<td>[法]梅洛·庞蒂</td>
<td>北京大学出版社</td>
<td>8.4(29人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1416721/" target="_blank" rel="noopener">世界的散文</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>9.1(39人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1444014/" target="_blank" rel="noopener">行为的结构</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>7.9(27人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1048338/" target="_blank" rel="noopener">哲学赞词</a></td>
<td>[法]梅洛·庞蒂</td>
<td>商务印书馆</td>
<td>8.0(37人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1488873/" target="_blank" rel="noopener">模糊暧昧的哲学 : 梅洛-庞蒂传</a></td>
<td>安德烈·罗宾耐</td>
<td>北京大学出版社</td>
<td>7.3(18人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2122177/" target="_blank" rel="noopener">隐喻的身体 : 梅洛·庞蒂身体现象学研究</a></td>
<td>张尧均</td>
<td>中国美术学院出版社</td>
<td>7.9(57人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/20444327/" target="_blank" rel="noopener">画与真 : 梅洛-庞蒂与中国山水画境</a></td>
<td>姜宇辉</td>
<td>上海人民出版社</td>
<td>8.3(28人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/24754292/" target="_blank" rel="noopener">表达与存在 : 梅洛-庞蒂现象学研究</a></td>
<td>宁晓萌</td>
<td>北京大学出版社</td>
<td>7.5(20人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1289092/" target="_blank" rel="noopener">感性的诗学 : 梅洛-庞蒂与法国哲学主流</a></td>
<td>杨大春</td>
<td>人民出版社</td>
<td>7.7(19人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/2043104/" target="_blank" rel="noopener">梅罗-庞蒂历史现象学研究</a></td>
<td>佘碧平</td>
<td>复旦大学出版社</td>
<td>7.7(11人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="罗洛·梅"><a href="#罗洛·梅" class="headerlink" title="罗洛·梅"></a>罗洛·梅</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/3265824/" target="_blank" rel="noopener">存在之发现</a></td>
<td>[美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.1 (73人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4839259/" target="_blank" rel="noopener">心理学与人类困境</a></td>
<td>[美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.2 (74人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4839258/" target="_blank" rel="noopener">存在心理学</a></td>
<td>[美]施耐德/ [美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.4 (49人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3298292/" target="_blank" rel="noopener">存在之发现</a></td>
<td>[美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>7.9 (65人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3265210/" target="_blank" rel="noopener">创造的勇气</a></td>
<td>[美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.3 (169人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4834600/" target="_blank" rel="noopener">自由与命运</a></td>
<td>[美]罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.2 (175人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3265827/" target="_blank" rel="noopener">人的自我寻求</a></td>
<td>[美] 罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.9 (441人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5361804/" target="_blank" rel="noopener">焦虑的意义</a></td>
<td>[美] 罗洛·梅</td>
<td>广西师范大学出版社</td>
<td>8.3 (469人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4834599/" target="_blank" rel="noopener">爱与意志</a></td>
<td>[美] 罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.0(160人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/24720021/" target="_blank" rel="noopener">权力与无知:寻求暴力的根源</a></td>
<td>[美] 罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.4(71人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/4839259/" target="_blank" rel="noopener">心理学与人类困境</a></td>
<td>[美] 罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.2(74人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/11586652/" target="_blank" rel="noopener">祈望神话 : 祈望神话</a></td>
<td>[美] 罗洛·梅</td>
<td>中国人民大学出版社</td>
<td>8.5(24人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/10517480/" target="_blank" rel="noopener">存在 : 精神病学和心理学的新方向</a></td>
<td>[美]罗洛·梅，恩斯特·安杰</td>
<td>中国人民大学出版社</td>
<td>7.6(35人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="欧文·亚隆"><a href="#欧文·亚隆" class="headerlink" title="欧文·亚隆"></a>欧文·亚隆</h2><div class="table-container">
<table>
<thead>
<tr>
<th>书名</th>
<th>作者</th>
<th>出版社</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://book.douban.com/subject/2327264/" target="_blank" rel="noopener">日益亲近 : 心理治疗师与来访者的心灵对话</a></td>
<td>[美]欧文·亚隆</td>
<td>中国轻工业出版社</td>
<td>8.5(335人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5999507/" target="_blank" rel="noopener">妈妈及生命的意义</a></td>
<td>[美]欧文·亚隆</td>
<td>机械工业出版社</td>
<td>8.4(452人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3684193/" target="_blank" rel="noopener">叔本华的治疗</a></td>
<td>[美]欧文·亚隆</td>
<td>希望出版社</td>
<td>9.0(928人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/5940553/" target="_blank" rel="noopener">当尼采哭泣</a></td>
<td>[美]欧文·亚隆</td>
<td>机械工业出版社</td>
<td>8.8(2983人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26747834/" target="_blank" rel="noopener">在生命最深处与人相遇：欧文·亚隆思想传记 </a></td>
<td>[美]欧文·亚隆</td>
<td>机械工业出版社</td>
<td>8.2(53人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/3432953/" target="_blank" rel="noopener">爱情刽子手</a></td>
<td>[美]欧文·亚隆</td>
<td>希望出版社</td>
<td>8.9(576人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26304954/" target="_blank" rel="noopener">存在主义心理治疗</a></td>
<td>[美]欧文·亚隆</td>
<td>商务印书馆</td>
<td>9.5(210人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/1936756/" target="_blank" rel="noopener">诊疗椅上的谎言</a></td>
<td>[美]欧文·亚隆</td>
<td>四川大学出版社</td>
<td>8.6(1037人评价)</td>
</tr>
<tr>
<td><a href="https://book.douban.com/subject/26841312/" target="_blank" rel="noopener">给心理治疗师的礼物 </a></td>
<td>[美]欧文·亚隆</td>
<td>中国轻工业出版社</td>
<td>9.4(137人评价)</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 存在主义 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（10）：高宣扬讲存在主义（四）]]></title>
      <url>/2018/01/23/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8810%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>一方面，存在主义试图用对存在本身的研究替代传统哲学对存在者的研究，在这个意义上说是抽象的。但是同时它又是富有生命情感的，只要我们深深的热爱自己的生命，不断地对自己的生命能够提出发问，感受自己生命经历中的一切，你就会觉得，存在主义是非常的有吸引力的，而且是跟你的生命是密切相关的</p>
<a id="more"></a>
<p> <img src="http://omu7tit09.bkt.clouddn.com/莫奈 静物（1）.jpg" alt="莫奈 静物（1）"></p>
<p>我刚才讲，这个关于语言的问题是海德格尔在谈到诠释学以后，开始进一步去展开的。所以他反复的讲“语言是存在的家”，他经常不断的去引用，运用语言运用到非常惟妙惟肖的高度的，这样一些著名的诗人的语言来说明。我这里在192页，引用了海德格尔对瑞士诗人格奥尔格•特拉克尔，Georg Trakl，这是1887年到1914年的一位诗人。他写了一首诗，题目叫做《一个冬天的夜晚》。我想从头到尾地念一下这首诗。</p>
<p>《一个冬天的夜晚》<br>正当雪花临窗<br>教堂晚钟长久回响。<br>千家万户摆好餐桌<br>家庭供应丰盛得当。</p>
<p>不只一个人还在旅程<br>经茫茫道途终临家门。<br>金果硕硕神恩之树，<br>屹立大地郁郁葱葱。</p>
<p>游子安详入室，<br>心情之哀痛使门槛顿时僵直。<br>一道金光闪耀<br>餐桌上摆设着面包和美酒。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  静物（2）.jpg" alt="莫奈  静物（2）"></p>
<p>这首诗非常深刻的，为什么？一方面这首诗用的诗的语言，把人生在世的那种曲折的历程，以及一个人怎么样经受了各种生活的煎熬、各种痛苦和快乐，然后最后在一个大雪纷飞的夜晚，在圣诞节的前夕，好不容易回到了久别的故乡，来到自己的家。这说明什么，感受到人生在世就是一种永远在旅途中游荡和飘荡的、反复不定的、充满着烦和忧虑的生活历程。这个生活历程就好像把人从自己的家给抛出去，然后在世界上经历了动荡波折以后，又到晚年把握了生命的要旨，然后要回到自己的家了，感受到原来生活就是如此。</p>
<p>这首诗就是以这样一种题目，来表达人生在世是怎么样通过自己的艰苦的经历，然后通过语言的表述，来说明“语言是存在的家”。你如果说往事有感想，但是找不着语言来说出来。那不行的，最后还是懵懵懂懂的。所以这点是海德格尔特别强调说，你要生活有意义，就应该像诗人那样生活在世界上。这就意味着必须要，一方面要对生活、对世界、对存在有一个深刻的把握，但另一方面又要学会通过语言去把握这个世界。这两个是同时存在的，同时必须要下功夫的。</p>
<p>所以海德格尔在晚年的时候，特别一再的感受到，一再的表示，他对他出生的故乡的怀念。他认为对故乡的怀念和对故乡的爱，他那种乡愁是他一生中，进行哲学思维的一个永远无止境的动力。他认为家乡给他生命的一个开端，家乡的父老兄弟给予他的各种各样，对他的关切和关注。</p>
<p>所以他特别强调说，在这个意义上说，哲学就是不断的回到原来的出发点，回到我当初出发的老家那边。而这个老家里面，给他感受最深的是老家的父老兄弟所说的语言，他说。那个语言是最亲切的。当我回到我的老家去，听到我老家的那个人讲的话，土话和方言的时候，我突然感受到一种非常的情感，感受到世界原来如此。正是在父老兄弟们，最早的时候自己所接受的母语的训练中，就已经包含着他此后一系列的对生命、对存在的理解，它的种子和芽，发芽的芽。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  静物（4）.jpg" alt="莫奈  静物（4）"><br>所以他特别重视这个，一再强调哲学就是要不断地回来，不断的跟原来的出发点进行对话。而且也特别提到，哲学在本质上就是感谢，就是回答所有的养育他成长，给予他营养，给予他精神启发的老师、故乡、父老兄弟的一种感谢、一种对话。</p>
<p>那么我想,关于存在主义这样一个东西，我在这里要特别强调说，一方面，存在主义是很抽象的。由于它试图扭转整个哲学研究的方向，把对存在的问题，把它提到本体论的高度。而且等于是把用传统哲学对存在者的研究把它代替，把它替代、改换成为对存在本身的研究，在这个意义上说是很难理解的。</p>
<p>但是同时它又是特别富有生命的情感的，是一个只要我们去深深的热爱我们自己的生命，只要我们不断地对自己的生命能够提出发问，不断地感受到自己生命经历中的一切一切，大大小小，从大的到最细的部分，到你的每时每刻所发生的脉搏的跳动的感受，很深地去理解的话，你就会觉得，存在主义是非常的有吸引力的，而且是跟你的生命是密切相关的。</p>
<p>而且人在存在的路上，在存在于世界上的路上，难免进入到危险的境界，他说。有很多风险，甚至会犯错误，会走在存在的错道上去。但是你真正的对存在，对自己的存在很关切的人，他才能够通过不断的反思。慢慢的，最后就好像前面所念的特拉克尔的诗歌《一个冬天的夜晚》所说的，一个流浪者到最后经过了千辛万苦，最后回到了家，也就是说回到了存在本身，把握了存在的奥秘，这样是不容易的。所以人生本质上是流浪的。<br><img src="http://omu7tit09.bkt.clouddn.com/莫奈  静物（3）.jpg" alt="莫奈  静物（3）"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 海德格尔 </tag>
            
            <tag> 存在主义 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（9）：高宣扬讲存在主义（三）]]></title>
      <url>/2018/01/22/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%889%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>海德格尔一方面强调存在必须要通过此在的自我诠释去把握，另一方面在这个基础上，进一步提出了关于“语言是存在的家”，这样一个基本论断。因为人（此在）是唯一能够通过语言去把握存在的存在者</p>
<a id="more"></a>
<p>在海德格尔进行长期的研究之后，他提出了一个非常重要的方法，就是把诠释学纳入到存在哲学里面，纳入到存在论里面的基本的内容，作为存在论的基本内容。希腊神话中赫尔墨斯在传递神谕的时候，赫尔墨斯也承担了对各种神谕进行解释的权利。所以诠释学的原意，就是Hermeneutik，就是从Hermes。这个赫尔墨斯，诠释的神的名字，作为基本的基干、词干，然后衍生出来的。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  翁弗勒（1）.jpg" alt="莫奈  翁弗勒（1）"></p>
<p>但后来，到了基督教统治整个欧洲的时候，诠释学又变成了对《圣经》文本进行诠释的一种方法。这个方法在14到15世纪的时候，由于基督教的改革，特别是后来马丁・路德对《圣经》做了新的诠释。他特别强调，基督教的原意是通过教徒根据他个人的理性和他对自己的生活的体会，对《圣经》的每一句话进行自我诠释。所以教徒不应该把天主教会对《圣经》的诠释，把它作为一个神圣不可侵犯的教条来理解。</p>
<p>但是到了18世纪的时候，启蒙运动和浪漫主义兴起以后，德国的神学家和哲学家施莱尔马赫（Friedrich Schleiermacher, 1768-1834）又把这样一个原来意义上的，来自古希腊和罗马教会的诠释学改为对于文本的诠释的学问。那是施莱尔马赫所确定的。但是现在到了海德格尔这里，由于海德格尔特别强调，研究存在必须通过此在对存在的体会去进入到存在的本质。所以这就涉及到此在，它作为一个特殊的一个个人的亲在，他对自己的生活经历有自己特殊的体会。而且他有一种特殊的语言表达的能力，把自己亲在所经历的感悟，通过自己的特殊的、充满着自己的情感的语言、生活的语言，来表达出他对生存的经历的感受。这也就是他对存在的理解。</p>
<p>所以在海德格尔那里，诠释学从此以后，就从对文本的技术性的诠释变成为存在论的诠释。这是一个重大的转折。而且在西方的思想史上，这个贡献是很大的。就是在海德格尔这样的一个思想的指导下，海德格尔的学生,Hans-Georg Gadamer,伽达默尔，后来在20世纪60年代的时候，他进一步明确的提出了关于本体论的诠释学的一个新的转折。而且伽达默尔由于发表了《真理与方法》这本书，把本体论诠释学和哲学诠释学进一步扩大，变成为人文社会科学的一个新的基本逻辑，这是后话。</p>
<p>但是在这里我要谈到的是此在对亲在的体会，通过他自己的自我诠释来表达对存在的理解。这是在海德格尔的《存在与时间》里面特别强调的。这一点我们要特别重视。也就是说，我们自己也必须要意识到这点，要对存在、对于人生在世有所理解，而且对这种理解能够不断的总结、不断的改进、不断的提升。通过这种提升来进一步感悟自己的对人生的的理解，也就是说提升自己的对存在的理解，那么必须要特别重视，关于对自己的在世过程的诠释，这样一个方法。<br>  <img src="http://omu7tit09.bkt.clouddn.com/莫奈  翁弗勒（2）.jpg" alt="莫奈  翁弗勒（2）"><br>海德格尔特别提到说，“此在”是通过自我言说来表达和不断提升自己对亲在的体验的。因此此在的自我言说、自我诠释，是此在再次生存的一个基本途径、基本方法、基本态度。就在这个诠释过程中，一方面表现了此在对世界的看法，对自己的生存和存在的理解，另一方面也体现了此在通过语言这个通道跟存在发生关系。在这里，就在这一点上，海德格尔特别强调，存在和语言的关系，人是一个唯一的，能够通过存在、能够通过语言去把握存在的这样一个存在者，这样一个存在。</p>
<p>所以必须要看到，能够如何通过自我表述、自我展现、自我言说来去临近、靠近，越来越靠近存在的本质。这是人有这个本事。他可以通过语言的表述、通过语言的体会的诠释，越来越深入的到存在的神秘的内部，去了解存在到底是什么。因此，海德格尔后来就是，一方面强调存在必须要通过此在的自我诠释去把握，另一方面他进一步在这个基础上，提出了关于“语言是存在的家”，这样一个基本论断。他特别提到说，Die Sprache ist das Haus des Seins。</p>
<p>“语言是存在的家”这句话，这句话是海德格尔后来在1927年发表他的《存在与时间》之后，在30年代之后，慢慢的进一步对存在、对存在语言的问题，进行了更深入的专门的研究。海德格尔越来越意识到，通过这一点意识到，要了解存在必须研究语言。因为前面已经说了，要了解存在必须通过此在，但是此在是唯一的一种能够同时了解和把握语言的艺术的存在，所以语言就成为了此在进入到存在的内部。它的进入到语言，通过语言进入到存在的家的内部，去了解存在的这样一个唯一的存在。</p>
<p>他特别强调，为了了解语言是存在的家，有必要对于存在有深刻的把握，同时又对语言有深刻的把握，这样一个存在者进行研究。这样一种对语言和存在同时有深刻把握的是什么呢？是谁呢？他认为是诗人。</p>
<p>诗人，Le Poète。他说，诗人之所以成为诗人，就在于诗人最懂得通过语言去把握存在。诗人之所以能够通过语言去把握存在，是因为诗人他最深刻的把握了存在的本质，他同时又能够通过他的语言，去把存在的本质表达出来。所以在这个意义上说，他后来，海德格尔特别强调，要特别重视研究诗歌，研究诗的语言。在这当中，海德格尔后来就特别越来越热爱德国天才的诗人――荷尔德林（Hölderlin）。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  翁弗勒（3）.jpg" alt="莫奈  翁弗勒（3）"></p>
<p>《存在主义》这本书的第四章，海德格尔进行哲学思想的有关诗歌的这部分，我在这里写了不少。在这里我顺便列举了一下。海德格尔特别提到，他说“语言就是语言”，这句话好像是同义反复。但是他特别强调“语言就是语言”，但前面的语言和后面的语言不一样。当说“语言就是语言”的时候，前面说的语言是指的一般人所理解的语言，但是后面的“就是语言”那个语言，后一个语言，他强调的是真正的语言。</p>
<p>所以“语言就是语言”。但是“语言就是语言”并不是所有人都明白。为什么“语言就是语言”？在这里海德格要强调的是说，语言就是那个真正作为存在的语言。语言就是作为存在的家的语言。他要说的是这个。所以当他说，语言本身，既非语言之外之他物，使语言成为语言，语言之为语言才使我们处于无底的深渊之上。所以他说，语言它的奥秘不是通过语言之外的其它物向我们展示，而是语言本身显示了语言的奥秘。</p>
<p>因为语言这个东西，只要是你有思想的话，当你使用语言的时候，你会感受到语言的那种既容易又难的这样的一个悖论。在语言中，在你使用语言的艺术中，你会体会到语言跟我们的生存，跟我们的生存于世的这样一个感受，有着非常深刻的然而是非常复杂的关系。有的时候我们想要用语言表达我们对生活的感受，但是恰恰是语言本身，又限制我们去表达对生活的感受。因为我们感受到说，用语言不容易，就好像我们生活在世界上不容易一样。你不要以为说，当我遇到一种处境的时候，我好像就可以很自在地表达我的感受。其实不然。你如果要真正地表达你的感受，你就会感受到，用语言去表达你的生活感受是非常难的，难到比登天还难。</p>
<p>所以他前面引用了哈曼（Johann Georg Hamann）的一封信，他特别提到，语言这个东西别小看它，好像我们人人都在讲话。但实际上语言是当你好好的去体会、去理解的时候，当你把语言跟你的生命连在一起的时候，你会感受到语言是一个无底的深渊，是一个没法把握的一种东西。他引用哈曼的一封信提到说，他说，语言这种东西弄得我们无法表达我们所要表达的东西。在这段话里面，他们特别强调，语言本身的强大的威力和它的价值，就在于它能够为我们人类的思维、精神活动以及一切属于人类属性的因素，提供一个永无边界、永无止境的线索，使得我们进入到一个无限广阔的维域。但是恰恰是因为这样，所以它既没有底基，也没有“他物”的限制，它就是不受一般存在物的时空限制的“存在”本身。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  翁弗勒（4）.jpg" alt="莫奈  翁弗勒（4）"></p>
<p>因此，语言是一个什么呢？是一个既靠近你，但同时又非常远离你。你要把握它，你要使用它，好像随手可得，但是你要真正的用语言去表达你的心里的感受的时候，你就感受到语言又是很难的，是需要一个你对你的生存有深刻的理解，有一种非常恰当的，就好像前面熊伟老师所说的，要对生活对存在一种恬然澄明的一种感受、感悟。就好像佛教所说的，你要真正的能够成佛，跟那个说，人人都能成佛，但是你真的是哪一个时候，你突然感受到，原来生活就是如此的时候，这个时候你才懂得你的这个世界到底是什么。而且也就在这个时候，你才懂得，其实世界的本质，我不用语言表达，我自己都把握了。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 海德格尔 </tag>
            
            <tag> 存在主义 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（8）：高宣扬讲存在主义（二）]]></title>
      <url>/2018/01/21/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%888%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>有一个存在者，对存在本身非常关注，而且有所体会。这个存在者就是人。海德格尔将其称之为“此在”。每个人都无可逃脱。在他亲自来到这个世界上，去经历自己的生命历程的时候，他每时每刻都遭遇到，关于你到底做什么选择，你到底要行使什么样的选择的自由的问题。此在的最根本、最基本的状态就是“烦”</p>
<a id="more"></a>
<p>因为刚才其实是把一般的问题提出来，就是关于研究存在主义，必须首先把握现象学这个方法。所谓“存在”的现象，就是它能够在被观察的时候，被观察的现象跟我们观察的主体之间的感情，和我们的生命的内部的情感、对生命的感悟，能够相互交流起来。然后使得观察过程中，被观察的对象，能够随着我们的观察者自己的生命的体验的活跃，而使被观察的对象重新地显现出来，再现出来。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  费康海（1）.jpg" alt="莫奈  费康海（1）"></p>
<p>这个再现是再，一再的再，再现，再次地显示出来。所以，在这点上，熊先生特别强调，要准确的、要深刻的把握，关于存在和存在者之间的区别，后来他经过很多的考察，他特别对这个，一方面强调存在者和存在的差异，但同时也去考察存在者和存在的关系。就在研究这个差异和关系的同时，他发现有这么一个存在者是非常特殊的存在者。这个存在者，他对存在本身非常关注，而且有所体会。这个存在者是什么？这个存在者就是人，但这不是传统所说的那种抽象的、一般的人，而是一个一个有特殊的生命经历，有特殊的生命感悟的那个个人。这个个体，每一个都是不一样的。就在这些不同的个体中，他对自身的存在都有自己的特殊的感悟。</p>
<p>举例子来说，我作为一个经过哲学训练，有过哲学思维能力，而且又对自己的经历特别关切的一个人。我一个特殊的人，不是你，不是他，而是我这个人。我作为这样一个特殊存在者，我对我的每时每刻的经历，我都有自己的经验和体会。不但有这个体会，而且我对它充满了感情，我对它不断的在生命中一再的去反思，一再的去重复的去考虑，每次考虑都有不同的结果。这样一个经历，使得我有资格、有能力、有兴趣去再现我自己的存在。</p>
<p>用哲学的概念和范畴来说，这样一个特殊的，对自己的存在特别关切，会不断的去关怀自己的存在的，这样一个存在者，特殊的存在者，海德格尔把他称之为“此在”，原文叫做“Dasein”。</p>
<p>所以每个人都无可逃脱的，在他亲自来到这个世界上，去经历自己的生命历程的时候，他每时每刻都遭遇到，关于你到底做什么选择，你到底要行使什么样的选择的自由的问题。在这个意义上说，也可以说每时每刻，对每个人来说来到这个世界上，他总是要遭遇到生和死的问题。为什么？你选择这个和选择那个，有的时候不是那么，简单地说，只是在一个具体问题上，你选择这个，选择那个，实际上你是在选择你自己要走什么路，你是要怎么生活，你离开你的死亡有多远的问题。这是一个很关键、很重要的一个问题，所以海德格尔特别强调，死亡其实不是一个只有到最后，好像是到年龄老了，好像是活到一定阶段以后，该死，要到没法逃脱自然的安排，你到八九十岁，七八十岁，可能那时候面临死亡。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  费康海（2）.jpg" alt="莫奈  费康海（2）"></p>
<p>其实存在主义认为不是这样，其实死亡天天每时每刻都伴随着你，就是严格意义上说，他的意思就是说，每时每刻你都有选择生命，生和死的问题，但是生和死的问题在不同的时刻，展现出不同的重要性和它的不同的意义。但总之，所有的时刻都是面临着生和死的问题。</p>
<p>在这个问题上，熊伟认为，他说，这个是他的原话，他说，“真自由必然是反身而诚”。什么叫反身而诚？就是对自己很忠诚、很诚实，是什么就是什么。就是自己能够对自己非常明了，我到底是什么样的人，我应该选择什么样的道路？这应该很清楚。而这种清楚只有你反复的，在每时每刻中，你都非常严格的，非常高标准的去思考自己的问题。思考你要选择什么？思考你面对的这个世界到底是什么样？你都能够不放松任何一个时候，去进行分析，进行反省，那么在这个时候，这叫做真正的，就是反身而诚，对自己诚，就是这个意思。</p>
<p>他说，“真自由必然是反身而诚，乐莫大焉”。“必须要从‘我在世’开始，以至于‘在到死中去’视死如归”。什么意思？就是这个“在”就是存在，存在到死中去。不但每时每刻，从我在世开始。而且必须要考虑到，每个在世同时又是面临着自己的死亡的问题，关系到自己如何死亡的问题。因为你选择什么，就是意味着你将来怎么死的问题。这是一个非常，也就是说，真正的对自己的存在负责任的一个人，当他去体验自己的亲在的时候，他必须对每一个时刻，他所面临的抉择，做出的选择，看作是自己对生死存亡的一个最大问题。以这样的严肃的心情去看待，你才能够对这个存在，对在世能够有所把握。你才真正的进入到，你是真正的生活，真正的生命，是这样。不然的话你是白过了。</p>
<p>所以他说，也是特别提到的，他说，我重复说，“现象学必须从‘我在世’开始，以至于‘在到死中去’视死如归，此亦即活的历史”。这也就是活的历史。“一言以蔽之，海德格尔的现象学是‘在者的在的学问，即存在论’”。“此在”是揭示存在的奥秘的一个钥匙，“此在”是进入到存在的神秘世界的一个入口，在这点上要特别注意。</p>
<p>所以熊伟说，“天地之大，谁能‘恬然澄明’地体会到‘我在’，谁就找到‘在’；”熊伟先生用佛教和道教，和老子所说的“恬然澄明”这四个字，来概括的说明通过对于存在，对自己的生存于世，特别关注的人的这种反思和体验，感觉到世界和我的存在的关键，因而真正的领会到原来世界就是如此，生命就是如此，到那个时候，他就是佛教说的，到了一个全然领悟的新的世界。</p>
<p>他说“此外在其他任何地方再也找不到‘在’”。再也不找不到存在，“而只能找到‘在者’而已”。在这里，当读者，希望你们在看海德格尔《存在与时间》的部分的时候，要把握此在，但同时也特别提到，这个此在的最根本的、最基本的状态就是“烦”，这个“烦”的原文就是，德文叫做Sorge。熊伟先生特别强调说，“如果说自由属于自己，但也不能忘记与我们‘共在’的‘他人’”。</p>
<p>也就是说，自由固然是自己可以做决定的时候，我面对着世界的时候，我到底做什么选择。但是当你选择的时候，固然是你自己选择，但是你不得不面对一个他人的问题。你选择你是要从你自己出发，你有你自己选择的自由。但是就是当你去选择自由的时候，你不能不考虑有他人的存在，这就是说，他人跟你永远是共在。刚才我讲到关于“此在”、“在世”、“存在”的时候，它的最基本形态叫做烦，萨特（Sartre）也同样是这样理解。</p>
<p>所以萨特的第一本著作，关于存在主义的第一本著作，是用小说的形式写的，叫做《呕吐》（La Nausée），在1938年写成的。在1934年以前，从1931年到1934年，萨特研究和学习海德格尔的《存在与时间》，而且也学习了胡塞尔的现象学，试图从现象学里面找到一个研究方法，去揭示人生的奥秘。<br> <img src="http://omu7tit09.bkt.clouddn.com/莫奈  费康海（3）.jpg" alt="莫奈  费康海（3）"></p>
<p>烦就是呕吐的时候的那种感觉，在萨特看来，这是活生生的人，活的个体，对于他在实际生活中所遭遇的每一个现象的这样或那样的生活感应。为什么呕吐？因为我这个个人的在世，在世的每一个时刻，我所面对的各种事情，都不是我这个人所愿意的，都不是符合我个人的喜爱，所以，一切都是令人恶心，令人作呕的。他说，我固然，我作为个体，我此在，我人生在世到这个世界上。但是，我首先是无缘无故的、不知所以然的被我的父母生出来。用他的话来说，被抛在这个世界上。我不知道怎么一回事，我也不知道我父母怎么把我生出来的。把我生出来以后，又面临着一系列的这样一个，我所遭遇到的各种各样，我所不能选择的世界，各种复杂的关系。然后当我自己去选择自己的自由的时候，我又遇到周围的各种各样的他人的约束和他们的干扰。于是，我的存在于世，就成为我不得不选择的那种选择，成为我所讨厌的选择；也就是说，我每时每刻都面临着“被强迫的选择”，我只好在恶心状态中过日子。</p>
<p>我无法逃脱跟他人的关系，所以他说这种无法逃脱，就好像是，萨特用一个很形象的话，叫做像黏液一样黏在你的身上。他人就像黏液一样粘在你身上，你要你逃避它不行，你关起门来，你以为你自己在屋里很孤孤单单的，你可以不去见别人，不去跟别人说话，好像你可以逃避他人。事实上你逃避不了。</p>
<p>但是简单的只是说“烦”还不够，所以海德格尔他的深刻之处就在于，在《存在与时间》里面，当他谈到，分析此在的、生存于世的这种“烦”的具体的过程的时候，重要的在于他描述了这个“烦”的细节。这个细节是到现在为止，任何人看了以后都会，当你对你的存在和对你的命运非常关切的话，你会感受到他的描述几乎好像就是真的是切合你的状况一样。这种烦是什么烦？就是他这么一系列的产生了，它这个概念一个一个展开，关于在烦的时候什么样的，叫做情态，感情的情，情态。</p>
<p>因为欧洲的启蒙运动特别重视理性。19世纪中叶，出现了像波德莱尔这样的一个作家和思想家，对理性的问题提出怀疑，要重新的思考现代性的问题，那么他特别重视关于情感的问题，特别重视情感跟理性之间的矛盾和它的相互渗透。只有通过对情感的深入的去分析，高度重视情感的成分，你才能够更好地理解什么叫理性。因为理性没有孤立的或抽象的、干巴巴的理性。在人生的生命中，所有的理性的发明、理性的思考，都伴随着一系列经历中的情感的变化，带着很深刻的情感。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/莫奈  费康海（4）.jpg" alt="莫奈  费康海（4）"></p>
<p>所以在海德格尔的《存在与时间》里面，他尤其重视情感的问题。所以他提出了一个新的概念，德文原文叫Befindlichkeit。特别强调理性和人生在世的遭遇当中，所产生的一种非常复杂的感情，一种生活情态；使得每个人都不得不在生活中，带着不同的生活情态，去面对这个“令人烦恼”的生活世界，人们不得不处理感情因素同理性因素的相互关系，要在生活中一再地面临情感与理性的协调问题，使人陷入理性与情感之间相互协调的难题之中，把人的自由问题，变成非常复杂的生活情态难题。这一点，海德格尔在分析中进行了非常恰当的一个分析，而且可以说扭转了当时西方传统的人性论中对人的看法。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 海德格尔 </tag>
            
            <tag> 存在主义 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（7）：高宣扬讲存在主义（一）]]></title>
      <url>/2018/01/20/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%887%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>海德格尔在1927年发表《存在与时间》这本书的时候特别提到，他的存在哲学最重要的贡献就在于把“存在”这样一个最基本的哲学问题重新提出来。存在主义特别强调的是研究作为活现象的存在本身的自我显现</p>
<a id="more"></a>
<p>我首先要跟大家一起分享的和讨论的，是我刚刚在去年年底出版的《存在主义》这本书，这是因为这本书所谈论的问题比较重要，而且也跟我们每个人的生活的命运和我们的生命的命运，又紧紧相关。更清楚地说，这本《存在主义》所讨论的，都是关于人生的基本问题；这个基本问题，在任何时候以及对任何一个人，都是性命攸关的。</p>
<p>  <img src="http://omu7tit09.bkt.clouddn.com/莫奈  悬崖（1）.jpg" alt="莫奈  悬崖（1）"></p>
<p>存在主义这个思潮，早在19世纪末的时候，就已经在欧洲出现了，但那个时候还没有成为一个正式的流行的思潮；当时已经被人们提出来了，主要是结合了当时欧洲社会的动荡和文化的危机，才出现了关于存在主义的一些论题。</p>
<p>这些论题的提出，首先是在文学界提出来的，当时丹麦的哲学家Søren Kierkegaard（索伦・克尔凯郭尔），他提出了关于存在主义的一些新的命题。他的新命题主要是认为人生是孤独的。这一命题，他是根据基督教的思想的传统，结合当时欧洲的危机，认为人生是很孤独的，而且充满着忧虑，所以关于，存在主义，所讨论的一些关于人生的问题，特别是关于人到底应该怎么样对待自己的生活？怎么样对待人生中遇到的危机，当遭遇到了各种意想不到的命运的时候，如何去处理，如何去面对？这样一个问题，在19世纪末就被突出地提出来了。Søren Kierkegaard在当时提出了这样一个（问题），能够把它从哲学上加以总结并提出来，宣示了存在主义的最早的呼声。</p>
<p>与此同时，俄国的著名小说家陀思妥耶夫斯基（Фёдор  Михайлович  Достоевский），也在他的好几本重要的小说里面，特别是《卡拉马佐夫兄弟》（братъя  карамазовы）这本书里面，已经很深刻的描述了，人生在世所遭遇的各种苦难，各种经历，以及这种经历给予人的心理的冲击，使得人对人生的基本态度，产生了所谓忧虑，所谓各种各样的烦恼。</p>
<p>所以存在主义应该说是很早就提出来，但是只有到了第一次世界大战爆发的时候，也就是说在1914年到1918年，第一次世界大战的战争的浩劫，给欧洲人带来了极大的冲击的情况下，存在主义才进一步被哲学家重新的进行讨论，并且加以系统的总结。</p>
<p>达达主义和超现实主义同样的，都是一个类似存在主义的思潮，跟存在主义几乎是有一个同样的基调，都认为人生不可把握，而且认为社会是很黑暗的。他们几乎都是采取对社会进行否定和颠覆的这样一种态度，而且对人生感到，要活着就必须要反抗，这样一个问题。</p>
<p>这样一个思潮产生以后，首先集中在德国和法国，一直到现在，一直在流传开来，而且它在理论上成为了哲学的一个派别以后，它更加深入人心，因为它触动了人类的灵魂本身。所以它不是一个，单纯只是在一个历史阶段中短期的一种思潮。而是已经可以说，把握了人心的最薄弱的那个环节，使得它能够很赤裸裸的，揭露人性中的那些最脆弱的部分。这一点是具有普遍意义的。</p>
<p>这本书是为了纪念我的老师熊伟先生。熊伟先生诞辰105年了，这本书的第一章的照片里面，我特别把熊伟老师在跟我合照的照片，把它登在那里面。因为熊伟先生是中国第一位，直接的聆听存在主义大师德国的海德格尔（Martin Heidegger）的中国学者。他30年代在弗莱堡听课以后，又在40年代回到中国。当时也是第一个，把海德格尔的基本思想、他的存在哲学传播到中国的著名学者。<br>  <img src="http://omu7tit09.bkt.clouddn.com/莫奈  悬崖（2）.jpg" alt="莫奈  悬崖（2）"><br>熊伟先生作出了重要的贡献，就是他能够把海德格尔用德语表述的艰难的、很艰涩的一些思想，结合中国的传统思想，特别是道家的思想、老子的思想，关于自然无为的思想，结合在一起，然后用中国的语言很准确地表达了。但是现象学尽管难懂，只有把握了现象学的原则，你才能够懂得存在主义。</p>
<p>在传统的科学思维、传统的哲学思维中，那些现象是固定不变的一个客观的对象，它是什么就是什么。一个杯子，高度、长度、宽度、形式等等，这都是现成的在那。所以我们的任务似乎就是要去测定它到底是什么样的。是什么样，我们就说什么样。但现象学不是这样，现象学要是让它“现象”出来。所谓要“现象”出来，（熊伟）他说就是，要使得它成为我们的感性的眼睛或智慧的眼睛的对象。感性的眼睛和智慧的眼睛什么意思？</p>
<p>后面他又讲，实际上所谓感性的和智慧的眼睛，就是我们的有它自身的生命力的眼睛，我们的眼睛是有它自己的生命的，这样的有生命的眼睛在观看现象的时候，总是带有感情的去看它；而且，在不同的时候看，眼睛所看到的现象，就不一样。所以，我们的眼睛所看到的现象，已经不是一个不动的“主体”去孤立地观看那些“客观”存在于外界，单纯地作为一种与我们无关的“对象”，而是通过我们的有生命力的眼睛的观看，</p>
<p>我们所看到的现象，实际上是已经在我们的眼睛与外界现象之间，在我们的有生命的眼睛与同样有自身生命的现象之间，建立了一种活生生的关联。用胡塞尔（Husserl）的话来说，就是构成了活的主体与同样活的客体之间的相互联系，这种联系，既不是单纯的主体性，也不是单纯的客体性，而是在两者之间的相互关系性，是在不同场合中产生的相互关系性；后来，胡塞尔由此特别强调一种“生活世界”的概念，强调任何观察，都是在特定的生活世界中发生，以此试图克服传统主观主义与客观主义的简单对立。这样一来，通过现象学的研究，把观察中的人与被观察的世界现象，搭起一个相互连接的桥梁，强调不同的人在不同的环境中所观察到的现象的差异性和连贯性。</p>
<p>海德格尔举例子，梵谷（梵高）画了一幅画，这幅画画得很简单，就是画了一支农靴，一个农民用过的靴子。然后，海德格尔就分析说，因为梵谷自己出身很贫寒，他自己经历了很多的曲折。他做过贫苦的工人，做过煤矿工人，做过给神父做服务的那些，在教会里面服务的小生，总之他经历了很多苦难。而且他后来到巴黎以后，住在巴黎北郊，郊区的农村里，他特别注意到，他所住的很贫寒的咖啡店的楼阁，用很便宜的房租在那边住。梵谷住在那里，对那里产生了感情，喜欢那个地方，虽然远离市中心，但是他能够亲眼看到，在他的咖啡店的小阁楼，看到的一片的麦田，他感到很亲切，因为什么？</p>
<p>  <img src="http://omu7tit09.bkt.clouddn.com/莫奈  悬崖（3）.jpg" alt="莫奈  悬崖（3）"><br>因为他经常看到，来了到麦田里劳作的那些农民，他们的辛劳，他们怎么样从夏天到秋天到冬天到春天，这么一年春夏秋冬的，一年又一年的辛勤劳作。经历了风雨的吹袭，然后他们的靴子里面，靴子上留下了许许多多的泥巴，这些泥巴记载了穿这个靴子的农民，他怎么样辛劳的劳动。而且同时的也在那里，等于是把辛苦劳动的农民，跟天地之间进行交流，这样一来，农民在种地的时候，他自己的这种生活的经历，他的情感，对这个世界的看法，都活生生的展现在那个靴子上。靴子上的每一个洞，每一个泥土，每个不同的痕迹，它的水汽也好，它的灰尘也好，都很形象地显示了、再现了农民的、他亲历的生活。</p>
<p>所以这就是，通过这个靴子，海德格尔做了很深刻的分析，说那个靴子不是死的，不是仅仅作为对象的靴子。因为作为对象的靴子，人人都看出是一个靴子，但是那个靴子当我看的时候，当梵谷看的时候，你看的时候都不一样。因为你们不同的人生、经验，使得你对出现在你面前的这个靴子，产生了不同的效果。那么这个靴子就很自然的在你面前，好像是一个活生生的在那边，一个活的物，在那边展现出来。这就是它所谓的现象，是吧？现象，每次在不同的时候，在不同的人面前，都以不同的人和不同的生活经历作为不同的背景，以不同的方式“显现”出来。这就是现象学的一个非常重要的原理。只有把握了这个原理，你才能够懂得，为什么存在主义研究人生的时候，必须要通过现象学这个方法，所以这是一个很重要的（原理）。</p>
<p>所以他说，这就是现象学所提到的，关于现象学研究的是“事情本身”。因为胡塞尔曾经对现象学做了一个概括，说现象学无非就是要回到“事情本身”。所以现象学的“事情本身”，就是要使各种不同的现象，在我们面前和当下“现象出来”，活生生地“显现”出来。</p>
<p>熊伟先生把这个“现象出来”，用他的自己的话把它说出来。让我们再说一遍，他说，所谓“现象出来”，就是，他说了，“要让现象‘在出来’”。“在出来”三个字，这个“在”就是“存在”，把它简写叫“在”。因此，“在”是活的，是因时因地而变的。要现象“在出来”，就是要现象存在出来。所谓现象存在出来，就是现象自己显现自己。所以在这个意义上说，所以，我说在，也就是存在，是自我生成的显现过程。而“在”的这种自我显现，就是存在本身。一切存在，都是自我显现的现象，也正因为这样，一切现象，通过它自己的自我显现，显示出它的活生生的性质；不仅在不同的环境，而且，对不同的观察者，面对不同的现象，这个观察者，他都以不同的姿态和不同的方式，把不同的或现象，“显现出来”。</p>
<p>由此可见，一切现象，由于都是自我显现，绝不是如同自然科学所研究的“客观对象”那样，都是统一的客观对象；这样一来，真正的现象，它们不可能都是千遍一律的，更不是死板不变的“客体”。</p>
<p>   <img src="http://omu7tit09.bkt.clouddn.com/莫奈  悬崖（4）.jpg" alt="莫奈  悬崖（4）"></p>
<p>他特别强调，熊伟先生说，只有通过“在”的自我显现，一切在者才有可能存在于世。这句话又是很重要，因为在这里面涉及到，存在主义所说的“存在”这个概念和“存在者”的区别。存在者，注意，就多一个字。存在和存在者的区别等于是，现象学和存在主义同传统的形而上学思考、同传统的科学思考的区别。为什么这么说？存在主义和现象学特别强调的是，现象学和存在主义都是研究存在本身，研究作为活现象的存在的自我显现。在这里，存在本身自我显现，所以它特别强调存在是活的，是有生命的，是存在自己存在出来；它既不是存在者，也不是被传统形而上学所扭曲的那种从现实的存在抽象出来的某种“本体”。存在它自己显出来，而它这个显出来，是在我跟它的关系中发生的。这是现象学和存在主义的基本原则。</p>
<p>但是科学思维不是这样。科学思维把它当作认识的对象，认为这个“在”不是存在，而是存在者，它用存在者来代替存在。为什么？因为这个杯子，如果说在科学研究中，可是我一再说，这个杯子是作为对象，它不是存在本身，在科学思维面前，它是一个存在者，一个已经在那里存在的存在者，作为一个客观的对象，它存在在那里。现象学和存在主义所关心的，不是作为存在者的现象，而是作为存在本身的现象。</p>
<p>所以海德格尔在1927年发表《存在与时间》（SEIN UND ZEIT）这本书的时候，他特别提到，他的存在哲学的最重要的贡献就在于，把存在这样一个最基本的哲学问题，重新提出来。而“存在”这个问题，从古希腊，就是公元前六世纪以后，由于苏格拉底和柏拉图，把他们之前，希腊的自然哲学的那种自然思考的方式，把它给篡改成为，改变成为以人为主体的一种认识真理的方式，存在就因此变成了作为对象的存在者。</p>
<p>由于柏拉图等人的这种旋转，把“存在”最重要的一个哲学问题，把它变成为存在者的问题。这样一来，就把“存在”这样一个原来最基本的希腊的最早的问题，把它给忘记了2000年。所以存在主义的问题，就是要重新的把这个存在提出来，要去一再强调，哲学研究的对象不是存在者，不是现成的现象，而是存在本身。而所谓存在本身就是，这个“存在”是靠它自己“在”出来，靠它自己的显现显象出来，是这样的。</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 海德格尔 </tag>
            
            <tag> 存在主义 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python编程系列（2）：JSON]]></title>
      <url>/2018/01/20/python%E7%BC%96%E7%A8%8B%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9AJSON/</url>
      <content type="html"><![CDATA[<h2 id="一、Json简介"><a href="#一、Json简介" class="headerlink" title="一、Json简介"></a>一、Json简介</h2><p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯（包括C, C++, C#, Java, JavaScript, Perl, Python等）。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成。</p>
<a id="more"></a>
<p>存储在SQL数据库中的数据往往是规整的，比如这里有一个SQLite数据库的例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">id|code|name|area|area_land|area_water|population|population_growth|birth_rate|death_rate|migration_rate|created_at|updated_at</div><div class="line">1|af|Afghanistan|652230|652230|0|32564342|2.32|38.57|13.89|1.51|2015-11-01 13:19:49.461734|2015-11-01 13:19:49.461734</div><div class="line">2|al|Albania|28748|27398|1350|3029278|0.3|12.92|6.58|3.3|2015-11-01 13:19:54.431082|2015-11-01 13:19:54.431082</div><div class="line">3|ag|Algeria|2381741|2381741|0|39542166|1.84|23.67|4.31|0.92|2015-11-01 13:19:59.961286|2015-11-01 13:19:59.961286</div></pre></td></tr></table></figure>
<p>上述的数据由行和列组成，其中每列映射到一个已定义的属性，如id或者name。其中每一行代表一个国家，每一列代表了这个国家一些特征。但随着数据量的增加，在存储的时候我们通常不知道数据的确切的结构，这被称为非结构化数据。一个很好的例子就是网站上的访客列表，下面是发送到服务器的事件列表的示例:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&#123;'event_type': 'started-mission',</div><div class="line"> 'keen': &#123;'created_at': '2015-06-12T23:09:03.966Z',</div><div class="line">  'id': '557b668fd2eaaa2e7c5e916b',</div><div class="line">  'timestamp': '2015-06-12T23:09:07.971Z'&#125;,</div><div class="line"> 'sequence': 1&#125;</div><div class="line"> </div><div class="line">&#123;'event_type': 'started-screen',</div><div class="line"> 'keen': &#123;'created_at': '2015-06-12T23:09:03.979Z',</div><div class="line">  'id': '557b668f90e4bd26c10b6ed6',</div><div class="line">  'timestamp': '2015-06-12T23:09:07.987Z'&#125;,</div><div class="line"> 'mission': 1,</div><div class="line"> 'sequence': 4,</div><div class="line"> 'type': 'code'&#125;</div><div class="line"> </div><div class="line">&#123;'event_type': 'started-screen',</div><div class="line"> 'keen': &#123;'created_at': '2015-06-12T23:09:22.517Z',</div><div class="line">  'id': '557b66a246f9a7239038b1e0',</div><div class="line">  'timestamp': '2015-06-12T23:09:24.246Z'&#125;,</div><div class="line"> 'mission': 1,</div><div class="line"> 'sequence': 3,</div><div class="line"> 'type': 'code'&#125;,</div></pre></td></tr></table></figure>
<p>上面列出了三个独立事件。每个事件都有不同的字段，有些字段嵌套在其他字段中。这种类型的数据很难在常规的SQL数据库中存储。所以这种非结构化数据通常以JavaScript对象表示法(JSON)格式存储。JSON是一种将列表和字典等数据结构编码成字符串的方法，这样来确保它们易于被机器读取。尽管JSON以Javascript开头，但它实际上只是一种格式，可以通过任何语言读取。</p>
<p>Json对象是一个无序的“‘名称/值’对”集合。一个对象以“{”（左括号）开始，“}”（右括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。</p>
<p>它的值可以是双引号括起来的字符串（string）、数值(number)、true、false、 null、对象（object）或者数组（array）。这些结构可以嵌套。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15253600039550.gif" alt=""></p>
<h2 id="二、Python编码和解析Json"><a href="#二、Python编码和解析Json" class="headerlink" title="二、Python编码和解析Json"></a>二、Python编码和解析Json</h2><p>Python有很大的JSON支持。我们可以将列表和字典转换为JSON，并将字符串转换为列表和字典。JSON数据看起来很像Python中的字典（dictionary），其中存储了键和值。</p>
<p>使用 JSON 函数需要导入 json 库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import json</div></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>json.dumps</td>
<td>将 Python 对象编码成 JSON 字符串</td>
</tr>
<tr>
<td>json.loads</td>
<td>将已编码的 JSON 字符串解码为 Python 对象</td>
</tr>
</tbody>
</table>
</div>
<p>将Python的字典结构导出到json使用<code>json.dumps()</code>，将json读成Python的字典结构，使用<code>json.loads()</code>。<br>如果不是针对string操作而是对文件操作，分别使用<code>json.load()</code>函数和<code>json.dump()</code>函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">from pprint import pprint </div><div class="line">  </div><div class="line">data = [</div><div class="line"> &#123;&apos;event_type&apos;: &apos;started-mission&apos;,</div><div class="line"> &apos;keen&apos;: &#123;&apos;created_at&apos;: &apos;2015-06-12T23:09:03.966Z&apos;,</div><div class="line">  &apos;id&apos;: &apos;557b668fd2eaaa2e7c5e916b&apos;,</div><div class="line">  &apos;timestamp&apos;: &apos;2015-06-12T23:09:07.971Z&apos;&#125;,</div><div class="line"> &apos;sequence&apos;: 1&#125;,</div><div class="line"> </div><div class="line">&#123;&apos;event_type&apos;: &apos;started-screen&apos;,</div><div class="line"> &apos;keen&apos;: &#123;&apos;created_at&apos;: &apos;2015-06-12T23:09:03.979Z&apos;,</div><div class="line">  &apos;id&apos;: &apos;557b668f90e4bd26c10b6ed6&apos;,</div><div class="line">  &apos;timestamp&apos;: &apos;2015-06-12T23:09:07.987Z&apos;&#125;,</div><div class="line"> &apos;mission&apos;: 1,</div><div class="line"> &apos;sequence&apos;: 4,</div><div class="line"> &apos;type&apos;: &apos;code&apos;&#125;</div><div class="line">]</div><div class="line"></div><div class="line"></div><div class="line">json_str = json.dumps(data, sort_keys=True, indent=1, separators=(&apos;,&apos;, &apos;: &apos;))</div><div class="line"></div><div class="line">data = json.loads(json_str)</div><div class="line"></div><div class="line">print type(json_str)</div><div class="line">print type(data)</div><div class="line">pprint(data)</div><div class="line"></div><div class="line"># Writing JSON data to file</div><div class="line">with open(&apos;data.json&apos;, &apos;w&apos;) as f:</div><div class="line">    json.dump(data, f)</div><div class="line">    </div><div class="line"># Reading data back</div><div class="line">with open(&apos;data.json&apos;, &apos;r&apos;) as f:</div><div class="line">    data = json.load(f)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">&lt;type &apos;list&apos;&gt;</div><div class="line">[&#123;u&apos;event_type&apos;: u&apos;started-mission&apos;,</div><div class="line">  u&apos;keen&apos;: &#123;u&apos;created_at&apos;: u&apos;2015-06-12T23:09:03.966Z&apos;,</div><div class="line">            u&apos;id&apos;: u&apos;557b668fd2eaaa2e7c5e916b&apos;,</div><div class="line">            u&apos;timestamp&apos;: u&apos;2015-06-12T23:09:07.971Z&apos;&#125;,</div><div class="line">  u&apos;sequence&apos;: 1&#125;,</div><div class="line"> &#123;u&apos;event_type&apos;: u&apos;started-screen&apos;,</div><div class="line">  u&apos;keen&apos;: &#123;u&apos;created_at&apos;: u&apos;2015-06-12T23:09:03.979Z&apos;,</div><div class="line">            u&apos;id&apos;: u&apos;557b668f90e4bd26c10b6ed6&apos;,</div><div class="line">            u&apos;timestamp&apos;: u&apos;2015-06-12T23:09:07.987Z&apos;&#125;,</div><div class="line">  u&apos;mission&apos;: 1,</div><div class="line">  u&apos;sequence&apos;: 4,</div><div class="line">  u&apos;type&apos;: u&apos;code&apos;&#125;]</div></pre></td></tr></table></figure>
<p>在编码JSON的时候，这里我们为了获得漂亮的格式化字符串，可以使用 json.dumps() 的indent参数。 它会使得输出和pprint()函数效果类似。</p>
<p>默认的类型对应如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>JSON</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td>object</td>
<td>dict</td>
</tr>
<tr>
<td>array</td>
<td>list</td>
</tr>
<tr>
<td>string</td>
<td>unicode</td>
</tr>
<tr>
<td>number (int)</td>
<td>int, long</td>
</tr>
<tr>
<td>number (real)</td>
<td>float</td>
</tr>
<tr>
<td>true</td>
<td>True</td>
</tr>
<tr>
<td>false</td>
<td>False</td>
</tr>
<tr>
<td>null</td>
<td>None</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三、其他数据类型与Json之间的编码和解码"><a href="#三、其他数据类型与Json之间的编码和解码" class="headerlink" title="三、其他数据类型与Json之间的编码和解码"></a>三、其他数据类型与Json之间的编码和解码</h2><p>一般来讲，JSON解码会根据提供的数据创建dicts或lists。 如果你想要创建其他类型的对象，可以给 json.loads() 传递object_pairs_hook或object_hook参数。 例如，下面是演示如何解码JSON数据并在一个OrderedDict中保留其顺序的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">from collections import OrderedDict</div><div class="line">s = &apos;&#123;&quot;name&quot;: &quot;ACME&quot;, &quot;shares&quot;: 50, &quot;price&quot;: 490.1&#125;&apos;</div><div class="line">data = json.loads(s, object_pairs_hook=OrderedDict)</div><div class="line">data</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">OrderedDict([(u&apos;name&apos;, u&apos;ACME&apos;), (u&apos;shares&apos;, 50), (u&apos;price&apos;, 490.1)])</div></pre></td></tr></table></figure>
<p>下面是如何将一个JSON字典转换为一个Python对象例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">s = &apos;&#123;&quot;name&quot;: &quot;ACME&quot;, &quot;shares&quot;: 50, &quot;price&quot;: 490.1&#125;&apos;</div><div class="line"></div><div class="line">class JSONObject:</div><div class="line">    def __init__(self, d):</div><div class="line">        self.__dict__ = d</div><div class="line">        </div><div class="line">data = json.loads(s, object_hook=JSONObject)</div></pre></td></tr></table></figure>
<p>这里，JSON解码后的字典作为一个单个参数传递给 <code>__init__()</code> 。 然后，就可以随心所欲的使用了，比如作为一个实例字典来直接使用它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data.name</div><div class="line">data.price</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ACME</div><div class="line">490.1</div></pre></td></tr></table></figure>
<p>对象实例通常并不是JSON可序列化的。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">class Point:</div><div class="line">    def __init__(self, x, y):</div><div class="line">        self.x = x</div><div class="line">        self.y = y</div><div class="line"></div><div class="line">p = Point(2, 3)</div><div class="line">json.dumps(p)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TypeError: &lt;__main__.Point instance at 0x10aa97c20&gt; is not JSON serializable</div></pre></td></tr></table></figure>
<p>如果你想序列化对象实例，你可以提供一个函数，它的输入是一个实例，返回一个可序列化的字典。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def serialize_instance(obj):</div><div class="line">    d = &#123; &apos;__classname__&apos; : type(obj).__name__ &#125;</div><div class="line">    d.update(vars(obj))</div><div class="line">    return d</div></pre></td></tr></table></figure>
<p>下面是如何使用这些函数的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def serialize_instance(obj):</div><div class="line">    d = &#123; &apos;__classname__&apos; : type(obj).__name__ &#125;</div><div class="line">    d.update(vars(obj))</div><div class="line">    return d</div><div class="line"></div><div class="line"></div><div class="line">p = Point(2,3)</div><div class="line">s = json.dumps(p, default=serialize_instance)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&apos;&#123;&quot;y&quot;: 3, &quot;x&quot;: 2, &quot;__classname__&quot;: &quot;instance&quot;&#125;&apos;</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Python </category>
            
        </categories>
        
        
        <tags>
            
            <tag> JSON </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python编程系列（1）：正则表达式]]></title>
      <url>/2018/01/19/python%E7%BC%96%E7%A8%8B%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      <content type="html"><![CDATA[<p>对文本进行处理在数据科学实践中必不可少的一环，业界的文本数据往往杂乱无章，而且数量及其庞大，当我们需要对文本进行片段匹配时，就要求我们利用计算机来批量地在文本中检索某种模式。正则表达式（Regular Expression）就是可以进行文本匹配的一种高级模式，它是一些由字符和特殊符号组成的字符串，它可以按照某种模式匹配一系列有相似特征的字符串。</p>
<a id="more"></a>
<p>最简单的正则表达式就是普通字符串，它仅仅可以匹配其自身。比如正则表达式“python”只可以匹配字符串“python”。正则表达式的强大之处在于特殊符号的应用，特殊符号定义了字符集合、子组匹配以及模式的重复次数。正是这些特殊符号使得一个正则表达式可以匹配字符串集合而不只是一个字符串。</p>
<h2 id="一、元字符"><a href="#一、元字符" class="headerlink" title="一、元字符"></a>一、元字符</h2><p>下图列出了Python支持的正则表达式元字符和语法：<br><img src="http://omu7tit09.bkt.clouddn.com/15163742862820.png" alt=""></p>
<h3 id="1-1-择一匹配符号“-”"><a href="#1-1-择一匹配符号“-”" class="headerlink" title="1.1 择一匹配符号“|”"></a>1.1 择一匹配符号“|”</h3><p>表示从多个模式中选择一个，用于分割不同的正则表达式，可以匹配不止一个字符串，等同于逻辑“或”。例如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bat | bet | bit    # 可以匹配字符串bat或者bet或者bit</div></pre></td></tr></table></figure>
<h3 id="1-2-任意字符匹配符号“-”"><a href="#1-2-任意字符匹配符号“-”" class="headerlink" title="1.2 任意字符匹配符号“.”"></a>1.2 任意字符匹配符号“.”</h3><p>“.”号可以匹配除了换行符以为的任何字符（Python正则表达式有一个编译标记[S或DOTALL]能够使“.”匹配换行符），要匹配“.”号自身，必须使用反斜线转译符号“.”。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">f.o  #能够匹配f和o之间加上任意一个字符的样式，如：fao、f9o、f#o</div><div class="line">..   #能够匹配任意两个字符</div></pre></td></tr></table></figure>
<h3 id="1-3-匹配字符串开始“-”或结尾“-”"><a href="#1-3-匹配字符串开始“-”或结尾“-”" class="headerlink" title="1.3 匹配字符串开始“^”或结尾“$”"></a>1.3 匹配字符串开始“^”或结尾“$”</h3><p>匹配字符串以什么开始的，可以使用脱字符“^”或\A;<br>匹配字符串以什么结束的，可以使用美元符“$”或\Z;<br>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">^From  #任何以From开始的字符串</div><div class="line">tcsh$  #任何以tcsh结尾的字符串</div><div class="line">^subject:hi$   #任何由单独的字符串subject：hi构成的字符串</div></pre></td></tr></table></figure>
<h3 id="1-4-匹配单词边界：“-b”、“-B”"><a href="#1-4-匹配单词边界：“-b”、“-B”" class="headerlink" title="1.4 匹配单词边界：“\b”、“\B”"></a>1.4 匹配单词边界：“\b”、“\B”</h3><p>\b：匹配单词的边界（单词前或后），而不在乎单词中间的字符<br>\B：匹配单词中间的字符，而不在乎单词边界的字符<br>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">er\b   #可以匹配“never”中的“er”，但不能匹配“verb”中的“er”，只关心后边</div><div class="line">er\B   #能匹配“verb”中的“er”，但不能匹配“never”中的“er”,只关心中间</div><div class="line">\bthe  #匹配任何以the开头的字符串</div></pre></td></tr></table></figure>
<h3 id="1-5-字符集“-”"><a href="#1-5-字符集“-”" class="headerlink" title="1.5 字符集“[ ]”"></a>1.5 字符集“[ ]”</h3><p>当想要匹配指定的某些字符的时候，使用字符集是很方便的。<br>注意：字符集只适用于单字符的情况。也就是说[ab]表示只从ab中选择一个</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">b[ae]t  #匹配bat、或bet</div><div class="line">[01][ab]  #匹配0a、0b、1a、1b</div></pre></td></tr></table></figure>
<h3 id="1-6-字符集中的范围“-”和否定“”"><a href="#1-6-字符集中的范围“-”和否定“”" class="headerlink" title="1.6 字符集中的范围“-”和否定“”"></a>1.6 字符集中的范围“-”和否定“<sup><a href="#fn_" id="reffn_"></a></sup>”</h3><p>-:表示一个字符的范围<br>^:不匹配指定字符集里的任意字符</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">z.[0-9]   #字母z后面跟着任何一个字符，然后跟着一个数字</div><div class="line">[^aeiou]  #一个非元音字符</div><div class="line">[^\t\n]   #不匹配制表符或\n</div><div class="line">[&quot;-a]   #在一个ASCII系统中，位于“&quot;”和“a”之间的字符，即34-97之间的字符</div></pre></td></tr></table></figure>
<h3 id="1-7-特殊符号（-，-，？，-）"><a href="#1-7-特殊符号（-，-，？，-）" class="headerlink" title="1.7 特殊符号（*，+，？，{}）"></a>1.7 特殊符号（*，+，？，{}）</h3><p>*：匹配其左边的正则表达式出现零次或多次的情况。<br>+：匹配一次或多次出现的正则表达式。<br>？：匹配零次或一次出现的正则表达式。<br>{N}、{M,N}: 匹配前面的正则表达式N次或M～N次</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[dn]ot?  #字母d或n后面跟一个o，然后后面最多再跟一个t。例如：do、no、dot、not</div><div class="line">0？[1-9] #一个1到9的数字，前面跟或不跟一个0</div><div class="line">[0-9]&#123;15,16&#125;  #匹配15或16个数字。例如信用卡号码</div><div class="line">&lt;/?[^&gt;]+&gt;  #匹配全部有效的（和无效的）HTML标签</div></pre></td></tr></table></figure>
<h3 id="1-8-特殊字符"><a href="#1-8-特殊字符" class="headerlink" title="1.8 特殊字符"></a>1.8 特殊字符</h3><p>\d：十进制数字，相当于[0-9]<br>\D：非十进制数字的字符，相当于<sup><a href="#fn_0-9" id="reffn_0-9">0-9</a></sup><br>\w：全部字母数字，相当于[A-Za-z0-9]<br>\W：非字母数字的字符,相当于<sup><a href="#fn_A-Za-z0-9" id="reffn_A-Za-z0-9">A-Za-z0-9</a></sup><br>\s: 空格字符<br>\S: 非空格字符</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">\w+-\d+  #一个由字母数字组成的字符串和一串由连字符分隔的数字</div><div class="line">[A-Za-z]\w* #第一个是字母，其余是字母或数字</div><div class="line">\d&#123;3&#125;-\d&#123;3&#125;-\d&#123;4&#125;  #美国电话号码格式，例如800-555-1212</div><div class="line">\w+@\w+\.com  #以xxx@yyy.com格式表示的简单电子邮件地址</div></pre></td></tr></table></figure>
<h3 id="1-9-圆括号指定分组"><a href="#1-9-圆括号指定分组" class="headerlink" title="1.9 圆括号指定分组"></a>1.9 圆括号指定分组</h3><p>有时候除了进行匹配操作外，我们还想要提取所匹配的子组，例如：\w+-\d+,这个正则表达式想要分别保存第一部分的字母和第二部分的数字，该怎么实现？我们可能这样做的原因是对于任何成功的匹配，我们想要看到匹配的字符串究竟是什么。如果为两个子模块都加上圆括号，例如(\w+)-(\d),然后就能够分别访问每一个匹配的子组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">\d+(\.\d*)?  #匹配浮点数的字符串，如：“5”、“5.”、“5.009”等</div></pre></td></tr></table></figure>
<h3 id="1-10-扩展表示法"><a href="#1-10-扩展表示法" class="headerlink" title="1.10 扩展表示法"></a>1.10 扩展表示法</h3><p>可以参考上面表格的讲解结合下面的例子就能懂了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Windows(?=95|98|NT|2000)  #能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”</div><div class="line">Windows(?!95|98|NT|2000)  #能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”</div><div class="line">(?&lt;=95|98|NT|2000)Windows  #能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”</div><div class="line">(?&lt;!95|98|NT|2000)Windows  #能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”</div><div class="line">industr(?:y|ies)  #就是一个比“industry|industries”更简略的表达式</div><div class="line"> </div><div class="line">(?:\w+\.)*  #以点结尾的字符串，如google.</div><div class="line">(?#comment)  #不做匹配，只做注释</div><div class="line">(?=.com)  #一个字符串后面跟着.com才做匹配</div><div class="line">(?!.net)  #一个字符串后面跟的不是.net才做匹配</div><div class="line">(?&lt;=800-)  #字符串前面出现800-才做匹配</div><div class="line">(?&lt;!192\.168\.)  # 字符串前面不是192.168。才做匹配，过滤掉一类ip地址</div><div class="line">(?(1)y|x)  #如果匹配组1（\1）存在，就与y匹配，否则就与x匹配</div></pre></td></tr></table></figure>
<h2 id="二、re模块"><a href="#二、re模块" class="headerlink" title="二、re模块"></a>二、re模块</h2><p>Python语言中使用re模块的方法支持正则表达式。这里列出re模块常见的函数以方便查询（后面会介绍主要的函数使用方法）</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-01-23 上午11.41.37.png" alt="屏幕快照 2018-01-23 上午11.41.37"></p>
<p>下面将分开解释上面的部分函数：</p>
<h3 id="2-1-使用match-和search-匹配字符串，使用group-查看结果"><a href="#2-1-使用match-和search-匹配字符串，使用group-查看结果" class="headerlink" title="2.1 使用match()和search()匹配字符串，使用group()查看结果"></a>2.1 使用match()和search()匹配字符串，使用group()查看结果</h3><p>re.match() :从字符串开始的位置匹配，成功返回匹配的对象，失败返回None<br>re.search(): 扫描整个字符串来进行匹配，成功返回匹配的对象，失败返回None</p>
<blockquote>
<p>例1：比较match() 和 search()的区别</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">m = re.match(&apos;foo&apos;, &apos;seafood&apos;)</div><div class="line">if m is not None: print(&quot;match-&quot; + m.group())</div><div class="line"></div><div class="line">m = re.search(&apos;foo&apos;, &apos;seafood&apos;)</div><div class="line">if m is not None: print(&quot;search-&quot; + m.group())</div><div class="line"></div><div class="line">#结果是：search-foo</div></pre></td></tr></table></figure>
<blockquote>
<p>例2: match()函数从起始位开始匹配</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">m = re.match(&apos;foo&apos;, &apos;foo&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;能匹配-&quot; + m.group())</div><div class="line"></div><div class="line">m = re.match(&apos;foo&apos;, &apos;bar&apos;)</div><div class="line">if m is not None: print(&quot;不能匹配-&quot; + m.group())</div><div class="line"></div><div class="line">m = re.match(&apos;foo&apos;, &apos;food on the table&apos;)</div><div class="line">if m is not None: print(&quot;从开始位置进行匹配-&quot; + m.group())</div><div class="line"></div><div class="line">#能匹配-foo</div><div class="line">#从开始位置进行匹配-foo</div></pre></td></tr></table></figure>
<blockquote>
<p>例3: 匹配多个值（使用择一表达式”|”）</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">bt = &apos;bat|bet|bit&apos;</div><div class="line"></div><div class="line">m = re.match(bt, &apos;bat&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;1能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(bt, &apos;blt&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;2能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(bt, &apos;he bit me&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;3能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.search(bt, &apos;he bit me&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;4能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">#结果：</div><div class="line">#   1能匹配-bat</div><div class="line">#   4能匹配-bit</div></pre></td></tr></table></figure>
<blockquote>
<p>例4: 匹配任何单个字符。点号”.”除了换行符\n和非字符，都能匹配</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">bt = &quot;.end&quot;</div><div class="line"></div><div class="line">m = re.match(bt, &apos;bend&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;bend能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(bt, &apos;end&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;end能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(bt, &apos;\nend&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;\nend能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.search(bt, &apos;the end.&apos;)</div><div class="line">if m is not None:</div><div class="line">    print(&quot;the end.能匹配-&quot; + m.group())</div><div class="line"></div><div class="line">#结果：</div><div class="line">#   bend能匹配-bend</div><div class="line">#   the end.能匹配- end</div></pre></td></tr></table></figure>
<blockquote>
<p>例5: 匹配小数点</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">bt = &quot;3.14&quot;</div><div class="line">pi_bt = &quot;3\.14&quot;  #表示字面量的点号 （dec.point）</div><div class="line"></div><div class="line">m = re.match(bt, &apos;3.14&apos;)    #点号匹配</div><div class="line">if m is not None:</div><div class="line">    print(&quot;3.14能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(pi_bt, &apos;3.14&apos;)  #精确匹配</div><div class="line">if m is not None:</div><div class="line">    print(&quot;精确匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">m = re.match(bt, &apos;3014&apos;)    #点号匹配0</div><div class="line">if m is not None:</div><div class="line">    print(&quot;3014能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">#结果：</div><div class="line"># 3.14能匹配-3.14</div><div class="line"># 精确匹配-3.14</div><div class="line"># 3014能匹配-3014</div></pre></td></tr></table></figure>
<blockquote>
<p>例6： 使用字符集”[ ]”</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">bt = &quot;[cr][23][dp][o2]&quot;</div><div class="line"></div><div class="line">m = re.match(bt, &apos;c3po&apos;)    #点号匹配</div><div class="line">if m is not None:</div><div class="line">    print(&quot;c3po能匹配-&quot; + m.group())</div><div class="line"></div><div class="line"></div><div class="line">#结果：</div><div class="line"># c3po能匹配-c3po</div></pre></td></tr></table></figure>
<blockquote>
<p>例7: 重复、特殊字符</p>
</blockquote>
<p>正则表达式: \w+@\w+.com可以匹配类似nobody@xxx.com的邮箱地址，但是类似nobody@xxx.yyy.aaa.com的地址就不能匹配了。这时候我们可以使用<em> 操作符来表示该模式出现零次或者多次：\w+@(\w+.)</em>\w+.com</p>
<blockquote>
<p>例8: 分组</p>
</blockquote>
<p>group()可以访问每个独立的子组<br>groups()获取一个包含所有匹配子组的元组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import re</div><div class="line">&gt;&gt;&gt; m = re.match(&apos;(\w\w\w)-(\d\d\d)&apos;, &apos;abc-123&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">&apos;abc-123&apos;</div><div class="line">&gt;&gt;&gt; m.group(1)</div><div class="line">&apos;abc&apos;</div><div class="line">&gt;&gt;&gt; m.group(2)</div><div class="line">&apos;123&apos;</div><div class="line">&gt;&gt;&gt; m.groups()</div><div class="line">(&apos;abc&apos;, &apos;123&apos;)</div><div class="line"></div><div class="line">&gt;&gt;&gt; m = re.match(&apos;ab&apos;, &apos;ab&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">&apos;ab&apos;</div><div class="line">&gt;&gt;&gt; m.groups()</div><div class="line">( )</div></pre></td></tr></table></figure>
<blockquote>
<p>例9: 匹配字符串起始和结尾</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">m = re.search(&apos;^the&apos;,&apos;the end.&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">&apos;the&apos;</div><div class="line">&gt;&gt;&gt; m = re.search(&apos;^the&apos;,&apos;sthe end.&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</div><div class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;</div><div class="line">&gt;&gt;&gt; m = re.search(r&apos;\bthe&apos;,&apos;bite the dog&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">&apos;the&apos;</div><div class="line"></div><div class="line">&gt;&gt;&gt; m = re.search(r&apos;\bthe&apos;,&apos;bitethe dog&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</div><div class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;</div><div class="line"></div><div class="line">&gt;&gt;&gt; m = re.search(r&apos;\Bthe&apos;,&apos;bitethe dog&apos;)</div><div class="line">&gt;&gt;&gt; m.group()</div><div class="line">&apos;the&apos;</div></pre></td></tr></table></figure>
<h3 id="2-2-使用findall-、finditer-查找每一次出现的位置"><a href="#2-2-使用findall-、finditer-查找每一次出现的位置" class="headerlink" title="2.2 使用findall()、finditer()查找每一次出现的位置"></a>2.2 使用findall()、finditer()查找每一次出现的位置</h3><p>final() 以列表的形式返回所有能匹配的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import re</div><div class="line">&gt;&gt;&gt; re.findall(&apos;car&apos;, &apos;car sscare&apos;)</div><div class="line">[&apos;car&apos;, &apos;car&apos;]</div></pre></td></tr></table></figure>
<p>finaliter()返回一个顺序访问每一个匹配结果（Match对象）的迭代器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.finditer(r&apos;(th\w+) and (th\w+)&apos;,s, re.I).next().group(1)</div><div class="line">&apos;This&apos;</div><div class="line">&gt;&gt;&gt; re.finditer(r&apos;(th\w+) and (th\w+)&apos;,s, re.I).next().group(2)</div><div class="line">&apos;That&apos;</div></pre></td></tr></table></figure>
<h3 id="2-3-使用sub-和subn-搜索和替换"><a href="#2-3-使用sub-和subn-搜索和替换" class="headerlink" title="2.3 使用sub()和subn()搜索和替换"></a>2.3 使用sub()和subn()搜索和替换</h3><p>两个函数都可以实现搜索和替换功能，将某字符串中所有匹配正则表达式的部分进行某种形式的替换。不同点是subn()还返回一个表示替换了多少次的总数，和返回结果一起以元组的形式返回。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.sub(&apos;[ae]&apos;,&apos;X&apos;,&apos;abcdef&apos;)</div><div class="line">&apos;XbcdXf&apos;</div><div class="line">&gt;&gt;&gt; re.subn(&apos;[ae]&apos;,&apos;X&apos;,&apos;abcdef&apos;)</div><div class="line">(&apos;XbcdXf&apos;, 2)</div></pre></td></tr></table></figure>
<p>进行替换的时候，还可以指定替换的顺序，原理是使用匹配对象的group()方法除了能够获取匹配分组编号外，还可以使用\N，其中N表示要替换字符串中的分组的编号，通过编号就能指定替换的顺序。<br>例如：将美式日期MM/DD/YY{,YY}格式转换成DD/MM/YY{,YY}格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.sub(r&apos;(\d&#123;1,2&#125;)/(\d&#123;1,2&#125;)/(\d&#123;2&#125;|\d&#123;4&#125;)&apos;,r&apos;\2/\1/\3&apos;,&apos;2/20/91&apos;)</div><div class="line">&apos;20/2/91&apos;</div><div class="line">&gt;&gt;&gt; re.sub(r&apos;(\d&#123;1,2&#125;)/(\d&#123;1,2&#125;)/(\d&#123;2&#125;|\d&#123;4&#125;)&apos;,r&apos;\2/\1/\3&apos;,&apos;2/20/1991&apos;)</div><div class="line">&apos;20/2/1991&apos;</div></pre></td></tr></table></figure>
<h3 id="2-4-在限定模式上使用split-分隔字符串"><a href="#2-4-在限定模式上使用split-分隔字符串" class="headerlink" title="2.4 在限定模式上使用split()分隔字符串"></a>2.4 在限定模式上使用split()分隔字符串</h3><p>re模块的split（）可以基于正则表达式的模式分隔字符串。但是当处理的不是特殊符号匹配多重模式的正则表达式时，re.split()和str.split()的工作方式相同，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.split(&apos;:&apos;, &apos;str1:str2&apos;)</div><div class="line">[&apos;str1&apos;, &apos;str2&apos;]</div><div class="line">&gt;&gt;&gt; &apos;str1:str2&apos;.split(&apos;:&apos;)</div><div class="line">[&apos;str1&apos;, &apos;str2&apos;]</div></pre></td></tr></table></figure>
<p>但当处理复杂的分隔时，就需要比普通字符串分隔更强大的处理方式,例如下面匹配复杂情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; DATA = (&apos;Mountation View, CA 94040&apos;, &apos;sunnyvale, CA&apos;, &apos;Los Altos, 94023&apos;, &apos;Palo Alto CA&apos;,&apos;Cupertino 95014&apos;)</div><div class="line">&gt;&gt;&gt; for datum in DATA: print(re.split(&apos;, |(?= (?:\d&#123;5&#125;|[A-Z]&#123;2&#125;)) &apos;,datum))</div><div class="line">... </div><div class="line">[&apos;Mountation View&apos;, &apos;CA&apos;, &apos;94040&apos;]</div><div class="line">[&apos;sunnyvale&apos;, &apos;CA&apos;]</div><div class="line">[&apos;Los Altos&apos;, &apos;94023&apos;]</div><div class="line">[&apos;Palo Alto&apos;, &apos;CA&apos;]</div><div class="line">[&apos;Cupertino&apos;, &apos;95014&apos;]</div></pre></td></tr></table></figure>
<p>上述的正则表达式：当一个空格紧跟在5个数字或2个字母后面时就用split语句分隔。当遇到“，”也用split函数分隔。</p>
<h3 id="2-5-扩展符号"><a href="#2-5-扩展符号" class="headerlink" title="2.5 扩展符号"></a>2.5 扩展符号</h3><p>通过使用(?iLmsux)系列选项，可以直接在正则表达式里面指定一个活着多个标记。以下是使用re.I/IGNORECASE的示例，第二个是使用re.M/MULTILINE实现多行混合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r&apos;(?i)yes&apos;,&apos;yes? Yes. YES!!!&apos;)</div><div class="line">[&apos;yes&apos;, &apos;Yes&apos;, &apos;YES&apos;]</div><div class="line">&gt;&gt;&gt; re.findall(r&apos;(?i)th\w+&apos;,&apos;The quickest way is through this tunnel.&apos;)</div><div class="line">[&apos;The&apos;, &apos;through&apos;, &apos;this&apos;]</div><div class="line">&gt;&gt;&gt; re.findall(r&apos;(?im)(^th[\w ]+)&apos;, &quot;&quot;&quot;</div><div class="line">... This is the first,</div><div class="line">... another line,</div><div class="line">... that line,it&apos;s the best</div><div class="line">... &quot;&quot;&quot;)</div><div class="line">[&apos;This is the first&apos;, &apos;that line&apos;]</div></pre></td></tr></table></figure>
<p>通过使用“多行”，能够在目标字符串中实现跨行搜索，而不必将整个字符串视为单个实体。</p>
<p>下一个例子用来演示re.S/DOTALL，该标记表示点号（.）能够用来表示\n符号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r&apos;th.+&apos;,&quot;&quot;&quot;</div><div class="line">... The first line</div><div class="line">... the second line</div><div class="line">... the third line</div><div class="line">... &quot;&quot;&quot;)</div><div class="line">[&apos;the second line&apos;, &apos;the third line&apos;]</div><div class="line">&gt;&gt;&gt; re.findall(r&apos;(?s)th.+&apos;,&quot;&quot;&quot;</div><div class="line">... The first line</div><div class="line">... the second line</div><div class="line">... the third line</div><div class="line">... &quot;&quot;&quot;)</div><div class="line">[&apos;the second line\nthe third line\n&apos;]</div></pre></td></tr></table></figure>
<p>re.X/VERBOSE标记允许用户通过抑制在正则表达式中使用空白符来创建更易读的正则表达式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.search(r&apos;&apos;&apos;(?x)</div><div class="line">... \((\d&#123;3&#125;)\) #区号</div><div class="line">... [ ]  #空白符</div><div class="line">... (\d&#123;3&#125;) #前缀</div><div class="line">... -  #横线</div><div class="line">... (\d&#123;4&#125;) #终点数字</div><div class="line">... &apos;&apos;&apos;,&apos;(800) 555-1212&apos;).groups()</div><div class="line">(&apos;800&apos;, &apos;555&apos;, &apos;1212&apos;)</div></pre></td></tr></table></figure>
<p>(?:…)符号可以对部分正则表达式进行分组，但是不会保存该分组用于后续的检索或应用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r&apos;http://(?:\w+\.)*(\w+\.com)&apos;,</div><div class="line">... &apos;http://google.com http://www.google.com http://code.google.com&apos;)</div><div class="line">[&apos;google.com&apos;, &apos;google.com&apos;, &apos;google.com&apos;]</div><div class="line">&gt;&gt;&gt; re.search(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?:\d&#123;4&#125;)&apos;,</div><div class="line">... &apos;(800) 555-1212&apos;).groupdict()</div><div class="line">&#123;&apos;areacode&apos;: &apos;800&apos;, &apos;prefix&apos;: &apos;555&apos;&#125;</div></pre></td></tr></table></figure>
<p>可以同时使用(?P<name>)和(?P=name)符号。前者通过使用一个名称标识符而不是使用从1开始增加到N的增量数字来保存匹配，如果使用数字来保存匹配结果，我们就可以通过使用\1、\2、…,\N来索引，如下所示，可以使用一个类似风格的\g<name>来检索它们。</name></name></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.sub(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?:\d&#123;4&#125;)&apos;,</div><div class="line">... &apos;(\g&lt;areacode&gt;) \g&lt;prefix&gt;-xxxx&apos;, &apos;(800) 555-1212&apos;)</div><div class="line">&apos;(800) 555-xxxx&apos;</div></pre></td></tr></table></figure>
<p>使用后者，可以在同一个正则表达式中重用模式。例如，验证一些电话号码的规范化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bool(re.match(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?P&lt;number&gt;\d&#123;4&#125;) (?P=areacode)-(?P=prefix)-(?P=number) 1(?P=areacode)(?P=prefix)(?P=number)&apos;, &apos;(800) 555-1212 800-555-1212 18005551212&apos;))</div><div class="line">True</div></pre></td></tr></table></figure>
<p>使用（？x）使代码更易读：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; bool(re.match(r&apos;&apos;&apos;(?x)</div><div class="line">... \((?P&lt;areacode&gt;\d&#123;3&#125;)\)[ ](?P&lt;prefix&gt;\d&#123;3&#125;)-(?P&lt;number&gt;\d&#123;4&#125;)</div><div class="line">... [ ]</div><div class="line">... (?P=areacode)-(?P=prefix)-(?P=number)</div><div class="line">... [ ]</div><div class="line">... 1(?P=areacode)(?P=prefix)(?P=number)</div><div class="line">... &apos;&apos;&apos;,&apos;(800) 555-1212 800-555-1212 18005551212&apos;))</div><div class="line">True</div></pre></td></tr></table></figure>
<p>可以使用(?=…)和(?!…)符号在目标字符串中实现一个前视匹配：</p>
<p>(?=…)字符串后面跟着…才适配</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r&apos;\w+(?= van Rossum)&apos;,</div><div class="line">... &apos;&apos;&apos;</div><div class="line">... Guido van Rossum</div><div class="line">... Tim Peters</div><div class="line">... Alex Martelli</div><div class="line">... Just van Rossum</div><div class="line">... Raymond Hettinger</div><div class="line">... &apos;&apos;&apos;)</div><div class="line">[&apos;Guido&apos;, &apos;Just&apos;]</div></pre></td></tr></table></figure>
<p>(?!…)字符串后面不跟着…才适配：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r&apos;(?m)^\s+(?!noreply|postmaster)(\w+)&apos;,</div><div class="line">... &apos;&apos;&apos;</div><div class="line">...  sales@phptr.com</div><div class="line">...  postmaster@phptr.com</div><div class="line">...  eng@phptr.com</div><div class="line">...  noreply@phptr.com</div><div class="line">...  admin@phptr.com</div><div class="line">... &apos;&apos;&apos;)</div><div class="line">[&apos;sales&apos;, &apos;eng&apos;, &apos;admin&apos;]</div></pre></td></tr></table></figure>
<p>比较re.findall()和re.finditer()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; [&apos;%s@awcom&apos; % e.group(1) for e in re.finditer(r&apos;(?m)^\s+(?!noreply|postmaster)(\w+)&apos;,</div><div class="line">... &apos;&apos;&apos;</div><div class="line">...  postmaster@phptr.com</div><div class="line">...  noreply@phptr.com</div><div class="line">...  admin@phptr.com</div><div class="line">...  eng@phptr.com</div><div class="line">...  sales@phptr.com</div><div class="line">... &apos;&apos;&apos;)]</div><div class="line">[&apos;admin@awcom&apos;, &apos;eng@awcom&apos;, &apos;sales@awcom&apos;]</div></pre></td></tr></table></figure>
<p>条件正则表达式匹配，假定拥有一个特殊字符，它仅仅包含字母x和y，两个字母必须由一个跟着另外一个，不能同时拥有相同的两个字母：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; bool(re.search(r&apos;(?:(x)|y)(?(1)y|x)&apos;, &apos;xy&apos;))</div><div class="line">True</div><div class="line">&gt;&gt;&gt; bool(re.search(r&apos;(?:(x)|y)(?(1)y|x)&apos;, &apos;xx&apos;))</div><div class="line">False</div></pre></td></tr></table></figure>
<h2 id="三、实例"><a href="#三、实例" class="headerlink" title="三、实例"></a>三、实例</h2><p>在UNIX系统中，who命令会展示登录的用户信息。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">➜  ~ who</div><div class="line">sl       console  Nov 21 08:59 </div><div class="line">sl       ttys000  Nov 21 09:09 </div><div class="line">sl       ttys001  Nov 21 10:30 </div><div class="line">➜  ~</div></pre></td></tr></table></figure>
<p>如果想按照空格（多个，数量不确定）分隔的话，可以使用\s\s+，下面创建一个程序，将保存在文件whodata.txt中的数据读出来：<br>先将who的数据保存在whodata.txt文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">➜  ~ who &gt; /Users/sl/Desktop/whodata.txt</div></pre></td></tr></table></figure>
<p>然后执行下面的程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"></div><div class="line">f = open(&apos;whodata.txt&apos;,&apos;r&apos;)</div><div class="line">for eachLine in f:</div><div class="line">    print(re.split(r&apos;\s\s+&apos;, eachLine))</div><div class="line">f.close()</div></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[&apos;sl&apos;, &apos;console&apos;, &apos;Nov 21 08:59&apos;, &apos;&apos;]</div><div class="line">[&apos;sl&apos;, &apos;ttys000&apos;, &apos;Nov 21 09:09&apos;, &apos;&apos;]</div><div class="line">[&apos;sl&apos;, &apos;ttys001&apos;, &apos;Nov 21 10:30&apos;, &apos;&apos;]</div></pre></td></tr></table></figure>
<p>优化上面的程序：</p>
<p>上面的程序，who命令是在脚本外部执行的，每次手动重复做这件事让人很厌倦，我们可以通过调用os.popen()命令（现在已经被subprocess模块替代）将这个命令的执行在脚本内部实现。另外我们使用str.rstrip()去除尾部的\n，程序如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line">import os</div><div class="line"></div><div class="line">f = os.popen(&apos;who&apos;, &apos;r&apos;)</div><div class="line">for eachLine in f:</div><div class="line">    print(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip()))</div><div class="line">f.close()</div><div class="line">#结果：</div><div class="line">[&apos;sl&apos;, &apos;console&apos;, &apos;Nov 21 08:59&apos;]</div><div class="line">[&apos;sl&apos;, &apos;ttys000&apos;, &apos;Nov 21 09:09&apos;]</div><div class="line">[&apos;sl&apos;, &apos;ttys001&apos;, &apos;Nov 21 10:30&apos;]</div></pre></td></tr></table></figure>
<p>还可以使用with语句，可以使上下文管理对象变得更简易：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line">import os</div><div class="line"></div><div class="line">with os.popen(&apos;who&apos;, &apos;r&apos;) as f:</div><div class="line">    for eachLine in f:</div><div class="line">        print(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip()))</div></pre></td></tr></table></figure>
<p>如果要适配python2和python3的话，可以避免使用print（）,而使用两个版本中都有的函数distutils.log.warn()，并将其转换成printf名来使用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line">import os</div><div class="line">from distutils.log import warn as printf</div><div class="line">with os.popen(&apos;who&apos;, &apos;r&apos;) as f:</div><div class="line">    for eachLine in f:</div><div class="line">        printf(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip()))</div></pre></td></tr></table></figure>
<p>生成随机数的例子，用于希望练习从中匹配、搜索正则表达式使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">from random import randrange, choice</div><div class="line">from string import ascii_lowercase as lc</div><div class="line">from sys import maxsize</div><div class="line">from time import ctime</div><div class="line"></div><div class="line">tlds = (&apos;com&apos;, &apos;edo&apos;, &apos;net&apos;, &apos;org&apos;, &apos;gov&apos;)</div><div class="line"></div><div class="line">for i in range(randrange(5, 11)):</div><div class="line">    dtint = randrange(maxsize) / 3000000000</div><div class="line">    dtstr = ctime(dtint)</div><div class="line">    llen = randrange(4, 8)</div><div class="line">    login = &apos;&apos;.join(choice(lc) for j in range(llen))</div><div class="line">    dlen = randrange(llen, 13)</div><div class="line">    dom = &apos;&apos;.join(choice(lc) for j in range(dlen))</div><div class="line">    print(&apos;%s::%s@%s.%s::%d-%d-%d&apos; % (dtstr, login, dom, choice(tlds), dtint, llen, dlen))</div><div class="line"></div><div class="line">#随机产生的结果</div><div class="line">Mon Jun 24 08:07:21 2024::nunzkre@iqdhccpw.gov::1719187641-7-8</div><div class="line">Sun May 21 12:23:33 2062::dxlyq@kupbixskweqj.edo::2915411013-5-12</div><div class="line">Thu Sep  2 18:27:12 1999::vuhihly@hdgaimdma.com::936268032-7-9</div><div class="line">Mon Jul 30 16:45:03 2007::vygxw@diwdeqkq.net::1185785103-5-8</div><div class="line">Thu Jul  8 01:50:54 1971::mjxs@dmcuo.com::47757054-4-5</div><div class="line">Thu Sep  1 03:02:30 2005::djld@eohculuz.gov::1125514950-4-8</div><div class="line">Sun Nov 27 01:23:35 2011::cjvf@atvmdgxupi.gov::1322328215-4-10</div><div class="line">Thu Aug  1 18:36:36 2024::phasko@flcfkvb.org::1722508596-6-7</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Python </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> re </tag>
            
            <tag> 正则表达式 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（6）：西方哲学前沿~新古典自由主义的破冰之旅]]></title>
      <url>/2018/01/17/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%886%EF%BC%89%EF%BC%9A%E8%A5%BF%E6%96%B9%E5%93%B2%E5%AD%A6%E5%89%8D%E6%B2%BF%20%E2%80%94%E2%80%94%20%E6%96%B0%E5%8F%A4%E5%85%B8%E8%87%AA%E7%94%B1%E4%B8%BB%E4%B9%89%E7%9A%84%E7%A0%B4%E5%86%B0%E4%B9%8B%E6%97%85/</url>
      <content type="html"><![CDATA[<blockquote>
<p>时间：2018年3月12日<br>地点：中国人民大学国学馆228<br>主讲：周濂</p>
</blockquote>
<p>德鲁克在《经济人的末日》1994年的再版序言中说：“可以确定的是，我们现在的世界，或许跟之前的所有社会一样，疯狂错乱。但偏执不是治愈疯狂世界的良方。相反，要在疯狂的环境中生存，更需要保持清醒。”现代性的根本宗旨就是凭借人类理性去营建和维系一个理性、有序。可控和可理解的社会秩序。虽然二十世纪的历史充满灾难和无序，但在一个上帝盾形的时代，我们只能将现代性的自由平等理想坚持下去，让自己的行为理性起来，正如的德鲁克所言，唯有如此，我们才有机会拥有一个正直的、有意义的、有成就感的人生和一个正直的社会。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.09.png" alt="屏幕快照 2018-03-12 下午4.56.09"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.50.40.png" alt="屏幕快照 2018-03-12 下午4.50.40"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.12.png" alt="屏幕快照 2018-03-12 下午4.52.12"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.17.png" alt="屏幕快照 2018-03-12 下午4.52.17"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.24.png" alt="屏幕快照 2018-03-12 下午4.52.24"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.28.png" alt="屏幕快照 2018-03-12 下午4.52.28"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.30.png" alt="屏幕快照 2018-03-12 下午4.52.30"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.33.png" alt="屏幕快照 2018-03-12 下午4.52.33"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.38.png" alt="屏幕快照 2018-03-12 下午4.52.38"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.51.41.png" alt="屏幕快照 2018-03-12 下午4.51.41"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.45.png" alt="屏幕快照 2018-03-12 下午4.52.45"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.48.png" alt="屏幕快照 2018-03-12 下午4.52.48"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.52.png" alt="屏幕快照 2018-03-12 下午4.52.52"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.52.58.png" alt="屏幕快照 2018-03-12 下午4.52.58"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.04.png" alt="屏幕快照 2018-03-12 下午4.53.04"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.07.png" alt="屏幕快照 2018-03-12 下午4.53.07"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.13.png" alt="屏幕快照 2018-03-12 下午4.53.13"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.17.png" alt="屏幕快照 2018-03-12 下午4.53.17"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.19.png" alt="屏幕快照 2018-03-12 下午4.53.19"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.23.png" alt="屏幕快照 2018-03-12 下午4.53.23"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.28.png" alt="屏幕快照 2018-03-12 下午4.53.28"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.32.png" alt="屏幕快照 2018-03-12 下午4.53.32"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.36.png" alt="屏幕快照 2018-03-12 下午4.53.36"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.38.png" alt="屏幕快照 2018-03-12 下午4.53.38"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.44.png" alt="屏幕快照 2018-03-12 下午4.53.44"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.48.png" alt="屏幕快照 2018-03-12 下午4.53.48"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.50.png" alt="屏幕快照 2018-03-12 下午4.53.50"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.56.png" alt="屏幕快照 2018-03-12 下午4.53.56"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.53.57.png" alt="屏幕快照 2018-03-12 下午4.53.57"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.01.png" alt="屏幕快照 2018-03-12 下午4.54.01"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.05.png" alt="屏幕快照 2018-03-12 下午4.54.05"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.09.png" alt="屏幕快照 2018-03-12 下午4.54.09"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.12.png" alt="屏幕快照 2018-03-12 下午4.54.12"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.16.png" alt="屏幕快照 2018-03-12 下午4.54.16"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.21.png" alt="屏幕快照 2018-03-12 下午4.54.21"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.24.png" alt="屏幕快照 2018-03-12 下午4.54.24"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.29.png" alt="屏幕快照 2018-03-12 下午4.54.29"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.33.png" alt="屏幕快照 2018-03-12 下午4.54.33"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.39.png" alt="屏幕快照 2018-03-12 下午4.54.39"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.42.png" alt="屏幕快照 2018-03-12 下午4.54.42"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.47.png" alt="屏幕快照 2018-03-12 下午4.54.47"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.50.png" alt="屏幕快照 2018-03-12 下午4.54.50"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.52.png" alt="屏幕快照 2018-03-12 下午4.54.52"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.54.57.png" alt="屏幕快照 2018-03-12 下午4.54.57"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.00.png" alt="屏幕快照 2018-03-12 下午4.55.00"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.03.png" alt="屏幕快照 2018-03-12 下午4.55.03"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.05.png" alt="屏幕快照 2018-03-12 下午4.55.05"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.08.png" alt="屏幕快照 2018-03-12 下午4.55.08"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.10.png" alt="屏幕快照 2018-03-12 下午4.55.10"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.14.png" alt="屏幕快照 2018-03-12 下午4.55.14"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.17.png" alt="屏幕快照 2018-03-12 下午4.55.17"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.19.png" alt="屏幕快照 2018-03-12 下午4.55.19"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.21.png" alt="屏幕快照 2018-03-12 下午4.55.21"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.26.png" alt="屏幕快照 2018-03-12 下午4.55.26"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.29.png" alt="屏幕快照 2018-03-12 下午4.55.29"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.33.png" alt="屏幕快照 2018-03-12 下午4.55.33"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.36.png" alt="屏幕快照 2018-03-12 下午4.55.36"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.38.png" alt="屏幕快照 2018-03-12 下午4.55.38"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.45.png" alt="屏幕快照 2018-03-12 下午4.55.45"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.49.png" alt="屏幕快照 2018-03-12 下午4.55.49"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.52.png" alt="屏幕快照 2018-03-12 下午4.55.52"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.54.png" alt="屏幕快照 2018-03-12 下午4.55.54"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.55.56.png" alt="屏幕快照 2018-03-12 下午4.55.56"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.00.png" alt="屏幕快照 2018-03-12 下午4.56.00"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.02.png" alt="屏幕快照 2018-03-12 下午4.56.02"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.07.png" alt="屏幕快照 2018-03-12 下午4.56.07"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.09.png" alt="屏幕快照 2018-03-12 下午4.56.09"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.14.png" alt="屏幕快照 2018-03-12 下午4.56.14"><br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-03-12 下午4.56.28.png" alt="屏幕快照 2018-03-12 下午4.56.28"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 新古典自由主义 </tag>
            
            <tag> 哈耶克 </tag>
            
            <tag> 罗尔斯 </tag>
            
            <tag> 诺齐克 </tag>
            
            <tag> 德鲁克 </tag>
            
            <tag> 自由 </tag>
            
            <tag> 平等 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（5）：《伯林谈话录》摘]]></title>
      <url>/2018/01/15/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%885%EF%BC%89%EF%BC%9A%E3%80%8A%E4%BC%AF%E6%9E%97%E8%B0%88%E8%AF%9D%E5%BD%95%E3%80%8B%E6%91%98/</url>
      <content type="html"><![CDATA[<h3 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h3><p>他反对那种以为可以依据科学的、政治的、甚至美学的价值在人世间创造一个乌托邦的主张。鉴于人类历史实际上是各种经常相互碰撞的价值和思想的产生地及其变化发展的实验场所这一事实，伯林追溯多元论在伦理学、政治学和美学等领域的出现。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%284%29.jpeg" alt="1"><br>但是，如果我说的不错，不光最终解决这个概念本身是不切实际的，而且，各种价值之间也不可避免地是相互碰撞的、不可协调的。最终解决的可能性（即使我们忘记了这个词组带有希特勒时期的恐怖感）会制造出一种幻觉，一种非常危险的幻觉。因为，如果人们真的相信这种解决是可能的，那么，为了达到这个目标付出多少都绝不为过：为了使人类永远公正、幸福、富于创造性以及和谐协调，有什么不可以为此付出的呢？为了做成这样的蛋卷，我们可以打破无限数量的鸡蛋，这就是列宁、托洛茨基以及我们所了解的波尔布特的信念。既然我知道通往社会问题最终解决的唯一正确道路，我也就知道人类车队必须沿着什么路线走；因为你没有我这种知识，你就不能有选择自由，哪怕是最低限度的选择自由，否则你就达不到目的地。你声明采取某种方式将使你更幸福、更自由，或将使你有自由呼吸的空间，而我知道你这样想是错误的。我知道你需要什么，人民大众需要什么。如果出现由于无知或恶意而酿成的反抗，那就必须振亚下去，为了大多数人永远幸福，消灭成千上万人也许是必要的。除了心甘情愿地将他们全都牺牲掉，我们，明白此中道理的我们，又有什么选择？“（扭曲的人性之材）</p>
<p>《往事与随想》赫尔岑  选择概念（关键地位）</p>
<p>在思想史的工作中，伯林研究了那些勇敢地、公开地跟占统治地位的理性体系作斗争的思想，赞赏他们的观点和立场。他特别重视这些思想家的自由思想。正式对这种普遍存在的自由思想的肯定和褒赏，显示了他的思想史研究具有重要意义。通过他的研究工作，伯林向我们宣示，在人类历史上没有绝对的价值，而且，人类历史与众多悲剧性后果相伴，充满着那些企图通过坚信最终绝对真理而避免做出悲剧性选择的人们的困苦。</p>
<p>普列汉诺夫   伯林：”普列汉诺夫的确是一位富有才华的马克思主义著作家。我完全被他的书迷住了，因为他学识渊博，说理精辟，行文机智，情趣横生，极富吸引力。他是真正的马克思主义之父“</p>
<p>赫尔岑  伯林：”赫尔岑成了我的人生楷模。他是一个非常杰出的作家，一个敏锐的真正的政治思想家，非常有独创性。他的自传大概是我一生中度过的最精彩的自传，比卢梭的自传还要好。正是赫尔岑使我爱上了社会思想史和政治思想史，这就是我研究思想史的真正的开端。“</p>
<p>《日瓦戈医生》  帕斯捷尔纳克  伯林：”他当然是一位伟大的诗人。这么说吧，诗人可以有两种类型。第一种诗人，写诗时是诗人，而写散文诗是作家，像普希金。第二种诗人，写诗时是诗人，而写散文时也是诗人。帕斯捷尔纳克就属于第二种类型的诗人。他的散文总是诗化的散文，我看他本质上不是一个散文作家。他是晚近俄罗斯伟大的诗人之一，他的小说是伟大的诗化小说。尽管他置身于那虚浮造作的时代中心，却能诚实地描写爱——男主人公对女主人公的爱，极少有作家能够做到这点。正是他的诗使他赢得俄国人和阅读俄文作品的外国人的广泛钦佩，实际上，只有约瑟夫·布罗茨基的成就可与他相媲美。阿赫玛托娃和曼德尔施塔姆都差得远。依我看，帕斯捷尔纳克在各方面都堪称是活着的最优秀的俄国诗人。但是并非所有天才都表里如一，帕斯捷尔纳克也同样如此。他谈起话来稀奇古怪，经常让听者捉摸不透，但总让你感到才气逼人。再也没有比听他谈话更迷人的事了。据我的体会，只有弗吉尼亚·伍尔夫谈到某些东西时像他那样迷人。当然弗吉尼亚·伍尔夫有点狂妄。“</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2820%29.jpeg" alt=""><br>马雅可夫斯基  伯林：”他是一个大胆的革新者，一个惊人的雄辩家，一个真正的革命者，但是，他的诗作我看比不上帕斯捷尔纳克、曼德尔施塔姆和阿赫玛托娃。“</p>
<p>如果你对某些思想有兴趣，并引起你思考一些问题，那么你就不能不考虑这些思想的历史。因为思想不是单子，他们不是在真空中产生的，而跟人们的信念、生活方式、人生观和世界观紧密相连。思想之间相互碰撞和影响，并不断呈现，成为所谓”智性气候“的组成部分，他和物质因素一样，形成人们的行为和感情，并且历史地变迁着。</p>
<p>哲学不是一种积累性的学科。古代那些基本的哲学思想、观点、理论和见解现在仍然是哲学的中心内容。他们有其特定的横贯历史的生命。</p>
<p>哲学，如果教得好的话，其用处之一就是让人透过政治上冠冕堂皇的辞藻，识别各种谬论、欺骗、恶作剧、赘疣、感情上的讹诈，以及各种各样的诡辩和伪装，他能大大增强人们的批判能力。</p>
<p>哲学不外是要在看不到办法的地方力图去寻找问题的答案。自我理解是哲学的主要目的之一，哲学的目标就是要理解人、事物、词语三者之间的相互关系。</p>
<p>在叔本华那里我没有发现包罗万象的形而上学大厦那样的东西。人们可能对叔本华的体系一无所知，但照样能从他许多尖锐的有时是深刻的见解中获得教益。黑格尔的体系在我看来，似乎是希腊神话中的独眼巨人波吕斐摩斯的阴森森的黑洞，一进去就出不来，每一个脚印都指示一条道路，正如拉丁诗人所说的那样。</p>
<p>伯林：”他（指施特劳斯）和他的门生都相信，善于恶、对与错都直接得自某种先天的启示，某种”形而上学之眼“，也就是靠使用柏拉图式的那种无缘分享的理性官能。柏拉图、亚里士多德、《圣经》、犹太教法典、迈蒙尼德，也许还有阿奎那和中世纪的其他经院哲学家，都通晓什么样的人生才是最美好的，他们的门生现在也执着于此，而我却没有这种荣幸。“</p>
<h3 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h3><p>世界上存在的一切不外乎就是人、物和人脑中的观念————目标、情感、希望、畏惧、选择、想象的情景和所有其他形式的人类经验。这就是我所认识的全部东西。我无法做到无所不知。也许有一个永恒真理和永恒价值的世界，有一种只有真正的思想家才能具有的魔眼，而这只属于恐怕我永远无法进入的极少数精英的领地。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/u=2134419765,406510865&amp;fm=21&amp;gp=0.jpg" alt="2"></p>
<p>在关键时期，在历史转折关头，当各种因素大体上平衡地出现的时候，个人以及他们的抉择的行动，本身不一定可被预见（确实很少被预见），但却能决定历史的进程。我不相信历史是一部戏剧（这是赫尔岑使用的概念，他认为历史不是一部多幕剧，一部由上帝或大自然赋予主题的演出，不是有图像可辨的地毯）。而马克思和黑格尔都认为历史是一部有结局的多幕剧，它在达到高潮之后（在马克思看来，要经过可怕的冲突、苦难、灾祸）天堂之门将会启开，那就是戏剧的收场，历史（马克思称为史前时期）便会从此结束，一切事物都永恒协调，人们将合乎理性地合作共事。</p>
<p>维科和赫尔德相信历史进程有一定的形式，但不相信历史是一部有结局的戏剧。</p>
<p>我感兴趣的是维科和赫尔德的文化多元性的信念。实际上，每种文化都有自己的重心，各种文化有着各不相同的、新颖的、不可预见的思想及其互相冲突的倾向。维科最先理解到，文化就是世界相对于社会的意义，就是男男女女对于他们自身与别人和环境发生关联的集体意识；文化影响思想、情感、行为、举动的特定形式；文化是多种多样的。维科划分不同时期的文化，赫尔德则对不同时代的不同民族的文明做了区分。</p>
<p>我认为历史不是一个呆板的单线条的进步过程。伏尔泰说，历史是理性、知识和艺术品创造不断进步的过程，有时被可怕的干扰所打破，突然陷入野蛮状态。</p>
<p>预言是一种普遍却难以信赖的活动。我在维科和赫尔德的著作中经所读到的，是人类历史固有的文化多样性的观点。历史并不直线行走，不同文化之间相互作用，有时就是因果性质的作用。通向未来或过去的道路不存在唯一的钥匙，他不好跟自然科学作类比。后者的定律对重复出现的因果链是开放的，这样的因果链能总结为一般的规律。</p>
<p>我们能够理解不同民族和地区人们的生活方式（即使他们的生活方式跟我们的差异很大，即使他们憎恨我们或有时候被我们所谴责），这样的事实表明，我们大家能够穿越时空进行沟通。当我们认了理解了那些与我们在文化上有很大差别的群体的时候，即意味着某种强大的富于同情心的理解、洞察和共感的存在。即便其他文化排斥我们，依靠移情的想象力，我们也可以设想，为什么他们会产生这样的思想和感情，并采取相应的行动达到预定的目标。</p>
<p>一般性的价值观是有的，这是关于人类的经验事实，莱布尼茨称为事实的真理而不是原理性的真理。不同时空的芸芸众生，绝大多数人都共同拥有某些价值观，不论这些价值观是否自觉明晰，也不论他们在态度、举止和行动上的表现如何。另一方面，人与人之间、社会与社会之间，又确实存在着很大的差异。如果你确实了解了个人之间、团体之间、民族之间、各个完整的文明之间所存在的差异，运用想象进入他们的思想、情感世界、设想你自己置身于他们的生存环境中会怎样认知世界并审视自己与他们的关系，那么，即使你对所观察到的东西很反感（全部了解当然并不等于全部谅解），也肯定会减少盲目的偏执和狂热。想象会产生狂热，但通过想象洞察了不同于自己的境况，结果必定能减少狂热。</p>
<p>我认为，人们在把一个有思考里的人称为疯子或神经错乱者时务必小心谨慎。迫害不是来自神经错乱，而产生于把骇人听闻的谬误深信为真理，进而导致罄竹难书的恶果。</p>
<p>了解自己及他人，懂得理性的方法，掌握作为知识和全部科学基础的证据，以及力图验证直观确定性，这些对我们来说都有着根本的重要性。人权这个观念建立在一个正确的信念之上，那就是普遍存在着某种特定的品性。自由、正义、对幸福的追求、真诚、爱。这符合整个人类的利益，而不只是符合作为这个或那个民族、宗教、职业、身份的成员的利益。满足这些要求，保护人们这些要求不被忽视或否认，都是正当的。</p>
<p>你必须懂得什么是正义，什么是自由，什么是社会契约，并对不同类别的自由、权威、义务作出区分等等。政治理论的分野往往围绕“为什么有些人要服从理你些人”这个中心问题来展开。多数政治理论都是对这个问题的回答。实质上不是为什么服从，而是为什么应该服从和服从到什么程度。<br><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2821%29.jpeg" alt="2"><br>关于消极自由的问题是：拦在我面前有什么障碍要排除？其他人怎样妨碍着我？其他人这样做是有意的还是无意的？是间接的还是有制度依据的？</p>
<p>关于积极自由的问题是：谁管我？别人管还是自己管？如果是别人，他凭借什么权利？什么权威？如果我有权自主，自己管自己，那么，我会不会失去这个权利？能不能丢掉这个权利？放弃这个权利再恢复这个权利？具体怎么做？还有，谁制定法律？或谁执行法律？征求过我的意见吗？是多数人在统治吗？为什么？是因为上帝、牧师、还是党？是出于公共舆论的压力？传统的压力？还是摄于什么权威？</p>
<p>积极自由在正常生活中虽然更重要，但与消极自由相比更频繁地被歪曲和滥用。历史上虚伪的积极自由所造成的危害比现代虚伪的消极自由所造成的危害更大。</p>
<p>真诚地相信错误的东西是很危险的，是没有道德价值或精神价值的，至少是令人遗憾的。</p>
<p>自由社会的好处在于容许各种各样相互冲突的意见存在而不被压制。</p>
<p>多元论确认：既然对于道德和政治问题以至任何价值问题不可能有一个最终的解答，并且，人们给出的或有权给出的某些解答是相互矛盾的，那么，在实际生活的某些领域，有些价值便可能变得互不相容，这样，如果要避免破坏性的冲突的话，就应该妥协，而最低限度的宽容，不管你情不情愿，都是必不可少的。</p>
<p>人们可以选择一种生活或另一张生活，而不能同时过两种生活；没有更为根本的标准用来决定正确的选择；既然选择这种也行，选择那种也行，在客观上就不能说一种生活优于另一种生活。它是人们想做什么和成为什么的问题。</p>
<p>浪漫主义认为，价值不是发现而是创造出来的，生活的目的就是生活本身。生活就是生活，没有目的。</p>
<p>政治哲学的任务是审视生活。要做的事就是审查为实现各种社会目标而提出的种种主张的合理性，检查为确定和实现这些目标而采取的种种方法的正当性。政治哲学要力图澄清构成有关观点的词和概念，使人们能理解自己相信的是什么，自己的行动表示什么。政治哲学还对那些维护或者反对人们所追求的各种目标的辩论作出评价，并防止麦克米兰所引述的胡说八道。</p>
<p>对人类的问题，追求一种唯一的、最后的、普遍的解决，无异于是追求海市蜃楼。对于人类生活破坏严重的，莫过于那种迷信了：凡美好生活都是跟政治或者军事力量相联结的。</p>
<p>多数英国哲学家似乎都太单薄、太技术化；跟英国哲学家相比，多数法国哲学家似乎都太含糊、太夸饰。</p>
<p>我认为马基雅维利是指出现实的各种价值是相互冲突的第一人。依照马基雅维利的观点，你可以选择做一个罗马人或一个基督教徒和殉道者，或者起码可以做一个当权者统治下的受害者。</p>
<p>任何真正的问题在某种意义上都是当代的问题。<br><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2823%29.jpeg" alt=""></p>
<p>我认为维科是理解了并告诉我们什么是人类文化的第一人。他不自觉地确立了文化的观念。就我所知，在他之前没有谁有过这样的想法，要努力去重构人们是如何看待生活在周围环境中的自己，如何看待（或感受）与自己发生关联的自然界和其他人————作为在时间中持续存在下来的一类生灵。他反思思想、情感、世界观等各类行为以及肉体的、情绪的、理智的、精神的等多种反应的本质，而正是这些行为和反应构成了文化。如果你想了解人们怎样生活，你必须了解他们的崇拜仪式，文字的内涵，他们通常运用什么类型的想象、明喻、隐喻，他们如何吃、喝、抚养小孩，如何看待自己，如何过私人的、社会的、经济的和政治的生活等等。作为一种模式的文化不是一个孤立的有机体，而是一种存在方式，树立这种理念正是维科对思想史的主要贡献。维科的值得重视的观点是：各不相同的人类思想、行为、感情和行动是互相联结和互相启发的。</p>
<p>米什莱按照维科的思路，认为历史就是社会跟自然力量作斗争并力图运用自然力量去创造让人们能生存和发展的生活方式的历史。人的历史是跟自然界，跟各种力量，跟一切人为的和非人为的障碍进行斗争的历史，这就是米什莱关于人类从各种羁绊中朝向自我解放不断进步的观点。</p>
<p>我们谈论自然界，但我们所知道的自然界仅是我们在外部世界所发现的东西。我们也看见和感触我们的身体，但我们还能说出他作为人之具体化有什么样的感受，这是一种“内在的审视”；人既是观察者，又是行动者。这就是“新科学”的大概意思。理解对于意图、感情、希望、恐惧、努力、意识和无意识的认识，而科学是对处在空间中的物体的认识。换句话说，我们可以看见桌子是什么样的，但我们看不见桌子为什么是这样的。理解过去的文化就是去理解前任所追求的东西；他们怎样看待与他人发生关联的自己，怎样看待世界以及生活在这个世界中的自己。</p>
<h3 id="其三"><a href="#其三" class="headerlink" title="其三"></a>其三</h3><p>在赫尔德看来，“归属于”的意思是，你说什么，不必多做解释，大家就能了解；你的姿态、语言、所有参与交流的因素，不需敬你熟悉的人作介绍，大家都能把握。是语言、习惯、姿态或本能的反应创造了联合和团结，即创造了具有自己特色的观点、文化和社会共同体。</p>
<p>我一开始读维科的著作简直就被他迷住了。我总是从接受邀请做讲演或写文章开始研究的。</p>
<p>唯一真实的东西是精神，是人与上帝的关系，人与人的关系，别无其他了。内在的精神，个人的灵魂的底蕴，内心世界，这是唯一实在的东西，至于礼节、学问和教阶制度，统统不在话下。<br><img src="http://omu7tit09.bkt.clouddn.com/timg%20%285%29.jpeg" alt="2"></p>
<p>赫尔德乐观地相信，人类大花园中的所有花卉都能和谐地生长，各种文化都能相互激励，为创造这种和谐的境界作出自己的贡献。绝不主张政治上的民族主义，政治上的民族主义必然大致侵略和培植民族自豪感。一个民族不是一个国家，而是一个文华实体，同一民族的人说共同的语言，生活在共同的地域，有着共同的习惯、共同的历史额共同的传统。</p>
<p>依我的看法，强烈的民族主义不过是耻辱心理的表现。高度发达的民族不会产生民族主义。民族主义是对伤害的反应。民资注意对一切事物均构成威胁。民族主义就等于我们对自己说，因为我们是德国人或法国人，所以我们是最优秀的人，我们完全有权做我们要做的事，一旦你把一切行为的根据放在民族这个超越个人的权威上，那就会扩展到政党，到阶级，到教会，通往压迫的道路便从此打开了。</p>
<p>你不能阻止科学的进步，造成灾难的不是武器，而在于使用武器的人。智能的进步是不能阻止的，人们所能做的是防止科学的滥用。廓清腐败的社会，荡涤一切污泥浊水，然后再向前进。</p>
<p>熊彼特正确的说过，那些相信观念必定绝对不变的人是偶像崇拜者。文明意味着必须允许变化的可能，意味着永不停息地去追求自己信奉的理想，为之献身也在所不惜。</p>
<p>不同的个人、集团。文化之间可以沟通，因为人的价值并非无限地多；他们共属于一条水平线，即客观的常常又相互矛盾的人类价值，在他们之间必须进行（常常是痛苦的）选择。</p>
<p>我自己感觉不到有这种既在现实生活之内又超越现实生活的实在。我不是宗教徒。但我对信教者的宗教体验评价颇高。我深深地被犹太教堂也包括基督教堂和伊斯兰教寺院中的宗教仪式所打动。我想，不理解信教是怎么一回事的人恐怕也不理解人为什么而活着。因此干巴巴的无神论者都是瞎子聋子，不了解人生的深刻体验，或者说不了解人生的内在底蕴，就像瞎子不能欣赏美景一样。光有感觉能力的人不能充分理解他人，包括信教者、不信教者、神秘主义者、儿童、诗人、艺术家等等。</p>
<p>我有一种深信不疑的看法，有些道德的、社会的和政治的价值是相互抵触的，任何一个社会总有一些价值是不能彼此调和的。换句话说，人们爱以生存的某种最终的价值，不光在实践上而且在原则上、在概念上都是不可兼得的，或者说不可彼此结合的。没有哪一个精于心机的人，同时又是无所计较，一切都听其自然的人。你不能把充分的自由跟充分的平等结合起来。给狼充分自由就不能同时也给羊有充分自由。正义和慈悲，知识和幸福，如此等等，都可能相互冲突，不可兼得。既然是这样，人类的问题（归根到底是如何生活的问题）就不可能全都求得完满的解决。这不是因为实际上有困难，找不到妥善的解决方法，而是因为这些价值本身在概念性质上都是有缺陷的。乌托邦式的解决在原则上没有缺陷，可以成立，但这样的解决是企图把不可结合的东西结合起来。某些人类的价值之所以不能相互结合，就因为他们本身是不能并存的。因此只能在彼此之间进行选择。选择可能很痛苦，如果你选择A，你就得忍痛失去B。在最终的各种人类价值之间不可避免要作出这样的选择。在任何可以想象的社会，选择都可能是痛苦而且是不可避免的。互不相容的价值本身始终是不能相容的。我们所能做的是防止选择太痛苦，这就意味着，我们需要有一种机制，使得人们对各种价值的追求尽可能不违背自己深刻的道德信念。在多元化的自由社会里，不可避免要作出各种妥协和折中，经过权衡利弊而避免最坏的情况。再三斟酌，取其一方。平等多重要？自有多重要？正义多重要？慈悲多重要？善良对重要？真理多重要？掂量掂量就知道了。知识和幸福也不总是牢牢结合的。一个知道自己患了癌症的人不会因为有了这种知识而感到幸福；无知会使他少一些自由，但同时却使他觉得多了一些幸福。这就是说，人生问题的某种最终解决，没有普遍适用的始终不变的可行保准。那些相信可能有完美无缺的社会的人必定以为，为了实现这种美好的社会，作出多大牺牲都是必要的，为了达到这种理想的目标，付出多大的代价都是值得的。他们想，如果必须要流血才能创造这种美好的社会，那么就流血吧，不管流谁的血，也不管流多少血。不打破鸡蛋怎么能做出上等的蛋卷，课时人们一旦养成打破鸡蛋的习惯，他们久不久罢手，鸡蛋打破了，蛋卷却没有做成。凡是以为对人生问题可以求得最终解决的这种狂热的信念，不能把导致灾难、痛苦、流血和可怕的压迫。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=5191751&auto=1&height=32"></iframe>

]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 以赛亚·伯林 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（4）：增长黑客的炼成]]></title>
      <url>/2018/01/13/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%884%EF%BC%89%EF%BC%9A%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%E7%9A%84%E7%82%BC%E6%88%90/</url>
      <content type="html"><![CDATA[<p>这篇文章是《增长黑客》的思维导图整理。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/第一章：增长黑客.png" alt="第一章：增长黑客"><br><img src="http://omu7tit09.bkt.clouddn.com/第二章：正确的产品.png" alt="第二章：正确的产品"><br><img src="http://omu7tit09.bkt.clouddn.com/第三章：获取用户 .png" alt="第三章：获取用户 "><br><img src="http://omu7tit09.bkt.clouddn.com/第四章：激发活跃.png" alt="第四章：激发活跃"><br><img src="http://omu7tit09.bkt.clouddn.com/第五章：提高留存.png" alt="第五章：提高留存"><br><img src="http://omu7tit09.bkt.clouddn.com/第六章：增加收入.png" alt="第六章：增加收入"><br><img src="http://omu7tit09.bkt.clouddn.com/第七章：病毒传播.png" alt="第七章：病毒传播"></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 增长黑客 </tag>
            
            <tag> 产品 </tag>
            
            <tag> 用户 </tag>
            
            <tag> 活跃 </tag>
            
            <tag> 留存 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（3）：娱乐时代的疾呼]]></title>
      <url>/2018/01/11/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%883%EF%BC%89%EF%BC%9A%E5%A8%B1%E4%B9%90%E6%97%B6%E4%BB%A3%E7%9A%84%E7%96%BE%E5%91%BC/</url>
      <content type="html"><![CDATA[<p>在地铁上看完了<a href="https://book.douban.com/subject/1062193/" target="_blank" rel="noopener">《娱乐至死》</a>，写点感想，做个串讲。</p>
<p>如果仅仅从书名“娱乐至死”的字面意思理解，我们或许都会误解，以为作者在书中定会着力指责那些活跃于电视荧幕的娱乐节目的缔造者们，将这个时代的浅薄与庸俗归咎于嬉皮笑脸的闪亮明星们，因为他们集体攻陷了普罗大众的闲暇时间，把他们一个个变成了“娱乐时代”的俘虏，从而变相地“奴役”了我们。</p>
<a id="more"></a>
<p>但其实波兹曼真正的意图并不在此。</p>
<p><div align="center"><img src="http://omu7tit09.bkt.clouddn.com/100229-2.jpg" alt="100229-2"></div></p>
<p>开篇，波兹曼满怀激情地拥泵并阐释了麦克卢汉的“媒介即隐喻”的理论，我们会因为媒介的存在形式的更替变化（诸如纸媒、电媒）而在一个相当长的时期内受到其影响，这个过程是潜移默化的，最终触及到人类社会的生活方式。因为特定的媒介形式会偏好特定的内容，长期来讲，这些“特定媒介形式提供的特定内容”就会慢慢地塑造出人的新的生活方式甚至是整个人类社会的文化特征。</p>
<p>再者，波兹曼在此基础上阐发了“媒介即认识论”，我们每个人日常获得的信息大都是经过媒介报道的，因为自身的限制我们无法跳脱出媒介所呈现的信息，久而久之，就会在不知不觉中认为媒介中传递的信息便是对我们身处的社会真实的描绘，但事实上这仅仅是我们的一厢情愿，那些有态度的媒体们会因为其价值倾向或者利益诱导等等原因，对取材于“客观世界”的信息进行层层筛选把关之后，再进行“滤镜式”报道。这就导致了我们的“内心直观世界”和“客观的社会环境”之间不仅仅隔着不可知的成分，还因为有偏倚的媒体中介的存在而变得“扑朔迷离”。因此“真理的定义至少有一部分来自传递信息的媒体的性质，进而影响我们对“真理”或者是“外部世界”的定义与评价。</p>
<p>随后，波兹曼用了大量的篇幅描述了在印刷机统治下的社会环境与思想市场，在那个年代，文学家狄更斯访问美国时所得到的待遇简直可以和当代最引人注目的明星相媲美；开创美国的元勋都是学养高深的知识分子，他们深受印刷术带来的深度思考的福利；整个社会环境推崇的是严肃的、有序的、观点明确的、具有逻辑性的公众对话；美国人用白纸黑字来表明态度、表达思想、制定法律、销售商品、创造文学和宣扬宗教。而这一切都是通过印刷术实现的，印刷文字有语义、可释义的、有逻辑，建立在其上文化也正因为如此而变得稳重而成熟。波兹曼把那个时代叫做“阐释年代”，阐释是一种思想的模式，一种学习的方法，一种表达的图解。所有成熟的话语所拥有的特征，都被偏爱阐释的印刷术发扬光大；富有逻辑的复杂思维、高度的理性和秩序、超常的冷静与客观都在印刷术的带领下变得触手可及。印刷机不仅仅是一种机器，它更代表着一种话语结构。</p>
<p><div align="center"><img src="http://omu7tit09.bkt.clouddn.com/104121.jpg" alt="104121"></div></p>
<p>然而让波兹曼深感缅怀的是，印刷术媒介在时代更迭中逐渐被电报、电视取代，再也回不到那个“阐释时代”。电报出现之时，便携带着无知无序无能的基因，它让公众话语变得无聊散乱，让我们的时间支离破碎，使我们的注意力被割裂。因为电报本身的性质就注定了它只适合于传播转瞬即逝的信息，各种各样的信息排着队被报道，一个报道完了便是下一个，很快就会被取代。新闻就是电报这个媒介所衍生出来的媒体内容，新闻往往是危言耸听、结构零散、没有目标受众，它就像一个个口号，毫无连贯性地进入读者的视野。在这样的语言中，没有关联，没有语境，没有历史，没有任何意义，它们拥有的是用趣味代替复杂而连贯的思想。读者们都随之变成了“知道分子”，他们知道很多事，但他们并不深入分析。</p>
<p>随后，电视来了，它成了新认识论的指挥中心。所有人都可以在电视中寻找一隅之地，所有公众感兴趣的话题都可以在电视上呈现。而电视的思维方式和印刷术的思维方式是格格不入的，电视对话会助长语无伦次和无聊琐碎；“严肃的电视”本来就是一个自相矛盾的表述，因为电视因为其本身的性质，注定了它只有一个声音——娱乐。</p>
<p>对，电视是娱乐性的，但这完全不会对文化造成任何的威胁，如果电视只是单纯地展示娱乐的内容，这是一件皆大欢喜的事情；但潜伏在背后的危机，我们或许很难察觉，那就是当所有的内容都已娱乐的方式表现出来，就完全变味了。这种变味发生在当严肃的思想被搬到荧幕面前被娱乐性地思考，发生在当需要严谨对话的社会话题以娱乐化的形式呈现在电视前。当思想、社会、文化、哲学、政治、经济这些厚重的语词和娱乐性的电视挂起钩的时候，“虚无主义”便出现了。波兹曼毫不客气地以轻蔑的语调嘲笑那些妄想利用电视机来提高文化修养的人，电视无法延伸或扩展文字文化，相反，电视只能攻击他们。毫不掩饰的揭示掩藏在电视节目超现实外壳下的，是反交流的理论，它抛弃逻辑、理性和秩序。他戏谑地说道，在美学中，这种理论被称为“达达主义”；在哲学中，它被称为“虚无主义”；在精神病学中，它被称为“精神分裂症”；如果用舞台术语来说，它可以被称为“杂耍”。</p>
<p>究竟电视是如何引致这种娱乐性的？因为人们喜欢看到有动感的画面，稍纵即逝但却斑斓夺目，正因为此决定了电视无法容忍思想，它必须来迎合人们对视觉快感的需求，来适应娱乐业的发展。电视新闻一切以简短为宜，做到不让观众有精神紧张之感，反之，要以富于变化和新奇的动作不断刺激观众的感官。你不必注意概念和角色，不要在同一个问题上多停留几秒。因为这种种的原因，严肃、连贯、缜密的思考天生与电视就是不相容的。</p>
<p><div align="center"><img src="http://omu7tit09.bkt.clouddn.com/104803.jpg" alt="104803"></div></p>
<p>电视的这种娱乐性渗透进了教育：电视通过控制人们的时间、注意力和认知习惯获得了控制人们教育的权力，但这种控制是无力，因为不像阅读获得的往往和我们原来储存的知识相关，而由电视上获得的往往是一些具体的片段，不具备推断性。它也同样渗透进了宗教：电视最大的长处是它让具体的形象进入我们的心里，电视里的宗教形象往往会因为电视的这种长处而惹人注目，这就违背了传教的本意：他本是将一些教义深入人心，而电视压根没有这种将抽象的概念留在我们脑海中的功能，他把我们的目光转置那些传教者的仪态和举止，他们成了受教者的上帝，而不是在教义中的上帝。</p>
<p>在最后波兹曼借由赫胥黎发出警告，有两种方法可以让文化精神枯萎，一种是奥威尔式的——文化成为一个监狱，另一种是赫胥黎式的——文化成为一场滑稽戏。我们应该借助赫胥黎而不是奥威尔来理解电视和其他图像形式对于民主国家的基础所造成的威胁，更明确地说，是对信息自由所造成的威胁。我们要担心的是电视信息的过剩，而不是政府的限制；我们的文化部是赫胥黎式的，而不是奥威尔式的，它想尽一切办法让我们不断地看电视，但是我们看到的是使信息简单化的一种媒介，它使信息变得没有内容、没有历史、没有语境，也就是说，信息被包装成为娱乐。在一个科技发达的时代里，造成精神毁灭的敌人更可能是一个满面笑容的人，而不是那种一眼看上去就让人心生怀疑和仇恨的人。</p>
<p>波兹曼开始大声呼喊：在弥尔顿、培根、伏尔泰、歌德和杰弗逊这些前辈的精神的激励下，我们一定会拿起武器保卫和平。但是，如果我们没有听到痛苦的哭声呢？谁会拿起武器去反对娱乐？当严肃的话语变成了玩笑，我们该向谁抱怨，该用什么样的语气抱怨？对于一个因为大笑过度而体力衰竭的文化，我们能有什么救命良方？</p>
<p>但似乎只有深刻而持久地意识到信息的结构和效应，消除对媒介的神秘感，我们才有可能对电视，或电脑，或任何其他媒介获得某种程度的控制。刻意地疏远那些信息形式还不足够，我们要学会用理性去解读文化中的象征，用思维去抵御那些虚假的思想面孔，当这个时代每个人都自觉的意识到，赫胥黎的预言正在悄然上演，那电视就或许真的可以做回它最擅长的事情——真正的娱乐，而严肃的思想也会回归本位，躺在在厚重的书卷里等待读者的共鸣。</p>
<p><div align="center"><img src="http://omu7tit09.bkt.clouddn.com/104992.jpg" alt="104992"></div></p>
<blockquote>
<p>补：</p>
</blockquote>
<p>前几天在看菲利普·罗斯的文章时，也看到类似的担忧：“捷克斯洛伐克成了一个自由民主的消费社会的时候，你们这些作家就会发现自己受到了许多新的对手的折磨，而再奇怪不过的是，那个压迫人的、枯燥无味的极权社会曾保护你们免于这些折磨。特别让人不安的将是威胁文学、教育和语言的那个无处不在的、威力无比的主要对手。我可以向你保证，不会再有对抗的人群在瓦茨拉夫广场集合起来去推翻暴政，也不会有哪位剧作家知识分子被愤怒的群众抬举来救赎民族之魂，使其脱离因对手将所有人类话语减弱之后而使之所处的昏聩状态。我正在谈论的是将所有一切变得浅薄无聊的商业电视——并不是说因为被愚蠢的国家审查所控制才没有人想看的少数频道，而是说十几、二十几个因为娱乐性才被众多人整日观看的乏味的陈词滥调的频道。你和作家同行们最终摆脱了极权体制下的知识监狱。欢迎你们进入全部娱乐的世界。你们不知道失去了什么。或者说你们知道吗？”</p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 娱乐至死 </tag>
            
            <tag> 波兹曼 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（2）：阅读月记【2017.12】]]></title>
      <url>/2018/01/05/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E9%98%85%E8%AF%BB%E6%9C%88%E8%AE%B0%E3%80%902017.12%E3%80%91/</url>
      <content type="html"><![CDATA[<p>这个月利用上下班乘坐地铁的时间阅读了五本书，分别是波兹曼的《娱乐至死》、黑塞的《悉达多》、菲利普·罗斯的《人性的污秽》和《行话》，还有一本池建强的《MacTalk：人生元编程》。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15167888900590.jpg" alt=""></p>
<a id="more"></a>
<p>这篇文章只对《娱乐至死》写了一些感想和串讲，其他几本就相应做了一下摘录。</p>
<h2 id="一、波兹曼《娱乐至死》"><a href="#一、波兹曼《娱乐至死》" class="headerlink" title="一、波兹曼《娱乐至死》"></a>一、波兹曼《娱乐至死》</h2><h2 id="二、黑塞《悉达多》"><a href="#二、黑塞《悉达多》" class="headerlink" title="二、黑塞《悉达多》"></a>二、黑塞《悉达多》</h2><p>摘录了一些句子：</p>
<ol>
<li>读海涅的诗，我要成为社会民主党人，我要学习海涅</li>
<li>他的杯子没有满，他的智怀没有饱，他的灵魂不安宁，他的心情不平静。</li>
<li>在他的身上看不到寻求，看不到欲望，看不到刻意，没有任何努力的痕迹——有的只是光明与安详。</li>
<li>但是这个清晰可敬的教导有一样东西没有包含到，它没有包含大智大慧的世尊自己经历的秘密——有别于千千万万个人的他独自的秘密。这就是在听您讲道的时候我想到并意识到的。这就是为什么我要继续走我自己的路的原因。我不再寻找另外的更好的教导了，因为我知道这样的教导是没有的。我要离开所有的教导和所有的教师，我要单独去达到我的目标，不然，就死掉。</li>
<li>思考的本质就是认识根源，只有通过思考，种种感觉才会变成知识，变成真实，开始成熟，不致迷失。</li>
<li>他必须自己去亲身体验。</li>
<li>除了一件小小的东西，一件极其微小的东西之外，凡是圣贤和思想家所有的一切，他们一概都不缺乏。他所缺乏的那个小小的东西，就是对于所有生命圆融统一的认识。有许多次，悉达多甚至怀疑这个知识，这个思想，怀疑它是不是真有这么大的价值，是不是它或许仅仅是思想家们天真的自我陶醉的说辞，因为思想家们也许不过是一批会思想的孩子而已。</li>
<li>他感觉到这位一动也不动的倾听者是在吸收他的自白，如同一棵大树吸收雨水一样。他感觉到这位静坐倾听的人就是河流的本身，就是神与梵天永恒的化身。</li>
<li>它们互相融入彼此，思念者的悲叹、聪明者的欢笑、愤慨者的哭泣、垂死者的呻吟，全都是互相交织在一起，互相连锁在一起，以千百种不同的方式纠缠在一起。所有的声音、所有的目标、所有的怀念、所有的悲哀、所有的欢乐、所有的善良和邪恶，这一切一切交汇在一起，就是生活之流，就是人生的音乐。</li>
<li>每一个真理的反面也同样是真实的。举例来说，一个真理，如果它是片面的真理，就只能以文字表达出来，也只能局限在文字能力的范围内。每一样事物，要是用思想来思考和用文字来表达，都是单方面的，都只是一半的真理，它缺乏完全、圆满和统一。</li>
<li>歌文达，这个世界不是不完美的，也不是在沿着一条漫长的途径逐渐向着完美发展。不，每个瞬间世界都是完美的，每一个罪恶都已经在它之中携含了恩赦，所有的孩童都是潜在的老人，所有的婴儿在身上都带着死亡，所有垂死的人必获得永恒的生命。</li>
<li>语言无法将思想完全表达出来，思想一旦被语言表达出来，总是会与心中的原意有所不同，有所曲解，显得有点愚蠢。</li>
<li>每一张面孔都是最后难免一死的生物，都是整个空幻界中的一个激情的、痛苦的例子。然而他们却没有一个死掉，他们只是不断地变幻，不断地重生，不断地有新的面孔，只有作为纽带的时间站在一张面孔和另一张面孔之间。</li>
</ol>
<h2 id="三、菲利普·罗斯《人性的污秽》"><a href="#三、菲利普·罗斯《人性的污秽》" class="headerlink" title="三、菲利普·罗斯《人性的污秽》"></a>三、菲利普·罗斯《人性的污秽》</h2><p>摘录了一些句子：</p>
<ol>
<li>1998年的夏天,在新英格兰应该是酷暑加骄阳,而在棒球场上,则该是一个白色本垒打战神和一个褐色本垒打战神之间所进行的神话般的比拼,然而那个夏天席卷全美的却是虔诚与贞洁的大狂欢,因为突然,恐怖主义被口交所代替:一位精力旺盛、面相年轻的中年总统和一个举止轻狂、使人神魂颠倒的二十一岁雇员在白宫的椭圆形办公室里,像两个十来岁孩子在停车场上调情似的。这使得美国最古老的公众再次燃起了激情,从历史的角度来看,也许是它最为不可靠、最具颠覆性的快感:伪君子的狂喜。</li>
<li>我自己则梦到一面大旗,仿佛是一幅基督画像,以达达派手法,将白宫从东到西包裹起来,上面撰写着如下铭文:这里住着的是一个人。这就是那个夏天,即使破烂摊、残害他人肢体罪或大杂烩都被十亿次地证明比这个人的思想或那个人的道德更为精妙。就是在那个夏天,人们脑子里想的都是一位总统的阳具,而他的生活,以其所有无耻的污秽,又一次使得美国不知所措。</li>
<li>我把所有一切的颠鸾倒凤和快乐都归功于伟哥。没有伟哥,这一切都不会发生。没有伟哥,我就会对世界有一个与我年龄相称的看法以及全然不同的生活目标。没有伟哥,我就不会受情欲干扰,而拥有举止规范的年长绅士的尊严。我就不会做没意思的事。我就不会做不体面的、草率的、考虑不周的,而且对所有相关的人都有着潜在危害的事。没有伟哥,我就可以继续在我的晚年发展一个有经验的、受过教育的、荣誉退休的,并早已放弃声色犬马享乐的老年人的那种客观、包容的视角,我就可以继续做深刻的哲理性总结,并一如既往地对青年人进行坚定不移的道德感化,而不至于将自己推回到不断出现的性冲动的紧急状态之中。感谢伟哥,我终于明白了宙斯缘何需要各种多情的化身。他们应当给伟哥起那个名字。他们应当叫伟哥宙斯。”</li>
<li>诀窍便是从(又是霍桑)“一个孤独的大脑与它自己的交流”中寻得养分。秘诀在于从诸如霍桑那样的人身上觅得养分,从才华横溢的已逝者身上觅得养分。</li>
<li>越战老兵是些在战后岁月里亲身经受了生活中一切罪孽的人:离婚、酗酒、毒品、犯罪、警察、牢房、毁灭性的精神压抑、无可控制的哭泣、想尖叫、要砸东西、双手颤抖、身体痉挛、面部紧绷、从头到脚大汗淋漓。由于重温枪林弹雨、刺眼的爆炸、血肉横飞的场面</li>
<li>父亲从不发脾气。父亲有另外的办法叫你服输。用言词。用话语。用他所谓的“乔叟的、莎士比亚的、狄更斯的语言”,用任何人都别想从你身上夺走的英语,用西尔克先生以浑厚的嗓音说出的始终完美、清晰、满怀激情的英语道白,仿佛即使在日常对话中他也是在朗诵马克·安东尼在恺撒尸体旁发表的演说。</li>
<li>有些人只是一味捶打沙袋,科尔曼不,科尔曼思索,与他在学校里或在赛跑时所用的方式一样:把一切不相干的东西都排除出去,不让任何不相干的东西钻进来,一心一意只关注这一件事,题目,比赛,考试——不论必须掌握的是什么,一律成为这一件事。他能够在学习生物学时那样做,他能在短跑时那样做,他能在拳击时那样做。不仅不受任何外部动静的干扰,任何内心活动也都置之度外。如果赛场上人群中有人冲他喊叫,他能充耳不闻,如果与之相斗的人是他最好的朋友,他也可以视而不见。比赛过后,他们有的是时间重修旧好。他设法强制自己无视感情,不论是恐惧、犹豫,甚至友谊——要有这些感情,但和他自己脱钩。比方说,当他进行假想拳斗时,他并不仅仅是全身放松,他同时还设想有另外一个人存在,在脑子里和另外一个人进行一场秘密打斗。临赛时,即使另外那人完全是真实的——臭气熏天的,鼻涕满脸的,汗流浃背的,正在眼前挥拳的——那家伙仍然无从得知你在想什么。没有一名教师要求得到对这个问题的答案。你在场上获得的答案你秘而不宣,当你的秘密大白于天下时,它可以出自各种渠道,唯独不经过你的嘴巴。</li>
<li>懦夫在未死以前,就已经死过好多次;勇士一生只死一次。在我所听到过的一切怪事之中,人们的贪生怕死是一件最奇怪的事情,因为死本来是一个人免不了的结局,它要来时谁也不能叫它不来。</li>
<li>随着两大防护墙的消失——大哥在海外,父亲死了——他重新充电,自由自在地想当什么就当什么,自由自在地追求最高的目标,他骨子里有信心当独特的我,自由到他父亲无从想象的地步,自由得正如他父亲不自由一般。不仅摆脱了他父亲,而且摆脱了他父亲忍受的一切。强迫。羞辱。阻挠。伤痛和故作姿态和羞耻。内心饱尝的失败及挫折的煎熬。自由地走上大舞台。自由地勇往直前,从事大事业。自由地上演无拘无束、自我定位的有关我们、他们和我的戏剧。</li>
<li>他自童年起所向往的就是自由:不当黑人,甚至不当白人——就当他自己,自由自在。他不想以自己的选择侮辱任何人,也不是在企图模仿他心目中的哪一位优等人物,或对他的或她的种族提出某种抗议。他知道,在循规蹈矩的人眼中,世上的一切都早有安排,都是一成不变的,他们永远也不会认为他做得对。但不敢越雷池一步、固守正确的界限向来不是他的目标,他的目标是决不将自己的命运交由一个敌视他的世界以愚昧和充满仇恨的意图主宰,必须由他自己的意志决定。</li>
<li>社会之所以成其为社会的一切——它不断变动的力量、无处不在的利与害的潜网、激烈的争权夺利的战斗、无休无止的吞并降服、派系的纵横捭阖、狡诈的道德术语、习以为常的仁厚独裁、变幻不定的稳定的幻觉——社会之成其为社会的一切,始终如此、必须如此的一切,对他们而言,居然跟康涅狄格的扬基人眼中的亚瑟王朝一样陌生。</li>
<li>心平气和地接受不如自我放逐宏伟辉煌的东西,并放弃对自身力量压倒性的挑战。与自己的失败以一种谦和的态度共处,重新以理性作为生活准则,抹去伤痛和愤怒。倘若不屈服,则静静地不屈服——平静地。带着尊严的沉思冥想</li>
<li>欲望乃万变之源。一切毁灭都可从中找到答案。</li>
<li>突然开始用傻瓜的思维模式思考问题:突然把每件事和每个人都往最好的方面想,完全抛弃对别人的怀疑、自我谨慎、自我怀疑,以为自己的一切困难都迎刃而解了,一切的困扰都不复存在了,不仅忘记了自己身在何处,而且忘记了自己是如何到达的,拱手交出勤勉、纪律、寸土必争的韧劲……就好像每个人的单斗都可以放弃了,就好像一个人可以随手捡回和扔掉自我——独具个性的、不可改变的、从一开始战斗就是为了它而进行的自我。</li>
<li>因为他的信条,因为他目空一切的、傲慢的“我不是你们中的一分子,我不能容忍你们,我不属于你们黑人的我们”的信条。反对他们的我们的伟大英勇的斗争——瞧他现在的德性!为争取宝贵的个性而进行的激烈斗争,他为反对黑人命运所进行的单枪匹马的反抗——瞧,这个蔑视一切的伟人落到了什么地步!这就是你,科尔曼,来寻找生活深层意义的地方。一个充满爱的世界,那是你原来拥有的,可是你却为了这个而抛弃了那个!你所作的悲剧性、鲁莽的行为!而且不仅对你自己——对我们大家,对欧内斯廷,对瓦特,对母亲,对我,对在坟墓中的我,对在坟墓中的我父亲。</li>
<li>“美国海军”,文身只说了这些,高度仅有四分之一英寸的几个字,用蓝色颜料刺在一个蓝色铁锚的两个蓝色臂膀之间,铁锚本身有两三英尺长。就军人的文身而言,是个非常简朴的图案,而且谨慎地、恰恰安置在右胳膊肩关节下,无疑是个相当容易隐藏的文身。但当他回想他如何将它刺上去时,它不仅成为一个唤起他生命中最糟糕夜晚狂乱情景的标记,而且成为一个唤起潜伏在狂乱背后之一切的标记——它是他全部的历史,他的英雄主义与羞耻不可分割性的缩影。镶嵌在那个文身里的正是他的一个真实、完整的自我形象,其中可见无法磨灭的身世,如同根深蒂固事物的原型,因为文身恰恰象征着永远无可变更的一切,其中也包含着巨大的业绩,包含着外部势力,不可预知未来的整个链接,一切暴露的危险,以及一切隐藏的危险——甚至生命的无意义性都隐含在那个小小的、傻乎乎的蓝色文身之中。</li>
<li>她在他们那个年龄早看过了所有的黑泽明,所有的塔科夫斯基,所有的费利尼,所有的安东尼奥尼,所有的法斯宾德,所有的沃特缪勒,所有的撒提亚吉特·雷,所有的雷内·克莱尔,所有的文·温德斯,所有的特吕福、戈达尔、夏布罗尔、利斯奈、罗米尔、雷诺阿,而这些孩子只看过星球大战。</li>
<li>他在里面,正和福妮雅一起,相互保护着对方,不受任何外人的侵扰——彼此,对对方而言,构成了整个世界。他们在里面跳舞,很可能光着身子,超越人世的苦难,置身于一个植根于世俗欲望的非凡的天堂里,在那里他们的结合是一出他们倾注生命中所有的愤怒与失望的戏剧。</li>
<li>音乐家们的确揭示了我们生命中最年轻、最天真的思想以及对于非现实、不可能实现的东西的根深蒂固的渴望。</li>
<li>他将假装说世界属于我们,而我会让他这么假装,然后我也会假装。但是,为什么不呢？我能跳……但他得记住。</li>
<li>我们留下污秽,我们留下踪迹,我们留下我们的印记。污染、残酷、欺凌、谬误、粪便、精液——要待在这儿就别无二致。和反抗无关。和恩赐或救赎无关。在每个人的身上。存储于内心。与生俱来。无可描述。污秽先于印记。没有留下印记之前便已存在。污秽完全是内在的,不需留印记。污秽先于反抗,是包围反抗并扰乱一切的解释与理解。这就是为什么所有的净化行为纯属玩笑,而且还是个野蛮的玩笑。纯洁的幻想是极其可怕的,是疯狂的。对纯洁的追求其实质倘若不是更严重的不纯洁,又会是什么呢？她所有关于污秽的话归结起来无非是说它是不可逃避的。这,自然,便是福妮雅的阐释:我们无可避免地都是被污染的角色。心甘情愿地接受这可怕的、原始的不纯净状态吧。她像希腊人,像科尔曼的希腊人,像他们供奉的神。无人不是小心眼。争吵。械斗。恨。谋杀。交媾。他们的宙斯成天只想操女的——女神,女人,母牛,母熊——不仅以他自身的形象出现,还更为令人兴奋地将自己装扮成兽类,作为一头公牛气势雄劲地凌驾于女性之上,化做一只扑打着双翼的白天鹅以异乎寻常的方式进入她的身体。对这位众神之王而言,肌肤之乐永无穷尽,花样翻新也层出不穷。欲望所带来的一切疯狂。放荡。堕落。最粗野的欢乐。还有妻子的怒气。不要那绝对孤独,绝对隐晦,作为现在、过去以及永远唯一主宰的穷极无聊地整日为犹太人操心的希伯来上帝。不要那完全无性别的基督神人和他降孕怀胎的母亲以及所有某种精致的超凡性所激发的罪恶感与羞耻感,而选择纠缠于冒险之中、具有鲜活表达力、朝秦暮楚、沉醉于声色犬马、精力充沛地与他丰富多彩的生活联姻、从不单独行事、从不偷偷摸摸的希腊的宙斯。而选择神圣的污秽。对福妮雅·法利来说,伟大的反映现实的宗教,倘若,通过科尔曼她有所了解的话,如同希伯来幻象所言,是以上帝的形象创造的,好吧,但并不是我们的上帝——是他们的。上帝淫荡。上帝腐败。如果真有过上帝的话,是个活生生的神,是以人的形象出现的上帝。</li>
<li>虽然世上满是那种自以为他们将你或你的邻居看透了的人,实际上未知的东西却深不可测。关于我们的真相是无穷无尽的,谎言也同样如此。</li>
</ol>
<h2 id="四、菲利普·罗斯《行话》"><a href="#四、菲利普·罗斯《行话》" class="headerlink" title="四、菲利普·罗斯《行话》"></a>四、菲利普·罗斯《行话》</h2><ol>
<li>与我们闭着眼睛什么也看不见，光溜溜地呱呱坠地不同，罗斯先生一出场，指甲、毛发、牙齿都已长齐，他说话流利，技巧娴熟，机智幽默，富有生气，具有名家风范。</li>
<li>卡夫卡的“小说一直所坚持的就是，看上去似乎难以想象的幻觉和毫无希望的诡论其实正是构成我们现实的东西”。</li>
<li>他们都在小说里直露地描写了性，但并不认为那是渲染色情，而是认为性是人的最深层的东西，最能体现人的本质，最能放射出异常强烈的光芒。</li>
<li>他在倾听时专心、安静，就像金花鼠在石墙上发现了陌生的东西一样。</li>
<li>正常人在生理构造上注定要从事具有目的的活动，无所事事或者无目标的工作（如奥斯威辛集中营的工作）则导致痛苦和萎缩症。</li>
<li>这部分及整部书给我留下深刻印象的是，一颗实际、高尚的科学心灵的思考在多大程度上使你得以生存。在我看来，你的幸存并非由兽性的生物力量或者难以置信的运气所决定，而是由你的专业性格所决定：讲究精确的人，追求秩序原理的实验控制者，他所重视的一切都被颠覆。就算你是恶魔般的机器中一个编了号的部件，你也是一个总在用一颗系统化的心灵去理解的部件。在奥斯威辛集中营里，你自语道“我思考得太多”以抵制，“我过于开化”。但对我而言，那位思考太多的开化人与那位幸存者是分不开的。科学家与幸存者是同一个人。</li>
<li>卡夫卡依仗的是内在的精神世界，企图对现实达到某种掌握，而我经历了集中营和森林这样实实在在、具体细致的经验世界。</li>
<li>第二次世界大战中的犹太人经历并非属于“历史”范畴。我们遭遇到了原始神秘的力量，一种神秘的潜意识。我们对其中所含的意义没有任何了解，时至今日我们仍然无法了解。这个世界似乎是理性的（有火车、发车时间、火车站和工程师），但这些只是想象的旅行、谎言和诡计，只有深奥的非理性冲动才能虚构出来。</li>
<li>他们的目盲和耳聋，以及他们只全神贯注于自己的事务等构成了他们单纯的一部分。那些凶手是功利主义者，知道他们需要什么。单纯的人总是不幸、滑稽的受害者，从没有及时听到危险的信号，混杂起来，乱糟糟一堆，最后掉在了陷阱里。那些缺陷使我感到陶醉。我爱上了它们。犹太人施用诡计统治世界的神话原来是被在某种程度上夸大了。</li>
<li>捷克斯洛伐克地下出版所产生的环境有其独特性。由外国军队支持的政权——由占领者建立的政权知道，这个政权只有符合占领者的意愿才能存在——害怕批评。它还知道，任何一种精神生活最终都导致对自由的向往。所以，它毫不犹豫地禁止实际上所有捷克斯洛伐克文化，使作家无法写作，画家无法展览，科学家——特别是社会科学领域——无法进行独立的研究；它摧毁大学，大多数情况下任命俯首帖耳的职员当教授。突然间被投入到这个大灾难之中的国家被动地接受着这一切，至少一度如此，无奈地看着最近崇拜的人物一个接一个地消失。</li>
<li>文学没有必要四处搜寻政治现实或者担忧兴衰更替的统治体系；文学可以超越这些，而且仍然可以回答统治体系在人们心中所激发出来的问题。</li>
<li>捷克斯洛伐克成了一个自由民主的消费社会的时候，你们这些作家就会发现自己受到了许多新的对手的折磨，而再奇怪不过的是，那个压迫人的、枯燥无味的极权社会曾保护你们免于这些折磨。特别让人不安的将是威胁文学、教育和语言的那个无处不在的、威力无比的主要对手。我可以向你保证，不会再有对抗的人群在瓦茨拉夫广场集合起来去推翻暴政，也不会有哪位剧作家知识分子被愤怒的群众抬举来救赎民族之魂，使其脱离因对手将所有人类话语减弱之后而使之所处的昏聩状态。我正在谈论的是将所有一切变得浅薄无聊的商业电视——并不是说因为被愚蠢的国家审查所控制才没有人想看的少数频道，而是说十几、二十几个因为娱乐性才被众多人整日观看的乏味的陈词滥调的频道。你和作家同行们最终摆脱了极权体制下的知识监狱。欢迎你们进入全部娱乐的世界。你们不知道失去了什么。或者说你们知道吗？</li>
<li>如同卡夫卡独身一人一样，据说舒尔茨与女性保持着长期热烈的联系，通过信函过着大部分的色情生活。</li>
<li>作为一个文化史概念，东欧是俄罗斯，其具体的历史位居拜占庭世界。波希米亚、波兰、匈牙利，就如奥地利一样，从来就不是东欧的一部分。从最初起，他们就参与了西欧文明伟大的冒险，如哥特文化、文艺复兴、宗教改革——这一运动的摇篮之地确切地说就在这一地区。而现代文化的最伟大的脉搏在中欧跳动——心理分析、结构主义、十二音技术、巴托克音乐、卡夫卡和穆西尔的小说新美学等。战后中欧被俄罗斯文明吞并（或者至少是其主要部分），致使西方文化失去了其关键的重心</li>
<li>幽默感是辨认的可靠标志。从那时起，我就对一个没有幽默感的世界感到恐惧。</li>
<li>人们的愚蠢在于为一切都提供一个答案，小说的智慧在于对一切都提出一个问题</li>
<li>小说家教育读者把世界当成一个问题来看。这种态度中包含着智慧和宽容。在一个建立于极度神圣的肯定之上的世界里，小说就无法存在。极权主义的世界，无论是建立在什么主义之上，都是一个答案的世界，而不是问题的世界。在这个世界里，小说没有地位。不管怎么说，在我看来，似乎全世界的人当今都喜欢判断而不喜欢理解，喜欢回答而不喜欢提问，结果小说的声音被人类吵闹的、愚蠢的肯定声音所淹没。</li>
<li>这就是作为作家的代价，对过去始终难以忘怀——痛苦、激动、拒绝，所有的一切。我坚信，这种抱着过去不放的思想，虽然没有希望，但它充满热情地渴望重构过去，使它发生变化。医生、律师，还有其他许多稳定的公民不会被一种持之以恒的记忆所折磨。他们以自己的方式和我们一样受到烦扰，只不过他们不知就里，因为他们不探究。</li>
<li>我秘密参与周围世界的一切，关注每一个人的每一点历史，这些材料都是故事和小说的必备基础。</li>
<li>你一旦长大成人，离开家庭，选择了作家这种孤独的生活，性爱就不可避免地成为你可以继续进入的最重要经验领域？</li>
<li>性爱的兴奋在很大程度上是与痛苦和分离联系在一起的。我的性生活对我来说是最重要的，相信对于每个人也如此。思考和完成它要花费很多的时间，但思考性爱是令人自豪的部分。对于我来说，它主要是秘密的，包含着神秘和劫掠的成分。我的日常生活和性生活不是一个整体——它们是分离的</li>
<li>贝娄推翻了一切：基于和谐、有序的叙述原则之上的写作选择，受惠于卡夫卡的《审判》、陀思妥耶夫斯基的《双重人格》（The Double）和《永久的丈夫》（The Eternal Husband）的小说家社会精神气质，以及一种很难说是因为喜欢闪光、色彩和足够的实体而产生的道德视角</li>
<li>《赫索格》是贝娄写作生涯中首部涉及性这个广泛领域并进行长时间探讨的小说。赫索格的女人对他来说至关重要，因为她们勾起了他的虚荣，激发了他的肉欲，引导了他的爱，引起了他的好奇。还因为可以表现出男人的聪明、魅力和俊美容颜从而使他滋生了男童般的快乐和喜悦——在她们的爱慕中他得到了确认。她们骂出的每一句难听的话，杜撰的每一个称号，头颅的每一次迷人的转动，手的每一次安慰性的触摸，嘴巴每一次愤怒的扭曲，他的女人们都以异性特有的他者性使赫索格神魂颠倒。</li>
<li>对无法预料的展现就是一切。把错误转变方向，那残酷的难料之事就是我们学童所学的无害的‘历史’，一切当时的难料之事被当做历史的必然编入历史，难以名状的恐怖被历史所掩盖，灾难转换成了史诗。</li>
</ol>
<h2 id="五、池建强《MacTalk：人生元编程》"><a href="#五、池建强《MacTalk：人生元编程》" class="headerlink" title="五、池建强《MacTalk：人生元编程》"></a>五、池建强《MacTalk：人生元编程》</h2><p><a href="https://plushunter.github.io/2018/01/05/日知录（8）：MacTips%20/" target="_blank" rel="noopener">日知录（8）：MacTips</a></p>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 娱乐至死 </tag>
            
            <tag> 悉达多 </tag>
            
            <tag> 人生的污秽 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[日知录（4）：启发式认知偏差]]></title>
      <url>/2018/01/02/%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A%E5%90%AF%E5%8F%91%E5%BC%8F%E8%AE%A4%E7%9F%A5%E5%81%8F%E5%B7%AE/</url>
      <content type="html"><![CDATA[<p>在不确定条件下，人类会采用一种启发式的思维方法，即根据以往（相同的或类似的甚至无关的）的经验来对当前情况进行判断。这是一种思考上的捷径，是解决问题的简单、笼统的策略，也称之为经验法则或拇指法则。</p>
<a id="more"></a>
<p>因为人类是认知的“吝啬鬼”，任性、懒惰。启发式推理会简化信息处理的过程。因此，当人们面对大量的信息和不确定性进行判断时，往往并不遵循贝叶斯法则，而是凭借直觉或者以往的经验进行判断。依赖“启发法”做出的决策带有不确定性，只能说可能是正确的结论，但如果所遗漏的因素和现象很重要，那么信息的缺损就会导致产生判断和估计上的严重偏差。</p>
<p>Kahneman和Tversky指出人们通常具有三种启发式推理方式：</p>
<ul>
<li>代表性启发法</li>
<li>可得性启发法</li>
<li>锚定与调整启发法</li>
</ul>
<p>这三种方法既可以得出正确的推理结果,也有可能导致错误的结论。</p>
<h2 id="一、代表性启发法"><a href="#一、代表性启发法" class="headerlink" title="一、代表性启发法"></a>一、代表性启发法</h2><p>在使用启发法时，首先会考虑到借鉴要判断事件本身或事件的同类事件以往的经验即以往出现的结果，这种推理过程称之为代表性启发法。</p>
<p>　　一般情况下，代表性是一个有用的启发法，但在分析以往经验，寻找规律或结果的概率分布的过程中,可能会产生严重的偏差,从而得到错误的启示，导致判断错误。</p>
<h3 id="1-1-代表性启发法的定义"><a href="#1-1-代表性启发法的定义" class="headerlink" title="1.1 代表性启发法的定义"></a>1.1 代表性启发法的定义</h3><ol>
<li>代表性会导致忽略样本大小。在分析事件特征或规律时，人们往往不能正确理解统计样本大小的意义，对总体进行统计的结果才是真正的结果，样本的数量愈接近真实的数量，统计的结果也就愈可信，样本愈小,与真实数量相差愈大,统计的结果愈不能反映真实的结果情况。代表性启发法是对同类事件以往所出现的各种结果进行统计分析，得到结果的概率分布，从而找出发生概率最大的结果即最可能发生的结果。 因此必须考察所有同类事件这个总体或者考察尽量多同类事件(大样本)但人们往往趋向于在很少的数据基础上很快地得出结论。</li>
<li>代表性会忽略判断的难易程度，即使面对的是一个复杂的难以判断的问题，也简单地去作出判断，或经常根据不规范的和与判断无关的描述轻易地作出判断，或经常会忽略掉不熟悉或是看不懂的信息，只凭自己能够理解和熟悉的信息去作出判断，这些忽略掉的信息可能对判断是关键的。
　　</li>
</ol>
<blockquote>
<p>代表性启发出现的背景</p>
</blockquote>
<ul>
<li>没有时间认真思考某个问题</li>
<li>负载信息过多以至于无法充分对其进行加工</li>
<li>认为问题不十分重要以至于不必太过思虑</li>
<li>缺乏做决策所需的可靠知识或者信息</li>
</ul>
<h3 id="1-2-Tom-W-实验"><a href="#1-2-Tom-W-实验" class="headerlink" title="1.2 Tom W 实验"></a>1.2 Tom W 实验</h3><p>1973年Kahneman及Tversky进行了一个名为“Tom W ”的著名实验，大概如下：给被试以下一段关于Tom W.的描述：“Tom W.智商很高，但是缺乏真正的创造力。他喜欢按部就班，把所有事情都安排得井然有序，写的文章无趣、呆板，但有时也会闪现一些俏皮的双关语和科学幻想。他很喜欢竞争，看起来不怎么关心别人的感情，也不喜欢和其他人交往。虽然以自我为中心，但也有很强的道德感。”</p>
<p>然后要被试估计，Tom W.最有可能是以下哪个专业的学生：企业管理，工程，教育，法律，图书，医学，社会学？想象一下如果你是其中一名被试，你会怎么回答。</p>
<p>结果，绝大多数被试都认为Tom W.最有可能是工程系学生。为什么呢？很有可能是因为Tom W.最像一个学工程学的学生。也就是说，对Tom W.的以上描述，与我们心目中一个理工科学生所应当具有的形象完全吻合（或者说代表了一个理工科学生的形象），所以我们认为Tom W.最有可能是工程系的学生。这就是典型的代表性启发式思维方式。当面对不确定的事件，我们往往根据其与过去经验的相似程度来进行判断或预测。说简单一点，就是基于（过去经验的）相似性来预测（当前事件的）可能性。到底个体A是否归属于群体B？如果个体A具有群体B的某些特征（具有相似性、代表性），则认为个体Ａ归属于群体Ｂ。</p>
<p>如果我们在公共汽车上看到一个人鬼鬼祟祟，像个小偷，则我们会认为他就是一个小偷，并提高警惕性。有时相似性确实和可能性有关，因此这种判断是正确的，但有时则可能会因此忽略其它相关信息而做出错误的判断。</p>
<p>比如在Tom W.的实验当中，被试就完全忽略了学生在各个专业中的基础比率（base rate）。就算上述7个专业的学生都一样多，那么任何一个学生是工程系的学生的概率和他是其它任何一个专业的学生的概率是一样的，即1/7。根据另外一组被试对所有学生在各个专业中所占的比率的估计，学工程学的学生应该比学其他专业的学生要更少，即还占不到1/7。如果考虑到这一点，那么任意抽一个学生出来（比如Tom W.），他是学工程学的可能性应该是很低的。这种在判断时忽略基础比率而导致的谬误就是所谓的基础比率谬误（base rate fallacy） 。</p>
<p>再看另外一个问题 ，Linda，31岁，单身、坦率，活泼，她学的专业是哲学。当她还是个学生时，就非常关注歧视和社会公正的问题，并且参加过反核武的示威游行活动。（Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.）问Linda更有可能是什么样的人？</p>
<ol>
<li>Linda是一个银行出纳员。</li>
<li>Linda是一个崇尚女权主义的银行出纳员。</li>
</ol>
<p>很多人都会选第二项。因为从对Linda描述更符合我们心目中女权主义者的形象（或者说代表了我们心目中女权主义者的形象），所以我们就更倾向于认为Linda是一个崇尚女权主义的银行出纳员。我们在这里就运用了启发式的判断，却没有注意到这样一个基本道理：两个独立的事件同时发生的概率不可能高于其中单个事件单独发生的概率，从而犯了一个所谓的结合谬误（conjunction falalcy）。Linda是崇尚女权主义的概率可能很高，Linda是一个银行出纳员的概率可能不高，但Linda同时既是银行出纳员又崇尚女权主义的概率就肯定低于前二者的概率了 。这个道理说出来很简单，大家心里都清楚，但一到实际中人们往往就不会运用。我们会犯这样一种错误的原因可能是因为对事件描述得越详尽，就越容易让我们产生联想，进而导致我们误以为事件越容易发生。</p>
<h3 id="1-3-赌徒谬误"><a href="#1-3-赌徒谬误" class="headerlink" title="1.3 赌徒谬误"></a>1.3 赌徒谬误</h3><p>在运用代表性启发法进行判断时还有可能会导致赌徒谬误（gambler’s fallacy），也称为蒙地卡罗谬误（The Monte Carlo Fallacy ），主要来源于这样一个故事 ：</p>
<p> 1913年8月18日，在蒙地卡罗的一间赌场里的轮盘 游戏中，黑色不可思议的连续出现了十五次，人们开始近乎疯狂的冲着去押红色。当黑色连续出现了二十次以后，人们还进一步加大了他们的赌注，因为大家都认为在黑色连续出现了二十次以后再出现黑色的可能性已经不到百万分之一了。结果黑色是创纪录的连续出现了二十六次！这间赌场因此挣得盆缽满盈。</p>
<p>大家都有这种感觉：似乎黑色已经连续出现太多次，不可能再出现了。这种想法很普遍。比如玩抛硬币，我告诉你前面抛的五次结果都是“正”，要你猜一猜，下一次会出现哪一面？肯定很多人会倾向于“反”面。前面已经出现过那么多次“正”面了，不可能还是“正”吧？连续出现6次“正”面的概率太低了。又比如人们在买彩票的时候，一般都不会选择上一次中奖已经出现过的号码，不可能连续两次中奖都有同一个号码。其实这也是支撑赌徒一直赌下去的重要心理原素之一：我已经输了那么多次了，无论怎么样也应该会赢一次吧。</p>
<p>这其实也是一种启发式的思维模式。我们认为不可能连续出现6次“正”面或是极端的连续26次黑色，或者连续两次中奖号码都有同一个数字，因为抛硬币、赌博、彩票等事件是随机的，这样的概率实在是太低了，根本就不像是随机事件，一个随机事件怎么可能有这么多巧合？  </p>
<p>但到底怎么样才叫“随机”？到底是“正反反正反正”还是”正正正正正正”更有可能出现？其实现实生活中随机事件看起来往往都不像是随机的，或者说随机事件并没有你想象的那么随机。所谓随机也就意味着事件与事件之间在统计学意义上是独立的（what makes a sequence random is that its members are statistically independent of each other），一件事情的发生在统计学意义上对另一件事情的发生没有任何影响。（the occurrence of one has no statistical effect upon the occurrence of the other）。随机事件是没有倾向性的，是不可预测的，是没有记忆功能的。因此，就算黑色已经连续出现了N次，下一次是红还是黑都是随机的，认为黑色不太可能再出现而疯狂的去押红色是没有道理的；不管“正”面已经连续出现了多少次，下一次的结果要不是正面就是反面，二者出现的概率都是50％。</p>
<p>但是竟然会出现连续26次黑色或者连续6次正面这种情况，还是让人难以接受。这是因为有时小样本不具有代表性，样本越小，与真实的数量相差越大，统计的结果越不能反映真实的情况。只有对总体进行统计的结果才是真正的结果，也就是说样本的数量越接近真实的数量，统计的结果也就越可信。如果只抛十次硬币，正反面出现的概率不一定是50%，什么情况都有可能发生，只有抛足够多次，才能得出正反面的概率是50%的结果。这提醒我们有时不要匆忙的作出判断或下结论，很有可能你看到的只不过是一个小样本。</p>
<h2 id="二、可得性启发法"><a href="#二、可得性启发法" class="headerlink" title="二、可得性启发法"></a>二、可得性启发法</h2><p>人们在什么情况下更愿意买地震保险？想象在汶川地震以前有位保险销售人员向你推销地震保险，相信你是打死都不会买。因为地震这个事情离你的生活实在是太遥远了，那是八竿子打不着的事情。但在发生汶川地震以后，情况就截然不同了，特别是你在电视上看到了地震的各种惨状以后，如果再有保险销售人员向你推销地震保险的话，你肯定会比以前更愿意掏钱。现在很多人去买房，总喜欢问这样一个问题：这房子能抗几级地震啊？如果我们仔细想想的话，人们的这种反应其实是非常可笑的，因为恰恰是因为刚刚发生过地震，所以在一段时间内再次发生的地震的可能性是很小很小的。那么为什么人们反而会在地震以后更愿意买地震保险呢？这就是可得性启发法（Availability heuristic）的影响。</p>
<p>很多时候我们做出判断是情绪性的、无意识的，非理性的。人们对于越容易想起来的事情（即在脑海中的印象更为深刻），会觉得越容易发生，这就是所谓的可得性启发，通过易得性来判断其可能性，即“<strong>人们倾向于根据客体或事件在知觉或记忆中的可得性程度来评估其相对频率，容易知觉到的或回想起的客体或事件被判定为更常出现 </strong>”。比如我们在判断是否要买地震保险时，脑海中首先浮现到的就是不久前发生的地震以及我们在电视上目睹的各种惨状，于是就会觉得地震确实很可怕，并且似乎离我们很近或者说很有可能会发生在我们自己的身上，因此就更愿意购买地震保险了。</p>
<p>对于可得性启发经常被提及的一个实验是这样的，问：以字母k开头的英文单词和第三个字母是K的英文单词相比，二者谁更多？大多数人认为以字母k开头的英文单词更多。因为人们很容易就想到以字母k开头的英文单词，比如keep,kill,kitchen等，但要想起第三个字母是K的英文单词就有些困难了，于是人们就会认为以字母k开头的英文单词会更多。实际上，第三个字母是k的单词是以k字母开头的单词的3倍。</p>
<p>有时候可得性启发式是正确的，越容易回想起来的事情确实越有可能发生，因为不断重复（容易）发生的事情，自然更容易在我们的脑海中留下深刻的印象。但是我们不仅仅只会记住那些经常发生的事情，事实证明，我们更喜欢那些生动的、形象的、具有情绪感染力的事件。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15148711597449.jpg" alt=""></p>
<p>大家都应该见过这张希望工程的宣传图片，相信许多人在看到这张照片以后，都会被深深的感动，可能本来没打算捐钱的结果也捐一点钱，本来打算只捐5块钱的结果捐了10块钱。我们通过这张图片，对贫困失学儿童有了更为直观的、生动的认识。</p>
<p>这就是所谓的一张图片胜过千言万语。不管我们是说帮助中国的失学儿童有多么多么的重要，还是说中国有多少多少的失学儿童并列举一堆统计数字，都不如一张具有视觉冲击力的图片更能打动我们的心。我们的行为很容易因此而受到影响——比如多捐一点钱，当然这是好的方面。（这也就是为什么在禁毒宣传或者交通安全宣传的时候总是喜欢摆一些恶心的照片出来）</p>
<p>此外，我们对自己亲身经历过的事情印象也会特别深刻。一个死于骑摩托车的亲戚比大量的统计数据更能影响你对摩托车的态度。结了婚的年轻人经常会为干家务的事情发生争吵，总认为自己干的家务活要比对方干得多。美国的心理学家做过这样一个调查，让妻子和丈夫各自评估自己所干的事占所有家务的比例，然后将两人的比例加起来，结果总是超过100%。这其实很好理解，这一方面是因为自利归因，另一方面也是因为自己干的家务活自己总是能记住，所以就总觉得自己干的要比对方多。</p>
<p>可得性启发会导致我们做出错误的决策。因为受可得性启发的影响，我们做出判断决策时，依据的可能就仅仅是一些经过我们头脑选择过的、印象最为深刻的例子。正如道金斯在《自私的基因》里说到的：经过选择的例子对于任何有价值的概括从来就不是重要的证据（Chosen examples are never serious evidence for any worthwhile generalization.）。因为例子永远都只能是一个例子，不具有任何的代表性。如果你遇到一个河南人把你的钱都骗光了，你能因此而得出结论说所有的河南人都是骗子，都是不可靠的吗？如果你不是遇到一个河南人如此，而是十个、一百个河南人都如此呢？那么也仅仅说明这十个、一百个河南人是不可靠的，但不能说所有河南人都是不可靠。</p>
<p>此外我们在举例时很容易忽略那些与我们已有观念不一致的信息，从而导致所谓的证实性偏差。塔布勒在《黑天鹅》中把过度举例称为“无知的经验主义”。想象一只火鸡，每天都被喂得饱饱的，每次的喂食都使它更加相信生命的一般法则在于每天得到“为他的最大利益着想”的友善人类的喂食。日子一天天的过去，它的信心也越来越充足。但感恩节也是一天天的临近，它被屠杀的危险也越来越大。当危险最大时它的安全感却达到了最大值。直到感恩节的前一天，它才真正的明白过来，它过去获得的知识和经验都是无关痛痒的甚至是虚假的。在某种程度上，我们所有人都是这只火鸡。</p>
<p>所以例子不应该作为论据来使用的。那么有人会说，你这篇文章里不就通篇都是例子吗？例子在这里只能是为了把更好的把一个道理解释清楚、使人更容易理解。也就是说我们应该用例子来阐释说明而非证明。但现实恰恰是人们很喜欢用例子来证明自己的观点——特别是那些最容易想到的例子。</p>
<p>比如有一次你乘坐A航空公司的航班，结果A航空公司把你的行李给弄丢了，你火冒三丈，觉得A公司的服务怎么这么差，以后再也不要坐这间公司的航班了。这其实和河南人的例子差不多，一次不愉快的经历可能只是一宗个案，并不具有任何代表性。</p>
<p>但有的人就会认为，保障乘客的行李安全是航空运输的最基本要求之一，如果连这一点都做不到的话，那么我们还有什么理由来相信它，就算是个案，这样的事情只要经历一次就足够了，难道还要再丢一次行李吗！这种想法是可以理解的，但却是没有道理的。在某种程度上，错误其实是不可避免的。管理学上有所谓的六西格玛质量管理法，我总是很怀疑这是否能够真的做到。就算是DNA复制这种高度精确的事件，都会不可避免的出现偏差（这正是进化的源泉之一），更何况人为的事件？因此，航空公司把乘客的行李弄丢其实是一个不可避免的错误，只是很不幸碰巧被你遇上了。这是一个负面的影响，导致你对整体产生了负面的认识。但是想象如果是一件正面的事件，比如你买彩票，一不小心中了五百万，那么你会如何认识中奖这样一件事情呢？难道你会因此而觉得彩票中奖是很容易，或者别人也应该中奖，或者你以后会更容易中奖吗？</p>
<p>当然我不是说航空公司把你行李弄丢是和中彩票一样的低概率事件，或者说是可以原谅的。而是要看到，不能仅仅因为一次丢失行李的不愉快的经历而对该公司的服务水平完全否定。如果真的要想知道A公司的服务水平到底怎么样？我们就需要调查不同航线准时到达率和行李丢失率的统计数据。只有统计数据才能相对真实的反映出客观情况。（这也让我想到了所谓的一票否决制，某些领导如果在某些事情上没有达标或者出了什么问题，那么就一概否决，这是不够科学的。没有被否决可能仅仅是运气好罢了。）</p>
<p>在现代传媒的作用下，可得性启发对人们判断决策的影响更加的大。因为媒体为了追求收视率，吸引眼球，总是喜欢过度报道一些特别的事件。正如人们戏言：狗咬人不是新闻，人咬狗才是新闻。但是如果你看电视看的太多并且懒得去想的话，那么你就会很容易认为满大街的人都在咬狗。比如人们总认为坐飞机很危险，事实上大量的统计数据表明，飞机并不会比坐汽车更危险，但为什么人们会有这样一种错误的判断？很大程度上是因为飞机失事更有新闻价值。如果是一宗普通的交通事故，媒体就不会有太多的报道，但如果是一架飞机坠毁了，那么媒体必定是连篇累牍的大肆报道。结果这些事情在人们的脑海里留下了极其深刻的印象，当我们需要作出判断时脑海中首先浮现的就是这些飞机失事的场景，因此就会得出结论：飞机非常的危险。当然人么更害怕坐飞机也有其他方面的原因，比如人类的生物本能，人们对于自己不能控制的事物的恐惧等等。</p>
<p>《魔鬼经济学》里有一个例子：假设有个一个8岁大的叫莫莉的孩子，她有两个最好的朋友，一个叫艾米，一个叫伊玛尼，两个朋友都住在附近。莫莉的父母知道艾米的家里放着一把枪，于是他们不许莫莉到艾米家玩。所以莫莉就经常跑到伊玛尼家玩，伊玛尼家的后院有个游泳池。莫莉的父母觉得自己的做法上是在保护莫莉，这样做是对的。</p>
<p>可根据统计资料显示，这种做法一点都不明智。平均来说，美国每1.1万个家庭游泳池就能溺死一个孩子。美国一共有600万个这样的游泳池，这也就是说，每年将近有550个不到10岁的孩子是溺死在游泳池里。相比之下，在美国，每100多万支枪才会杀死一个孩子。据估计，美国一共有2亿支枪，这就是说美国平均每年死于枪口下的孩子数量大约为175名。所以对于美国孩子来说，他们死于游泳池里的概率（1：11000）要远远大于死于枪口的概率（1：1000000）：也就是说莫莉在伊玛尼家的危险程度是在艾米家的100倍。</p>
<p>问题就在于为什么莫莉的父母会觉得枪会比游泳池更危险？这和人们对飞机的态度的原因是一样的。媒体更愿意报道一名丧心病狂的家伙开枪打死一名孩子而不是一名孩子在游泳池里淹死。各种奇闻异事更容易在人们的脑海中留下深刻的印象。因此在某种程度上我们对世界的认识是扭曲的，我们的身边充斥着各路媒体和大量信息，我们以为自己很清楚自己在想什么，实际上我们的世界观价值观已被他人所左右。从这个角度来看，信息越多误差越大。</p>
<p>我们如何摆脱可得性启发带来的偏差？这其实是不可能的，因为启发法是一种无意识的思维，也就意味着有时候我们连自己已经运用了启发法都不知道，更别说避免其错误了。但如果我们对启发法有更深入的了解，那么还是有助于我们减少在这一方面的错误的。感觉往往是不可靠的，不要轻易的下结论。当我们做出判断或得出一个结论时，必须要问自己：我的依据（论据）是什么？这些依据是否足以支撑我的判断？我们对那些感人或者骇人的一切生动的故事都必须保持足够的警惕，不要被我们的情绪所主宰，要更加注重统计数据，很多时候只有统计数据才能相对真实的反映出客观情况。</p>
<h2 id="三、锚定与调整启发法"><a href="#三、锚定与调整启发法" class="headerlink" title="三、锚定与调整启发法"></a>三、锚定与调整启发法</h2><p>这是一个地球人都知道的故事。一条巷子里有两家卖粥的小店。左边一个，右边一个，两家店的生意都很好，每天都是顾客盈门。可是，晚上盘点的时候，左边这个店总是比右边那个店每天多赚两三百块钱，而且每天都是这样，让人心生不解。</p>
<p>细心的人终于发现了其中的秘密。如果你走进右边那个粥店，服务员微笑着把你迎进去，给你盛好一碗粥，热情地问你：“您好!加不加鸡蛋?”一般情况下，喜欢吃鸡蛋的人，就会说加一个吧!于是服务员就会拿来一个鸡蛋;不喜欢吃鸡蛋的人，就会说不加，喝完粥结了帐就走了。可是，如果你走进左边那个粥店，服务员同样也是微笑着把你迎进去，给你盛好一碗粥，然后热情地问你：“您好!加一个鸡蛋还是加两个鸡蛋?”一般情况下，喜欢吃鸡蛋的人，就会说加两个;不喜欢吃鸡蛋的人，就会说加一个。就这样，一天下来，左边的这个粥店比右边的那个粥店每天要多卖出很多个鸡蛋，这就是它每天多出多出两三百块的原因。</p>
<p>大凡讲这个故事的人，都是在讲左边那家粥店的生意经。其实，粥店的生意经里蕴含着十分深刻的心理学道理。左边的那个粥店其实是在运用心理学中的锚定法，诱导消费者在不经意间做出有利于店家的选择。如果你是它的顾客，你的决策则是受到了心理锚定效应的影响，你不自觉地在粥店设定的条件下进行了决策选择。<br>　　<br>也许，多吃一个鸡蛋并没有什么大的问题，但是，如果在商业交易或谈判中，你多付出了10万或者100万，可能就是一个大问题，或者说是一个大损失。左边小店的服务员把顾客“锚定”在“加几个鸡蛋”上，而右边小店的服务员则把顾客“锚定”在“要不要加鸡蛋”上。在前一种情况下，顾客是在“加一个鸡蛋还是加两个鸡蛋”上进行选择或调整，而后一种情况下，顾客是在“加不加鸡蛋”上进行选择或调整，顾客有限的理性使很多的顾客没有充分地调整，使两个小店的生意大相径庭。</p>
<p>心理学家曾经运用一个随机转盘对人们进行测试。当转盘的指针停留在65%这个刻度时，要求被试回答：非洲国家的数量在联合国国家总数中，所占的百分比是大于65%还是小于65%?对于这个常识性的问题，大部分被试都会回答小于65%。实验者接着又问：“具体的比例是多少?”多数被试回答45%左右。</p>
<p>接下来，研究者又对一些从来没有参加这类测试活动的人进行了测试。当转盘的指针指向10%的时候，要求被试回答：非洲国家的数量在联合国国家总数中，所占的百分比是大于10%还是小于10%?</p>
<p>这也是一个常识性的问题，大部分被试会回答大于10%。实验者接着又问：“具体的比例是多少?”多数被试回答在25%左右。所有的被试都知道，转盘的数字是随机出现的，但是，他们的回答却明显地受到转盘先前给出的数字的影响——即使这些数字是无关的。也就是说，被试的答案被“锚定”在先前给出的无关数字上了。</p>
<p>外出旅游的时候，你看中了一件标价为3000元的紫砂壶，但你对紫砂壶的情况又不是很了解，结果你动用了所有的智慧与店主讨价还价，最终以 1500元成交，你感到很满意。因为，你花了1500元的价钱买了3000元的东西，而不是花了4000元的价钱买了3000元的东西。店主也很高兴，因为他把价值500元的东西随意标成了3000元，而店主的这个前置标价，对于你来说就是一种“锚定”，他让你始终围绕着这个似乎是随机的数字3000元来思维，这也就是为什么商家在一开始就标价很高的原因，因为，这种方法能够实现“双赢”：商家赚了钱，顾客“捡了便宜”。</p>
<p>这些现象也会出现在商业谈判之中，在信息不对称的情况下，你可能会“锚定”谈判对手，也可能会被对方所“锚定”。有一种十分简单的方法就可以打破这种思维锚定，那就是货比三家，充分地了解标的物的相关信息。</p>
<p>Kahneman和Tversky认为，无论是初出茅庐的新手，还是经验丰富的决策者，在面对复杂和模糊的问题时，经常会发生启发式认知偏差，只是偏差的几率、幅度大小不同而已。</p>
]]></content>
      
        <categories>
            
            <category> 日知录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 启发式认知偏差 </tag>
            
            <tag> 代表性启发法 </tag>
            
            <tag> 可得性启发法 </tag>
            
            <tag> 锚定与调整启发法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[读书记（1）：读书清单]]></title>
      <url>/2018/01/01/%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%BB%E4%B9%A6%E6%B8%85%E5%8D%95/</url>
      <content type="html"><![CDATA[<p>这是我本科实习期间利用闲暇时间整理出来的一系列清单，包含涉及人文社科各个学科的经典书籍。</p>
<p>我有很多渴望，而这其中尤为让我感受到歌德所说的“静谧的激情”的是这样一种渴望。我渴望深入人类文明最澄明精要的思想脉络，渴望与人类历史上最迷人的灵魂相伴，渴望在漫长的岁月中被这些浪漫的精神“缠绕”，而我相信，它们留下的书，正是我们通向这种渴望的路途中最朴实的一条。这份清单正是始于这种渴望，我期许通过它们去谋合我对文字的控制欲以及统领知识的野心，形成人文社科的阅读图景与思想路径，依循这条规定好的路径，边走边想，争取到那半点光辉。</p>
<a id="more"></a>
<p>翁贝托·艾科在他的《无限的清单》里，带着极度的痴狂带领着我们发现隐藏在伟大的作品中的迷人的清单，将那些本来漫无秩序的事物赋予秩序。在第十七章《混乱的枚举》中提及到博尔赫斯式的作品清单时说：“他们之所以开清单，并不是因为他们技穷，不晓得要如何说他们想说的事情，他们以开清单的方式来说他们想说的话，是出于他们对过度的喜爱，是出于骄傲，以及对文字的贪婪，还有，对多元、无限的知识——快乐的知识——贪求。”正是这些伟大的人和他们伟大的作品，在引诱着我们去探求更为鲜活的真理、更为纯粹的思想，也正因为每个人身上都携带着对知识渴求的基因，才会让这些随着伟大的头脑的消逝便无法生效的纸质文字在每个时代熠熠生辉。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/hahha.png" alt="hahha"></p>
<p>但之后的事情并没有按照规定好的方式前行，我时常感觉到我被这些“狂妄的清单”奴役了，甚至很长时间无法摆脱他们整齐划一的身影，它们赤裸裸地暴露了我对多元、无限和过度的探求欲望，他们就像是一位看似博学实则浅薄的教授在我指手画脚，指挥我、支使我、调度我的行动，让我失去了阅读的趣味，失去那种在无边无际的森林中肆意游荡然后发现原本看不见的风景的趣味，我就像是一只困兽，被这些并非观念、并非思想的索然无味的家伙给囚禁了。</p>
<p>我也曾有过很多类似的美学体验，当我一人徜徉于图书馆之中时，看到成千上万本归好类别的书层层叠叠的放置在书架上，尽管这是一个有限的空间，只能容纳下有限的纸张，但面对这些扑面而来的携带着无限知识模块的书籍，我感受到的是一种“不安的快感”，甚至是一种“绝望的快感”，一种即将窒息的无力感，就如同溺水之际一眼看不到海岸之时的垂死挣扎。于是索性就暂时“抛弃”了它们，用力挣脱它们的身影，离开它们，把它们闲置、一劳永逸地拖入垃圾箱，让他们荒芜，让他们丢失我最初给予它们的热忱与痴迷。</p>
<p>之后在没有清单梦魇困扰的的日子里，我逐渐醒悟，原来清单它只是一种有限的表达形式，它仅仅代表了一种无序生活中寻找各种秩序生活的可能性。而思想与精神无法像清单一样清晰地罗列排开，它们是无需安排的，它们有其内在的环路，我开始告诫自己，沿着思想走，而不是沿着书本走。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/嘻嘻.PNG" alt="嘻嘻"></p>
<p>但如果这些清单最终的用途仅仅是满足我的私欲和揭露我狂妄的野心，那它的命运也过于浅薄灰暗了，我不该让它们被我自私的占有，它们的命运也不绝于此，它们本该有更多的触角去和更多的读者相遇，见到更多的光与热爱，与更多美妙的灵魂互动。于是便有了这一系列的整理。</p>
<p>下面是它们的目录及链接，简单介绍一下，我把他们归类成了以下几个篇章：哲学篇、文学篇、诗歌篇、社会学篇、政治学篇、心理学篇、人类学篇、经济学篇、法学篇，每一个类目包含了该领域下经典的作品，主要按照作者来进行细分，当然也包含了一些比较不错的系列丛书，每一个作者或者系列都链接到了百度网盘，里面就是那些伟大的作品了，基本上是PDF格式，当初选择PDF格式也是因为阅读的时候可以看到纸质书的原貌，在选择的时候也尽量挑选了相对比较清晰的版本，尽量提升阅读体验。</p>
<p>最后，我想引用歌德的一句话：“阅读是读者与作者间的一次合谋。书已经翻开，你已经边缘性地进入这场阴谋，除了主动乃至假装愉快地参与，似乎别无选择······”</p>
<p>如果您有任何问题，请联系我的邮箱<a href="&#x6d;&#x61;&#x69;&#x6c;&#x74;&#111;&#58;&#49;&#56;&#56;&#49;&#x30;&#54;&#x39;&#x38;&#x39;&#50;&#x33;&#x40;&#x31;&#54;&#51;&#46;&#99;&#111;&#x6d;">&#49;&#56;&#56;&#49;&#x30;&#54;&#x39;&#x38;&#x39;&#50;&#x33;&#x40;&#x31;&#54;&#51;&#46;&#99;&#111;&#x6d;</a>。</p>
<hr>
<h3 id="清单的艺术：哲学篇"><a href="#清单的艺术：哲学篇" class="headerlink" title="清单的艺术：哲学篇"></a><strong>清单的艺术：哲学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1cygFt8" target="_blank" rel="noopener">阿多诺</a></th>
<th><a href="https://pan.baidu.com/s/1ge3LSAJ" target="_blank" rel="noopener">阿尔都塞</a></th>
<th><a href="https://pan.baidu.com/s/1i5xm0qL" target="_blank" rel="noopener">阿甘本</a></th>
<th><a href="https://pan.baidu.com/s/1pLl4S5P" target="_blank" rel="noopener">阿奎那</a></th>
<th><a href="https://pan.baidu.com/s/1i47L8yD" target="_blank" rel="noopener">艾耶尔</a></th>
<th><a href="https://pan.baidu.com/s/1eSKtdkA" target="_blank" rel="noopener">奥古斯丁</a></th>
<th><a href="https://pan.baidu.com/s/1slG8Qwl" target="_blank" rel="noopener">巴迪欧</a></th>
<th><a href="https://pan.baidu.com/s/1slC5kwl" target="_blank" rel="noopener">巴赫金</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1i4UMgaL" target="_blank" rel="noopener">柏格森</a></td>
<td><a href="https://pan.baidu.com/s/1dF8jO9j" target="_blank" rel="noopener">柏拉图</a></td>
<td><a href="https://pan.baidu.com/s/1c232lsk" target="_blank" rel="noopener">查尔斯.泰勒</a></td>
<td><a href="https://pan.baidu.com/s/1boMQPiB" target="_blank" rel="noopener">陈嘉映</a></td>
<td><a href="https://pan.baidu.com/s/1qYvCmWs" target="_blank" rel="noopener">德勒兹</a></td>
<td><a href="https://pan.baidu.com/s/1kVINcsZ" target="_blank" rel="noopener">德里达</a></td>
<td><a href="https://pan.baidu.com/s/1jI4BR7o" target="_blank" rel="noopener">詹姆士</a></td>
<td><a href="https://pan.baidu.com/s/1i5ncXtb" target="_blank" rel="noopener">张汝伦</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1dEGblQT" target="_blank" rel="noopener">邓安庆</a></td>
<td><a href="https://pan.baidu.com/s/1kU9dyDL" target="_blank" rel="noopener">邓晓芒</a></td>
<td><a href="https://pan.baidu.com/s/1qYQey8s" target="_blank" rel="noopener">狄尔泰</a></td>
<td><a href="https://pan.baidu.com/s/1c15nX08" target="_blank" rel="noopener">笛卡尔</a></td>
<td><a href="https://pan.baidu.com/s/1gfGnw7T" target="_blank" rel="noopener">蒂利希</a></td>
<td><a href="https://pan.baidu.com/s/1i5cTGih" target="_blank" rel="noopener">杜威</a></td>
<td><a href="https://pan.baidu.com/s/1nuCxh5v" target="_blank" rel="noopener">福柯</a></td>
<td><a href="https://pan.baidu.com/s/1hs1smck" target="_blank" rel="noopener">伽达默尔</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c2kvKU" target="_blank" rel="noopener">哈贝马斯</a></td>
<td><a href="https://pan.baidu.com/s/1bGNly2" target="_blank" rel="noopener">海德格尔</a></td>
<td><a href="https://pan.baidu.com/s/1hsH7jZA" target="_blank" rel="noopener">何怀宏</a></td>
<td><a href="https://pan.baidu.com/s/1b2AeIE" target="_blank" rel="noopener">何兆武</a></td>
<td><a href="https://pan.baidu.com/s/1qXPilf2" target="_blank" rel="noopener">黑格尔</a></td>
<td><a href="https://pan.baidu.com/s/1c1ILcQG" target="_blank" rel="noopener">胡塞尔</a></td>
<td><a href="https://pan.baidu.com/s/1i44xEXv" target="_blank" rel="noopener">怀特海</a></td>
<td><a href="https://pan.baidu.com/s/1o83NNzs" target="_blank" rel="noopener">张世英</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1i5q0PJf" target="_blank" rel="noopener">霍克海默</a></td>
<td><a href="https://pan.baidu.com/s/1o8QO3xC" target="_blank" rel="noopener">江怡</a></td>
<td><a href="https://pan.baidu.com/s/1i5tOCzN" target="_blank" rel="noopener">金观涛</a></td>
<td><a href="https://pan.baidu.com/s/1miL2QX6" target="_blank" rel="noopener">卡西尔</a></td>
<td><a href="https://pan.baidu.com/s/1miy2l8s" target="_blank" rel="noopener">康德</a></td>
<td><a href="https://pan.baidu.com/s/1boOSHtH" target="_blank" rel="noopener">柯林伍德</a></td>
<td><a href="https://pan.baidu.com/s/1nvvIrlZ" target="_blank" rel="noopener">克尔凯郭尔</a></td>
<td><a href="https://pan.baidu.com/s/1mhRsqKC" target="_blank" rel="noopener">克里希那穆提</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1nuAvpFz" target="_blank" rel="noopener">莱布尼茨</a></td>
<td><a href="https://pan.baidu.com/s/1boIhhKj" target="_blank" rel="noopener">赖尔</a></td>
<td><a href="https://pan.baidu.com/s/1minCWm0" target="_blank" rel="noopener">朗西埃</a></td>
<td><a href="https://pan.baidu.com/s/1skNKpWT" target="_blank" rel="noopener">勒维纳斯</a></td>
<td><a href="https://pan.baidu.com/s/1geUQn7P" target="_blank" rel="noopener">利科</a></td>
<td><a href="https://pan.baidu.com/s/1o7HKKiQ" target="_blank" rel="noopener">刘小枫</a></td>
<td><a href="https://pan.baidu.com/s/1boOSHtX" target="_blank" rel="noopener">卢克莱修</a></td>
<td><a href="https://pan.baidu.com/s/1hrBlyzq" target="_blank" rel="noopener">罗蒂</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1o8MeKm2" target="_blank" rel="noopener">罗兰.巴特</a></td>
<td><a href="https://pan.baidu.com/s/1ge63IZx" target="_blank" rel="noopener">罗素</a></td>
<td><a href="https://pan.baidu.com/s/1hsw85Ru" target="_blank" rel="noopener">洛克</a></td>
<td><a href="https://pan.baidu.com/s/1kVb3AHT" target="_blank" rel="noopener">马克思</a></td>
<td><a href="https://pan.baidu.com/s/1kVvM1qn" target="_blank" rel="noopener">梅洛.庞蒂</a></td>
<td><a href="https://pan.baidu.com/s/1i4E7iAh" target="_blank" rel="noopener">莫罗阿</a></td>
<td><a href="https://pan.baidu.com/s/1slHoVFJ" target="_blank" rel="noopener">尼采</a></td>
<td><a href="https://pan.baidu.com/s/1o8xcoaQ" target="_blank" rel="noopener">张一兵</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c2tsOHq" target="_blank" rel="noopener">倪梁康</a></td>
<td><a href="https://pan.baidu.com/s/1c1Zab08" target="_blank" rel="noopener">帕斯卡尔</a></td>
<td><a href="https://pan.baidu.com/s/1nvOiDXf" target="_blank" rel="noopener">培根</a></td>
<td><a href="https://pan.baidu.com/s/1nvilosL" target="_blank" rel="noopener">皮亚杰</a></td>
<td><a href="https://pan.baidu.com/s/1miEbudU" target="_blank" rel="noopener">齐泽克</a></td>
<td><a href="https://pan.baidu.com/s/1hs6HTcS" target="_blank" rel="noopener">萨特</a></td>
<td><a href="https://pan.baidu.com/s/1nuBHjeh" target="_blank" rel="noopener">塞涅卡</a></td>
<td><a href="https://pan.baidu.com/s/1cnruqE" target="_blank" rel="noopener">舍斯托夫</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1jIktmDo" target="_blank" rel="noopener">石里克</a></td>
<td><a href="https://pan.baidu.com/s/1jI4BSs2" target="_blank" rel="noopener">叔本华</a></td>
<td><a href="https://pan.baidu.com/s/1gfm1hdh" target="_blank" rel="noopener">斯宾诺莎</a></td>
<td><a href="https://pan.baidu.com/s/1nvAB1lJ" target="_blank" rel="noopener">唐君毅</a></td>
<td><a href="https://pan.baidu.com/s/1ctwU5K" target="_blank" rel="noopener">王元化</a></td>
<td><a href="https://pan.baidu.com/s/1bpuGOWZ" target="_blank" rel="noopener">维科</a></td>
<td><a href="https://pan.baidu.com/s/1c1Gdphm" target="_blank" rel="noopener">维特根斯坦</a></td>
<td><a href="https://pan.baidu.com/s/1hrQK2OW" target="_blank" rel="noopener">俞吾金</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1cM3iCu" target="_blank" rel="noopener">西蒙娜·薇依</a></td>
<td><a href="https://pan.baidu.com/s/1i5y8vD7" target="_blank" rel="noopener">西塞罗</a></td>
<td><a href="https://pan.baidu.com/s/1jI7tIIE" target="_blank" rel="noopener">休谟</a></td>
<td><a href="https://pan.baidu.com/s/1jIeT0Q6" target="_blank" rel="noopener">雅斯贝尔斯</a></td>
<td><a href="https://pan.baidu.com/s/1skY9Dox" target="_blank" rel="noopener">亚当斯密</a></td>
<td><a href="https://pan.baidu.com/s/1miQSzY4" target="_blank" rel="noopener">亚里士多德</a></td>
<td><a href="https://pan.baidu.com/s/1gflfpcv" target="_blank" rel="noopener">杨祖陶</a></td>
<td><a href="https://pan.baidu.com/s/1bEf1uu" target="_blank" rel="noopener">于连</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1hsAfABi" target="_blank" rel="noopener">美学</a></td>
<td><a href="https://pan.baidu.com/s/1hsH7kgO" target="_blank" rel="noopener">科学哲学</a></td>
<td><a href="https://pan.baidu.com/s/1jIeT0Se" target="_blank" rel="noopener">政治哲学</a></td>
<td><a href="https://pan.baidu.com/s/1c2pUZr6" target="_blank" rel="noopener">道德哲学</a></td>
<td><a href="https://pan.baidu.com/s/1geOK9Fl" target="_blank" rel="noopener">逻辑学</a></td>
<td><a href="https://pan.baidu.com/s/1pLcbzo3" target="_blank" rel="noopener">现象学</a></td>
<td><a href="https://pan.baidu.com/s/1qYmiOzU" target="_blank" rel="noopener">心灵哲学</a></td>
<td><a href="https://pan.baidu.com/s/1pLFraUB" target="_blank" rel="noopener">分析哲学</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1bp0MxrD" target="_blank" rel="noopener">二十世纪西方哲学译丛</a></td>
<td><a href="https://pan.baidu.com/s/1mh9HuEs" target="_blank" rel="noopener">国外经典哲学教材译丛</a></td>
<td><a href="https://pan.baidu.com/s/1i5tiAPZ" target="_blank" rel="noopener">世纪人文系列丛书</a></td>
<td><a href="https://pan.baidu.com/s/1c2jPwAk" target="_blank" rel="noopener">西方名著入门</a></td>
<td><a href="https://pan.baidu.com/s/1b9vGBG" target="_blank" rel="noopener">西方社会科学读本</a></td>
<td><a href="https://pan.baidu.com/s/1nv67DG1" target="_blank" rel="noopener">人文译丛</a></td>
<td><a href="https://pan.baidu.com/s/1i4QIwE5" target="_blank" rel="noopener">人文与社会译丛</a></td>
<td><a href="https://pan.baidu.com/s/1eSaXUSi" target="_blank" rel="noopener">后现代交锋丛书</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c2lRqe8" target="_blank" rel="noopener">当代西方哲学家评传</a></td>
<td><a href="https://pan.baidu.com/s/1c2EX3FE" target="_blank" rel="noopener">德国哲学 辑刊</a></td>
<td><a href="https://pan.baidu.com/s/1hsgZ13Y" target="_blank" rel="noopener">汉译哲学</a></td>
<td><a href="https://pan.baidu.com/s/1jI8FFWa" target="_blank" rel="noopener">哲学工具书系列</a></td>
<td><a href="https://pan.baidu.com/s/1pKWBKM3" target="_blank" rel="noopener">西方哲学史</a></td>
<td><a href="https://pan.baidu.com/s/1dFHDuF3" target="_blank" rel="noopener">通识系列</a></td>
<td><a href="https://pan.baidu.com/s/1o87R582" target="_blank" rel="noopener">现代性研究译丛</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1i4QczU9" target="_blank" rel="noopener">孔子</a></td>
<td><a href="https://pan.baidu.com/s/1pLoSDGv" target="_blank" rel="noopener">老子</a></td>
<td><a href="https://pan.baidu.com/s/1gfzsvKn" target="_blank" rel="noopener">梁漱溟</a></td>
<td><a href="https://pan.baidu.com/s/1mhUFZ3i" target="_blank" rel="noopener">孟子</a></td>
<td><a href="https://pan.baidu.com/s/1nvqsDNr" target="_blank" rel="noopener">牟宗三</a></td>
<td><a href="https://pan.baidu.com/s/1eS6LTuy" target="_blank" rel="noopener">王阳明</a></td>
<td><a href="https://pan.baidu.com/s/1c2pplN6" target="_blank" rel="noopener">熊十力</a></td>
<td><a href="https://pan.baidu.com/s/1eShJ6tc" target="_blank" rel="noopener">余英时</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1nvh5rT3" target="_blank" rel="noopener">朱熹</a></td>
<td><a href="https://pan.baidu.com/s/1gfm1hh5" target="_blank" rel="noopener">庄子</a></td>
<td><a href="https://pan.baidu.com/s/1jItgI4u" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：文学篇"><a href="#清单的艺术：文学篇" class="headerlink" title="清单的艺术：文学篇"></a><strong>清单的艺术：文学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1c2pplRi" target="_blank" rel="noopener">D.H.劳伦斯</a></th>
<th><a href="https://pan.baidu.com/s/1ge7PC8V" target="_blank" rel="noopener">E.M.福斯特</a></th>
<th><a href="https://pan.baidu.com/s/1kUBfr3h" target="_blank" rel="noopener">J.M.库切</a></th>
<th><a href="https://pan.baidu.com/s/1sl4xDPr" target="_blank" rel="noopener">T·S·艾略特</a></th>
<th><a href="https://pan.baidu.com/s/1mi25Xwc" target="_blank" rel="noopener">V.S.奈保尔</a></th>
<th><a href="https://pan.baidu.com/s/1jIjDpxs" target="_blank" rel="noopener">阿·托尔斯泰</a></th>
<th><a href="https://pan.baidu.com/s/1pLEBc4F" target="_blank" rel="noopener">阿波利奈尔</a></th>
<th><a href="https://pan.baidu.com/s/1i5tOCAT" target="_blank" rel="noopener">阿多尼斯</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1o7AZkYA" target="_blank" rel="noopener">阿赫玛托娃</a></td>
<td><a href="https://pan.baidu.com/s/1o7UBZCm" target="_blank" rel="noopener">阿克萨科夫</a></td>
<td><a href="https://pan.baidu.com/s/1pLDVn9x" target="_blank" rel="noopener">阿来</a></td>
<td><a href="https://pan.baidu.com/s/1gf4IgKF" target="_blank" rel="noopener">德波顿</a></td>
<td><a href="https://pan.baidu.com/s/1pLNyC23" target="_blank" rel="noopener">阿摩司·奥兹</a></td>
<td><a href="https://pan.baidu.com/s/1jI9v9OU" target="_blank" rel="noopener">阿瑟·库斯勒</a></td>
<td><a href="https://pan.baidu.com/s/1c2pplRE" target="_blank" rel="noopener">阿斯塔菲耶夫</a></td>
<td><a href="https://pan.baidu.com/s/1pLywhS3" target="_blank" rel="noopener">耶利内克</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1dF74a21" target="_blank" rel="noopener">埃利蒂斯</a></td>
<td><a href="https://pan.baidu.com/s/1kVrcE4B" target="_blank" rel="noopener">艾丽丝·默多克</a></td>
<td><a href="https://pan.baidu.com/s/1pLkOWyB" target="_blank" rel="noopener">艾丽斯·沃克</a></td>
<td><a href="https://pan.baidu.com/s/1i5AJ5PV" target="_blank" rel="noopener">艾特玛托夫</a></td>
<td><a href="https://pan.baidu.com/s/1dF95S4p" target="_blank" rel="noopener">爱伦堡</a></td>
<td><a href="https://pan.baidu.com/s/1mi9r9Ja" target="_blank" rel="noopener">爱伦坡</a></td>
<td><a href="https://pan.baidu.com/s/1kVrcE4R" target="_blank" rel="noopener">爱默生</a></td>
<td><a href="https://pan.baidu.com/s/1kUZ1ISJ" target="_blank" rel="noopener">安.兰德</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1geFXccV" target="_blank" rel="noopener">安德烈.纪德</a></td>
<td><a href="https://pan.baidu.com/s/1kUDNfbL" target="_blank" rel="noopener">安德烈耶夫</a></td>
<td><a href="https://pan.baidu.com/s/1i5cTGvb" target="_blank" rel="noopener">安东尼·伯吉斯</a></td>
<td><a href="https://pan.baidu.com/s/1hsEj3Hq" target="_blank" rel="noopener">安妮·普鲁</a></td>
<td><a href="https://pan.baidu.com/s/1csGZwy" target="_blank" rel="noopener">安徒生</a></td>
<td><a href="https://pan.baidu.com/s/1kUOC6kn" target="_blank" rel="noopener">奥尔罕.帕慕克</a></td>
<td><a href="https://pan.baidu.com/s/1pLFrbab" target="_blank" rel="noopener">巴别尔</a></td>
<td><a href="https://pan.baidu.com/s/1jIEftD8" target="_blank" rel="noopener">巴尔扎克</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1hs8dJCC" target="_blank" rel="noopener">巴塞尔姆</a></td>
<td><a href="https://pan.baidu.com/s/1i53sfid" target="_blank" rel="noopener">邦达列夫</a></td>
<td><a href="https://pan.baidu.com/s/1kU6vAeV" target="_blank" rel="noopener">保罗.策兰</a></td>
<td><a href="https://pan.baidu.com/s/1kVQo5an" target="_blank" rel="noopener">保罗·科埃略</a></td>
<td><a href="https://pan.baidu.com/s/1hrKEJdy" target="_blank" rel="noopener">朱利安.巴恩斯</a></td>
<td><a href="https://pan.baidu.com/s/1c1WsoH6" target="_blank" rel="noopener">朱天文</a></td>
<td><a href="https://pan.baidu.com/s/1pLOe86v" target="_blank" rel="noopener">张炜</a></td>
<td><a href="https://pan.baidu.com/s/1pLFrbIV" target="_blank" rel="noopener">张贤亮</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1sl0tXU1" target="_blank" rel="noopener">贝克特</a></td>
<td><a href="https://pan.baidu.com/s/1jImvgnc" target="_blank" rel="noopener">本·奥克瑞</a></td>
<td><a href="https://pan.baidu.com/s/1gfH9s9H" target="_blank" rel="noopener">本哈德·施林克</a></td>
<td><a href="https://pan.baidu.com/s/1qYzadcg" target="_blank" rel="noopener">本雅明</a></td>
<td><a href="https://pan.baidu.com/s/1gfcrH1x" target="_blank" rel="noopener">彼得·阿克罗伊德</a></td>
<td><a href="https://pan.baidu.com/s/1b7tRUe" target="_blank" rel="noopener">别尔嘉耶夫</a></td>
<td><a href="https://pan.baidu.com/s/1slBPkJV" target="_blank" rel="noopener">波德莱尔</a></td>
<td><a href="https://pan.baidu.com/s/1o85jIUu" target="_blank" rel="noopener">波德里亚</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1kUSGouz" target="_blank" rel="noopener">勃兰兑斯</a></td>
<td><a href="https://pan.baidu.com/s/1cCtKKI" target="_blank" rel="noopener">勃留索夫</a></td>
<td><a href="https://pan.baidu.com/s/1nvrE8pr" target="_blank" rel="noopener">勃洛克</a></td>
<td><a href="https://pan.baidu.com/s/1c2waF08" target="_blank" rel="noopener">博尔赫斯</a></td>
<td><a href="https://pan.baidu.com/s/1eSplaJg" target="_blank" rel="noopener">布尔加科夫</a></td>
<td><a href="https://pan.baidu.com/s/1bpCXFSb" target="_blank" rel="noopener">布莱希特</a></td>
<td><a href="https://pan.baidu.com/s/1gfANIJx" target="_blank" rel="noopener">布朗肖</a></td>
<td><a href="https://pan.baidu.com/s/1jHWLZfg" target="_blank" rel="noopener">蔡骏</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1pL63ESr" target="_blank" rel="noopener">陈忠实</a></td>
<td><a href="https://pan.baidu.com/s/1mhBJmDq" target="_blank" rel="noopener">迟子建</a></td>
<td><a href="https://pan.baidu.com/s/1i5H5mYx" target="_blank" rel="noopener">川端康成</a></td>
<td><a href="https://pan.baidu.com/s/1o83NNCY" target="_blank" rel="noopener">茨维塔耶娃</a></td>
<td><a href="https://pan.baidu.com/s/1cfjPWa" target="_blank" rel="noopener">村上春树</a></td>
<td><a href="https://pan.baidu.com/s/1qXKy6rq" target="_blank" rel="noopener">达夫妮.杜穆里埃</a></td>
<td><a href="https://pan.baidu.com/s/1mi69F2w" target="_blank" rel="noopener">达里奥·福</a></td>
<td><a href="https://pan.baidu.com/s/1hrOIA4s" target="_blank" rel="noopener">大江健三郎</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1pKS7Wkb" target="_blank" rel="noopener">但丁</a></td>
<td><a href="https://pan.baidu.com/s/1slOa5K5" target="_blank" rel="noopener">狄更斯</a></td>
<td><a href="https://pan.baidu.com/s/1hspgTne" target="_blank" rel="noopener">东野圭吾</a></td>
<td><a href="https://pan.baidu.com/s/1jHYNNOq" target="_blank" rel="noopener">董桥</a></td>
<td><a href="https://pan.baidu.com/s/1i5H5mZV" target="_blank" rel="noopener">杜鲁门·卡波特</a></td>
<td><a href="https://pan.baidu.com/s/1eRIbegU" target="_blank" rel="noopener">多丽丝莱辛</a></td>
<td><a href="https://pan.baidu.com/s/1eS0awbo" target="_blank" rel="noopener">菲利普·罗斯</a></td>
<td><a href="https://pan.baidu.com/s/1kVteC31" target="_blank" rel="noopener">费定</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1bp12xK7" target="_blank" rel="noopener">马卡宁</a></td>
<td><a href="https://pan.baidu.com/s/1slV1Z57" target="_blank" rel="noopener">迪伦马特</a></td>
<td><a href="https://pan.baidu.com/s/1qY3JJ4o" target="_blank" rel="noopener">孚希特万格</a></td>
<td><a href="https://pan.baidu.com/s/1hs8dJEG" target="_blank" rel="noopener">福克纳</a></td>
<td><a href="https://pan.baidu.com/s/1miJ0Ywg" target="_blank" rel="noopener">冈察尔</a></td>
<td><a href="https://pan.baidu.com/s/1sl5dDgx" target="_blank" rel="noopener">冈察洛夫</a></td>
<td><a href="https://pan.baidu.com/s/1jIjDp2I" target="_blank" rel="noopener">高尔基</a></td>
<td><a href="https://pan.baidu.com/s/1bpsETMB" target="_blank" rel="noopener">高尔斯华绥</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c26Qfza" target="_blank" rel="noopener">高行健</a></td>
<td><a href="https://pan.baidu.com/s/1boVEsY7" target="_blank" rel="noopener">戈迪默</a></td>
<td><a href="https://pan.baidu.com/s/1slC5luP" target="_blank" rel="noopener">歌德</a></td>
<td><a href="https://pan.baidu.com/s/1miafEdE" target="_blank" rel="noopener">格非</a></td>
<td><a href="https://pan.baidu.com/s/1gf0Erjd" target="_blank" rel="noopener">格雷厄姆·格林</a></td>
<td><a href="https://pan.baidu.com/s/1o8KIOFg" target="_blank" rel="noopener">格列科娃</a></td>
<td><a href="https://pan.baidu.com/s/1hrDnnUk" target="_blank" rel="noopener">果戈理</a></td>
<td><a href="https://pan.baidu.com/s/1qYtAueC" target="_blank" rel="noopener">哈珀·李</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1i53XXWD" target="_blank" rel="noopener">海明威</a></td>
<td><a href="https://pan.baidu.com/s/1cucU7s" target="_blank" rel="noopener">海涅</a></td>
<td><a href="https://pan.baidu.com/s/1o8FZiKM" target="_blank" rel="noopener">海因里希.伯尔</a></td>
<td><a href="https://pan.baidu.com/s/1nvxKsad" target="_blank" rel="noopener">韩松</a></td>
<td><a href="https://pan.baidu.com/s/1i4RyyDB" target="_blank" rel="noopener">荷尔德林</a></td>
<td><a href="https://pan.baidu.com/s/1csaZgE" target="_blank" rel="noopener">赫尔岑</a></td>
<td><a href="https://pan.baidu.com/s/1i5AdH3J" target="_blank" rel="noopener">赫塔·米勒</a></td>
<td><a href="https://pan.baidu.com/s/1jIOoNDO" target="_blank" rel="noopener">黑塞</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1i5b7J6X" target="_blank" rel="noopener">亨利.米勒</a></td>
<td><a href="https://pan.baidu.com/s/1i5Eh4OH" target="_blank" rel="noopener">亨利希·曼</a></td>
<td><a href="https://pan.baidu.com/s/1geQg80b" target="_blank" rel="noopener">科塔萨尔</a></td>
<td><a href="https://pan.baidu.com/s/1o8vayxk" target="_blank" rel="noopener">华兹华斯</a></td>
<td><a href="https://pan.baidu.com/s/1bCJ1dc" target="_blank" rel="noopener">惠特曼</a></td>
<td><a href="https://pan.baidu.com/s/1dE9V8Y1" target="_blank" rel="noopener">霍达</a></td>
<td><a href="https://pan.baidu.com/s/1dFCnIRB" target="_blank" rel="noopener">霍夫曼</a></td>
<td><a href="https://pan.baidu.com/s/1c2IvwFQ" target="_blank" rel="noopener">霍普特曼</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1sl9Nr7f" target="_blank" rel="noopener">济慈</a></td>
<td><a href="https://pan.baidu.com/s/1nvpCFqx" target="_blank" rel="noopener">加缪</a></td>
<td><a href="https://pan.baidu.com/s/1skHECvr" target="_blank" rel="noopener">加西亚.马尔克斯</a></td>
<td><a href="https://pan.baidu.com/s/1nuM66yl" target="_blank" rel="noopener">贾平凹</a></td>
<td><a href="https://pan.baidu.com/s/1pKVpVvl" target="_blank" rel="noopener">蒋勋</a></td>
<td><a href="https://pan.baidu.com/s/1hsLAUWG" target="_blank" rel="noopener">杰弗里·尤金尼德斯</a></td>
<td><a href="https://pan.baidu.com/s/1hshF1PU" target="_blank" rel="noopener">芥川龙之介</a></td>
<td><a href="https://pan.baidu.com/s/1nvT8r6T" target="_blank" rel="noopener">金庸</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1jH4vIJW" target="_blank" rel="noopener">聚斯金德</a></td>
<td><a href="https://pan.baidu.com/s/1kVCbw9L" target="_blank" rel="noopener">君特.格拉斯</a></td>
<td><a href="https://pan.baidu.com/s/1b7ZNEA" target="_blank" rel="noopener">卡夫卡</a></td>
<td><a href="https://pan.baidu.com/s/1qYHWQN6" target="_blank" rel="noopener">富恩特斯</a></td>
<td><a href="https://pan.baidu.com/s/1hsKpgG0" target="_blank" rel="noopener">卡内蒂</a></td>
<td><a href="https://pan.baidu.com/s/1miIfcZQ" target="_blank" rel="noopener">卡萨诺瓦</a></td>
<td><a href="https://pan.baidu.com/s/1c11QiTa" target="_blank" rel="noopener">卡赞扎基斯</a></td>
<td><a href="https://pan.baidu.com/s/1eSrSRJk" target="_blank" rel="noopener">科尔姆.托宾</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1cjT5BS" target="_blank" rel="noopener">科马克·麦卡锡</a></td>
<td><a href="https://pan.baidu.com/s/1hsvm7qs" target="_blank" rel="noopener">克莱斯特</a></td>
<td><a href="https://pan.baidu.com/s/1mhBJnfE" target="_blank" rel="noopener">克里斯塔·沃尔夫</a></td>
<td><a href="https://pan.baidu.com/s/1c1WsoCS" target="_blank" rel="noopener">冯内古特</a></td>
<td><a href="https://pan.baidu.com/s/1miiTlDe" target="_blank" rel="noopener">库普林</a></td>
<td><a href="https://pan.baidu.com/s/1pL3LNvl" target="_blank" rel="noopener">拉·艾里森</a></td>
<td><a href="https://pan.baidu.com/s/1c2nnoY0" target="_blank" rel="noopener">拉斯普京</a></td>
<td><a href="https://pan.baidu.com/s/1hrYmbOS" target="_blank" rel="noopener">莱蒙特</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1sll0H5N" target="_blank" rel="noopener">莱蒙托夫</a></td>
<td><a href="https://pan.baidu.com/s/1eSlhn4A" target="_blank" rel="noopener">莱辛</a></td>
<td><a href="https://pan.baidu.com/s/1pKHcKFh" target="_blank" rel="noopener">兰波</a></td>
<td><a href="https://pan.baidu.com/s/1qXJI8Ao" target="_blank" rel="noopener">劳伦斯·斯特恩</a></td>
<td><a href="https://pan.baidu.com/s/1qYvCo36" target="_blank" rel="noopener">勒克莱齐奥</a></td>
<td><a href="https://pan.baidu.com/s/1eRR45JO" target="_blank" rel="noopener">勒萨日</a></td>
<td><a href="https://pan.baidu.com/s/1o8FtkWM" target="_blank" rel="noopener">雷巴科夫</a></td>
<td><a href="https://pan.baidu.com/s/1c2MzcXE" target="_blank" rel="noopener">雷马克</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1qYiKYmS" target="_blank" rel="noopener">雷蒙德卡佛</a></td>
<td><a href="https://pan.baidu.com/s/1dEXCeNR" target="_blank" rel="noopener">李欧梵</a></td>
<td><a href="https://pan.baidu.com/s/1o8eLviq" target="_blank" rel="noopener">里尔克</a></td>
<td><a href="https://pan.baidu.com/s/1pLHsVHL" target="_blank" rel="noopener">理查·赖特</a></td>
<td><a href="https://pan.baidu.com/s/1eRDrv2q" target="_blank" rel="noopener">列夫.托尔斯泰</a></td>
<td><a href="https://pan.baidu.com/s/1hsF5w80" target="_blank" rel="noopener">列斯科夫</a></td>
<td><a href="https://pan.baidu.com/s/1qXM0y3i" target="_blank" rel="noopener">林贤治</a></td>
<td><a href="https://pan.baidu.com/s/1c2qBb6S" target="_blank" rel="noopener">刘慈欣</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1nvSSt7N" target="_blank" rel="noopener">鲁迅</a></td>
<td><a href="https://pan.baidu.com/s/1jIMmTxk" target="_blank" rel="noopener">路遥</a></td>
<td><a href="https://pan.baidu.com/s/1dEDZxfF" target="_blank" rel="noopener">伦茨</a></td>
<td><a href="https://pan.baidu.com/s/1sl9Nr9j" target="_blank" rel="noopener">罗伯.格里耶</a></td>
<td><a href="https://pan.baidu.com/s/1kVEJpqB" target="_blank" rel="noopener">罗伯特.波拉尼奥</a></td>
<td><a href="https://pan.baidu.com/s/1eRP2DuM" target="_blank" rel="noopener">罗曼罗兰</a></td>
<td><a href="https://pan.baidu.com/s/1bzmvRw" target="_blank" rel="noopener">洛扎诺夫</a></td>
<td><a href="https://pan.baidu.com/s/1bo6BOh5" target="_blank" rel="noopener">骆以军</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1bo7XNBp" target="_blank" rel="noopener">马丁·瓦尔泽</a></td>
<td><a href="https://pan.baidu.com/s/1mh9bsHM" target="_blank" rel="noopener">马尔罗</a></td>
<td><a href="https://pan.baidu.com/s/1geQg82v" target="_blank" rel="noopener">马家辉</a></td>
<td><a href="https://pan.baidu.com/s/1i4G9d8h" target="_blank" rel="noopener">马拉默德</a></td>
<td><a href="https://pan.baidu.com/s/1i4NuG3F" target="_blank" rel="noopener">略萨</a></td>
<td><a href="https://pan.baidu.com/s/1o8FtkX8" target="_blank" rel="noopener">马雅可夫斯基</a></td>
<td><a href="https://pan.baidu.com/s/1gfrkXOJ" target="_blank" rel="noopener">玛·金·罗琳斯</a></td>
<td><a href="https://pan.baidu.com/s/1c2LJdJU" target="_blank" rel="noopener">阿特伍德</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1kURuqN1" target="_blank" rel="noopener">杜拉斯</a></td>
<td><a href="https://pan.baidu.com/s/1c2zy7eC" target="_blank" rel="noopener">迈克尔·翁达杰</a></td>
<td><a href="https://pan.baidu.com/s/1qY83zMs" target="_blank" rel="noopener">迈克尔·坎宁安</a></td>
<td><a href="https://pan.baidu.com/s/1eSm3mfw" target="_blank" rel="noopener">麦尔维尔</a></td>
<td><a href="https://pan.baidu.com/s/1jImvgJc" target="_blank" rel="noopener">麦家</a></td>
<td><a href="https://pan.baidu.com/s/1mhQClsO" target="_blank" rel="noopener">曼德尔施塔姆</a></td>
<td><a href="https://pan.baidu.com/s/1miquIMk" target="_blank" rel="noopener">毛姆</a></td>
<td><a href="https://pan.baidu.com/s/1i58bL1j" target="_blank" rel="noopener">梅列日科夫斯基</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1eSD77Tw" target="_blank" rel="noopener">梅特林克</a></td>
<td><a href="https://pan.baidu.com/s/1jIAHCDS" target="_blank" rel="noopener">蒙田</a></td>
<td><a href="https://pan.baidu.com/s/1dFjNz1J" target="_blank" rel="noopener">弥尔顿</a></td>
<td><a href="https://pan.baidu.com/s/1hsChCgO" target="_blank" rel="noopener">米兰·昆德拉</a></td>
<td><a href="https://pan.baidu.com/s/1bpFF9Hd" target="_blank" rel="noopener">米沃什</a></td>
<td><a href="https://pan.baidu.com/s/1kVkXiAV" target="_blank" rel="noopener">莫迪亚诺</a></td>
<td><a href="https://pan.baidu.com/s/1kVf7wEV" target="_blank" rel="noopener">莫里亚克</a></td>
<td><a href="https://pan.baidu.com/s/1kVb3B4V" target="_blank" rel="noopener">穆齐尔</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1qYSMres" target="_blank" rel="noopener">那多</a></td>
<td><a href="https://pan.baidu.com/s/1kVKOWB1" target="_blank" rel="noopener">纳博科夫</a></td>
<td><a href="https://pan.baidu.com/s/1c2Nfb6S" target="_blank" rel="noopener">纳丁·戈迪默</a></td>
<td><a href="https://pan.baidu.com/s/1c2uEL0G" target="_blank" rel="noopener">娜塔莉·萨洛特</a></td>
<td><a href="https://pan.baidu.com/s/1miTArkk" target="_blank" rel="noopener">涅克拉索夫</a></td>
<td><a href="https://pan.baidu.com/s/1pLtCv4B" target="_blank" rel="noopener">聂鲁达</a></td>
<td><a href="https://pan.baidu.com/s/1dFdbSiP" target="_blank" rel="noopener">诺曼·梅勒</a></td>
<td><a href="https://pan.baidu.com/s/1c2LdnJa" target="_blank" rel="noopener">帕斯捷尔纳克</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1dFlPrax" target="_blank" rel="noopener">帕特里克·怀特</a></td>
<td><a href="https://pan.baidu.com/s/1dFgzE17" target="_blank" rel="noopener">帕乌斯托夫斯基</a></td>
<td><a href="https://pan.baidu.com/s/1mhBdoLU" target="_blank" rel="noopener">潘海天</a></td>
<td><a href="https://pan.baidu.com/s/1c1VCv0c" target="_blank" rel="noopener">培根</a></td>
<td><a href="https://pan.baidu.com/s/1bppreRt" target="_blank" rel="noopener">菲兹杰拉德</a></td>
<td><a href="https://pan.baidu.com/s/1dE3kbst" target="_blank" rel="noopener">皮兰德娄</a></td>
<td><a href="https://pan.baidu.com/s/1eSs8Q6e" target="_blank" rel="noopener">蒲宁</a></td>
<td><a href="https://pan.baidu.com/s/1jH9f4Vk" target="_blank" rel="noopener">普里什文</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1boUswEV" target="_blank" rel="noopener">普鲁斯特</a></td>
<td><a href="https://pan.baidu.com/s/1nvilpdJ" target="_blank" rel="noopener">普吕多姆</a></td>
<td><a href="https://pan.baidu.com/s/1dEMgJ7b" target="_blank" rel="noopener">普希金</a></td>
<td><a href="https://pan.baidu.com/s/1kVgTyQz" target="_blank" rel="noopener">契诃夫</a></td>
<td><a href="https://pan.baidu.com/s/1pK8VrlD" target="_blank" rel="noopener">恰佩克</a></td>
<td><a href="https://pan.baidu.com/s/1c1Bt7Ja" target="_blank" rel="noopener">乔纳森·弗兰岑</a></td>
<td><a href="https://pan.baidu.com/s/1hsLAUZ6" target="_blank" rel="noopener">乔伊斯·奥兹</a></td>
<td><a href="https://pan.baidu.com/s/1qYMaFxi" target="_blank" rel="noopener">丘特切夫</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1miiTlFE" target="_blank" rel="noopener">邱妙津</a></td>
<td><a href="https://pan.baidu.com/s/1i5itxfJ" target="_blank" rel="noopener">裘帕・拉希莉</a></td>
<td><a href="https://pan.baidu.com/s/1i5jFszj" target="_blank" rel="noopener">让.雅克.卢梭</a></td>
<td><a href="https://pan.baidu.com/s/1hs2EkAw" target="_blank" rel="noopener">萨冈</a></td>
<td><a href="https://pan.baidu.com/s/1eS8hWn4" target="_blank" rel="noopener">萨特</a></td>
<td><a href="https://pan.baidu.com/s/1eR3hZkI" target="_blank" rel="noopener">塞弗尔特</a></td>
<td><a href="https://pan.baidu.com/s/1dFLHiQ9" target="_blank" rel="noopener">塞林格</a></td>
<td><a href="https://pan.baidu.com/s/1boFzuNX" target="_blank" rel="noopener">塞普尔维达</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1o7UCarK" target="_blank" rel="noopener">塞万提斯</a></td>
<td><a href="https://pan.baidu.com/s/1pLIEWeZ" target="_blank" rel="noopener">三岛由纪夫</a></td>
<td><a href="https://pan.baidu.com/s/1qY83zOS" target="_blank" rel="noopener">莎士比亚</a></td>
<td><a href="https://pan.baidu.com/s/1mhJQIe8" target="_blank" rel="noopener">舍斯托夫</a></td>
<td><a href="https://pan.baidu.com/s/1jHDjkCQ" target="_blank" rel="noopener">沈从文</a></td>
<td><a href="https://pan.baidu.com/s/1cfjQCA" target="_blank" rel="noopener">施尼茨勒</a></td>
<td><a href="https://pan.baidu.com/s/1cBDPUY" target="_blank" rel="noopener">施托姆</a></td>
<td><a href="https://pan.baidu.com/s/1mi9rAys" target="_blank" rel="noopener">张爱玲</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1kVOmEMV" target="_blank" rel="noopener">石黑一雄</a></td>
<td><a href="https://pan.baidu.com/s/1b1UkG2" target="_blank" rel="noopener">菲茨杰拉德</a></td>
<td><a href="https://pan.baidu.com/s/1gfhH4Gz" target="_blank" rel="noopener">司汤达</a></td>
<td><a href="https://pan.baidu.com/s/1eSlhn7W" target="_blank" rel="noopener">斯蒂芬.茨威格</a></td>
<td><a href="https://pan.baidu.com/s/1cHJyu6" target="_blank" rel="noopener">斯特林堡</a></td>
<td><a href="https://pan.baidu.com/s/1hrQK3Co" target="_blank" rel="noopener">苏珊.桑塔格</a></td>
<td><a href="https://pan.baidu.com/s/1nvh5s7n" target="_blank" rel="noopener">苏童</a></td>
<td><a href="https://pan.baidu.com/s/1hs5wcBi" target="_blank" rel="noopener">苏伟贞</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1slmMKu5" target="_blank" rel="noopener">梭罗</a></td>
<td><a href="https://pan.baidu.com/s/1kU9d0r1" target="_blank" rel="noopener">索尔·贝娄</a></td>
<td><a href="https://pan.baidu.com/s/1geDpteV" target="_blank" rel="noopener">索尔仁尼琴</a></td>
<td><a href="https://pan.baidu.com/s/1slgbppj" target="_blank" rel="noopener">太宰治</a></td>
<td><a href="https://pan.baidu.com/s/1pKXXLYb" target="_blank" rel="noopener">泰戈尔</a></td>
<td><a href="https://pan.baidu.com/s/1bpdfTGb" target="_blank" rel="noopener">唐·德里罗</a></td>
<td><a href="https://pan.baidu.com/s/1eS50dFW" target="_blank" rel="noopener">唐诺</a></td>
<td><a href="https://pan.baidu.com/s/1nv5RMlV" target="_blank" rel="noopener">特兰斯特勒默</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1jIy6sjc" target="_blank" rel="noopener">铁凝</a></td>
<td><a href="https://pan.baidu.com/s/1bZKeHS" target="_blank" rel="noopener">屠格涅夫</a></td>
<td><a href="https://pan.baidu.com/s/1hsF5wDA" target="_blank" rel="noopener">托马斯.哈代</a></td>
<td><a href="https://pan.baidu.com/s/1jHUKkz8" target="_blank" rel="noopener">托马斯曼</a></td>
<td><a href="https://pan.baidu.com/s/1hsOsKok" target="_blank" rel="noopener">托马斯·品钦</a></td>
<td><a href="https://pan.baidu.com/s/1pKIyGaf" target="_blank" rel="noopener">托妮·莫里森</a></td>
<td><a href="https://pan.baidu.com/s/1qYDTjze" target="_blank" rel="noopener">陀思妥耶夫斯基</a></td>
<td><a href="https://pan.baidu.com/s/1ge3fP9D" target="_blank" rel="noopener">瓦·格罗斯曼</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1hsmzdOg" target="_blank" rel="noopener">王安忆</a></td>
<td><a href="https://pan.baidu.com/s/1i52CiGX" target="_blank" rel="noopener">王德威</a></td>
<td><a href="https://pan.baidu.com/s/1bp56kWJ" target="_blank" rel="noopener">王尔德</a></td>
<td><a href="https://pan.baidu.com/s/1pLj3fuj" target="_blank" rel="noopener">王晋康</a></td>
<td><a href="https://pan.baidu.com/s/1miKgRva" target="_blank" rel="noopener">王蒙</a></td>
<td><a href="https://pan.baidu.com/s/1gfNEPij" target="_blank" rel="noopener">王小波</a></td>
<td><a href="https://pan.baidu.com/s/1jIpIN3K" target="_blank" rel="noopener">威廉·格纳齐诺</a></td>
<td><a href="https://pan.baidu.com/s/1miIfdgc" target="_blank" rel="noopener">维·贡布罗维奇</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1miJ0YZe" target="_blank" rel="noopener">维科</a></td>
<td><a href="https://pan.baidu.com/s/1i5itxdz" target="_blank" rel="noopener">翁贝托.艾柯</a></td>
<td><a href="https://pan.baidu.com/s/1c1ZGf1e" target="_blank" rel="noopener">伍尔夫</a></td>
<td><a href="https://pan.baidu.com/s/1gfpOW7p" target="_blank" rel="noopener">西奥多.德莱塞</a></td>
<td><a href="https://pan.baidu.com/s/1geYTVgv" target="_blank" rel="noopener">西尔维娅・普拉斯</a></td>
<td><a href="https://pan.baidu.com/s/1eSMvdZG" target="_blank" rel="noopener">西格斯</a></td>
<td><a href="https://pan.baidu.com/s/1qYPy3Kg" target="_blank" rel="noopener">张大春</a></td>
<td><a href="https://pan.baidu.com/s/1eSET1Pw" target="_blank" rel="noopener">张洁</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1i4Pw6pf" target="_blank" rel="noopener">希梅内斯</a></td>
<td><a href="https://pan.baidu.com/s/1o7BFl0I" target="_blank" rel="noopener">席勒</a></td>
<td><a href="https://pan.baidu.com/s/1nvBXyfZ" target="_blank" rel="noopener">夏多布里昂</a></td>
<td><a href="https://pan.baidu.com/s/1skFCHLN" target="_blank" rel="noopener">萧伯纳</a></td>
<td><a href="https://pan.baidu.com/s/1cuIPTs" target="_blank" rel="noopener">萧红</a></td>
<td><a href="https://pan.baidu.com/s/1nv9p4Vn" target="_blank" rel="noopener">新井一二三</a></td>
<td><a href="https://pan.baidu.com/s/1eRUSjxK" target="_blank" rel="noopener">雪莱</a></td>
<td><a href="https://pan.baidu.com/s/1jIL6Xcq" target="_blank" rel="noopener">亚历克斯·哈里</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1kVmsXT1" target="_blank" rel="noopener">亚马多</a></td>
<td><a href="https://pan.baidu.com/s/1i5fB8Eh" target="_blank" rel="noopener">阎连科</a></td>
<td><a href="https://pan.baidu.com/s/1nuFeWaP" target="_blank" rel="noopener">叶赛宁</a></td>
<td><a href="https://pan.baidu.com/s/1mhYJGe8" target="_blank" rel="noopener">叶芝</a></td>
<td><a href="https://pan.baidu.com/s/1c4SpRo" target="_blank" rel="noopener">伊迪丝·华顿</a></td>
<td><a href="https://pan.baidu.com/s/1skFCHM9" target="_blank" rel="noopener">伊恩·麦克尤恩</a></td>
<td><a href="https://pan.baidu.com/s/1slgbprn" target="_blank" rel="noopener">伊凡·克里玛</a></td>
<td><a href="https://pan.baidu.com/s/1pL41JtT" target="_blank" rel="noopener">伊夫林·沃</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1qXJcBj2" target="_blank" rel="noopener">内米洛夫斯基</a></td>
<td><a href="https://pan.baidu.com/s/1jIMSLsa" target="_blank" rel="noopener">伊姆雷</a></td>
<td><a href="https://pan.baidu.com/s/1nu6XwrJ" target="_blank" rel="noopener">卡达莱</a></td>
<td><a href="https://pan.baidu.com/s/1dFexJVV" target="_blank" rel="noopener">卡尔维诺</a></td>
<td><a href="https://pan.baidu.com/s/1qXTRYd2" target="_blank" rel="noopener">伊沃.安德里奇</a></td>
<td><a href="https://pan.baidu.com/s/1bplnvIn" target="_blank" rel="noopener">易卜生</a></td>
<td><a href="https://pan.baidu.com/s/1o8USh2U" target="_blank" rel="noopener">尤瑟纳尔</a></td>
<td><a href="https://pan.baidu.com/s/1gfetI0N" target="_blank" rel="noopener">约翰.厄普代克</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1gfetI0N" target="_blank" rel="noopener">约翰·肯尼迪·图尔</a></td>
<td><a href="https://pan.baidu.com/s/1slHoWyD" target="_blank" rel="noopener">约翰·韦恩</a></td>
<td><a href="https://pan.baidu.com/s/1c2rXahy" target="_blank" rel="noopener">约瑟夫.海勒</a></td>
<td><a href="https://pan.baidu.com/s/1miTArpu" target="_blank" rel="noopener">布罗茨基</a></td>
<td><a href="https://pan.baidu.com/s/1i4YPXel" target="_blank" rel="noopener">扎米亚京</a></td>
<td><a href="https://pan.baidu.com/s/1o7XtQTO" target="_blank" rel="noopener">詹姆斯.乔伊斯</a></td>
<td><a href="https://pan.baidu.com/s/1bpjR13x" target="_blank" rel="noopener">鲍德温</a></td>
<td><a href="https://pan.baidu.com/s/1mikpi1y" target="_blank" rel="noopener">文化生活译丛</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1qYWjXHM" target="_blank" rel="noopener">文学批评理论</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：诗歌篇"><a href="#清单的艺术：诗歌篇" class="headerlink" title="清单的艺术：诗歌篇"></a><strong>清单的艺术：诗歌篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1i5tOCAT" target="_blank" rel="noopener">阿多尼斯</a></th>
<th><a href="https://pan.baidu.com/s/1jH9f5wM" target="_blank" rel="noopener">阿赫玛托娃</a></th>
<th><a href="https://pan.baidu.com/s/1i5ncZqd" target="_blank" rel="noopener">艾略特</a></th>
<th><a href="https://pan.baidu.com/s/1dEOOILb" target="_blank" rel="noopener">奥登</a></th>
<th><a href="https://pan.baidu.com/s/1qYr81dU" target="_blank" rel="noopener">拜伦</a></th>
<th><a href="https://pan.baidu.com/s/1eSzux4u" target="_blank" rel="noopener">波德莱尔</a></th>
<th><a href="https://pan.baidu.com/s/1eRBp3jk" target="_blank" rel="noopener">茨维塔耶娃</a></th>
<th><a href="https://pan.baidu.com/s/1pKArjLp" target="_blank" rel="noopener">张枣</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1o8KIP9g" target="_blank" rel="noopener">狄金森</a></td>
<td><a href="https://pan.baidu.com/s/1cbLTsq" target="_blank" rel="noopener">谷川俊太郎</a></td>
<td><a href="https://pan.baidu.com/s/1nuRqhgH" target="_blank" rel="noopener">哈维尔</a></td>
<td><a href="https://pan.baidu.com/s/1bpi59O7" target="_blank" rel="noopener">海涅</a></td>
<td><a href="https://pan.baidu.com/s/1skCoUQD" target="_blank" rel="noopener">荷尔德林</a></td>
<td><a href="https://pan.baidu.com/s/1dFsAVPB" target="_blank" rel="noopener">惠特曼</a></td>
<td><a href="https://pan.baidu.com/s/1jIEL07G" target="_blank" rel="noopener">济慈</a></td>
<td><a href="https://pan.baidu.com/s/1ge5hMPP" target="_blank" rel="noopener">亚非诗选</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1hrOIBI0" target="_blank" rel="noopener">莱蒙托夫</a></td>
<td><a href="https://pan.baidu.com/s/1kUSaojP" target="_blank" rel="noopener">兰波</a></td>
<td><a href="https://pan.baidu.com/s/1c4Sq30" target="_blank" rel="noopener">里尔克</a></td>
<td><a href="https://pan.baidu.com/s/1miiTmre" target="_blank" rel="noopener">马雅可夫斯基</a></td>
<td><a href="https://pan.baidu.com/s/1hsiv0w8" target="_blank" rel="noopener">曼德尔施塔姆</a></td>
<td><a href="https://pan.baidu.com/s/1b1Ulo6" target="_blank" rel="noopener">聂鲁达</a></td>
<td><a href="https://pan.baidu.com/s/1gfpOXft" target="_blank" rel="noopener">帕斯捷尔纳克</a></td>
<td><a href="https://pan.baidu.com/s/1jHJUNn4" target="_blank" rel="noopener">泰戈尔</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1o7NQwsE" target="_blank" rel="noopener">托马斯.哈代</a></td>
<td><a href="https://pan.baidu.com/s/1pLbv5xx" target="_blank" rel="noopener">王尔德</a></td>
<td><a href="https://pan.baidu.com/s/1i57vPAX" target="_blank" rel="noopener">希梅内斯</a></td>
<td><a href="https://pan.baidu.com/s/1miGJlyC" target="_blank" rel="noopener">席勒</a></td>
<td><a href="https://pan.baidu.com/s/1dFKviOp" target="_blank" rel="noopener">雪莱</a></td>
<td><a href="https://pan.baidu.com/s/1mi25YD6" target="_blank" rel="noopener">叶赛宁</a></td>
<td><a href="https://pan.baidu.com/s/1kVel7Ll" target="_blank" rel="noopener">叶芝</a></td>
<td><a href="https://pan.baidu.com/s/1sln2KGH" target="_blank" rel="noopener">域外诗丛</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1nvHxg6H" target="_blank" rel="noopener">域外诗歌精品评析</a></td>
<td><a href="https://pan.baidu.com/s/1hs8JNpi" target="_blank" rel="noopener">20世纪世界诗歌译丛</a></td>
<td><a href="https://pan.baidu.com/s/1skTPNC5" target="_blank" rel="noopener">西葡诗选</a></td>
<td><a href="https://pan.baidu.com/s/1qY61AmK" target="_blank" rel="noopener">德语诗选</a></td>
<td><a href="https://pan.baidu.com/s/1bp12yS3" target="_blank" rel="noopener">英诗选集</a></td>
<td><a href="https://pan.baidu.com/s/1pK8VrSZ" target="_blank" rel="noopener">拉美诗选</a></td>
<td><a href="https://pan.baidu.com/s/1qYV4dyc" target="_blank" rel="noopener">俄罗斯诗选</a></td>
<td><a href="https://pan.baidu.com/s/1jInGU7W" target="_blank" rel="noopener">诺贝尔获得者诗选</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c2Gt5zq" target="_blank" rel="noopener">日本诗选</a></td>
<td><a href="https://pan.baidu.com/s/1c1Gdq0o" target="_blank" rel="noopener">法国诗选</a></td>
<td><a href="https://pan.baidu.com/s/1mhNU1mw" target="_blank" rel="noopener">美国诗选</a></td>
<td><a href="https://pan.baidu.com/s/1pLEBd8n" target="_blank" rel="noopener">二十世纪外国大诗人丛书</a></td>
<td><a href="https://pan.baidu.com/s/1jIrKI0Y" target="_blank" rel="noopener">十位外国诗人</a></td>
<td><a href="https://pan.baidu.com/s/1nvGbgfB" target="_blank" rel="noopener">外国文学名家诗篇</a></td>
<td><a href="https://pan.baidu.com/s/1o87R6NC" target="_blank" rel="noopener">世界诗库</a></td>
<td><a href="https://pan.baidu.com/s/1qXRQjaW" target="_blank" rel="noopener">诗苑译林</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1dEJyZ0d" target="_blank" rel="noopener">外国诗歌丛书</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：法学篇"><a href="#清单的艺术：法学篇" class="headerlink" title="清单的艺术：法学篇"></a><strong>清单的艺术：法学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1i5MUOE5" target="_blank" rel="noopener">波斯纳</a></th>
<th><a href="https://pan.baidu.com/s/1bpLfiI7" target="_blank" rel="noopener">德沃金</a></th>
<th><a href="https://pan.baidu.com/s/1gfu8Cr5" target="_blank" rel="noopener">德肖维茨</a></th>
<th><a href="https://pan.baidu.com/s/1jIAbCO2" target="_blank" rel="noopener">哈特</a></th>
<th><a href="https://pan.baidu.com/s/1dF74bkP" target="_blank" rel="noopener">贺卫方</a></th>
<th><a href="https://pan.baidu.com/s/1jI08lUm" target="_blank" rel="noopener">卡尔·施米特</a></th>
<th><a href="https://pan.baidu.com/s/1kV27YHh" target="_blank" rel="noopener">梁治平</a></th>
<th><a href="https://pan.baidu.com/s/1clVyUi" target="_blank" rel="noopener">刘星</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1gf91Gpd" target="_blank" rel="noopener">苏力</a></td>
<td><a href="https://pan.baidu.com/s/1jIy6som" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：心理学篇"><a href="#清单的艺术：心理学篇" class="headerlink" title="清单的艺术：心理学篇"></a><strong>清单的艺术：心理学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1mhZzJ9A" target="_blank" rel="noopener">荣格</a></th>
<th><a href="https://pan.baidu.com/s/1pLUQhWB" target="_blank" rel="noopener">阿德勒</a></th>
<th><a href="https://pan.baidu.com/s/1c1J1fGW" target="_blank" rel="noopener">陈鹤琴</a></th>
<th><a href="https://pan.baidu.com/s/1slNu2Yx" target="_blank" rel="noopener">弗洛姆</a></th>
<th><a href="https://pan.baidu.com/s/1dFlPrgP" target="_blank" rel="noopener">弗洛伊德</a></th>
<th><a href="https://pan.baidu.com/s/1gfnNcm3" target="_blank" rel="noopener">卡伦霍妮</a></th>
<th><a href="https://pan.baidu.com/s/1jIlFcSe" target="_blank" rel="noopener">拉康</a></th>
<th><a href="https://pan.baidu.com/s/1nu8tytF" target="_blank" rel="noopener">马斯洛</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1i5itxjB" target="_blank" rel="noopener">20世纪西方现代心理学</a></td>
<td><a href="https://pan.baidu.com/s/1gfbBQP9" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：社会学篇"><a href="#清单的艺术：社会学篇" class="headerlink" title="清单的艺术：社会学篇"></a><strong>清单的艺术：社会学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1nuLkAxj" target="_blank" rel="noopener">鲍德里亚</a></th>
<th><a href="https://pan.baidu.com/s/1qY619VY" target="_blank" rel="noopener">鲍曼</a></th>
<th><a href="https://pan.baidu.com/s/1qYoQIz6" target="_blank" rel="noopener">彼得·伯格</a></th>
<th><a href="https://pan.baidu.com/s/1kVnEWrd" target="_blank" rel="noopener">布迪厄</a></th>
<th><a href="https://pan.baidu.com/s/1mi1QckC" target="_blank" rel="noopener">迪尔凯姆</a></th>
<th><a href="https://pan.baidu.com/s/1i51qi5B" target="_blank" rel="noopener">福柯</a></th>
<th><a href="https://pan.baidu.com/s/1jIrKHXO" target="_blank" rel="noopener">福山</a></th>
<th><a href="https://pan.baidu.com/s/1nuQApOP" target="_blank" rel="noopener">格尔茨</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1gf5xYtP" target="_blank" rel="noopener">哈耶克</a></td>
<td><a href="https://pan.baidu.com/s/1dEOiHdZ" target="_blank" rel="noopener">吉登斯</a></td>
<td><a href="https://pan.baidu.com/s/1ca0kyi" target="_blank" rel="noopener">雷蒙.阿隆</a></td>
<td><a href="https://pan.baidu.com/s/1sll0HGh" target="_blank" rel="noopener">马克思.舍勒</a></td>
<td><a href="https://pan.baidu.com/s/1o7FIRpg" target="_blank" rel="noopener">马克思.韦伯</a></td>
<td><a href="https://pan.baidu.com/s/1nvUomzn" target="_blank" rel="noopener">曼海姆</a></td>
<td><a href="https://pan.baidu.com/s/1qY3dOug" target="_blank" rel="noopener">米德</a></td>
<td><a href="https://pan.baidu.com/s/1slT0muD" target="_blank" rel="noopener">西美尔</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1bo9ZC3X" target="_blank" rel="noopener">殷海光</a></td>
<td><a href="https://pan.baidu.com/s/1bpe1Q6j" target="_blank" rel="noopener">郑杭生</a></td>
<td><a href="https://pan.baidu.com/s/1i5H5nKx" target="_blank" rel="noopener">亚历山大</a></td>
<td><a href="https://pan.baidu.com/s/1o8wwnuq" target="_blank" rel="noopener">科塞</a></td>
<td><a href="https://pan.baidu.com/s/1kU8xxMR" target="_blank" rel="noopener">其他</a></td>
<td><a href="https://pan.baidu.com/s/1dE67NgP" target="_blank" rel="noopener">社会学家茶座</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：政治学篇"><a href="#清单的艺术：政治学篇" class="headerlink" title="清单的艺术：政治学篇"></a><strong>清单的艺术：政治学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1mhYdQgo" target="_blank" rel="noopener">德沃金</a></th>
<th><a href="https://pan.baidu.com/s/1pLDpo4j" target="_blank" rel="noopener">福山</a></th>
<th><a href="https://pan.baidu.com/s/1bBxD2e" target="_blank" rel="noopener">国际政治</a></th>
<th><a href="https://pan.baidu.com/s/1i4Oa91r" target="_blank" rel="noopener">哈贝马斯</a></th>
<th><a href="https://pan.baidu.com/s/1qYsUz3u" target="_blank" rel="noopener">哈耶克</a></th>
<th><a href="https://pan.baidu.com/s/1kVkrltX" target="_blank" rel="noopener">汉娜.阿伦特</a></th>
<th><a href="https://pan.baidu.com/s/1o7JgGOU" target="_blank" rel="noopener">亨廷顿</a></th>
<th><a href="https://pan.baidu.com/s/1o7PSsXG" target="_blank" rel="noopener">吉登斯</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1mi1kdmg" target="_blank" rel="noopener">刘军宁</a></td>
<td><a href="https://pan.baidu.com/s/1i4JqRKp" target="_blank" rel="noopener">刘瑜</a></td>
<td><a href="https://pan.baidu.com/s/1c1Bt7Uw" target="_blank" rel="noopener">卢梭</a></td>
<td><a href="https://pan.baidu.com/s/1pL7jCAv" target="_blank" rel="noopener">罗尔斯</a></td>
<td><a href="https://pan.baidu.com/s/1c2uELEg" target="_blank" rel="noopener">马克思.韦伯</a></td>
<td><a href="https://pan.baidu.com/s/1jIhBvCU" target="_blank" rel="noopener">秦晖</a></td>
<td><a href="https://pan.baidu.com/s/1nuFKUG5" target="_blank" rel="noopener">施特劳斯</a></td>
<td><a href="https://pan.baidu.com/s/1jH9f5jw" target="_blank" rel="noopener">汪晖</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1bpg3Kcv" target="_blank" rel="noopener">许纪霖</a></td>
<td><a href="https://pan.baidu.com/s/1bpg3Kcv" target="_blank" rel="noopener">以赛亚.伯林</a></td>
<td><a href="https://pan.baidu.com/s/1nuRqgFf" target="_blank" rel="noopener">政治法律社会</a></td>
<td><a href="https://pan.baidu.com/s/1qYgJfNA" target="_blank" rel="noopener">周保松</a></td>
<td><a href="https://pan.baidu.com/s/1nvR6vZr" target="_blank" rel="noopener">刘擎</a></td>
<td><a href="https://pan.baidu.com/s/1hsmzea4" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：人类学篇"><a href="#清单的艺术：人类学篇" class="headerlink" title="清单的艺术：人类学篇"></a><strong>清单的艺术：人类学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1kUTwlsr" target="_blank" rel="noopener">博厄斯</a></th>
<th><a href="https://pan.baidu.com/s/1c17VHao" target="_blank" rel="noopener">布迪厄</a></th>
<th><a href="https://pan.baidu.com/s/1eR3NWC6" target="_blank" rel="noopener">迪尔凯姆</a></th>
<th><a href="https://pan.baidu.com/s/1c2ffLJE" target="_blank" rel="noopener">格尔茨</a></th>
<th><a href="https://pan.baidu.com/s/1i58bLI1" target="_blank" rel="noopener">马林诺夫斯基</a></th>
<th><a href="https://pan.baidu.com/s/1o7PmrdK" target="_blank" rel="noopener">马塞尔·莫斯</a></th>
<th><a href="https://pan.baidu.com/s/1slSKpK5" target="_blank" rel="noopener">米德</a></th>
<th><a href="https://pan.baidu.com/s/1hr5fMRi" target="_blank" rel="noopener">普理查德</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1pLaF1Bd" target="_blank" rel="noopener">人物传记</a></td>
<td><a href="https://pan.baidu.com/s/1nuHMN6T" target="_blank" rel="noopener">萨林斯</a></td>
<td><a href="https://pan.baidu.com/s/1c2o9l76" target="_blank" rel="noopener">斯特劳斯</a></td>
<td><a href="https://pan.baidu.com/s/1hr4zPEs" target="_blank" rel="noopener">特纳</a></td>
<td><a href="https://pan.baidu.com/s/1gfB3Hr9" target="_blank" rel="noopener">王铭铭</a></td>
<td><a href="https://pan.baidu.com/s/1eSaXVP4" target="_blank" rel="noopener">庄孔韶</a></td>
<td><a href="https://pan.baidu.com/s/1c2FD8fI" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="清单的艺术：经济学篇"><a href="#清单的艺术：经济学篇" class="headerlink" title="清单的艺术：经济学篇"></a><strong>清单的艺术：经济学篇</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://pan.baidu.com/s/1mi5nRs8" target="_blank" rel="noopener">阿玛蒂亚.森</a></th>
<th><a href="https://pan.baidu.com/s/1pLedspX" target="_blank" rel="noopener">博弈论</a></th>
<th><a href="https://pan.baidu.com/s/1pLFXgyn" target="_blank" rel="noopener">布坎南</a></th>
<th><a href="https://pan.baidu.com/s/1nuSChr7" target="_blank" rel="noopener">丹尼尔·卡尼曼</a></th>
<th><a href="https://pan.baidu.com/s/1miN4JTQ" target="_blank" rel="noopener">周其仁</a></th>
<th><a href="https://pan.baidu.com/s/1nv7nBxn" target="_blank" rel="noopener">哈耶克</a></th>
<th><a href="https://pan.baidu.com/s/1sl4xEO1" target="_blank" rel="noopener">加里.贝克尔</a></th>
<th><a href="https://pan.baidu.com/s/1i55ZXWD" target="_blank" rel="noopener">卡尔·门格尔</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pan.baidu.com/s/1mhAxv0O" target="_blank" rel="noopener">科斯</a></td>
<td><a href="https://pan.baidu.com/s/1kU4ZEkB" target="_blank" rel="noopener">约瑟夫·阿罗</a></td>
<td><a href="https://pan.baidu.com/s/1dF8POIH" target="_blank" rel="noopener">茅于轼</a></td>
<td><a href="https://pan.baidu.com/s/1dEYYdxR" target="_blank" rel="noopener">米尔顿.弗里德曼</a></td>
<td><a href="https://pan.baidu.com/s/1jI1UdJw" target="_blank" rel="noopener">米塞斯</a></td>
<td><a href="https://pan.baidu.com/s/1pLn6Pv1" target="_blank" rel="noopener">庞巴维克</a></td>
<td><a href="https://pan.baidu.com/s/1mhI4Pja" target="_blank" rel="noopener">钱颖一</a></td>
<td><a href="https://pan.baidu.com/s/1i4BjsB7" target="_blank" rel="noopener">斯蒂格勒</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1c2k5wPy" target="_blank" rel="noopener">斯蒂格利茨</a></td>
<td><a href="https://pan.baidu.com/s/1pLHsWaB" target="_blank" rel="noopener">托马斯·谢林</a></td>
<td><a href="https://pan.baidu.com/s/1eSD78kE" target="_blank" rel="noopener">汪丁丁</a></td>
<td><a href="https://pan.baidu.com/s/1skAnhIt" target="_blank" rel="noopener">韦森</a></td>
<td><a href="https://pan.baidu.com/s/1hso0X3A" target="_blank" rel="noopener">行为经济学</a></td>
<td><a href="https://pan.baidu.com/s/1o8IbbnW" target="_blank" rel="noopener">张维迎</a></td>
<td><a href="https://pan.baidu.com/s/1bp0MypL" target="_blank" rel="noopener">张五常</a></td>
</tr>
<tr>
<td><a href="https://pan.baidu.com/s/1b1oqmE" target="_blank" rel="noopener">当代世界十大经济学派丛书</a></td>
<td><a href="https://pan.baidu.com/s/1jIKlbLk" target="_blank" rel="noopener">新制度经济学</a></td>
<td><a href="https://pan.baidu.com/s/1slE7eMx" target="_blank" rel="noopener">其他</a></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      
        <categories>
            
            <category> 读书记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[日知录（3）：心理账户]]></title>
      <url>/2017/12/12/%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%883%EF%BC%89%EF%BC%9A%E5%BF%83%E7%90%86%E8%B4%A6%E6%88%B7/</url>
      <content type="html"><![CDATA[<p>2002年10月9日，瑞典皇家科学院将诺贝尔经济学奖授予普林斯顿大学心理学教授DannielKahneman和乔治·梅森大学经济学教授VernonSmith。根据瑞典皇家科学院的新闻公报，卡尼曼“将心理学的深入分析融入到了经济学中，从而为一个崭新的经济学研究领域奠定了基础”。1981年，DannielKahneman及其合作者AmosTversky在《科学》杂志发表论文，研究人们决策过程的认知心理规律。文章介绍了“演出实验”</p>
<a id="more"></a>
<ul>
<li>【实验情境A】：你打算去剧院看一场演出，票价是10美元，在你到达剧院的时候，发现自己丢了一张10美元的钞票。你是否会买票看演出？实验表明：88%的调查对象选择会；12%的调查对象选择不会。（调查对象为183人）</li>
<li>【实验情境B】：你打算去看一场演出而且花10美元钱买了一张票。在你到达剧院的时候，发现门票丢了。如果你想看演出，必须再花10美元，你是否会买票？实验结果表明：46%的调查对象选择会，54%的调查对象不会。（调查对象为200人）</li>
</ul>
<p>Kahneman和Tversky认为:两种实验情境出现明显不同结果的原因在于:在考虑情境A的决策结果时，人们把丢失的10美元钞票和买演出票的10美元分别考虑；而在情境B中，则把已经购买演出票的钱和后来买票的钱放在同一个账户估价，一部分人觉得“太贵了”改变自己的选择。为此，Kahneman和Tversky引入理查德·萨勒教授（RichardThaler）提出的“心理账户”概念，对实验结果进行了深入的分析。</p>
<h2 id="一、概念发展"><a href="#一、概念发展" class="headerlink" title="一、概念发展"></a>一、概念发展</h2><h3 id="1-1-理查德·萨勒"><a href="#1-1-理查德·萨勒" class="headerlink" title="1.1 理查德·萨勒"></a>1.1 理查德·萨勒</h3><p>1980年，芝加哥大学著名行为金融和行为经济学家理查德·萨勒（RichardThaler）首次提出“Psychic Accounting（心理账户）”概念，用于解释个体在消费决策时为什么会受到“沉没成本效应（sunk cost effert）”的影响。萨勒认为：人们在消费行为中之所以受到“沉没成本”的影响，一个可能的解释是卡尼曼教授等提出的“前景理论”，另一个可能的解释就是推测个体潜意识中存在的“心理账户系统”（Psychic Accounting system）。人们在消费决策时把过去的投入和现在的付出加在一起作为总成本，来衡量决策的后果。这种对金钱分门别类的分账管理和预算的心理过程就是“心理账户”的估价过程。</p>
<h3 id="1-2-丹尼尔·卡尼曼"><a href="#1-2-丹尼尔·卡尼曼" class="headerlink" title="1.2 丹尼尔·卡尼曼"></a>1.2 丹尼尔·卡尼曼</h3><p>1981年，丹尼尔·卡尼曼和特韦尔斯基(Amos Tversky)在对“演出实验”的分析中使用“Psychological Account(心理账户)”概念，表明消费者在决策时根据不同的决策任务形成相应的心理账户。卡尼曼认为，心理账户是人们在心理上对结果(尤其是经济结果)的分类记账、编码、估价和预算等过程。</p>
<p>1984年，卡尼曼教授和特韦尔斯基教授认为“心理账户”概念用“mental account”表达更贴切。卡尼曼认为:人们在做出选择时，实际上就是对多种选择结果进行估价的过程。究竟如何估价，最简单也最基本的估价方式就是把选择结果进行获益与损失(得失)的评价。因此，他提出了“值函数”假设和“决策权重”函数来解释人们内在的得失评价机制。</p>
<h3 id="1-3-理查德·萨勒"><a href="#1-3-理查德·萨勒" class="headerlink" title="1.3 理查德·萨勒"></a>1.3 理查德·萨勒</h3><p>1985年，萨勒教授发表“心理账户与消费者行为选择”一文，正式提出“心理账户”理论，系统地分析了心理账户现象，以及心理账户如何导致个体违背最简单的经济规律。萨勒认为:小到个体、家庭，大到企业集团，都有或明确或潜在的心理账户系统。在作经济决策时，这种心理账户系统常常遵循一种与经济学的运算规律相矛盾的潜在心理运算规则，其心理记账方式与经济学和数学的运算方式都不相同。因此经常以非预期的方式影响着决策，使个体的决策违背最简单的理性经济法则。萨勒列举了4个典型现象阐明心理账户对传统经济规律的违背，并提出了心理账户的“非替代性”特征。</p>
<h3 id="1-4-特韦尔斯基"><a href="#1-4-特韦尔斯基" class="headerlink" title="1.4 特韦尔斯基"></a>1.4 特韦尔斯基</h3><p>1996年Tversky提出，心理账户是一种认知幻觉，这种认知幻觉影响金融市场的投资者，使投资者们失去对价格的理性关注，从而产生非理性投资行为。Kivetz(1999)认为，心理账户是人们根据财富的来源不同进行编码和归类的心理过程，在这一编码和分类过程中“重要性-非重要性”是人们考虑的一个维度。有学者从行为的角度对“心理账户”进行定义，认为心理账户是个人或家庭用来管理、评估和记录经济活动的一套认知操作系统，这套认知操作系统导致一系列非理性的“心理账户”决策误区。</p>
<h3 id="1-5-理查德·萨勒"><a href="#1-5-理查德·萨勒" class="headerlink" title="1.5 理查德·萨勒"></a>1.5 理查德·萨勒</h3><p>1999年，萨勒发表“mental accounting matters”一文，这是对近20年“心理账户”研究的一个总结。在文章中，萨勒认为:心理账户的三个部分最受关注，首先是对于决策结果的感知以及决策结果的制定及评价，心理账户系统提供了决策前后的损失——获益分析;第二个部分涉及特定账户的分类活动，资金根据来源和支出划分成不同的类别(住房、食物等)，消费有时要受制于明确或不明确的特定账户的预算;第三个部分涉及账户评估频率和选择框架，账户可以是以每天、每周或每年的频率进行权衡，时间限定可宽可窄。因此，“心理账户”是人们在心理上对结果(尤其是经济结果)的编码、分类和估价的过程，它揭示了人们在进行(资金)财富决策时的心理认知过程。</p>
<h2 id="二、心理账户的非替代性（non-fungibility）"><a href="#二、心理账户的非替代性（non-fungibility）" class="headerlink" title="二、心理账户的非替代性（non-fungibility）"></a>二、心理账户的非替代性（non-fungibility）</h2><p>按照传统的微观经济学理论，金钱不会被贴上标签，它具有替代性(fungibility)，事实上，越来越多的实证研究表明:</p>
<blockquote>
<p>人们并不是把所有的财富放在一个整体账户进行管理，每一元钱与每一元钱可以很好的替换与转移。相反，人们根据财富来源与支出划分成不同性质的多个分账户，每个分账户有单独的预算和支配规则，金钱并不能容易地从一个账户转移到另一个账户。</p>
</blockquote>
<p>萨勒将这种金钱不能很好转移，不能完全替换的特点称之为“非替代性”。萨勒教授在研究中发现金钱非替代性的一些表现:</p>
<ol>
<li>【不同来源】：由不同来源的财富而设立的心理账户之间具有非替代性，例如意外之财和辛苦得来的钱不具替代性。一般来说，人们会把辛苦挣来的钱存起来不舍得花，而如果是一笔意外之财，可能很快就花掉。</li>
<li>【不同消费项目】：不同消费项目而设立的心理账户之间具有非替代性。我们来看一个案例:王先生非常中意商场的一件羊毛衫，价格为1250元，他觉得贵而舍不得买。月底的时候他妻子买下羊毛衫作为生日礼物送给他，他非常开心。尽管王先生的钱和他的妻子的钱是同一家庭的钱，为什么同样的钱以不同的理由开支心理感觉不同?研究表明:自己花费购买羊毛衫，属于生活必需开支，1250元太贵了;而作为生日礼物送给丈夫，属于情感开支。因此人们欣然接受昂贵的礼品却未必自己去买昂贵的物品。可见，为不同的消费项目设立的心理账户之间具有非替代性。</li>
<li>【不同存储方式】：不同存储方式导致心理账户的非替代性。萨勒教授举的一个实例。约翰先生一家存了15000美元准备买一栋理想的别墅，他们计划在5年以后购买，这笔钱放在商业账户上的利率是10%;可最近他们刚刚贷款11000美元买了一部新车，新车贷款3年的利率是15%，为什么他不用自己的15000美元存款买新车呢?通常，人们对已经有了预定开支项目的金钱，不愿意由于临时开支挪用这笔钱，对这个家庭来说，存起来买房的钱，已经放在了购房这一预定账户上，如果另外一项开支(买车)挪用了这笔钱，这笔钱就不存在了。从理性上说，家庭的总财富不变。但因为财富改变了存放的位置，固定账户和临时账户具有非替代性，人们的心理感觉不一样。</li>
</ol>
<h2 id="三、心理账户的运算规则"><a href="#三、心理账户的运算规则" class="headerlink" title="三、心理账户的运算规则"></a>三、心理账户的运算规则</h2><p>在日常经济活动中，人们是如何操纵和管理心理账户，这些经济交易在人们心里是如何评估和被体验的呢?萨勒认为:人们在进行各个账户的心理运算时，实际上就是对各种选择的损失-获益进行估价，称之为“得与失的构架(the framing of gains and losses)”，人们在心理运算的过程中并不是追求理性认知上的效用最大化，而是追求情感上的满意最大化。</p>
<p>情感体验在人们的现实决策中起着重要的作用，他将这种运算称之为“享乐主义的加工”（hedonic editing）</p>
<h3 id="3-1-值函数的假设"><a href="#3-1-值函数的假设" class="headerlink" title="3.1 值函数的假设"></a>3.1 值函数的假设</h3><p>为了更好地探讨心理账户的价值运算如何影响人们的经济决策行为，卡尼曼教授在“前景理论”中提出了“值函数”(value function)这一概念。与以往经济理论中的“效用函数”(utility function)相比，值函数有三个重要的特征。</p>
<ol>
<li>值函数是人们在决策行为时对于某个参照点的相对得失的详细说明，人们的“得与失”是个相对概念而不是期望效用理论的绝对概念。人们对某一决策结果的主观判断是相对于某个自然参照点而言，而不是绝对的财富或经济。因此，参照点的变化会引起人们主观估价的变化，人们更关注的是围绕参照点引起的改变而不是绝对水平。</li>
<li>“得与失”都表现出敏感性递减的规律。值函数的曲线是一条近似“S”形的曲线，右上角的盈利曲线为下凹形(concave)，左下角的亏损曲线为上凸形(convex)(如图所示)。离参照点(坐标交叉的原点)愈近的差额人们愈加敏感，越是远离参照点的差额越不敏感。因此，不管是获得还是损失，人们感觉到10元到20元的差额似乎比1000元到1010元的差额更大，这反映了价值曲线的边际递减特征。</li>
<li>损失规避。卡尼曼教授认为:同等数量的损失比获益对人的影响更大，因此在决策的时候人们尽量回避损失，表现在价值函数曲线上，损失曲线的斜率比获益曲线的斜率更大(如图1所示)，用公式表示为V(X)&lt;-V(-X)。例如损失1000元钱所带来的痛苦比获得1000元奖金而带来的愉悦更强烈。因此，面临损失时，人们是风险偏好的;面临获得时，人们是风险规避的。</li>
</ol>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-12-19 下午4.00.29.png" alt="屏幕快照 2017-12-19 下午4.00.29"></p>
<p>由于值函数的三个典型特征，对心理账户的运算规则至少有三个启示:</p>
<ol>
<li>相同的决策结果表述为损失或者获益会改变人们的风险决策偏好;</li>
<li>设计不同的参照点会改变人们对决策结果的认知;</li>
<li>同样的价格差额在不同的原始价格下，影响作用是不同的。</li>
</ol>
<h3 id="3-2-得与失的编码规则"><a href="#3-2-得与失的编码规则" class="headerlink" title="3.2 得与失的编码规则"></a>3.2 得与失的编码规则</h3><p>根据上述值函数的特点，萨勒在关于心理账户的研究中，将值函数在得与失的不同组合结果中的偏好情况作了分析。</p>
<blockquote>
<p>规则一：两笔盈利应分开</p>
</blockquote>
<p>假如两笔收入X、Y均为正，分开价值为$V(X)+V(Y)$，整合值为$V(X+Y)$。因价值曲线在右上角为凸形，所以$V(X)+V(Y)&gt;V(X+Y)$，个体更偏好分开体验(如图)。假如想送朋友两件礼物——一套衣服和一个健身器，最好分两次送。每次送一件礼物所带来的心理体验比一次送两件礼物的心理体验高。</p>
<blockquote>
<p>规则二：两笔损失应整合</p>
</blockquote>
<p>两笔支出对个体而言是“损失”，因价值曲线在左下角为凹形，所以$V(-X)+V(-Y)&lt;V(-X-Y)$，个体更偏好整合价值。这一规律可以解释生活中的很多现象，比如开会收取会务费时，最好一次收齐并留有余地，若有额外开支一次次增收，虽然数量不多，会员仍会牢骚满腹。</p>
<blockquote>
<p>规则三:大得小失应整合</p>
</blockquote>
<p>两笔收入一正一负:X，-Y，且余额为正，即$X&gt;Y$，从价值曲线看应是$V(X)+V(-Y)&lt;V(X-Y)$，所以人们更偏好整合。这条规则给人们的启示是，如果你有一个大的好消息和一个小的坏消息，应该把这两个消息一起告诉别人。如此整合，坏消息带来的痛苦会被好消息带来的快乐所冲淡，负面效应也就小得多。</p>
<blockquote>
<p>规则四:小得大失应具体分析</p>
</blockquote>
<p>两笔收入一正一负:X，-Y，且余额为负，即X&lt;Y，此时应分两种情况:</p>
<p>其一，小得大失且悬殊很大，应分开估价。从图中看出$V(X)+V(-Y)&gt;V(X-Y)$，因此，分开估价的心理体验要好，这种现象称为“银衬里(silver lining)”规则。例如(40，-6000)，人们更愿意分开估价，因为价值曲线在-6000元附近相对较平缓，40元的获得与6000元的损失相比几乎没有减少损失的作用，分开估价还能得到40元收益的感觉。</p>
<p>其二，小得大失且悬殊不大，应整合。如(40，-50)，人们更偏好整合价值。表现为$V(X-Y)&gt;V(X)+V(-Y)$。整合估价时，人们在心理会把损失从50元降低到10元，这样的损失就显得小了，心理体验更好，整合估价的作用体现出来。</p>
<p>萨勒进一步把这四条规则概括为:</p>
<ol>
<li>分离收益;</li>
<li>整合损失;</li>
<li>把小损失与大收益整合一起;</li>
<li>把小收益从大损失中分离出来。</li>
</ol>
<p>以上心理账户的运算规则对于理解和解释现实经济决策行为有重要的指导意义。</p>
<h2 id="四、应用研究"><a href="#四、应用研究" class="headerlink" title="四、应用研究"></a>四、应用研究</h2><h3 id="4-1-价格感知——绝对值优惠与相对值优惠"><a href="#4-1-价格感知——绝对值优惠与相对值优惠" class="headerlink" title="4.1 价格感知——绝对值优惠与相对值优惠"></a>4.1 价格感知——绝对值优惠与相对值优惠</h3><p>1982年，特维尔斯基教授和卡尼曼教授通过设计以下情景实验引入“心理账户”与消费者购买决策行为的研究。</p>
<ul>
<li>【实验情景A】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是125美元，计算器的价格是15美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是10美元。请问:你会去另一个商场买计算器吗?</li>
<li>【实验情景B】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是15美元，计算器的价格是125美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是120美元。请问:你会去另一个商场买计算器吗?</li>
</ul>
<p>在这两个情境中，其实都是对“是否开车20分钟从140美元的总购物款中节省5美元”做出选择。然而，实验对象在两个情境中的回答却不一样。在情境A中，68%的实验对象选择去另一家商场;而在情境B中，只有29%的实验对象选择开车去另一家商场。选择偏好发生了逆转。</p>
<p>卡尼曼提出，消费者在感知价格的时候，是从三个不同的心理账户进行得失评价的。一个是最小账户(minimal account)，就是不同方案所优惠的绝对值。在本实验中的最小账户就是5美元。另一个是局部账户(topical account)，也可称为相对值账户。例如，在实验情境A中开车前往另一家店的“局部账户”表现为计算器价格从15美元降为10美元(相对差额为1/3);而在实验情境B中的“局部账户”表现为计算器价格从125美元降为120美元(相对差额为1/25)。第三个是综合账户(comprehensive account)，综合账户就是总消费账户，该实验的综合账户为140美元。</p>
<p>卡尼曼认为，在上面的实验中，消费者是自发运用了局部账户，即通过相对优惠值来感知价格。情境A有33.3%的优惠;而情境B仅有4%的优惠。因此，人们的购买行为发生了反转。表现为在实验情境A中，68%的实验对象选择去另一家商场;而在实验情境B中，却只有29%。</p>
<p>此后，Philip Moon,Kevin Keasey,Darren Duxbury对卡尼曼的研究进行了重复实验并且提出，当优惠超过某个阈限值的时候，消费者对绝对优惠值同样非常敏感。绝对值优惠与相对值优惠之间存在一种关系。</p>
<h3 id="4-2-行为生命周期理论——心理账户在消费领域的应用"><a href="#4-2-行为生命周期理论——心理账户在消费领域的应用" class="headerlink" title="4.2 行为生命周期理论——心理账户在消费领域的应用"></a>4.2 行为生命周期理论——心理账户在消费领域的应用</h3><p>经典的生命周期假说和持久收入假说是凯恩斯以后消费函数理论最重要的发展，但他们的理论是建立在完全理性人的假设之上的。例如，生命周期假说就认为:人总是能够深谋远虑，在任何时候都会考虑几十年以后的长远利益，并站在这种高度，根据一生的总财富来合理安排一生中每个阶段的消费，使一生的总效用达到最大。这显然和人们实际的消费行为不符，这种过于理性化的理论也无法解释现实中的许多经济现象。</p>
<p>1988年Shefrin和Thaler提出行为生命周期理论(behavior life cycle hypothesis)修正了传统的生命周期假说，使之能更好地描述现实中人们的消费行为。行为生命周期理论的两个最重要的概念是自我控制和心理账户。</p>
<p>行为生命周期理论引入“心理账户”理论解释消费行为。消费者根据生命周期不同财富的来源和形式，将它们划分为三个心理账户:现期可花费的现金收入账户(I)，现期资产账户(A)和未来收入账户(F)。行为生命周期理论认为:不同账户的财富对消费者的决策行为是不同的。现金收入账户消费的诱惑力最大，因此，将这个账户的收入不消费而储蓄起来的心理成本也最大;现期资产账户的诱惑力和储蓄的心理成本居中;未来收入账户的诱惑力和储蓄的心理成本最小。由于不同的心理账户对消费者的诱惑不同，所以，消费者倾向于较多地通过现金收入账户消费，而较少通过现期资产账户消费，几乎不通过未来收入账户消费。不仅不同的心理账户对消费者的诱惑是不同的，而且同一个心理账户，其中的财富余额不同，对消费者的诱惑也不同。财富余额越多，诱惑越大。</p>
<p>行为生命周期理论的消费函数可表示为$C=f(I,A,F)$，且有:$1≈C/I&gt;C/A&gt;C/F≈0$。这就是说，现金收入账户的边际消费倾向最大，接近于1;现期资产账户次之;未来收入账户最小，接近0。和生命周期持久收入假说的消费函数相比，行为生命周期理论在分析消费者行为时强调的是心理方面的因素，这些心理因素主要是通过心理账户加以描述。所以，心理账户的划分及其性质是理解行为生命周期理论的关键。</p>
<h3 id="4-3-关于消费预算的研究"><a href="#4-3-关于消费预算的研究" class="headerlink" title="4.3 关于消费预算的研究"></a>4.3 关于消费预算的研究</h3><p>1994年Heath和Soll发现，消费者有为不同的消费支出账户设置心理预算的倾向，并且严格控制该项目支出不超过合适的预算。例如，每个月的娱乐支出300元，每个月的日常餐饮消费1000元等。如果一段时间购买同一支出项目的总消费额超过了预算，人们会停止购买该类产品。即使在同一个消费项目中，不同的消费有不同的预算标准，同是娱乐消费，看电影的消费是200元人民币，买一本武打小说的消费是50元人民币。他们通过实验证明:人们当前在某一类项目的消费支出会减少他们未来在同一类项目的支出，而对其他项目的支出几乎没有什么影响。这是心理账户对每个消费项目会设定一个预算控制。</p>
<p>1996年，Chip和Soll研究认为，心理账户通过心理预算调节人们的消费行为。表现在:人们会为不同的消费设置预算，但预算通常会低估或者高估购买特定商品的价格，因此常使人们产生“穷鬼”和“大富翁”的认知错觉，从而出现消费不足和过度消费的消费误区。他们通过三个实验证明了心理账户的分类预算对消费决策的重要作用。</p>
<p>2006年，EldarShafir和RichardThaler发表Investnow,drinklater,spendnever一文，研究表明:在购买和消费暂时分离的商品交易中，人们会建构多种框架的心理账户。奢侈品的购买更多的被认为是一种“投资”而不是一种消费，因此，当消费很早以前购买的高档产品时，通常被编码为“免费”的或者是储蓄。但如果消费方式不是按原意愿进行时，对该产品的消费预算就会发挥作用。</p>
<h3 id="4-4-行为资产组合理论（BPT）——心理账户在金融投资领域的应用"><a href="#4-4-行为资产组合理论（BPT）——心理账户在金融投资领域的应用" class="headerlink" title="4.4 行为资产组合理论（BPT）——心理账户在金融投资领域的应用"></a>4.4 行为资产组合理论（BPT）——心理账户在金融投资领域的应用</h3><p>心理账户在金融投资决策领域最广泛的应用是投资组合结构的运用。根据理性投资组合理论，投资者应该只关心他们投资组合的期望收益，而不应该关注某个特定投资部分的收益。可事实相反，投资者倾向于把他们的资金分成安全账户(保障他们的财富水平)和风险账户(试图作风险投机的买卖)。</p>
<p>1997 年 Fisher 和 Statman 提出:人们在投资时会把 资金分别放在不同的投资账户中，即使是基金公司 也建议投资者建立一个资产投资的金字塔，把现金放在金字塔的最低层，把基金放在中间层，把股票放在金字塔的最高层。2000年，Shefrin和Statman提出了行为资产组合理论（Behavioral portfolio theory，BPT-MA）下图就是一个典型的分层金字塔结构，从底端到顶端是按照其风险程度由低到高排列的，从右到左是按其收入价值由低到高 的顺序排列[14]。模型中的每层是根据安全性、潜力 性和期望值这三者相关的投资需求设计的。底层是 为投资者提供安全性而设计的证券，包括货币市场 基金和银行存款保证，上一层是债券，再上一层是 股票和房地产。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-12-20 上午10.04.40.png" alt="屏幕快照 2017-12-20 上午10.04.40"></p>
<p>在行为金融理论中，行为投资组合理论是建立 在卡尼曼和特维尔斯基的前景理论之上的一个框架 体系。它认为投资者的资产结构应该是金字塔式的 分层结构(这里的层就是心理账户)，投资者对其 资产分层进行管理，每一层对应投资者的一个目标。 底层是投资者为避免贫穷而设立的，所以，其投资 对象通常是短期国债、大额可转让存单、货币市场 基金等有稳定收益、风险小的证券;高层是为使其 富有而设立的，其投资对象通常是外国股票、成长 性股票、彩票等高风险、高收益证券。Shefrin 和 Statman 设计了投资者只有一个心理账户和两个心 理账户的行为资产组合模型，并给出了模型的最优 解。当投资者有两个心理账户时，他们分别在低期 望水平和高期望水平两个心理账户建立投资模型， 并在两个账户之间分配资金。</p>
<p>此外，巴比雷斯和黄明(Barberis and Ming Huang)于 2001 年发表了题为“心理账户、损失规 避与个股回报”的论文，提出了一个较为完整的、 具体的刻画投资者心态的投资模型。并研究了在 两种心理账户下公司股票的均衡回报:一种是投资 者只对所持有的个股价格波动损失规避;另一种是 投资者对所持有的证券组合价格波动损失规避。该 模型在结合心理学、信息学和社会学研究成果的基 础上，对投资者与外部信息之间的互动关系做了崭 新的诠释，对投资者的心态及其决策过程做了具体 的刻画。为人们对投资决策的研究和资产定价的研 究提供了新的思路。</p>
]]></content>
      
        <categories>
            
            <category> 日知录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 心理账户 </tag>
            
            <tag> 卡尼曼 </tag>
            
            <tag> 行为经济学 </tag>
            
            <tag> 理查德·萨勒 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[日知录（2）：前景理论]]></title>
      <url>/2017/12/10/%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E5%89%8D%E6%99%AF%E7%90%86%E8%AE%BA/</url>
      <content type="html"><![CDATA[<p>所谓决策，就是在几个方案中选择一个方案，分为风险决策和非风险决策。在非风险决策中，各个方案的结果都是确定的。在风险决策中，有的方案有的结果是不确定的，即可能发生，也可能不发生。我们这里谈及的就是风险决策。</p>
<a id="more"></a>
<p>一般来说，风险决策研究有两个途径：</p>
<ul>
<li>一个是规范性途径，其基本问题是：人类的风险决策应该遵循怎样的规则？</li>
<li>一个是描述性途径，其基本问题是：人类的风险决策实际遵循怎样的规则？</li>
</ul>
<p>规范性途径往往被叫做决策逻辑学，描述性途径往往被叫做决策行为学或决策心理学。我们这里关注的是描述性途径。</p>
<p>关于风险决策的最早理论是期望效用理论，它最初只是规范性理论，但经济学家们逐渐把它当做描述性理论来使用——把它作为经济学中对人的决策行为的基本假定，即认为人的实际决策行为遵循期望效应理论，直到上世纪70年代处有的著名经济学家（如Arrow）仍然采取这种做法。</p>
<p>自1979年以来，Kahneman &amp; Tversky的一系列著作，对期望效用理论作为描述性理论的有效性提出了严峻的挑战，并提出前景理论作为合适的描述理论。由于前景理论能够精确解释、预言许多风险决策行为，它的影响越来越大；后来Kahneman把它应用于经济学领域，对经济学产生了深远的影响，为此，Kahneman于2002年获得了诺贝尔经济学奖。</p>
<p>这里首先介绍期望效用理论；然后介绍Kahneman等以实验事实对期望效用理论作为描述性理论的批评；最后介绍前景理论的主要内容。</p>
<h2 id="一、期望效用理论"><a href="#一、期望效用理论" class="headerlink" title="一、期望效用理论"></a>一、期望效用理论</h2><p>期望效用函数理论是20世纪50年代，冯·纽曼和摩根斯坦（von Neumann and Morgenstem）在公理化假设的基础上，运用逻辑和数学工具，建立了不确定条件下对理性人（rational actor）选择进行分析的框架。不过，该理论是将个体与群体合而为一的。后来，阿罗和德布鲁（Arrow and Debreu）将其吸收进瓦尔拉斯均衡的框架中，成为处理不确定决策问题的分析范式，进而构建起现代微观经济学并由此展开的包括宏观、金融、计量等在内的宏伟而又优美的理论大厦。</p>
<p>期望效用理论本来是作为规范性理论提出的，但后来在许多经济学著作中被应用为描述性理论，直到上个世纪70年代初仍然如此。</p>
<p>经济学上所使用的期望效用理论包括三方面的内容：Bayes框架；Savage公理；Bernoulli原则。它们分别由不同的人在不同的时期创建。以下分述之。</p>
<h3 id="1-1-Bayes框架"><a href="#1-1-Bayes框架" class="headerlink" title="1.1 Bayes框架"></a>1.1 Bayes框架</h3><p>早在1662年，Antoine Arnauld就写道：决定一个人必须做什么以获得好处或避免坏处，不仅必须考虑好处和坏处本身，而且必须考虑它发生或不发生的概率。</p>
<p>大约在100年之后出版的Bayes的遗著（1763）年把这个思想系统化、精确化，形成了所谓的风险决策的Bayes框架，其核心思想可以概括为两点：</p>
<ol>
<li>$ED(A_i)=\sum _jP_{ij}·D_{ij}$：即行动$A_i$的估计渴望度（estimated desirability，简称ED；后来改称期望效用，expected utility，EU）等于它的各个可能结果的渴望度$D_{ij}$乘以该可能结果出现的概率所得的积的和；或者说，行动$A_i$的估计渴望度等于它的各个结果的渴望度的加权和，权重为各个结果的概率。（注意，前提是各个可能结果互不相容。）</li>
<li>根据贝叶斯原则进行选择：选择有着最大估计渴望度的一个方案。</li>
</ol>
<p>请注意到Bayes决策框架所包含的两点假设：</p>
<ul>
<li>决策权重=概率本身</li>
<li>渴望度（效用）不依赖于参考点。Bayes决策框架并没有要求以一个参考点来衡量渴望度（效用）；实际上，在经济学中人们往往以财富的最终状态来计算效用。</li>
</ul>
<h3 id="1-2-Savage公理"><a href="#1-2-Savage公理" class="headerlink" title="1.2 Savage公理"></a>1.2 Savage公理</h3><p>Bayes决策框架一直沿用下来，并有进一步的发展。</p>
<p>Von Neumann &amp; Morgenstern（1944）发展出关于偏好（选择）的公理系统；而Ramsey（1931）和Savage（1954）继续发展了该公理系统，用主观概率代替客观概率，从而使概率理论和决策理论可以适用于更广泛的事情。Savage公理系统是这些公理系统中最为成熟的。其中的公理有：</p>
<ul>
<li>不变性公理（Invariance Axiom）：方案间的偏好顺序不依赖于方案的描述方式。</li>
<li>优势性公理（Dominance Axiom）：如果方案A在每个方面至少跟方案B一样好，而在至少一个方面比B更好，那么A应该比B更可取。</li>
<li>如果方案B由于方案A，那么它们与任一概率$p≠0$的结合所得的$(B,p)$一定优于$(A,p)$</li>
</ul>
<p>这些公理与直觉十分一致；也与Bayes决策框架完全一致。</p>
<h3 id="1-3-Bernoulli原则"><a href="#1-3-Bernoulli原则" class="headerlink" title="1.3 Bernoulli原则"></a>1.3 Bernoulli原则</h3><p>很早以前Bernoulli就指出人们通常是风险回避的。一个决策是风险回避的，是指：按照结果的表面值（如金额）计算，在确定的结果与有着相等或更高的期望值的不确定的结果之间，决策者选择了确定的结果，如，A. 确定得到80元；B.81%的可能得到100元，按结果的表面值计算，B的期望值高于A，按理应该选择B，但事实上大多数人们会选择A。</p>
<p>为了解释这个现象，Bernoulli提出：人们评价方案，不是用方案的金钱结果值，而是用这些金钱结果值的主观价值，而这个主观价值对于金钱值的函数曲线（效用曲线）是一条凹形的曲线，即$u’’(x)&lt;0$。通俗地说，随着x的不断增大，u的增长越来越慢。用这个原理能轻易解释上段提及的现象。假设80元的主观价值是72，由此可以推出100元的主观价值应该小于90（因为前面80元中每20元的主观价值是18，根据u增长越来越慢的原理，100元超出80元的那20元的主观价值应该小于18，因而100元的主观价值小于90），假设是85.于是，A方案的期望值是72，而B的期望值是$85*81%=68.85$。所以大多数人们选择了A方案。</p>
<p>于是，风险回避和$u’’(x)&lt;0$也成了一些经济学著作对人的决策行为的假定之一。</p>
<h3 id="1-4-小结与问题"><a href="#1-4-小结与问题" class="headerlink" title="1.4 小结与问题"></a>1.4 小结与问题</h3><p>经济学理论中常常把期望效用理论的上述Bayes框架，Savage公理和Bernoulli原则中的全部或部分作为对人的风险决策行为的基本假定，在此假定和其他假定的基础上构建经济学理论。</p>
<p>这种做法有明显的方法论问题：这些经验假定（经验命题）的真假并没有经过经验方法的系统判明。逻辑学家们提出Bayes框架，Savage公理的初衷是：高斯人们应该怎样决策，或者说怎样决策才是合乎理性的；他们并未考察人们的决策行为是否恰好符合这些规则。而Bernoulli原则知识根据某类风险决策事实归纳得出；并未建立在对各种类型的风险决策事实的全面考察上。人们的实际决策行为是否遵循上述规则，显然是个经验命题；而一个经验命题只有通过经验的方法才能判明它的真伪。而经济学家们，未经经验方法的判定，就把上述规则当做经验命题来使用，这种做法就存在方法论上的问题。</p>
<p>这种方法论上的问题使得采用期望效用理论作为描述性理论可能是错误的。而Kahneman &amp; Tversky的一系列实验表明：采用期望效用理论作为描述性理论确实是错误的。</p>
<h2 id="二、对期望效用理论的实验挑战"><a href="#二、对期望效用理论的实验挑战" class="headerlink" title="二、对期望效用理论的实验挑战"></a>二、对期望效用理论的实验挑战</h2><p>Kahneman &amp; Tversky的一系列实验对期望效用理论作为描述理论的有效性提出了挑战。下面是他们的部分实验。</p>
<h3 id="2-1-实验一"><a href="#2-1-实验一" class="headerlink" title="2.1 实验一"></a>2.1 实验一</h3><blockquote>
<p>期望效用理论的“决策权重=概率”成立吗？</p>
<p>问题一：<br>请选择：</p>
</blockquote>
<ul>
<li>A. 有80%的可能性得到4000元； 【20%】</li>
<li>B. 确定地得到3000元。【80%】</li>
</ul>
<p>方括号指的是该项被试的百分比，以下皆同。</p>
<blockquote>
<p>问题二：</p>
</blockquote>
<p>请选择：</p>
<ul>
<li>C. 有20%的可能性得到4000元；【65%】</li>
<li>D. 有25%的可能性得到3000元。【35%】</li>
</ul>
<blockquote>
<p>分析：</p>
</blockquote>
<p>如前1.1 所述，期望效用理论中，决策权重=概率。假设这点成立，那么有：</p>
<script type="math/tex; mode=display">U(A)=0.80*u(4000);</script><script type="math/tex; mode=display">U(B)=u(3000)</script><script type="math/tex; mode=display">U(C)=0.2*u(4000)</script><script type="math/tex; mode=display">U(D)=0.25*u(3000)</script><p>注意到$U(A),U(B)$分别乘以0.25就相应得到$U(C),U(D)$，所以有：<br>如果$U(A)<u(b)$，那么$u(c)<u(d)$；如果$u(a)>U(B)$，那么$U(C)&gt;U(D)$。即A、B之间的偏好顺序应该和C、D之间的偏好顺序相同。</u(b)$，那么$u(c)<u(d)$；如果$u(a)></p>
<p>但上述实验结果却是：虽然在A、B之间大部分偏好B；但在C、D之间大部分偏好C。即在集体水平上被试的偏好发生了逆转。另外，根据Kahneman &amp; Tversky的实验报告，在个体水平上，超过半数的被试的偏好也发生了逆转。</p>
<p>实验结果与假设推论相矛盾。可见，期望效用理论中的“决策权重=概率”这一假定并不符合人的真实的决策行为。</p>
<p>另外，由于C就是25%概率的A，D就是25% 的B，按照期望效用理论的替代性公理，A、B之间的偏好关系，与C、D之间的偏好关系相同，实验结果却是偏好发生了逆转。可见，期望效用理论的替代性公理在人们的实际行为中也不成立。</p>
<h3 id="2-2-实验二"><a href="#2-2-实验二" class="headerlink" title="2.2 实验二"></a>2.2 实验二</h3><blockquote>
<p>期望效用理论的不变性公理成立吗?</p>
</blockquote>
<p>下面是许多人都熟知的一个实验，但也许并非大家都知道其理论蕴涵。</p>
<blockquote>
<p>问题一：想象有一场流行病，预计将杀死600人。有两个预防方案，你希望哪个方案被征服采纳？</p>
</blockquote>
<ul>
<li>如果A方案被采纳，200人将被挽救； 【72%】</li>
<li>如果B方案被采纳，有 1/3 的可能性 600 人都被挽救，有 2/3 的可能性没有人能得到挽救。  【28%】</li>
</ul>
<blockquote>
<p>问题二：</p>
</blockquote>
<ul>
<li>如果方案 C 被采纳，400 人将死亡;  【22%】</li>
<li>如果方案 D 被采纳，有 1/3 的可能性没有人死亡并且有 2/3 的可能性 600 人都会死亡。  【78%】</li>
</ul>
<blockquote>
<p>分析</p>
</blockquote>
<p>期望效用理论的不变性公理认为:人们的选择不因描述方式不同而改变。 假设这点成立，那么，因为方案 A 等价于方案 C，而 B 等价于 D，所以 A,B 之间的偏好顺序与 C,D 之间的偏 好顺序应该是一样的。但事实上它们是相反的。假设有误。这说明人的决策行为并不遵循期望效用理论的不变性公理。</p>
<p>另外，上述偏好逆转发生的原因在于:在问题 1 中，被试被诱导把 600 人都会死作为参 考点，从而把问题表征为确定的 200 人被挽救与 1/3 可能的 600 人被挽救之比较;而在问题 2 中，被试被诱导把 0 人死亡作为参考点，从而把问题表征为确定的 400 人死亡与 2/3 可能 的 600 人死亡。问题表征上的这种不同进而导致了决策上的不同。由此我们看到:决策中， 人们并非以财富数值或生命个数的最终状态来计算主观价值，而是根据财富数值或生命个数 相对于参考点的值来计算主观价值。这也违反了期望效用理论。</p>
<h3 id="2-3-实验三"><a href="#2-3-实验三" class="headerlink" title="2.3 实验三"></a>2.3 实验三</h3><blockquote>
<p>期望效用理论的优势性公理成立吗?</p>
<p>问题一：<br>请选择</p>
</blockquote>
<ul>
<li>E.有25%的机会获得240元并有75%的机会失去760元； 【0%】</li>
<li>F. 有 25%的机会获得 250 元并有 75%的机会失去 750 元 。【100%】</li>
</ul>
<blockquote>
<p>问题二:</p>
</blockquote>
<p>假想你面临两组选择。请先仔细阅读两组选择，然后作出选择:</p>
<p>第一组选择：</p>
<ul>
<li>A. 固定得到 240 元【84%】</li>
<li>B. 25%的机会得到 1000 元，75%的机会得到 0 元。【16%】</li>
</ul>
<p>第二组选择：</p>
<ul>
<li>C.固定失去 750 元；【13%】</li>
<li>D.75%的机会失去 1000 元，25%的机会失去 0 元 【87%】</li>
</ul>
<blockquote>
<p>分析:</p>
</blockquote>
<p>期望效用理论的优势性公理认为:如果方案 A 在每个方面至少跟方案 B 一样好，而在至少一个方面比 B 更好，那么 A 应该比 B 更可取。假设这点成立，那么，由于 A 优于 B, D 优于 C，那么，A+D 应该优于 B+C;而注意到 A+D=E， B+C=F;所 以，E 应该优于 F。但事实相反，所有的被试都认为 F 优于 E。可见，人们的决策行为并不 遵循期望效用理论的优势性公理。</p>
<h3 id="2-4-实验四"><a href="#2-4-实验四" class="headerlink" title="2.4 实验四"></a>2.4 实验四</h3><blockquote>
<p>期望效用理论的“风险回避”原则总是成立吗?</p>
<p>问题一:</p>
</blockquote>
<p>请选择:</p>
<ul>
<li>A. 确定得到 1 元;</li>
<li>B. 1%的可能性得到 100 元，99%的可能性得到 0 元。 </li>
</ul>
<p>人们大多会选择 B.</p>
<blockquote>
<p>问题二：</p>
</blockquote>
<ul>
<li>C.确定失去 90 元;</li>
<li>D.有 90%的可能性失去 100 元，有 10%的可能性失去 0 元。</li>
</ul>
<p>人们大多会选择 D.</p>
<blockquote>
<p>分析</p>
</blockquote>
<p>经济学家所用的期望效用理论的“风险回避”原则认为:在按表面结果 值计算有着相等期望值的风险选项与确定选项之间，人们往往会选择确定项而回避风险项。但在上面这些决策问题上，大多数人们风险寻求，而非风险回避。可见期望效用理论的 “风险回避”原则在人们的实际决策行为中并不总是成立。</p>
<p>事实上，Kahneman 和 Tversky 的研究发现人们的风险态度有如下的模式:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>中、大概率</th>
<th>小概率</th>
</tr>
</thead>
<tbody>
<tr>
<td>得</td>
<td>风险回避</td>
<td>风险寻求</td>
</tr>
<tr>
<td>失</td>
<td>风险寻求</td>
<td>风险回避</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三、Kahneman和Tversky的前景理论"><a href="#三、Kahneman和Tversky的前景理论" class="headerlink" title="三、Kahneman和Tversky的前景理论"></a>三、Kahneman和Tversky的前景理论</h2><p>上述实验事实表明：人们的风险决策行为系统地偏离了期望效用理论。那么人们的风险决策行为究竟遵循怎样的规律呢？Kahneman和Tversky提出了前景理论（prospect theory）作为上述问题的答案。</p>
<p>前景理论的核心内容是：价值函数（曲线）与权重函数（曲线）。</p>
<h3 id="3-1-价值函数（曲线）"><a href="#3-1-价值函数（曲线）" class="headerlink" title="3.1 价值函数（曲线）"></a>3.1 价值函数（曲线）</h3><p>价值函数把表面价值如金额转化为决策价值，其具体形式是：</p>
<script type="math/tex; mode=display">
v\left(x\right)=\left\{\begin{array}{l}
    x^a \ (x>0)\\
    -\lambda\left(-x\right)^{\beta}\ (x<0)\
\end{array}\right.</script><p>其中x是表面价值如金额的得失，得为正，失为负；v为决策价值。在Kahneman &amp; Tversky的一个研究中，被试者们的中位参数$\alpha = \beta =0.88;\gamma = 2.25$；其他研究者也得到了相近的数值。</p>
<p>其曲线如下图所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15135751972602.jpg" alt=""></p>
<p>它有如下特点：</p>
<ul>
<li>其定义域不是财富，而是财富的变化、得失，即财富相对于某个参考点的差距，这个参考点往往是当前的财富状态。</li>
<li>整个函数是个递增函数；$v(0) = 0$</li>
<li>在得（Gains）的区域呈凹形，在失（LOSSES）的区域呈凸形（即当$x&gt;0$时，$v’’(x)<0$;当$x<0$时，$v''(x)>0$）。用通俗的话来说就是，随着$|x|$的不断增大，$v$的变化越来越小。例如，从0元到10元所引起的价值上的变化，要大于从100元到110元所引起的价值上的变化。</0$;当$x<0$时，$v''(x)></li>
<li>在失的区域的曲线比在得的区域的曲线更加陡峭。通俗地说，失去100元所带来的痛苦的程度要大于100元所带来的快乐的程度。</li>
</ul>
<h3 id="3-2-权重函数（曲线）"><a href="#3-2-权重函数（曲线）" class="headerlink" title="3.2 权重函数（曲线）"></a>3.2 权重函数（曲线）</h3><p>权重函数把概率转化为决策权重。当风险前景为两个结果时，其具体的形式如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-12-18 下午1.42.45.png" alt="屏幕快照 2017-12-18 下午1.42.45">    其中，p为概率，$w^+(p)$为得到时的决策权重，$w^-(p)$为失去时的决策权重。在Kahneman的一个实验中，求得被试们的中位参数$\lambda = 0.61,\delta =0.69$。其他研究者也得到相近的结果。</p>
<p>其曲线如下图所示：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-12-18 下午1.50.18.png" alt="屏幕快照 2017-12-18 下午1.50.18"></p>
<ul>
<li>其中的$w^+$为得到时的权重，$w^-$为失去时的权重。当概率较小时，同等概率下，$w^+$大于$w^-$；当概率中等或较大时，同等概率下，$w^+$小于$w^-$</li>
</ul>
<p>无论$w^+$还是$w^-$都有以下特点：</p>
<ul>
<li>$w(0)=0$；$w(1)=1$；$w$是$p$的递增函数。</li>
<li>给予小概率以过高的权重（注意，并非高估小概率，因为被试清除地直到概率是多少；只是小概率所对应的权重高于高于概率本身）</li>
<li>给予中、高概率以较小的权重，即低于概率自身的权重。</li>
<li>次确定性：$w(p)+w(1-p)&lt;1$，其中$0&lt;p&lt;1$</li>
<li>次比例性：$\frac{w(pq)}{w(p)}≤\frac{w(pqr)}{w(pr)}$</li>
<li>次可加性，包括：<ul>
<li>下端次可加性：$w(p+q)≤w(p)+w(q)$，其中$p+q&lt;1$</li>
<li>上端次可加性：$w(1)-w(1-p)≥w(p+q)-w(q)$，其中$p≠0$</li>
</ul>
</li>
</ul>
<p>计算各个前景的价值，比较并作出选择。应用$v(x)$函数和$w(p)$函数，就能求出各个结果的$v$值和$w$值；把这些$v$值和$w$值代入$V$公式，就能求出各个前景的V值：</p>
<script type="math/tex; mode=display">V=\sum^n_{i=-m}w_iv(x_i)</script><p>其中V为一个前景的心理价值；$w_i$和$v(x_i)$分别是第i个结果的决策权重和决策价值；结果分得失即正负，所以结果的下标也分正负。然后比较各个前景的V值，选出最高V值的前景</p>
<p>以上就是前景理论所描述的人们的决策行为规律。</p>
<p>用上述规律，不仅能够轻易而一致地解释前面提到的各种违背期望效用理论的实验现象；还能很好地解释和预言大量的其他实验现象。Kahneman后来把这个理论应用于经济学领域，对经济学的微观基础产生了巨大的影响，为此，Kahneman获得了2002年的诺贝尔经济学奖。</p>
<h2 id="四、通俗解释"><a href="#四、通俗解释" class="headerlink" title="四、通俗解释"></a>四、通俗解释</h2><p>在《赌客信条》一书中，作者孙惟微将前景理论归纳为5句话：</p>
<ul>
<li>“二鸟在林，不如一鸟在手”，在确定的收益和“赌一把”之间，多数人会选择确定的好处。所谓“见好就收，落袋为安。称之为“确定效应”。</li>
<li>在确定的损失和“赌一把”之间，做一个抉择，多数人会选择“赌一把”。称之为“反射效应”。</li>
<li>白捡的100元所带来的快乐，难以抵消丢失100元所带来的痛苦。称之为“损失规避”。</li>
<li>很多人都买过彩票，虽然赢钱可能微乎其微，你的钱99.99%的可能支持福利事业和体育事业了，可还是有人心存侥幸搏小概率事件。称之为“迷恋小概率事件”。</li>
<li>多数人对得失的判断往往根据参照点决定，举例来说，在“其他人一年挣6万元你年收入7万元”和“其他人年收入为9万元你一年收入8万”的选择题中，大部分人会选择前者。称之为“参照依赖”。</li>
</ul>
<h3 id="4-1-确定效用"><a href="#4-1-确定效用" class="headerlink" title="4.1 确定效用"></a>4.1 确定效用</h3><p>所谓确定效应（certainty effect），就是在确定的好处（收益）和“赌一把”之间，做一个抉择，多数人会选择确定的好处。用一个词形容就是“见好就收”，用一句话打比方就是“二鸟在林，不如一鸟在手”，正所谓落袋为安。</p>
<p>让我们来做这样一个实验。</p>
<ul>
<li>A.你一定能赚30000元。</li>
<li>B.你有80%可能赚40000元，20%可能性什么也得不到。<br>你会选择哪一个呢？实验结果是，大部分人都选择A。</li>
</ul>
<p>传统经济学中的“理性人”这时会跳出来批判：选择A是错的，因为40000×80%=32000，期望值要大于30000。这个实验结果是对“原理1”的印证：大多数人处于收益状态时，往往小心翼翼、厌恶风险、喜欢见好就收，害怕失去已有的利润。卡尼曼和特韦斯基称为“确定效应”（certainty effect），即处于收益状态时，大部分人都是风险厌恶者。</p>
<p>“确定效应”表现在投资上就是投资者有强烈的获利了结倾向，喜欢将正在赚钱的股票卖出。投资时，多数人的表现是“赔则拖，赢必走”。在股市中，普遍有一种“卖出效应”，也就是投资者卖出获利的股票的意向，要远远大于卖出亏损股票的意向。这与“对则持，错即改”的投资核心理念背道而驰。</p>
<h3 id="4-2-反射效应"><a href="#4-2-反射效应" class="headerlink" title="4.2 反射效应"></a>4.2 反射效应</h3><p>面对两种损害，你是会选择躲避呢，还是勇往直前？当一个人在面对两种都损失的抉择时，会激起他的冒险精神。在确定的坏处（损失）和“赌一把”之间，做一个抉择，多数人会选择“赌一把”，这叫“反射效应”。用一句话概括就是“两害相权取其轻”。</p>
<p>让我们来做这样一个实验。</p>
<ul>
<li>A.你一定会赔30000元。</li>
<li>B.你有80%可能赔40000元，20%可能不赔钱。</li>
</ul>
<p>你会选择哪一个呢？投票结果是，只有少数人情愿“花钱消灾”选择A，大部分人愿意和命运抗一抗，选择B。</p>
<p>传统经济学中的“理性人”会跳出来说，两害相权取其轻，所以选B是错的，因为（-40000）×80%=-32000，风险要大于-30000元。现实是，多数人处于亏损状态时，会极不甘心，宁愿承受更大的风险来赌一把。也就是说，处于损失预期时，大多数人变得甘冒风险。卡尼曼和特韦斯基称为“反射效应”（reflectioneffect）。</p>
<p>“反射效应”是非理性的，表现在股市上就是喜欢将赔钱的股票继续持有下去。统计数据证实，投资者持有亏损股票的时间远长于持有获利股票。投资者长期持有的股票多数是不愿意“割肉”而留下的“套牢”股票。</p>
<h3 id="4-3-损失规避"><a href="#4-3-损失规避" class="headerlink" title="4.3 损失规避"></a>4.3 损失规避</h3><p>如何理解“损失规避”？用一句话打比方，就是“白捡的100元所带来的快乐，难以抵消丢失100元所带来的痛苦”。前景理论最重要也是最有用的发现之一是：当我们做有关收益和有关损失的决策时表现出的不对称性。对此，就连传统经济学的坚定捍卫者 保罗·萨缪尔森，也不得不承认：“增加100元收入所带来的效用，小于失去100元所带来的效用。”</p>
<p>这其实是前景理论的第3个原理，即“损失规避”（loss aversion）：大多数人对损失和获得的敏感程度不对称，面对损失的痛苦感要大大超过面对获得的快乐感。</p>
<p>行为经济学家通过一个赌局验证了这一论断。</p>
<p>假设有这样一个赌博游戏，投一枚均匀的硬币，正面为赢，反面为输。如果赢了可以获得50000元，输了失去50000元。请问你是否愿意赌一把？请做出你的选择。</p>
<ul>
<li>A.愿意</li>
<li>B.不愿意</li>
</ul>
<p>从整体上来说，这个赌局输赢的可能性相同，就是说这个游戏的结果期望值为零，是绝对公平的赌局。你会选择参与这个赌局吗？但大量类似实验的结果证明，多数人不愿意玩这个游戏。为什么人们会做出这样的选择呢？这个现象同样可以用损失规避效应解释，虽然出现正反面的概率是相同的，但是人们对“失”比对“得”敏感。想到可能会输掉50000元，这种不舒服的程度超过了想到有同样可能赢来50000元的快乐。由于人们对损失要比对相同数量的收益敏感得多，因此即使股票账户有涨有跌，人们也会更加频繁地为每日的损失而痛苦，最终将股票抛掉。一般人因为这种“损失规避”（loss aversion），会放弃本可以获利的投资。</p>
<h3 id="4-4-迷恋小概率事件"><a href="#4-4-迷恋小概率事件" class="headerlink" title="4.4 迷恋小概率事件"></a>4.4 迷恋小概率事件</h3><p>买彩票是赌自己会走运，买保险是赌自己会倒霉。这是两种很少发生的事件，但人们却十分热衷。前景理论还揭示了一个奇特现象，即人类具有强调小概率事件的倾向。</p>
<p>何谓小概率事件？就是几乎不可能发生的事件。如天上掉馅饼，这就是个小概率事件。掉的是馅饼固然好，但如果掉下来的不是馅饼而是陷阱呢？当然也属于小概率事件。</p>
<p>面对小概率的赢利，多数人是风险喜好者。面对小概率的损失，多数人是风险厌恶者。<br>事实上，很多人都买过彩票，虽然赢钱可能微乎其微，你的钱99.99%的可能支持福利事业和体育事业了，可还是有人心存侥幸搏小概率事件。</p>
<p>同时，很多人都买过保险，虽然倒霉的概率非常小，可还是想规避这个风险。人们的这种倾向，是保险公司经营下去的心理学基础。</p>
<p>在小概率事件面前人类对风险的态度是矛盾的，一个人可以是风险喜好者，同时又是风险厌恶者。传统经济学无法解释这个现象。</p>
<p>小概率事件的另一个名字叫运气。侥幸，就是企求好运，邀天之幸。孔子很反感这种事，他说：“小人行险以侥幸。”庄子认为孔子是个“灯下黑”，他借盗跖之口评价孔子：“妄作孝弟，而侥幸于封侯富贵者也。”对小概率事件的迷恋，连圣人也不能免俗。</p>
<p>前景理论指出，在风险和收益面前，人的“心是偏的”。在涉及收益时，我们是风险的厌恶者，但涉及损失时，我们却是风险喜好者。但涉及小概率事件时，风险偏好又会发生离奇的转变。所以，人们并不是风险厌恶者，他们在他们认为合适的情况下非常乐意赌一把。归根结底，人们真正憎恨的是损失，而不是风险。</p>
<p>这种损失厌恶而不是风险厌恶的情形，在股市中常常见到。比如，我们持有一只股票，在高点没有抛出，然后一路下跌，进入了彻彻底底的下降通道，这时的明智之举应是抛出该股票，而交易费用与预期的损失相比，是微不足道的。</p>
<p>扪心自问，如果现在持有现金，还会不会买这只股票？你很可能不会再买吧，那为什么不能卖掉它买别的更好的股票呢？也许，卖了它后损失就成了“事实”吧。</p>
<h3 id="4-5-参照依赖"><a href="#4-5-参照依赖" class="headerlink" title="4.5 参照依赖"></a>4.5 参照依赖</h3><p>假设你面对这样一个选择：在商品和服务价格相同的情况下，你有两种选择：</p>
<ul>
<li>A.其他同事一年挣6万元的情况下，你的年收入7万元。</li>
<li>B.其他同事年收入为9万元的情况下，你一年有8万元进账。</li>
</ul>
<p>卡尼曼的这调查结果出人意料：大部分人选择了前者。事实上，我们拼命赚钱的动力，多是来自同侪间的嫉妒和攀比。我们对得与失的判断，是来自比较。嫉妒总是来自自我与别人的比较，培根曾言：皇帝通常不会被人嫉妒，除非对方也是皇帝。对此，美国作家门肯早有妙论：“只要比你小姨子的丈夫（连襟）一年多赚1000块，你就算是有钱人了。”</p>
<p>传统经济学认为金钱的效用是绝对的，行为经济学则告诉我们，金钱的效用是相对的。这就是财富与幸福之间的悖论。到底什么是“得”，什么是“失”呢？你今年收入20万元，该高兴还是失落呢？假如你的奋斗目标是10万元，你也许会感到愉快；假如目标是100万元，你会不会有点失落呢？所谓的损失和获得，一定是相对于参照点而言的。卡尼曼称为“参照依赖”（Reference Dependence）。</p>
<p>老张最幸福的时候是他在20世纪80年代做“万元户”的时候，虽然现在自己的村镇已经改造成了城市，拆迁补贴也让自己成为了“百万元户”，但他感觉没有当年兴奋，因为邻里都是“百万元户”了。</p>
<p>讲这个故事的用意不难明白，我们就不再进行烦琐的论证了 得与失都是比较出来的结果。传统经济学的偏好理论（Preference theory）假设，人的选择与参照点无关。行为经济学则证实，人们的偏好会受到单独评判、联合评判、交替对比及语意效应等因素的影响。</p>
<p>参照依赖理论：多数人对得失的判断往往根据参照点决定。一般人对一个决策结果的评价，是通过计算该结果相对于某一参照点的变化而完成的。人们看的不是最终的结果，而是看最终结果与参照点之间的差额。一样东西可以说成是“得”，也可以说成是“失”，这取决于参照点的不同。非理性的得失感受会对我们的决策产生影响。</p>
<p>综上，前景理论引申出五个基本结论</p>
<ul>
<li>确定效应：处于收益状态时，多数人是风险厌恶者。</li>
<li>反射效应：处于损失状态时，多数人是风险喜好者。</li>
<li>迷恋小概率事件：面对小概率的赢利，多数人是风险喜好者；面对小概率的损失，多数人是风险厌恶者。</li>
<li>损失规避：多数人对损失比对收益敏感。</li>
<li>参照依赖：多数人对得失的判断往往由参照点决定。</li>
</ul>
<p>简言之，人在面临获利时，不愿冒风险；而在面临损失时，人人都成了冒险家。而损失和获利是相对于参照点而言的，改变评价事物时的参照点，就会改变对风险的态度。</p>
]]></content>
      
        <categories>
            
            <category> 日知录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 卡尼曼 </tag>
            
            <tag> 行为经济学 </tag>
            
            <tag> 前景理论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[日知录（1）：MacTips]]></title>
      <url>/2017/12/05/%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9AMacTips%20/</url>
      <content type="html"><![CDATA[<blockquote>
<p>1.终端输入说英语</p>
</blockquote>
<p>说英语时我们当然希望有标准发音。在Mac中不需要字典，直接在终端里输入say yes，Mac就会说英语了。</p>
<a id="more"></a>
<blockquote>
<p>2.Spotlight快速打开程序</p>
</blockquote>
<p>很多刚开始使用Mac的用户，一般都知道Spotlight检索功能。事实上，用这个功能还可以快速打开程序。通过ctrl+space呼出，或者输入通讯或cont都可以找到通讯录这个程序，回车即可打开。</p>
<blockquote>
<p>3.Spotlight注释功能定位文件</p>
</blockquote>
<p>OS X的文件系统提供了Spotlight注释功能，可以帮助用户更有针对性地定位文件。选中一个文件或文件夹，command+i打开简介，在Spotlight注释功能中加入自己特定的关键词。关掉简介窗口，呼出Spotlight并输入刚才的关键词，可以准确定位到相关的文件或文件夹。</p>
<blockquote>
<p>4.使用sips命令批量处理图片</p>
</blockquote>
<p>如果你想批量修改一批图片（尺寸、旋转、反转等），但你不会或没有PS，可以使用sips命令高效完成这些功能，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">##把当前用户图片文件夹下的所有JPG图片宽度缩小为800px，高度按比例缩放</div><div class="line">sips -Z 800～/Pictures/.JPG</div><div class="line">##顺时针旋转90˚</div><div class="line">sips -r 90～/Pictures/.JPGwdhjf</div><div class="line">##垂直反转</div><div class="line">sips -f vertical ～/Pictures/.JPG</div></pre></td></tr></table></figure>
<p>更多命令可以用sips -h查看。</p>
<blockquote>
<p>5.把当前网页附加到待发送邮件中</p>
</blockquote>
<p>使用Safari浏览网页的时候，如果你想把当前页面通过邮件发送给自己或别人，使用command+i可以直接打开邮件并把当前网页附加到待发送的邮件中。</p>
<blockquote>
<p>6.快速删除文件和清空废纸篓</p>
</blockquote>
<p>在Finder中选中文件，使用command+delete删除文件。如果想彻底清除，使用shift+command+delete就会自动清空废纸篓。</p>
<blockquote>
<p>7.获悉目录空间</p>
</blockquote>
<p>在Mac下想知道某个目录下各个文件和子目录各占多少空间，不需要一个一个去查看。打开终端，在该目录下输入du-sh*，结果一目了然。</p>
<blockquote>
<p>8.英文自动完成</p>
</blockquote>
<p>当使用系统软件文本编辑、Pages和Keynote时，输入英文按esc键，系统会帮助你自动完成单词。比如你想输入“brilliance”，只需输入“brill”，按esc键，系统就会出现自动提示。如果某个应用，比如Safari的搜索框里esc是取消输入，那么使用fn+F5也可以达到这个效果。对于常写英文文档的人比较有帮助。</p>
<blockquote>
<p>9.文件操作</p>
</blockquote>
<p>在Finder中打开文件使用鼠标双击或按command+o键。和Windows不一样的是，选中文件回车是对文件重命名，而不是打开文件。</p>
<blockquote>
<p>10.显示隐藏文件</p>
</blockquote>
<p>在终端里输入ls-a，可以显示该目录下的隐藏文件。在Finder中按shift+command+.键可以显示隐藏文件，想恢复原来的设置，再按一遍shift+command+.即可。</p>
<blockquote>
<p>11.利用你的触发角</p>
</blockquote>
<p>OS X系统为用户提供了强大的Mission Control功能，今天为大家介绍其中的触发角。打开“系统偏好设置”→“Mission Control”→“触发角”，就可以对屏幕的四个角进行设置了。比如把左上角设置为将显示器置为睡眠状态，当我们暂时离开电脑时，顺手把鼠标移到左上角，屏幕就变黑了，非常方便。</p>
<blockquote>
<p>12.维护你的Mac</p>
</blockquote>
<p>Mac的OS X是一个使用起来非常简单的操作系统，一般情况下不需要装杀毒工具，大部分程序安装都非常简单，直接把后缀为app的程序拖进应用程序文件夹就可以了。但是，当你在使用系统时如果发现出现异常，那么就该进行日常维护了。<br>打开磁盘管理，选中你的系统盘，单击“修复磁盘权限”，对磁盘权限进行检查和修复。完成之后还可以手动执行维护脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo periodic daily</div><div class="line">sudo periodic weekly</div><div class="line">sudo periodic monthly</div></pre></td></tr></table></figure>
<p>也可以一次全部执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo periodic daily weekly monthly</div></pre></td></tr></table></figure>
<p>一般执行完这些操作后，你的Mac就会充满活力，可以继续上路了。这些操作可以定期执行。</p>
<blockquote>
<p>13.Mission Control</p>
</blockquote>
<p>设置中把“使窗口按应用程序成组”关掉，Mission Control的行为就会跟10.7以前的Exposé一样，不会把同一个程序的多个窗口叠在一起。对经常一个程序开很多窗口的程序员来说很有用。</p>
<blockquote>
<p>14.截图</p>
</blockquote>
<p>OS X提供了非常方便的截图工具，你可以随时随地截取屏幕画面。<br>shift+command+3：全屏幕截图；shift+command+4：通过鼠标选取截图。截取的图片默认存放在桌面上，以时间命名。系统默认截图格式是png，你可以通过如下命令修改截图文件类型，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.screencapture type -string JPEG</div></pre></td></tr></table></figure>
<blockquote>
<p>15.推荐几个有用的小工具</p>
</blockquote>
<p>◆TotalFinder：Finder的增强插件，Finder的插件，为Finder增加多标签（类似Chrome的多页签）、双面板、UI设置等功能。收费软件。<br>◆Breeze：窗口管理软件，option+1/2/3分别对应最大化窗口/左半屏幕窗口/右半屏幕窗口。收费软件。<br>◆Trillian：整合了MSN、GTalk、Twitter等，表现稳定，用户体验也不错。免费软件，可以从AppStore直接下载。<br>◆smcFancontrol：风扇控制软件，免费。OS X对风扇控制不敏感，CPU温度很高时才会增加风扇转速，那时机器表面已经比较热了。用这个软件可以自由控制风扇转速。夏天空调屋里一般3000～4000转就够了，冬天一般不需要开启。</p>
<blockquote>
<p>16.Mac的原生输入法</p>
</blockquote>
<p>我在Mac下曾经使用过很多输入法，包括百度、FIT、搜狗等，这是因为之前Mac的原生输入法太不给力了。不过现在的版本已经有了很大的改进，慢慢地也变成了常用输入法之一。今天就为大家介绍一些Mac输入法的操作技巧。<br>◆中英文混合输入：输入中文的时候，打开caps lock键，可以直接输入英文，关掉又切换回中文。<br>◆选词：通过-+号可以切换字或词，通过[]可以展开候选词列表并进行切换。<br>◆打开输入法偏好设置，可以设置自动校正模糊音。<br>◆用’可以进行手动分词，比如“fang’an（方案）”。<br>◆使用shift+6可以输入表情符号，比如“（☆_☆）”和“凸^-^凸”。<br>用习惯了，你会离不开这个输入法的。</p>
<blockquote>
<p>17.Safari的标签</p>
</blockquote>
<p>Safari是我在Mac上最常用的浏览器，Chrome也不错，但我更偏爱Safari。今天为大家介绍一下这个浏览器的标签使用。<br>当你想在新的标签页打开网页时，只需要按住command键，单击链接即可。使用Multi-Touch手势在标签页中切换。在触控板上，双指开合即可显示你打开的标签页。在标签视图中，双指轻扫可浏览不同标签页。<br>通过shift+command+左右方向键，可以快速在Safari中打开的标签中进行切换。</p>
<blockquote>
<p>18.监控Mac的运行状况</p>
</blockquote>
<p>◆top：打开终端输入top，可以显示目前系统的进程情况、CPU使用情况、内存使用情况、磁盘使用情况和进程的详细列表等信息，输入“?”会显示帮助信息，参考帮助你还可以自定义top显示的信息，输入“q”退出监控界面。<br>◆htop：htop是更聪明更高级的top，虽然不是Mac原生的，但安装非常方便。打开终端输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo port install htop</div></pre></td></tr></table></figure>
<p>命令结束就安装完成了。然后键入htop，你会看到一个更丰富的彩色的top，多个CPU、内存统计、uptime，以及更详细的进程信息。参考界面最底部的帮助信息还可以进行排序、展开和Kill。输入q退出监控界面。<br>◆系统的活动监视器：这个非常适合不喜欢终端的用户。从应用程序→实用工具可以找到活动监视器，打开后你会发现很类似Windows下的任务管理器，相信这个不需要给大家介绍了。</p>
<blockquote>
<p>19.批量复制文件</p>
</blockquote>
<p>例如你在一个目录下林林总总放了几百个文件，有图片、pdf、zip、doc等，你想把后缀为png、jpeg、gif的图片复制到另一个文件夹去，最简单的方式是什么？不是通过搜索把这些文件找出来，再全选复制到另一个文件夹下。而是进入该目录，执行这样一条命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp *.png *.jpeg *.gif /destpath</div></pre></td></tr></table></figure>
<p>如果想剪切，就把cp改为mv。</p>
<blockquote>
<p>20.程序切换</p>
</blockquote>
<p>在OS X中程序切换可以通过command+tab进行，command+tab进行顺序切换，command+shift+tab进行逆序切换，功能类似Win7的alt+tab。<br>OS X还提供了同组程序的切换，比如你打开了多个预览程序阅读pdf，你想在这些pdf之间切换阅读，这时候就可以使用command+`（esc下面的键）进行同组程序切换。</p>
<blockquote>
<p>21.远程复制</p>
</blockquote>
<p>OS X提供了基于SSH的远程复制命令scp，这个命令大部分Linux和Unix系统都会提供，使用该命令可以非常方便地在两台机器之间安全地复制文件，具体命令为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scp ./testfile.txt  username@10.10.10.22:/tmp</div></pre></td></tr></table></figure>
<p>回车后会要求你输入username的密码，只会将当前目录下的testfile.txt复制到另一台机器的tmp目录下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scp username@10.10.10.22:/tmp/testfile.txt  ./</div></pre></td></tr></table></figure>
<p>从远端复制到本地。</p>
<blockquote>
<p>22.OS X中的ftp</p>
</blockquote>
<p>这个问题有订阅者问过，总结一下，以下三种方式就够用了。<br>◆直接在命令行使用。打开终端输入ftp anonymous@ftp.mozilla.org，或者使用sftp通过SSH完成ftp的功能，例如sftp user@10.10.10.11。<br>◆使用第三方工具。比如FileZilla，用法和Windows类似。<br>◆利用OS X原生FTP工具。从Finder菜单栏中进入“前往”→“连接服务器…”，输入FTP服务器地址,如 ftp://ftp.mozilla.org , 单击地址栏右侧的“+”号按钮可以将当前地址加入“个人收藏服务器”，单击“连接”按钮，按照提示进行身份验证，成功后即可连接到FTP服务器。</p>
<blockquote>
<p>23.备份</p>
</blockquote>
<p>OS X提供了非常方便的备份工具TimeMachine（时间机器），我第一台Mac用的操作系统是Leopard，后来升级到Snow Leopard→Lion→MountainLion，再后来换新机器，但从未重装过系统。这对于Windows系统来说是不可想象的，这都得益于时间机器。我个人每周会备份一次，如果你觉得自己资料非常重要，可以每隔几小时备份一次。具体的用法我就不介绍了，可以参考官方介绍：<a href="http://support.apple.com/kb/HT1427?viewlocale=zh_CN。" target="_blank" rel="noopener">http://support.apple.com/kb/HT1427?viewlocale=zh_CN。</a></p>
<blockquote>
<p>24.inode和history</p>
</blockquote>
<p>◆inode：Mac的文件系统和Windows完全不同，文件所需信息都包含在这个inode（索引节点）里。每个文件都有inode，文件系统用inode来标识文件。简单来说就是inode包含了文件的元数据信息，文件名、文件内容，但不包含任何控制信息。inode是Unix/Linux系列文件系统设计的核心，有兴趣的同学可以上网查阅相关资料。对于普通用户用来，最直观的表现是，在Mac里，你可以对正在使用的文件改名，换目录，甚至放到废纸篓，都不会影响当前文件的使用。<br>◆history：打开终端输入history，所有的历史命令都会显示出来，想找某一条执行过的命令，还可以这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">history | grep apache</div></pre></td></tr></table></figure>
<p>找到左边的命令编号（例如1001），在终端输入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">!1001</div></pre></td></tr></table></figure>
<p>就可以执行原来那条命令了。</p>
<blockquote>
<p>25.Go2Shell</p>
</blockquote>
<p>通过Finder浏览文件的时候，常常需要在浏览的文件目录中打开终端进行操作，Go2Shell能够自动做到这一点。从App Store下载这个免费软件 <a href="https://itunes.apple.com/us/app/go2shell/id445770608?mt=12" target="_blank" rel="noopener">https://itunes.apple.com/us/app/go2shell/id445770608?mt=12</a>  ,下载完成后从应用程序文件夹把Go2Shell拖到Finder工具栏上，然后随便进入一个目录，单击Go2Shell图标，即可打开终端进入该目录。<br>Go2Shell支持原生终端、iTerm2和xterm，在终端输入open-a Go2Shell—args config即可进入配置界面，选择你喜欢的终端。</p>
<blockquote>
<p>26.Safari的阅读器</p>
</blockquote>
<p>Safari的阅读器是浏览器创新之一，在Safari之前，没有其他浏览器提供过这样的功能。当Safari发现结构优良的网页文档时，就会在地址栏右侧显示“阅读器”，单击就可以进入简洁的阅读模式，通过shift+command+r也可以进入。</p>
<p>阅读器已经提供了良好的网页阅读体验，对于分页文档甚至能够自动翻页阅读，但是我们还可以更进一步。比如我就觉得阅读器太窄了，视野不够宽阔。有类似需求的同学就可以通过Safari的扩展插件CustomReader进行个性化定义。</p>
<p>从<a href="http://canisbos.com/customreader" target="_blank" rel="noopener">http://canisbos.com/customreader</a> 下载CustomReader ,双击即可安装。安装之后到任何一个支持阅读器的网页，按下shift+command+r激活阅读器，再用ctrl+r调出配置页面，就可以配置你自己独享的个性化阅读器了。</p>
<blockquote>
<p>27.Remote Desktop Connection for Mac</p>
</blockquote>
<p>很多读者询问如何在Mac中通过远程桌面连接到Windows，这次统一答复一下，微软提供了专门的Remote Desktop Connection for Mac，免费，下载链接：<a href="http://www.microsoft.com/mac/remote-desktop-client。" target="_blank" rel="noopener">http://www.microsoft.com/mac/remote-desktop-client。</a></p>
<blockquote>
<p>28.文档的版本控制</p>
</blockquote>
<p>经常使用Keynote、Pages、Numbers和原生文本编辑器的用户，可以尝试使用文档的版本控制功能。对于经常编写文档的人来说，这个功能非常有用。大家可能没有注意到，当你把鼠标移至文档标题的时候，会出现一个小箭头，下拉可以看到浏览所有版本的选项，单击进入该文档的时间线，界面与Time Machine一模一样，你可以非常方便地找到任何一时间点你编辑过的内容，也可以随意恢复到任何一个版本而不会影响其他版本。非常酷的功能，并且好用。</p>
<blockquote>
<p>29.如何快速发送带附件的邮件</p>
</blockquote>
<p>在Windows中，我们可以右键单击文件发送到邮箱即可发送带附件的邮件。OS X也有类似功能，只不过叫共享。右键单击要发送的文件→“共享”→“电子邮件”即可。</p>
<blockquote>
<p>30.如何快速创建便笺</p>
</blockquote>
<p>便笺是我们很常用的功能，可以把一些临时性的文字内容贴到桌面上，大家是如何做的呢？复制文字，打开便笺程序，新建便笺，粘贴文字！Too young too complicated，我们只需要选中文字，然后shift+command+y，就行了。</p>
<blockquote>
<p>31.Mac的通用快捷键</p>
</blockquote>
<p>这部分内容之前陆续介绍过，但还是有人希望有个汇总，基于二八原则，我把最常用的快捷键罗列一下，对于非开发者，应该够用了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">command+tab                  任意情况下切换应用程序，向前循环</div><div class="line">shift+command+tab            切换应用程序，向后循环</div><div class="line">command delete               把选中的资源移到废纸篓</div><div class="line">shift+command+delete         清倒废纸篓（有确认）</div><div class="line">shift+option+command+delete   直接清倒废纸篓</div><div class="line">command+～                    同一应用程序多窗口间切换</div><div class="line">command+f                    呼出大部分应用程序的查询功能</div><div class="line">command+c/v/x                复制/粘贴/剪切</div><div class="line">command+n                    新建应用程序窗口</div><div class="line">command+q                    彻底退出当前应用程序</div><div class="line">command+l                    当前程序是浏览器时，可以直接定位到地址栏</div><div class="line">command+&quot;+/-&quot;                放大或缩小字体</div><div class="line">control+space                呼出Spotlight</div><div class="line">command+space                切换输入法</div></pre></td></tr></table></figure>
<p>对于最后两个快捷键，我个人比较习惯control+space切换输入法，所以做了自定义的配置。</p>
<blockquote>
<p>32.终端命令open</p>
</blockquote>
<p>我们之前介绍过如何在Finder中浏览文件时进入当前目录的shell界面，那个插件叫做Go2Shell。当然我们也会有在shell下打开当前目录的Finder的需求，运行如下命令即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">open .</div><div class="line">#当然open也可以打开其他目录，比如</div><div class="line">open /Users</div><div class="line">#open还可以直接打开文件、打开程序、指定程序打开文件、打开网址等，例如</div><div class="line">open a.txt</div><div class="line">open -a Safari</div><div class="line">open -a TextMate a.txt</div><div class="line">open http://news.sina.com.cn</div></pre></td></tr></table></figure>
<blockquote>
<p>33.介绍小软件——CatchMouse</p>
</blockquote>
<p>可以自定义快捷键快速在多个显示器内切换鼠标，非常方便，链接附上：<a href="https://itunes.apple.com/cn/app/catchmouse/id439700005?mt=12。" target="_blank" rel="noopener">https://itunes.apple.com/cn/app/catchmouse/id439700005?mt=12。</a></p>
<blockquote>
<p>34.激活窗口</p>
</blockquote>
<p>如果你在一个屏幕内打开了多个程序，除了当前激活的软件窗口，你还想看看其他窗口的内容，这时你直接单击其他窗口的话，原来的窗口就可能被遮挡或消失。如何保持原来的窗口一直处于最上层呢？非常简单，拖曳其他窗口的时候按住command键即可，原来的窗口会永远在最上面。</p>
<blockquote>
<p>35.文件检查器</p>
</blockquote>
<p>在Windows中大家经常选中多个文件，单击右键→“属性”可以查看这些文件的大小。在Mac里同样的操作（选中多个文件，单击右键→“显示简介”）弹出的是各个文件或文件夹的简介，这让很多人困惑不解。其实我们只要在单击右键的同时按住option键，“显示简介”就会变成“显示检查器”，单击“显示检查器”即可查看和操作批量文件。<br>另外，我还经常用这种方式浏览图片，比如选中多张图片，按住option键单击鼠标右键，选中“幻灯片显示xx项”，就可以全屏浏览图片了。</p>
<blockquote>
<p>36.屏幕放大镜</p>
</blockquote>
<p>有时我们需要放大屏幕做一些精细的操作，Ctrl+鼠标滚轮可以实现这一效果，如果你是键盘控，用option+command加上加减号也可以实现。</p>
<blockquote>
<p>37.语音识别</p>
</blockquote>
<p>Mountain Lion增加了语音识别的功能，具体的设置在“系统偏好设置”→“听写与语音”，可以设置听写语言、呼出窗口的快捷键等。我采用的是默认的快捷键，连续按fn键两次即可呼出语音识别窗口，这时候你就可以对Mac说话了。如果你想让Mac把你说的写下来，最好打开一个的文本编辑器并让光标处于可编辑状态。注意，该功能需要联网。</p>
<blockquote>
<p>38.time命令</p>
</blockquote>
<p>如果你想知道在终端执行的某个程序耗时多久，对CPU等的使用情况，可以输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">time python fib.py</div></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python fib.py  0.02s user 0.02s system 50% cpu 0.094 total</div></pre></td></tr></table></figure>
<blockquote>
<p>39.特殊字符输入</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">美元（$）              shift+4</div><div class="line">美分（¢）              option+4</div><div class="line">英镑（£）              option+3</div><div class="line">人民币（¥）            option+Y</div><div class="line">欧元（€）             shift+option+2</div><div class="line">连接号（–）           option+-</div><div class="line">破折号（—）           shift+option+-</div><div class="line">省略号（...）          option+;</div><div class="line">约等于（≈）           option+X</div><div class="line">度（°）                shift+option+8</div><div class="line">除号（÷）              option+/</div><div class="line">无穷大（∞）           option+5</div><div class="line">小于等于（≤）         option+,</div><div class="line">大于等于（≥）         option+.</div><div class="line">不等于（≠）           option+=</div><div class="line">圆周率（π）           option+P</div><div class="line">正负（±）              shift+option+=</div><div class="line">平方根（√）           option+V</div><div class="line">总和（∑）             option+W</div><div class="line">商标（™）             option+2</div><div class="line">注册（®）              option+R</div><div class="line">版权（©）              option+G</div></pre></td></tr></table></figure>
<blockquote>
<p>40.OS X三指轻拍查找功能</p>
</blockquote>
<p>OS X提供了三指轻拍查找的功能，什么意思呢？把光标移到一个单词上面，无需选中，三指轻拍，系统就会弹出词典显示相关单词的释义，非常方便。该功能可以在“系统偏好设置”→“触控板”里进行设置。</p>
<blockquote>
<p>41.F.lux调节屏幕色温</p>
</blockquote>
<p>推荐一款免费小软件F.lux。这个软件功能类似系统的亮度自动调节，不同的是它调节的是屏幕的色温。该软件能够根据时间来调节屏幕色温以达到保护眼睛的目的。有数据表明，4600～5000K的暖色有缓解眼部疲劳的作用。下载地址：<a href="http://stereopsis.com/flux/。" target="_blank" rel="noopener">http://stereopsis.com/flux/。</a><br>第一次打开应用，需要输入当前城市名称，搜索定位用户的当地时间。F.lux将根据日出及日落的时间来调节色温。在日出日落期间，屏幕色温和平时一样，对于RMBP来时就是6500K；日落之后，F.lux会逐渐地暖化你的屏幕。具体色温可以自定义。<br>我用了之后感觉还不错，大家可以试用下。</p>
<blockquote>
<p>42.让Mac不进入休眠状态</p>
</blockquote>
<p>如果你想离开电脑一段时间，又不想让电脑进入休眠状态，有个简单的命令可以帮助你做到这一点。在终端中输入：pmset noidle，即可。只要该命令一直运行，Mac就不会进入睡眠状态。关掉终端或ctrl+C可以取消该命令。pmset是OS X提供的命令行管理电源的工具，其功能远不止于此。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pmset -g    #查看当前电源的使用方案</div><div class="line">sudo pmset -b displaysleep 5    #设置电池供电时，显示器5分钟内进睡眠</div><div class="line">sudo pmset schedule wake &quot;02/01/13 20:00:00&quot;    #设置电脑在2013年2月1日20点唤醒电脑    ……</div></pre></td></tr></table></figure>
<p>感兴趣的可以使用man pmset查看详细信息。</p>
<blockquote>
<p>43.网络共享</p>
</blockquote>
<p>Mac提供了非常简单易用的Internet共享功能，可以作为一个轻量级的家庭无线路由使用。只要你的Mac能够上网，那么Phone和Pad等设备就都可以通过Wi-Fi共享Mac的网络，实现无线上网。具体的设置非常简单，打开“系统偏好设置”→“共享”→“互联网共享”，选择共享源（网卡或AirPort），并设置Wi-Fi的名称、密码、安全级别等属性，最后勾选左侧列表的“互联网共享”，根据提示操作即可。<br>这是一个我曾经认为大部分用户都知道的功能，后来发现几乎很少人使用或会用。</p>
<blockquote>
<p>44.快速查看</p>
</blockquote>
<p>OS X提供了非常方便的预览文件内容的功能。在Finder或桌面上，选中一个文件并按空格键，系统就会弹出预览界面。对于很多文件我们仅仅使用快速查看功能就可以浏览文件内容了，比如iWorks的Keynote、Pages和Numbers的文档，微软Office的文档，pdf，图片，视频，各类文本文件等。除了在Finder和桌面快速查看文件，我们还可以快速浏览邮件的附件。打开邮件程序，找到一个带有附件的邮件，选中附件并按空格键，就可以快速浏览附件内容。<br>我们还可以在终端操作的时候使用这个功能，例如qlmanage-p文件名，系统就会弹出快速查看窗口。</p>
<blockquote>
<p>45.显示桌面</p>
</blockquote>
<p>我们下载文件或临时文件经常会放到桌面上，在Windows里通过alt+d或点击显示桌面的图标即可，在Mac里如何实现呢？<br>有两种方式，都很方便。第一种是四指划开，该功能可以在触控板里设置。还有一种方式是通过快捷键command+F3，即可实现移开程序显示桌面的功能。当我们想把桌面的文件放入某个程序（比如当做邮件附件）时，可以配合command+tab实现。用鼠标拖动桌面文件，command+tag切换程序，然后把文件拖入该程序即可。</p>
<blockquote>
<p>46.应用程序的安装和卸载</p>
</blockquote>
<p>OS X中的应用程序和OSGi中使用的Bundle类似，都是把配置文件和程序封装在一个包里。对于普通用户来说，你在Launchpad中看到的所有程序都像一个图标，但这个图标不是Windows中的快捷方式，而是封装好的Bundle，从程序角度而言这是一个文件夹，对普通用户来说，知道点这个图标运行程序就行了。这种设计方式使得OS X中95%以上的软件的安装变得十分简单。如果你是从Windows转过来的话，你会认为安装和卸载简单得令人发指。安装程序就是把XXX.app拖进/Applications（应用程序文件夹），卸载就是把程序从该目录删掉。好吧，你可以这么理解，OS X中95%以上的软件都是Windows中的“绿色软件”。</p>
<blockquote>
<p>47.磁盘映像</p>
</blockquote>
<p>磁盘映像类似Windows中的iso，不过文件后缀为dmg。磁盘映像可以直接挂接到OS X中，其表现形式就像是磁盘分区。双击文件可以直接打开，打开后在Finder左边栏的设备中可以找到挂接好的磁盘映像。dmg是Mac下最常用的文件组织方式，几乎所有的安装程序都是以dmg方式发布的。一般情况下安装程序就是打开相关程序的dmg文件，里面有一个app和应用程序文件夹，把app拖入应用程序即可。另外我们也可以使用磁盘工具把dmg里的文件恢复为真正的硬盘文件，也可以制作dmg文件。</p>
<blockquote>
<p>48.复制目录下文件名列表</p>
</blockquote>
<p>如何复制某个目录下所有文件的文件名列表呢？非常简单，command+a，command+c。然后打开一个文本编辑器（比如TextMate），command+v即可。</p>
<blockquote>
<p>49.多点触控手势</p>
</blockquote>
<p>当我们用Safari浏览网页时，经常想回到之前浏览过的历史页面。使用多点触控手势可以非常容易直观地实现该功能。打开Safari浏览多个页面，然后使用双指左右轻扫，可以来回切换浏览页面。</p>
<p>另外，如果你在浏览时不小心关掉了一个标签页，使用command+z可以恢复最后关闭的那个标签页。</p>
<blockquote>
<p>50.OS X的预览程序</p>
</blockquote>
<p>OS X的预览程序可以打开各类图片和pdf等类型的文件，当你想查看某个图片或pdf的细节时，没必要用command++/-来缩放整个文件，使用`键可以呼出放大镜，细节一览无遗。</p>
<blockquote>
<p>51.显示/隐藏桌面内容快捷键</p>
</blockquote>
<p>我们经常会在桌面上堆满文件夹和文件，有时候会很方便，有时候会觉得很乱。其实我们可以通过以下命令来决定什么时候显示，什么时候隐藏：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">chflags hidden ～/Desktop/*     //隐藏桌面内容</div><div class="line">chflags nohidden ～/Desktop/*   //显示桌面内容</div></pre></td></tr></table></figure>
<p>如果觉得输入麻烦，用TextExpander或Alfred设置成snippet即可。</p>
<blockquote>
<p>52.按住option的快捷键</p>
</blockquote>
<p>OS X设置了一些快捷键用来快速打开显示器、Mission Control、键盘、声音等系统设置，具体是什么呢？你只要按住option，轮番把键盘最上方的那排键试一下就知道了，一般人我不告诉他。</p>
<blockquote>
<p>53.Space（空间）</p>
</blockquote>
<p>使用OS X，我们可以充分利用系统提供的多个Space，把不同的程序放到不同的Space，让我们的系统更有扩展性。如何增加Space呢？四指上推，在桌面的最上方会出现当前的Space，把鼠标移到Space列表的右侧，会出现一个带+号的空间，单击加号，即可增加一个Space。<br>那么如何把某个程序固定在某个Space打开呢？在某个Space打开程序，在Dock中找到这个程序图标，鼠标长按会出现一个菜单，“选项”→“分配给”，选“这个桌面”，下次再打开这个程序，就会自动进入设定的Space。</p>
<p>Space的排列方式可以在Mission Control里设置，比如选择按照使用情况自动排列等。</p>
<blockquote>
<p>54.隐藏程序</p>
</blockquote>
<p>当我们不想在使用当前程序的时候看到其他程序的时候，可以使用快捷键option+command+h，这时除了你正在使用的程序，其他所有的程序都会被隐藏起来，有助于你专心工作。想切换到其他程序时，可以使用command+tab。</p>
<blockquote>
<p>55.文件颜色标签的使用</p>
</blockquote>
<p>OS X的Finder提供了颜色标签的功能，可以直接为文件和文件夹标记颜色。我在很长一段时间都没有注意到这个功能，一次偶然的机会开始使用颜色标记文件，感觉非常方便。<br>比如我会在Finder的主目录下用颜色标明最常访问的文件夹。如果是电子书，可以用颜色表示阅读状态，例如绿色表示正在阅读，灰色表示读完了，橙色表示待阅读，等等。大家可以根据自己的习惯使用颜色标签，提高效率。</p>
<blockquote>
<p>56.利用邮件中的日期创建日历事件</p>
</blockquote>
<p>工作中我们总是通过邮件来通知会议和活动，这时邮件中往往有日期信息。我们可以利用这个信息直接创建日历事件。打开邮件，把鼠标移动到有效的日期信息上，会出现下拉菜单的按钮，单击后可以为日历添加事件，事件标题默认为邮件标题。</p>
<blockquote>
<p>57.AppleScript小程序</p>
</blockquote>
<p>今天为大家介绍用AppleScript实现一个示例小功能：清空废纸篓。打开AppleScript编辑器，输入如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">--操作对象是Finder</div><div class="line">tell application &quot;Finder&quot;</div><div class="line">--为isEmpty变量赋值</div><div class="line">set isEmpty to &quot;是否清空废纸篓！&quot;</div><div class="line">--显示确认对话框，单击“确认”程序继续执行，单击“取消”终止程序</div><div class="line">display dialog isEmpty</div><div class="line">--清空废纸篓</div><div class="line">empty the trash</div><div class="line">--通过语音说这事搞定了</div><div class="line">say &quot;It is done!&quot;</div><div class="line">end tell</div></pre></td></tr></table></figure>
<p>单击工具栏的“编译”按钮，检查没有错误后，单击“运行”即可，大家可以看看发生了什么。</p>
<blockquote>
<p>58.Homebrew</p>
</blockquote>
<p>Homebrew的功能和OS X自带的MacPorts很像，但是更为轻量级，由于大量利用了系统自带的库，安装方便，编译快速，实在是OS X系统开发中之必备工具。<br>安装方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ruby-e&quot;$（curl-fsSL https://raw.github.com/mxcl/homebrew/go）&quot;。</div></pre></td></tr></table></figure>
<p>使用方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install wget//安装wget工具。</div></pre></td></tr></table></figure>
<p>具体的使用请参考：<a href="https://github.com/mxcl/homebrew/wiki。" target="_blank" rel="noopener">https://github.com/mxcl/homebrew/wiki。</a></p>
<blockquote>
<p>59.根据文件名快速查找文件</p>
</blockquote>
<p>我们在OS X中查找文件或文件内容一般使用Spotlight或Alfred，这些功能在前面的Mac Tips中都介绍过，不过，如果你知道文件名的一部分，想更加快速地定位文件，那么就会用到命令行工具locate。</p>
<p>locate是Unix/Linux下的命令工具，基本原理就是通过定期更新系统的文件和文件名并把索引信息放入系统的数据库中，当通过locate查找文件时直接从数据库里取数据。而且locate可以查到Spotlight查不到的系统文件。基本的使用方法非常简单，比如你想找Nignx的配置文件在哪，只需输入：locate nginx.conf。</p>
<blockquote>
<p>60.设置用户登录选项</p>
</blockquote>
<p>OS X系统登录后会自动启动一些程序，比如Alfred、拼音输入法、风扇控制软件等等。有时我们会嫌多，有时又想增加一些启动项，在哪设置呢？打开“系统偏好设置”→“用户与群组”，选中“当前用户”，单击右边的“登录项”，你就会看到系统启动时加载的程序，可以随意删减，还能够设置启动后隐藏，非常方便。</p>
<blockquote>
<p>61.修改你的登录窗口</p>
</blockquote>
<p>我们默认登录OS X时，系统会显示登录用户列表，你需要用鼠标点一下要使用的用户，或者用光标键选择用户，出现登录框后输入密码登录。如果我们想不显示用户列表，直接输入用户名和密码登录怎么办呢？<br>打开“系统偏好设置”→“用户与群组”，单击左侧下方的“登录选项”（很奇怪很多人找不到这个），在右侧修改登录窗口为“名称和密码”。注销登录，这次大家就满意了。</p>
<blockquote>
<p>62.Mac的键盘</p>
</blockquote>
<p>很多人第一次用Mac的键盘时会发现，苹果也太抠门儿了，退格键没了，PageUP/PageDown/Home/End也没了。别担心，您不是还有delete键和上下左右方向键么？delete相对于退格键，fn+delete可以往前删，fn+上下左右方向键可以实现PageUP/PageDown/Home/End的功能，一个功能都不少。</p>
<blockquote>
<p>63.QuickTime</p>
</blockquote>
<p>很多人都会使用QuickTime Player看mp4或mov视频文件，但其功能远不止于此。option+command+n可以打开录像功能，ctrl+option+command+n可以打开录音功能，ctrl+command+n可以打开录制屏幕功能。最后一个功能非常适合做产品介绍或产品演示，大家可以试一试。遇到快捷键冲突的，在QuickTime的文件菜单中也可以找到这三项。</p>
<blockquote>
<p>64.Dropbox快速导入Mac</p>
</blockquote>
<p>有读者问如何把iPhone或iPad里的照片导入Mac，我自己用的办法是Dropbox，安装了Dropbox之后，每次用USB连接iPhone或iPad时，程序都会提示是否有新照片需要导入，导入后自动云端同步。不用Dropbox的同学，另外一个简单的方式是连接移动设备时，打开预览程序，单击文件，可以看到一个“从iPhone/iPad导入”的菜单，单击一下，后续你基本就知道该怎么做了。</p>
<p>当然还有其他方法，比如打开图像捕捉或iPhoto程序等。</p>
<blockquote>
<p>65.快速创建日历事件</p>
</blockquote>
<p>OS X提供了智能创建日历事件的功能。打开日历程序，单击左上角的“+”号，在弹出的输入框里输入：“明天上午9点到13点参加公司年会。”回车，看看效果如何？日历程序会准确地创建你想要的事件。大家可以试试其他写法。</p>
<blockquote>
<p>66.显卡监控软件gfxCardStatus</p>
</blockquote>
<p>现在大部分Mac都有两块显卡，集成显卡和独立显卡。OS X会根据不同的程序自动切换显卡，但有时候我们在电池供电的情况下会由于某些程序的原因一直使用独立显卡，会大大缩短待机时间，这时候就能用到这个软件了。gfxCardStatus能做的事情有两件。一件是手动切换显卡。另一件是监控现在系统在使用哪块显卡，如果是独立显卡的话，是因为哪个程序导致必须使用独显。下载地址：<a href="http://gfx.io。" target="_blank" rel="noopener">http://gfx.io。</a></p>
<blockquote>
<p>67.创建智能文件夹</p>
</blockquote>
<p>Finder提供了智能文件夹的功能，简单来说就是固化你的搜索条件，并形成文件夹存放在左侧边栏。</p>
<p>例如你想建一个文件大小大于1G的智能文件夹，使用快捷键option+command+n呼出新建智能文件夹界面，单击最右侧的加号，在条件选择第一栏选择“大小”，第二栏选择“大于”，第三栏输入“1G”，你就可以看到你的Mac上文件大于1G的列表，单击“存储”，命名后该文件夹就会出现在左侧边栏。随时单击随时动态监控自己的硬盘上有哪些超过1G的大文件。试试其他搜索条件吧！</p>
<blockquote>
<p>68.自动打开程序文稿</p>
</blockquote>
<p>OS X提供了自动恢复上次关闭程序时打开的文稿和窗口的功能。这就是说，如果你使用预览程序打开了5个PDF文件，用command+q关闭了预览程序，下次打开预览程序时，会自动恢复这5个PDF程序，包括文字选中的状态，阅读进度等信息。这个功能我非常喜欢，但有时候我们并不希望自动恢复，那么有两种方式可以关闭这个功能。</p>
<p>第一种：打开“系统偏好设置”→“通用”，选中“退出应用时关闭窗口”，这样所有的程序都不再具备恢复功能。</p>
<p>第二种：退出程序时使用option+command+q而不是command+q，相当于关闭所有文件并退出程序，下次打开时，这些文件就不会自动打开了.</p>
<blockquote>
<p>69.智能邮箱</p>
</blockquote>
<p>邮箱账户的创建相信一般的用户都可以正常操作，不知道你是否使用过OS X中Mail的智能邮箱功能呢？</p>
<p>打开邮件程序，单击“邮箱”→“新建智能邮箱”，在弹出的窗口中选择你的过滤条件。过滤条件非常灵活，可以定义与或关系，增加多个过滤条件，设置完成后保存即可。你会发现左侧栏多了一个智能邮箱，单击即可根据你设置的过滤条件找到那些符合条件的邮件。</p>
<blockquote>
<p>70.隐藏的VIP</p>
</blockquote>
<p>如果你的系统版本是10.8以上，那么你就会发现邮件程序中多了一个隐藏的VIP功能。随便找封邮件，把鼠标放在发件人或收件人的邮件地址上，会出现一个蓝色的选择框，单击其中的白色箭头，在下拉菜单中单击“添加到VIP”，你就会发现左边栏多了一个VIP分栏，单击加入的VIP用户，可以直接查看他们发送的邮件。</p>
<blockquote>
<p>71.在Finder中打开某个文件夹下所有子文件夹</p>
</blockquote>
<p>有时候我们希望在Finder中查看某个文件夹下的所有文件和子文件夹，怎么做到呢？把文件切换到列表视图（command+2），把排序方式设置为“不排序”，这时文件夹左侧会出现一个箭头。按住option键单击文件夹左侧的箭头，你就会发现所有的文件和文件夹都展现在眼前了。注意，如果该文件夹下文件太多，那么不建议使用，因为打开会需要很长时间。</p>
<blockquote>
<p>72.慢速动画</p>
</blockquote>
<p>所有具备动画效果的操作，按住shift键，会播放慢速动画。大家可以试试按住shift键的时候最小化窗口，效果非常酷。</p>
<blockquote>
<p>73.XtraFinder</p>
</blockquote>
<p>这个插件具备和TotalFinder类似的功能，支持tab、文件夹置顶、多窗口、剪切、全局热键等功能，重要的是这是一个完全免费的自由软件。<br>下载网址：<a href="http://www.trankynam.com/xtrafinder/。" target="_blank" rel="noopener">http://www.trankynam.com/xtrafinder/。</a></p>
<blockquote>
<p>74.MacBook待机仍可为iPhone供电</p>
</blockquote>
<p>我们平时会把iPhone接到MacBook上充电。事实上，把MacBook合上待机时，仍然可以为iPhone供电，大家可以试一下。如果你出游时会带上你的Mac，别忘了这也是一块大的移动电池。</p>
<blockquote>
<p>75.恢复截屏图片默认保存路径</p>
</blockquote>
<p>截屏图片存哪了？OS X自带截屏不好使了，截屏之后有“咔嚓”的程序运行声，但图片不知道去哪里了，如何修复？</p>
<p>OS X自带的截图文件是存储在桌面上的，你的可能是被修改过了，我们可以通过以下命令恢复默认路径：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults delete com.apple.screencapture location</div></pre></td></tr></table></figure>
<p>注销，重新登录，再次截屏看看文件是否保存在桌面上了。</p>
<blockquote>
<p>76.如何为OS X自带的字典增加中文字典</p>
</blockquote>
<p>目前OS X自带的字典程序是没有中文的，不过我们很容易为其扩展新字典。操作如下：</p>
<p>◆如果打开了字典程序，关闭。<br>◆到以下网址下载《朗道英汉字典》和《朗道汉英字典》，解压缩得到两个后缀为dictionary的文件：<a href="http://pan.baidu.com/share/link?shareid=249542&amp;uk=2617481269。" target="_blank" rel="noopener">http://pan.baidu.com/share/link?shareid=249542&amp;uk=2617481269。</a><br>◆把这两个文件复制到～/Library/Dictionaries下。<br>◆启动字典程序，你就会看到增加了《朗道英汉字典》和《朗道汉英字典》。</p>
<blockquote>
<p>77.文件共享</p>
</blockquote>
<p>在Mac之间进行文件共享有很多种方式，介绍两个最简单的，具备AirDrop功能的两台或多台Mac，在开着Wi-Fi的情况下打开AirDrop，就会找到同样打开AirDrop的Mac，把想传送的文件拖放到其他人的Mac头像上即可。</p>
<p>另一个就是利用系统的共享功能。打开“系统偏好设置”→“共享”，单击左侧栏的“文件共享”，在右侧区域配置即可。</p>
<blockquote>
<p>78.删除程序</p>
</blockquote>
<p>删除Mac上的程序有很多种方法，比如直接去应用程序文件夹下删除，用CleanApp删除，等等，今天介绍一个最好玩的。</p>
<p>打开Launchpad，按住option键，就会看到所有的程序图标都会像iOS图标那样晃动起来，单击图标左上角的叉，即可删除程序，和操作iOS一样。</p>
<blockquote>
<p>79.command+上下方向键</p>
</blockquote>
<p>这两个快捷键很多应用程序都支持，具体功能就是屏幕滚动到应用程序的顶部或底部，类似很多网站提供的“回到顶部/底部”功能。Safari、Chrome、Firefox、Pages、Evernote等默认都支持这样的功能。</p>
<p>在使用快捷键呼出Spotlight的时候，使用command+上下方向键还可以在搜索分组之间切换，非常方便。</p>
<blockquote>
<p>80.Mac上的阅读笔记类软件</p>
</blockquote>
<p>◆Kindle for Mac：支持视网膜屏，支持本地阅读和Amazon商店，支持中英文字典，电子阅读体验一流。遗憾的是不能整合中国和美国Amazon的账户，导致电子书商品也没法使用同一个账户阅读。免费。<br>◆Caffeinated：优秀的RSS阅读器，如果你还喜欢博客和传统阅读，那么推荐使用。<br>◆Pocket：免费。最好的稍后读App，支持标签分类、编辑等功能，支持afari、Chrome等插件，非常适合知识积累。收费。<br>◆Evernote：很好的笔记类App，5.0之后UI有了很大的改进，目前我所有的文章都是用Evernote管理。免费，有收费版本。<br>以上4个App在iPad、iPhone上也有相关应用，并且都支持云同步，合理使用对提高读写效率非常有帮助。</p>
<blockquote>
<p>81.查看电源状况</p>
</blockquote>
<p>按住option键，单击右上角的苹果图标，选择“系统信息”，在打开窗口的左侧栏中找到“电源”，单击即可查看电源的详细信息。主要的指标包括电池循环计数、状况等信息。如果您安装了Alfred，呼出后直接输“sys”，也可以找到系统信息。</p>
<p>如果想简单查看一下电池的使用状况，按住option键单击顶部工具栏上的电池图标，可以显示电池使用状况。如果出现“尽快更换”、“修理电池”等信息，那么有可能是电池出了问题，建议先重置系统管理控制器（SMC），如何重置可以去Apple的官方支持网站查一下。还没效果的话，可能就需要换电池了。</p>
<blockquote>
<p>82.Pixelmator</p>
</blockquote>
<p>这款图像处理软件号称是Mac上的精简版Photoshop，而且更为人性化，适合非专业人士使用，不是平面设计人员也可以作出非常专业的图像设计。MacTalk里很多配图我都使用这款软件加工过，很好用。收费软件，但值得拥有。推 荐 一 个Podcast视 频 教 程（RSS）：<a href="http://www.pixelmator.com/tutorials/itunes/。" target="_blank" rel="noopener">http://www.pixelmator.com/tutorials/itunes/。</a></p>
<blockquote>
<p>83.搜索命令mdfind</p>
</blockquote>
<p>mdfind是一个非常灵活的全局搜索命令，类似Spotlight的命令行模式，可以在任何目录对文件名、文件内容进行检索，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//搜索文件内容或文件名包含“苹果操作系统”的文件</div><div class="line">mdfind苹果操作系统</div><div class="line">//在桌面上搜索文件内容或文件名包含“苹果操作系统”的文件</div><div class="line">mdfind -onlyin ～/Desktop苹果操作系统</div><div class="line">//统计搜索到的结果</div><div class="line">mdfind -count -onlyin ～/Desktop苹果操作系统</div><div class="line">//搜索文件名包含“苹果操作系统”的文件</div><div class="line">mdfind -name苹果操作系统</div></pre></td></tr></table></figure>
<blockquote>
<p>84.元信息命令mdls</p>
</blockquote>
<p>mdls可以列出某个文件或文件夹的所有元数据信息，针对不同文件显示不同的元数据信息，例如文件创建时间、类型、大小等。如果是图片或音视频文件，则会显示更多元数据信息。使用方式非常简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mdls ～/Desktop/a.jpg</div></pre></td></tr></table></figure>
<p>如果想查看图片的ISO数据，可以使用如下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mdls ～/Desktop/a.jpg|grep ISO</div></pre></td></tr></table></figure>
<blockquote>
<p>85.功能键</p>
</blockquote>
<p>很多程序员在调试程序的时候总会用到F7、F8这些键，但在OS X里这些功能键默认分配了一些功能，想使用的话需要同时按fn+F8……</p>
<p>如果希望将F1～F12这些按键用作标准功能键而且不需要按fn，可以执行以下操作：打开“系统偏好设置”→“键盘”，选中“将F1、F2等键用作标准功能键”。启用此选项时，顶部一行按键将用作标准功能键（F1～F12），而不执行音量控制等特殊功能。启用此选项后，若要使用这些按键的特殊功能，请按fn，比如按fn+F8来播放音乐。</p>
<blockquote>
<p>86.查看文件信息的命令：file</p>
</blockquote>
<p>file可以查看相关文件的类型和属性，相对于mdls，这个更亲民一些，基本用法file xxx.png，大家感受一下。</p>
<blockquote>
<p>87.如何配置多种网络环境</p>
</blockquote>
<p>我自己无论在公司还是家里都是DHCP自动分配IP，所以不需要进行网络环境切换。但有些用户有时自动有时手动，需要多套网络配置方案，每次修改实在是太麻烦了。曾经有人问我Mac上是否有这样的第三方软件？我说没有，因为OS X的网络设置本身就提供了这样的功能。</p>
<p>打开“系统偏好设置”→“网络”，单击“位置”下拉菜单，找到“编辑位置”，打开后即可增删编辑多套网络设置，设置完成后保存。</p>
<p>这时单击屏幕左上角的苹果图标，在下拉菜单里增加了一个位置选项，里面就是你配置好的多种网络设置，单击切换即可。</p>
<blockquote>
<p>88.生成man page的PDF文档</p>
</blockquote>
<p>打开OS X的终端，通过man命令可以直接查看该命令的使用手册。但有时我们会觉得在命令行查看不太方便，如果可以提供一个PDF文档就完美了。这很容易做到，在终端输入如下命令，即可在预览程序打开grep的使用手册，另存为你需要的文件名即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">man -t grep | open -f -a Preview</div></pre></td></tr></table></figure>
<blockquote>
<p>89.虚拟机</p>
</blockquote>
<p>2006年Mac的硬件进行了重大的架构调整，开始全面采用Intel系列的CPU，Power渐行渐远。架构的调整和Bootcamp的推出，使得在Mac上安装双系统变得触手可及。基于Mac的虚拟机应用也开始出现。我刚开始使用Mac时是双系统的支持者，后来Windows用得越来越少，就比较推荐使用虚拟机了。</p>
<p>在OS X上主要有三款虚拟机软件：Parallels Desktop、Vmware Fusion和VirtualBox。简单给大家介绍一下。</p>
<p>◆Parallels Desktop：Parallels是OS X上一款优秀的虚拟机软件，最新版本是8。它支持多种操作系统，并对Windows有完美的支持。通过融合模式，可以让Windows程序运行起来象Mac的应用。提供把Vmware Fusion虚拟机迁移到PD上的功能。收费。</p>
<p>◆Vmware Fusion：Vmware在Windows和Linux下大名鼎鼎，Fusion是Mac版本，功能同样强大。收费。</p>
<p>◆VirtualBox：Sun推出的一款开源虚拟机，现在归Oracle了，未来走势不明。免费。</p>
<p>我个人首推Parallels Desktop，功能、性能和价格都不错，专注于桌面版，属上乘之选。我自己虚拟了Win7、Redhat Linux和Ubuntu等环境，作软件测试和搭建多机开发环境。</p>
<blockquote>
<p>90.如何开启root用户</p>
</blockquote>
<p>用过Linux/Unix系统的都知道root用户，它具备读写文件系统所有区域的特权，是最高级别的用户。OS X一样有root用户，只不过默认情况是不开启的。我们想在命令行执行需要root权限的操作时，可以在命令之前增加sudo指令，比如执行每日维护指令：sudo periodic daily，系统会提示你输入用户密码，执行root权限。在GUI（图形界面）执行root级别的命令时也会提示输入用户密码。一般情况下我们是不需要开启root用户的。</p>
<p>用惯了Linux系统的用户有时很想启用root用户，其实也很简单，打开Finder，输入shift+command+g，在前往文件夹中输入：/System/Library/CoreServices，然后在目录中找到“目录实用工具”并打开，解开左下角的小锁，然后单击顶部菜单的“编辑”，你就会看到启用或停用root用户的选项了。然后我们在命令行下执行su-，就可以切换到root目录下，root的默认目录是/var/root。</p>
<p>root有风险，启用须谨慎！</p>
<blockquote>
<p>91.隐藏的空间切换功能</p>
</blockquote>
<p>以前介绍过OS X中Space的使用，我们可以定义多个Space，每个程序都可以在特定的Space中打开，多手势上推下滑选择程序，也可以通过ctrl+数字切换Space，很方便。今天再为大家介绍一个隐藏的功能，就是通过四指双击触控板，可以在你最近使用的两个Space之间切换，这个功能就类似电视频道中的返回功能，当你使用了Space1中的一些App，切换到了Space4之后，就可以通过四指双击在Space1和Space4之间切换了，对于协同工作非常有效。典型的应用场景：Space1里编码，在Space4里参考各类文档。</p>
<p>功能开启，打开终端程序，输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.dock double-tap-jump-back -bool TRUE;  #功能开启</div><div class="line">killall Dock;     #重启Dock</div></pre></td></tr></table></figure>
<blockquote>
<p>92.免费的文本编辑器Imagine</p>
</blockquote>
<p>我个人觉得Imagine比OS X自带的TextEdit好，除了目前不支持iCloud外，基本涵盖了TE的功能，而且排版简约美观，可更换柔和的背景色，全屏写字非常舒服，对字体样式的支持很好，在富文本和纯文本间切换方便，我基本用Imagine替代了TextEdit。</p>
<p>下载地址：<a href="https://itunes.apple.com/cn/app/imagine/id566877440?mt=12。" target="_blank" rel="noopener">https://itunes.apple.com/cn/app/imagine/id566877440?mt=12。</a></p>
<blockquote>
<p>93.去除右键菜单的重复项</p>
</blockquote>
<p>OS X系统有个问题，某个程序反复安装后，选中某种类型的文件，单击右键→“打开方式”，你会看到不少重复的选项，我们可以用以下命令的除重复项。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/Support/lsregister -kill -r -domain local -domain system -domain user</div></pre></td></tr></table></figure>
<blockquote>
<p>94.如何分别设置Mac的鼠标和触控板的滚动方向</p>
</blockquote>
<p>很多人习惯鼠标使用相反的滚动方向，而触控板类似iPad那样自然滚动，问如何设置，当时我的回答是不知道，因为目前OS X的系统设置里，鼠标和触控板的设置是统一的。今天发现了一个免费的软件Scroll Reverser，可以实现鼠标和触控板的分别设置。</p>
<p>下载地址：<a href="http://www.macupdate.com/app/mac/37872/scroll-reverser。" target="_blank" rel="noopener">http://www.macupdate.com/app/mac/37872/scroll-reverser。</a></p>
<p>启动后程序显示在顶部菜单栏，设置简单明了，有需要的用户体验一下吧。</p>
<blockquote>
<p>95.如何让不支持Retina的Mac软件变成Retina App</p>
</blockquote>
<p>前两天有读者求推荐Mac下的FTP软件，我推荐了FileZilla，但这个软件是不支持Retina屏的，Retina用户使用这个软件会感觉整个世界都模糊了，结果搜索之下，发现了一个小软件，叫做Retinizer，顾名思义，就是把非Retina的软件Retina化，我用了一下，完美支持FileZilla。<br>下载地址：<a href="http://retinizer.mikelpr.com/。" target="_blank" rel="noopener">http://retinizer.mikelpr.com/。</a></p>
<blockquote>
<p>96.文件比较</p>
</blockquote>
<p>◆对于单个文件的比较，一般使用diff或vimdiff就可以了，比如：vimdiff destfile.txt sourcefile.txt vim会非常清晰地显示出文件的不同，还有很多快捷方式帮助你查看和操作文件，这个命令比较适合命令行爱好者。</p>
<p>◆对于大批量文件的比较，还是图形化比较工具更合适一些。OS X自带了FileMerge比较工具，可以满足部分需求，但对于中文编码文件或大文件经常会崩溃，很奇怪Apple一直不解决这个问题。</p>
<p>◆推荐一款收费软件：VisualDiffer（25元），UI、功能和稳定性都非常不错，实在是居家旅行、代码比较、查找问题的必备利器。</p>
<blockquote>
<p>97.FTP工具Cyberduck</p>
</blockquote>
<p>之前介绍Retinizer（普通软件Retina化）的时候提到了FTP软件FileZilla，我个人一般使用命令行下的ftp/sftp/scp等实现FTP软件的功能，但普通用户还是用图形界面的更方便些。今天再给大家介绍一个可以实现远程同步文件的FTP工具：Cyberduck。<br>Cyberduck除了可以实现FTP的基本功能外，还能支持远程同步。所谓同步，就是把远程和本地的两个目录进行比较，然后自动找出修改的文件上传到服务器。</p>
<p>具体操作就是通过ftp或sftp的方式登入远端服务器，选中某个文件夹，右键菜单里选择“同步”，再选择本地文件夹，就可以进行同步比较上传了，上传之前你最好确认下，更稳妥。</p>
<p>同样，这个软件也可以用Retinizer实现高清显示效果。</p>
<blockquote>
<p>98.文件重命名</p>
</blockquote>
<p>文件重命名的问题以前说过，但最近又有些用户问起，就再说一下。<br>如果你没有装任何插件的话，在Finder中重命名文件或文件夹的快捷键就是回车。打开文件用command+O，返回上级目录用command+向上的方向键。如果你装了原来推荐过的XtraFinder，可以把回车改为打开文件（与Windows操作类似），把option+R设置为文件重命名。<br>如果你在命令行下重命名文件，命令是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv oldname newname</div></pre></td></tr></table></figure>
<blockquote>
<p>99.多个用户登录一个程序</p>
</blockquote>
<p>Mac下有很多程序默认是单进程的，比如你不能打开多个邮件程序，不能打开多个Evernote，但有时我们可能会有这样的需求，那么用如下命令可以实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">open -n /Applications/XXX.app</div></pre></td></tr></table></figure>
<p>-n的含义是：Open a new instance of the application（s）even if one is already running.意思就是为正在运行的应用程序再开一个新实例。常用于多个账户登录一个程序，或软件比较等场景。</p>
<blockquote>
<p>100.强制关闭程序</p>
</blockquote>
<p>总有程序关闭不了，这时候我们就需要以下方法。</p>
<p>方法一：option+command+esc，调出强制退出应用程序的窗口，选择要退出的进程即可。</p>
<p>方法二：打开活动监视器，类似windows的任务管理器一样操作就好了。</p>
<p>方法三：命令行下的kill命令，比如想杀掉TextMate，首先用ps-ax|grep TextMate找到进程号，然后用kill-9进程号，即可。</p>
<p>至此，天下无杀不掉的进程。</p>
<blockquote>
<p>101.用AppleScript实现打开多实例程序</p>
</blockquote>
<p>之前介绍了通过open-n/Applications/XXX.app的方式打开多实例程序，有人在微博上问如何选中一个文件或程序，通过右键菜单打开新实例，而不是每次都去命令行操作。</p>
<p>我们可以通过Automator+AppleScript实现这个功能。<br>打开Automator，选择创建“服务”，在左侧选择“运行AppleScript”，双击打开程序窗口，在“（Yourscriptgoeshere）”处输入如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tell application &quot;Finder&quot;</div><div class="line">   try</div><div class="line">        set filename to POSIX path of（selection as text）</div><div class="line">        set fileType to（do shell script &quot;file -b &quot; &amp; filename）</div><div class="line">        if（fileType does not end with &quot;directory&quot;）or（filename end with &quot;app&quot;）then</div><div class="line">        do shell script &quot;open -n &quot; &amp; filename</div><div class="line">        end if</div><div class="line">        end try</div><div class="line">end tell</div></pre></td></tr></table></figure>
<p>在程序上方的选择框设定“文件和文件夹”、“任何应用程序”，然后保存，起个你喜欢的名字，比如叫“以新实例运行”。退出Automator。<br>选中文件或程序，右键→“服务”→“以新实例运行”，即可实现类似open-n的方式。</p>
<blockquote>
<p>102.Automator</p>
</blockquote>
<p>Automator是苹果公司为其操作系统OS X开发的一款软件。通过单击拖曳鼠标等操作就可以将一系列动作组合成一个工作流，从而帮助你自动完成一些复杂的重复工作。Automator还能横跨很多不同种类的程序，包括：查找器、Safari网络浏览器、iCal、地址簿或者其他的一些程序。在Automator中可以运行AppleScript。</p>
<p>在上一个技巧中我们通过Automator创建了一个服务，当你在Finder或桌面上选中文件时，在右键的“服务”菜单里增加了一个选项“以新实例运行”，是通过AppleScript实现的。下面说明一下程序功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">--通知Finder</div><div class="line">tell application &quot;Finder&quot;</div><div class="line">--异常处理</div><div class="line">try</div><div class="line"> --获取选中文件的全路径</div><div class="line"> set filename to POSIX path of（selection as text）</div><div class="line"> --通过脚本file -b获取文件类型</div><div class="line"> set fileType to（do shell script &quot;file -b &quot; &amp; filename）</div><div class="line"> --如果不是文件夹或以app结尾，执行open -n脚本</div><div class="line"> if（fileType does not end with &quot;directory&quot;）or（filename ends with &quot;app&quot;）then</div><div class="line">        do shell script &quot;open -n &quot; &amp; filename</div><div class="line">        end if</div><div class="line">        end try</div><div class="line">end tell</div></pre></td></tr></table></figure>
<p>这里考虑到了选中程序直接打开，或选中文件以默认程序打开的情况。</p>
<blockquote>
<p>103.用Safari默认查询引擎查询应用软件里的文字</p>
</blockquote>
<p>如果你想通过Safari的默认查询引擎查询某个应用软件里的文字，那么选中文字，然后使用shift+command+L即可跳转到Safari的搜索页面，非常方便。大部分应用都支持这个快捷键。</p>
<blockquote>
<p>104.旋转屏幕</p>
</blockquote>
<p>打开“系统偏好设置”，已经打开了的，退出重新打开。按住option+command键，单击显示器，在原来的亮度选项下方会出现一个“旋转”的选项，这时候你就可以旋转你的屏幕了。</p>
<blockquote>
<p>105.最近尝试录制视频时在屏幕上显示键盘快捷键的操作，</p>
</blockquote>
<p>ScreenFlow固然可以实现这个功能，不过99美元的价格让人感觉得不偿失。搜索之下找到了KeyCastr，简单设置了一下发现可以实现我需要的功能，项目托管在GitHub上，网址：<a href="https://github.com/sdeken/keycastr" target="_blank" rel="noopener">https://github.com/sdeken/keycastr</a>, 可以直接下载DMG包。还有一种方案是使用OS X原生的键盘显示，打开“系统偏好设置”→“语言与文本”→“输入源”，选中左边栏的第一项“键盘与字符显示程序”。关闭偏好设置，这时单击顶部Menu Bar的“语言”，会多出两项功能，单击“显示键盘显示程序”，就会在屏幕上出现一个模拟键盘。</p>
<p>这个方案的缺点是没法区分快捷键和普通字符输入，而且显示速度太快，不够醒目。</p>
<blockquote>
<p>106.复制截屏图片到剪贴板</p>
</blockquote>
<p>以前介绍过如何通过苹果自带的快捷键截屏并存储图片文件，例如shift+command+3和shift+command+4。现在发现如果在以上两个截屏动作中加入ctrl键，可以实现直接把图片保存在剪贴板中，而不是实体文件中，这样就可以通过command+v直接把截取的图片内容复制到图像处理软件或Pages、Keynote等文件中了。</p>
<blockquote>
<p>107.CheatSheet</p>
</blockquote>
<p>一生要记住多少快捷键呢？我都不知道自己记住了多少快捷键，很多快捷键是到了那个环境下才能想起来。但是毋庸置疑，快捷键可以大大提高我们的工作效率，在Mac环境下使用快捷键和不使用，几乎是两种体验。如何记住这些快捷键呢，有人开发了一款软件叫做CheatSheet，安装并打开之后，当你记不住快捷键的时候，按住command键两秒钟，就会弹出一个当前应用软件快捷键列表，不全，但是对大部分用户都够用了。</p>
<p>下载地址：<a href="http://www.cheatsheetapp.com/CheatSheet/" target="_blank" rel="noopener">http://www.cheatsheetapp.com/CheatSheet/</a></p>
<blockquote>
<p>108.HTML5Player</p>
</blockquote>
<p>现在越来越多的人开始看在线视频，目前大部分视频网站的播放器都是基于Flash技术的，而苹果一直对Flash很抵触，支持得也不好，Flash播一会Mac机身就会变热。另外现在的视频网站广告太多，页面花里胡哨也不适合观看。于是有位无聊的程序员做了一个HTML5播放器，可以把在线视频的播放转化成HTML5方式，并且去除广告。使用起来非常简单，只要把{原文}里的HTML5Player链接拖曳到Safari的书签栏，播放视频时单击书签栏上的HTML5Player书签，播放器就会自动转换，效果自己看吧。目前支持优酷、土豆、搜狐视频、爱奇艺、乐视网、QQ、迅雷离线，以及56视频的单视频播放页面。</p>
<p>相关链接：<a href="http://zythum.sinaapp.com/youkuhtml5playerbookmark/" target="_blank" rel="noopener">http://zythum.sinaapp.com/youkuhtml5playerbookmark/</a></p>
<blockquote>
<p>109.重建Spotlight索引</p>
</blockquote>
<p>以前给大家介绍过，在OS X中几乎不需要进行文档和文件夹管理，因为有Spotlight机制，可以瞬间找到你想要的文件，只要你记得这个文件的一点蛛丝马迹。</p>
<p>但是Spotlight也有出问题的时候，就是它的索引文件出事了，比如查找速度变慢，某些文件明明在硬盘上就是检索不到，等等，这时候就需要重建索引了。</p>
<p>打开终端程序，输入如下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo mdutil -i off /</div><div class="line">#该命令用来关闭索引</div><div class="line">sudo mdutil -E /</div><div class="line">#该命令用来删除索引</div><div class="line">sudo mdutil -i on /</div><div class="line">#该命令用来重建索引</div></pre></td></tr></table></figure>
<p>然后用快捷键呼出Spotlight菜单，随便输入一个词，就能看到提示，正在进行索引，并且显示完成重建索引需要的时间。<br>完成之后，Spotlight又可以运转如飞了。</p>
<p>有时候人在某个阶段也需要重建索引，保持初心。什么是初心，空空如也！不要成天得瑟你知道的那点事，多琢磨那些你还不知道的事儿。</p>
<blockquote>
<p>110.用键盘操作Dock和Menu Bar的菜单</p>
</blockquote>
<p>当我们想操作Dock或顶部菜单栏的时候，往往需要鼠标去选中Dock或菜单栏，但是我们往往是不希望去碰鼠标的，这时候快捷键就又开始发挥作用了。使用control+F2可以选中Menu Bar的菜单，通过左右键选择功能，回车执行；使用control+F3可以选中并显示Dock，通过左右键选择功能，回车执行。</p>
<p>该功能在全屏操作时尤其有效。对于F1、F2等不是标准功能键的设置，增加fn键即可。</p>
<blockquote>
<p>111.定义自己的快捷键</p>
</blockquote>
<p>我认为OS X是一个把GUI、程序进程和脚本结合得最好的操作系统，当然这样说可能有些读者不是很明白，这么说吧，OS X是一个定制化非常强的系统，很多人说OS X封闭，事实上OS X为用户预留了非常多的入口和切面，让你能够通过简单、简洁的办法进入系统做你想做的事情。<br>举例来说，对于普通用户，你可以通过键盘的快捷键设置定义自己的常用操作。对于程序员，你可以自己通过AppleScript、Shell、Automator等创建自己的服务，也可通过类似Alfred2这样的优秀工具编写自己的workflow。</p>
<p>今天给大家说说第一种。打开“系统偏好设置”→“键盘”→“键盘快捷键”，左侧栏里列出了各种功能的快捷键，比如Launchpad、Dock、Mission Control、截屏、服务等，大家可以在这些选项中定义和修改自己常用的快捷键，增加右键菜单，等等。</p>
<blockquote>
<p>112.选择文本</p>
</blockquote>
<p>用command+鼠标，可以选中不同位置的文本内容。<br>用option+鼠标，可以对文本进行块选。</p>
<blockquote>
<p>113.Dock中的文件夹</p>
</blockquote>
<p>这个功能非常适合普通用户使用。一般安装了系统后Dock右边会有几个默认的文件夹，事实上你可以把任何常用的文件夹拖到这个位置，不想要的拖到废纸篓即可移除。<br>Dock文件夹的显示方式提供了扇状、网格和列表三种方式，我一般使用网格和列表。但是还有一个隐藏的列表功能，更为实用些，可以在命令行输入如下命令开启：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.dock use-new-list-stack -bool TRUE; killall Dock</div></pre></td></tr></table></figure>
<p>这时候你再启动列表模式，就会发现列表显示方式不一样了，变得更加容易操作了。<br>另外，在列表和网格模式下，还可以通过command+/-来放大和缩小图标，非常方便。</p>
<blockquote>
<p>114.Finder的宽度</p>
</blockquote>
<p>Finder是OS X的默认文件管理器，它提供了多种显示方式，包括图标、列表、分栏和Cover Flow。其中分栏最为常用，通过键盘的方向键浏览多层级的文件非常方便。不过每个分栏的宽度都是系统默认宽度，如何改变这个默认宽度呢？用鼠标拖动分栏线时同时按住option键，这个默认宽度就随之改变了。</p>
<blockquote>
<p>115.Dashboard</p>
</blockquote>
<p>顾名思义，Dashboard就是OS X系统中的仪表盘，它可以在桌面上显示各种小功能块，比如字典、便笺、系统状态、天气预报等。<br>使用快捷键F12或单击Dock中的Dashboard可以运行Dashboard，运行方式有两种，可以在一个新的Space里打开，也可以在当前的Space里打开，可以在“偏好设置”→“Mission Control”中设置。我一般使用在当前Space里打开。</p>
<p>单击左下角的“+”号，可以为Dashboard添加功能块，“-”号可以删除已经添加的功能块。把鼠标移动到某个功能块时按住option键，该功能块会出现一个删除图标，单击也可删除。</p>
<p>如果你想添加更多的功能块，在单击“+”号时，右侧会显示更多的Widget，单击可以到网络上下载你需要的功能。</p>
<p>Dashboard还有一个Web Clip的功能，如果你添加了这个功能块，浏览网页看到特别喜欢的词句或图片，可以单击右键→“在Dashboard打开”，把这部分内容放入Dashboard。</p>
<blockquote>
<p>116.Dock文件夹的使用小技巧</p>
</blockquote>
<p>前面介绍过Dock文件夹的使用问题，再说一个小技巧。当我们打开Dock文件夹后，先打开某个文件所在文件夹时，按住command，单击该文件，就会打开Finder文件夹，并选中你刚才单击的文件。</p>
<blockquote>
<p>117.几个简单的命令</p>
</blockquote>
<p>介绍几个简单的命令。打开终端程序，输入date会显示当前日期，输入cal会显示日历，输入uptime会显示系统从开机到现在所运行的时间。</p>
<blockquote>
<p>118.神奇的option键</p>
</blockquote>
<p>前面介绍过option相关的快捷键和功能，比如选中多个文件，然后按option+右键，可以显示检查器，按住option单击顶部菜单的电池图标会显示电池状况，单击Wi-Fi图标会显示网络状况，单击备份……可以自己试试。别忘了最左边的苹果按钮，option+单击，在下拉菜单击关机、重启都是不提示的。</p>
<p>option+拖曳文件可以复制，按住option输入“=”，输出是“≠”，按住option和shift输入“=”，输出是“±”。</p>
<p>还有好多，没事的时候多按按option键，你会有很多意外的发现。</p>
<blockquote>
<p>119.音乐处理软件XLD</p>
</blockquote>
<p>XLD全程是X Lossless Decoder，是Mac平台上无损音乐播放、编码和转换工具，不仅支持APE、FLAC等无损音频，还支持读取音频CD，将音轨抓取出来之后创建音乐文件。</p>
<p>免费软件，喜欢的可以捐赠。</p>
<p>官网地址：<a href="http://tmkk.undo.jp/xld/index_e.html。" target="_blank" rel="noopener">http://tmkk.undo.jp/xld/index_e.html。</a></p>
<blockquote>
<p>120.保护你的数据文件</p>
</blockquote>
<p>在Mac下对某些文件或数据进行加密操作有两种方式：</p>
<p>第一种：“系统偏好设置”→“安全性与隐私”→“FileVault”→“打开FileVault…”，即可。FileVault是全盘加密技术，可以对磁盘上的所有文件进行加密，后果是系统速度会稍微变慢一点点，一般不建议采用。</p>
<p>第二种：创建磁盘映像文件，对磁盘映像进行加密处理，然后把需要保护的数据和文件放到这个磁盘映像中即可。具体方式如下：<br>打开“应用程序”→“实用工具”→“磁盘工具”，单击“新建映像”，在“加密”选项处选择“256位AES加密（更安全但速度较慢）”，这种加密算法是极其安全的。创建映像时输入两次密码，即可创建加密的磁盘映像文件。在创建时最好不要选择“在我的钥匙串中记住密码”，这样可以每次打开这个磁盘映像文件时都需要输入密码，可以达到最佳保护数据的作用。</p>
<blockquote>
<p>121.如何禁用通知</p>
</blockquote>
<p>很多时候写作或写代码时，不希望被打扰，这时候就需要把OX S的通知关掉。双指从触控板右侧滑入，呼出“通知中心”，在最顶部有一个“显示提示和横幅”的开关，关掉就会禁止通知，不过第二天会自动恢复这个通知设置。</p>
<p>更简单的做法是按住option键单击屏幕右上角的“通知”图标。</p>
<blockquote>
<p>122.Finder的工具栏</p>
</blockquote>
<p>我们可以把文件和程序拖到Finder的工具栏上，以便随时打开。但是想移除时会发现单击鼠标拖动是没法把这些图标移除的，这时候只要在单击拖动时加上command，你就会发现这些图标被销毁了。</p>
<blockquote>
<p>123.Spotlight搜索时文件的定位</p>
</blockquote>
<p>用Spotlight搜索的时候，搜到文件时，我们有时候会需要打开该文件所在的文件夹，这时候按住command*键，单击文件即可打开Finder，并定位到该文件所在文件夹。</p>
<blockquote>
<p>124.重新启动Finder</p>
</blockquote>
<p>Finder是OS X系统中的常驻程序，一般不需要退出，如果想重新启动Finder，有一个简单的方式：按住option键，右键单击Dock上的Finder图标，底部菜单会出现“重新开启”的选项，单击即可。同样的操作对其他Dock上的程序是强制退出。</p>
<blockquote>
<p>125.屏幕画中画</p>
</blockquote>
<p>之前介绍过屏幕放大功能，也就是通过option+command++/-可以放大和缩小屏幕，使用control+滚轮也可以。</p>
<p>不过这只是放大屏幕方式的一种表现形式，我们还可以通过辅助设置改为画中画模式，打开“系统偏好设置”→“缩放”→“缩放样式”，把全屏幕改为画中画即可，效果大家自己看吧。</p>
<blockquote>
<p>126.粘贴纯文本</p>
</blockquote>
<p>我们在网页或其他文档上复制文字的时候，会把文字格式一并复制下来，command+v会把文字格式都粘贴过去，如果我们只想粘贴纯文本，可以使用shift+option+command+v，大部分软件都支持这种方式复制纯文本。</p>
<blockquote>
<p>127.终端命令lsof</p>
</blockquote>
<p>有用户问，在倾倒废纸篓的时候，经常会提示该文件还在使用，不能删除，但是又不知道哪个程序在用，怎么办？</p>
<p>Unix下有一个命令叫做lsof，名字是list open files的缩写，顾名思义，就是查看打开的文件，在终端里输入lsof文件名，就可以找到打开这个文件的程序。关掉程序，就可以正常删除文件。当然lsof还有很多丰富的指令，感兴趣的用户自行Google吧。</p>
<blockquote>
<p>128.AirDrop的有线传输</p>
</blockquote>
<p>AirDrop默认只能通过Wi-Fi来传文件，如果电脑已经连了网线，但是没开Wi-Fi就不能用AirDrop了，有一个办法可以打开AirDrop通过有线传文件的特征。打开终端输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write com.apple.NetworkBrowser BrowseAllInterfaces 1</div></pre></td></tr></table></figure>
<p>然后选中Dock栏的Finder，按住option键，然后右键单击Finder图标，单击底部菜单项“重新开启”，Finder重启之后，即使你的电脑没开Wi-Fi，也可以用AirDrop给别人分享传文件了。</p>
<blockquote>
<p>129.切换程序时实现预览功能</p>
</blockquote>
<p>通过command+tab可以实现程序之间的切换，如果我们想在切换到某个程序的时候看看该程序组都在显示什么，可以按住command的同时按数字键1或上下方向键，系统会调出该程序的Exposé模式，这时你可以放开所有按键，用鼠标或方向键选择显示哪个程序窗口。</p>
<blockquote>
<p>130.Spotlight检索的高级技巧</p>
</blockquote>
<p>◆通过文件类型搜索文件，搜索格式是：<br>kind:文件类型——搜索关键字<br>比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">kind:app——搜索应用程序</div><div class="line">kind:bookmark——搜索书签和历史记录</div><div class="line">kind:contact——搜索联系人</div><div class="line">kind:document——搜索各类文档</div><div class="line">kind:word——搜索word</div><div class="line">kind:pages——搜索pages</div><div class="line">kind:key——搜索keynote</div><div class="line">kind:email——搜索email</div><div class="line">kind:event——搜索日历事件</div><div class="line">kind:folder——搜索文件夹</div><div class="line">kind:movies——搜索视频</div><div class="line">kind:music——搜索音乐</div><div class="line">kind:pdf——搜索pdf文件</div><div class="line">kind:pic——搜索图片</div><div class="line">……</div></pre></td></tr></table></figure>
<p>◆通过标签颜色搜索。如果你喜欢使用各种颜色的标签标注不同的文件夹，那么这个功能就用得上了。“label:红”就可以找到红色标签的文件和文件夹。<br>◆通过日期搜索。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">date:today——查看今天创建或修改的文件</div><div class="line">date:yesterday——查看昨天创建或修改的文件</div><div class="line">date:2013-05-01——查看2013年5月1日创建或修改的文件</div></pre></td></tr></table></figure>
<p>◆条件表达式。想搜索包含Mac不包含Windows的Keynote，可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kind:key Mac -Windows</div></pre></td></tr></table></figure>
<p>也可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kind:key Mac NOT Windows</div></pre></td></tr></table></figure>
<p> 我们可以使用+或-进行条件表达式求值，也可以通过NOT、AND和OR来检索，不过后者一定要大写，否则会被当做搜索内容处理。<br>有了以上4种搜索方式，天下再无搜不到的文档！</p>
]]></content>
      
        <categories>
            
            <category> 日知录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> macbook </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（40）：机器学习中的数据清洗与特征处理综述]]></title>
      <url>/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8840%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E7%BB%BC%E8%BF%B0/</url>
      <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>随着美团交易规模的逐步增大，积累下来的业务数据和交易数据越来越多，这些数据是美团做为一个团购平台最宝贵的财富。通过对这些数据的分析和挖掘，不仅能给美团业务发展方向提供决策支持，也为业务的迭代指明了方向。目前在美团的团购系统中大量地应用到了机器学习和数据挖掘技术，例如个性化推荐、筛选排序、搜索排序、用户建模等等，为公司创造了巨大的价值。<br>本文主要介绍在美团的推荐与个性化团队实践中的数据清洗与特征挖掘方法。主要内容已经在内部公开课”机器学习InAction系列”讲过，本博客的内容主要是讲座内容的提炼和总结。</p>
<a id="more"></a>
<h2 id="二、综述"><a href="#二、综述" class="headerlink" title="二、综述"></a>二、综述</h2><p><img src="http://omu7tit09.bkt.clouddn.com/15049332797637.png" alt=""><br>如上图所示是一个经典的机器学习问题框架图。数据清洗和特征挖掘的工作是在灰色框中框出的部分，即“数据清洗=&gt;特征，标注数据生成=&gt;模型学习=&gt;模型应用”中的前两个步骤。<br>灰色框中蓝色箭头对应的是离线处理部分。主要工作是</p>
<ul>
<li>从原始数据，如文本、图像或者应用数据中清洗出特征数据和标注数据。</li>
<li>对清洗出的特征和标注数据进行处理，例如样本采样，样本调权，异常点去除，特征归一化处理，特征变化，特征组合等过程。最终生成的数据主要是供模型训练使用。</li>
</ul>
<p>灰色框中绿色箭头对应的是在线处理的部分。所做的主要工作和离线处理的类似，主要的区别在于1.不需要清洗标注数据，只需要处理得到特征数据，在线模型使用特征数据预测出样本可能的标签。2.最终生成数据的用处，最终生成的数据主要用于模型的预测，而不是训练。<br>在离线的处理部分，可以进行较多的实验和迭代，尝试不同的样本采样、样本权重、特征处理方法、特征组合方法等，最终得到一个最优的方法，在离线评估得到好的结果后，最终将确定的方案在线上使用。<br>另外，由于在线和离线环境不同，存储数据、获取数据的方法存在较大的差异。例如离线数据获取可以将数据存储在Hadoop，批量地进行分析处理等操作，并且容忍一定的失败。而在线服务获取数据需要稳定、延时小等，可以将数据建入索引、存入KV存储系统等。后面在相应的部分会详细地介绍。</p>
<p>本文以点击下单率预测为例，结合实例来介绍如何进行数据清洗和特征处理。首先介绍下点击下单率预测任务，其业务目标是提高团购用户的用户体验，帮助用户更快更好地找到自己想买的单子。这个概念或者说目标看起来比较虚，我们需要将其转换成一个技术目标，便于度量和实现。最终确定的技术目标是点击下单率预估，去预测用户点击或者购买团购单的概率。我们将预测出来点击或者下单率高的单子排在前面，预测的越准确，用户在排序靠前的单子点击、下单的就越多，省去了用户反复翻页的开销，很快就能找到自己想要的单子。离线我们用常用的衡量排序结果的AUC指标，在线的我们通过ABTest来测试算法对下单率、用户转化率等指标的影响。</p>
<h2 id="三、特征使用方案"><a href="#三、特征使用方案" class="headerlink" title="三、特征使用方案"></a>三、特征使用方案</h2><p>在确定了目标之后，下一步，我们需要确定使用哪些数据来达到目标。需要事先梳理哪些特征数据可能与用户是否点击下单相关。我们可以借鉴一些业务经验，另外可以采用一些特征选择、特征分析等方法来辅助我们选择。具体的特征选择，特征分析等方法我们后面会详细介绍。从业务经验来判断，可能影响用户是否点击下单的因素有：</p>
<ul>
<li>距离，很显然这是一个很重要的特征。如果购买一个离用户距离较远的单子，用户去消费这个单子需要付出很多的代价。 当然，也并不是没有买很远单子的用户，但是这个比例会比较小。</li>
<li>用户历史行为，对于老用户，之前可能在美团有过购买、点击等行为。<br>用户实时兴趣。</li>
<li>单子质量，上面的特征都是比较好衡量的，单子质量可能是更复杂的一个特征。</li>
<li>是否热门，用户评价人数，购买数等等。</li>
</ul>
<p>在确定好要使用哪些数据之后，我们需要对使用数据的可用性进行评估，包括数据的获取难度，数据的规模，数据的准确率，数据的覆盖率等，</p>
<ul>
<li>数据获取难度：例如获取用户id不难，但是获取用户年龄和性别较困难，因为用户注册或者购买时，这些并不是必填项。即使填了也不完全准确。这些特征可能是通过额外的预测模型预测的，那就存在着模型精度的问题。</li>
<li>数据覆盖率：数据覆盖率也是一个重要的考量因素，例如距离特征，并不是所有用户的距离我们都能获取到。PC端的就没有距离，还有很多用户禁止使用它们的地理位置信息等。</li>
<li>用户历史行为，只有老用户才会有行为。</li>
<li>用户实时行为，如果用户刚打开app，还没有任何行为，同样面临着一个冷启动的问题。<br>数据的准确率</li>
<li>单子质量，用户性别等，都会有准确率的问题。</li>
</ul>
<h2 id="四、特征获取方案"><a href="#四、特征获取方案" class="headerlink" title="四、特征获取方案"></a>四、特征获取方案</h2><p>Ok，在选定好要用的特征之后，我们需要考虑一个问题。就是这些数据从哪可以获取？只有获取了这些数据我们才能用上。否则，提一个不可能获取到的特征，获取不到，提了也是白提。下面就介绍下特征获取方案。</p>
<ul>
<li>离线特征获取方案：离线可以使用海量的数据，借助于分布式文件存储平台，例如HDFS等，使用例如MapReduce，Spark等处理工具来处理海量的数据等。</li>
<li>在线特征获取方案：在线特征比较注重获取数据的延时，由于是在线服务，需要在非常短的时间内获取到相应的数据，对查找性能要求非常高，可以将数据存储在索引、kv存储等。而查找性能与数据的数据量会有矛盾，需要折衷处理，我们使用了特征分层获取方案，如下图所示。<img src="http://omu7tit09.bkt.clouddn.com/15049333997560.png" alt="">出于性能考虑。在粗排阶段，使用更基础的特征，数据直接建入索引。精排阶段，再使用一些个性化特征等。</li>
</ul>
<h2 id="五、特征与标注数据清洗"><a href="#五、特征与标注数据清洗" class="headerlink" title="五、特征与标注数据清洗"></a>五、特征与标注数据清洗</h2><p>在了解特征数据放在哪儿、怎样获取之后。下一步就是考虑如何处理特征和标注数据了。下面3节都是主要讲的特征和标注处理方法</p>
<h3 id="5-1-标注数据清洗"><a href="#5-1-标注数据清洗" class="headerlink" title="5.1 标注数据清洗"></a>5.1 标注数据清洗</h3><p>首先介绍下如何清洗特征数据，清洗特征数据方法可以分为离线清洗和在线清洗两种方法。</p>
<ul>
<li>离线清洗数据：离线清洗优点是方便评估新特征效果，缺点是实时性差，与线上实时环境有一定误差。对于实时特征难以训练得到恰当的权重。</li>
<li>在线清洗数据：在线清洗优点是实时性强，完全记录的线上实际数据，缺点是新特征加入需要一段时间做数据积累。</li>
</ul>
<h3 id="5-2-样本采样与样本过滤"><a href="#5-2-样本采样与样本过滤" class="headerlink" title="5.2 样本采样与样本过滤"></a>5.2 样本采样与样本过滤</h3><p>特征数据只有在和标注数据合并之后，才能用来做为模型的训练。下面介绍下如何清洗标注数据。主要是数据采样和样本过滤。</p>
<p>数据采样，例如对于分类问题：选取正例，负例。对于回归问题，需要采集数据。对于采样得到的样本，根据需要，需要设定样本权重。当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。采样的方法包括随机采样，固定比例采样等方法。</p>
<p>除了采样外，经常对样本还需要进行过滤，包括</p>
<ul>
<li>1.结合业务情况进行数据的过滤，例如去除crawler抓取，spam，作弊等数据。</li>
<li>2.异常点检测，采用异常点检测算法对样本进行分析，常用的异常点检测算法包括<br>偏差检测，例如聚类，最近邻等。<ul>
<li>基于统计的异常点检测算法</li>
<li>例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。全距(Range)，又称极差，是用来表示统计资料中的变异量数(measures of variation) ，其最大值与最小值之间的差距；四分位距通常是用来构建箱形图，以及对概率分布的简要图表概述。</li>
<li>基于距离的异常点检测算法，主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。</li>
<li>基于密度的异常点检测算法，考察当前点周围密度，可以发现局部异常点，例如LOF算法</li>
</ul>
</li>
</ul>
<h2 id="六、特征分类"><a href="#六、特征分类" class="headerlink" title="六、特征分类"></a>六、特征分类</h2><p>在分析完特征和标注的清洗方法之后，下面来具体介绍下特征的处理方法，先对特征进行分类，对于不同的特征应该有不同的处理方法。</p>
<p>根据不同的分类方法，可以将特征分为(1)Low level特征和High level特征。(2)稳定特征与动态特征。(3)二值特征、连续特征、枚举特征。</p>
<p>Low level特征是较低级别的特征，主要是原始特征，不需要或者需要非常少的人工处理和干预，例如文本特征中的词向量特征，图像特征中的像素点，用户id，商品id等。Low level特征一般维度比较高，不能用过于复杂的模型。High level特征是经过较复杂的处理，结合部分业务逻辑或者规则、模型得到的特征，例如人工打分，模型打分等特征，可以用于较复杂的非线性模型。Low level 比较针对性，覆盖面小。长尾样本的预测值主要受high level特征影响。 高频样本的预测值主要受low level特征影响。</p>
<p>稳定特征是变化频率(更新频率)较少的特征，例如评价平均分，团购单价格等，在较长的时间段内都不会发生变化。动态特征是更新变化比较频繁的特征，有些甚至是实时计算得到的特征，例如距离特征，2小时销量等特征。或者叫做实时特征和非实时特征。针对两类特征的不同可以针对性地设计特征存储和更新方式，例如对于稳定特征，可以建入索引，较长时间更新一次，如果做缓存的话，缓存的时间可以较长。对于动态特征，需要实时计算或者准实时地更新数据，如果做缓存的话，缓存过期时间需要设置的较短。</p>
<p>二值特征主要是0/1特征，即特征只取两种值：0或者1，例如用户id特征：目前的id是否是某个特定的id，词向量特征：某个特定的词是否在文章中出现等等。连续值特征是取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。枚举值特征主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。在实际的使用中，我们可能对不同类型的特征进行转换，例如将枚举特征或者连续特征处理为二值特征。枚举特征处理为二值特征技巧：将枚举特征映射为多个特征，每个特征对应一个特定枚举值，例如今天周几，可以把它转换成7个二元特征：今天是否是周一，今天是否是周二，…，今天是否是周日。连续值处理为二值特征方法：先将连续值离散化（后面会介绍如何离散化)，再将离散化后的特征切分为N个二元特征，每个特征代表是否在这个区间内。</p>
<h3 id="6-1-特征归一化，离散化，缺省值处理"><a href="#6-1-特征归一化，离散化，缺省值处理" class="headerlink" title="6.1 特征归一化，离散化，缺省值处理"></a>6.1 特征归一化，离散化，缺省值处理</h3><p>主要用于单个特征的处理。</p>
<ul>
<li><p>归一化<br>不同的特征有不同的取值范围，在有些算法中，例如线性模型或者距离相关的模型像聚类模型、knn模型等，特征的取值范围会对最终的结果产生较大影响，例如二元特征的取值范围为[0，1]，而距离特征取值可能是[0，正无穷)，在实际使用中会对距离进行截断，例如[0，3000000]，但是这两个特征由于取值范围不一致导致了模型可能会更偏向于取值范围较大的特征，为了平衡取值范围不一致的特征，需要对特征进行归一化处理，将特征取值归一化到［0，1］区间。常用的归一化方法包括1.函数归一化，通过映射函数将特征取值映射到［0，1］区间，例如最大最小值归一化方法，是一种线性的映射。还有通过非线性函数的映射，例如log函数等。2.分维度归一化，可以使用最大最小归一化方法，但是最大最小值选取的是所属类别的最大最小值，即使用的是局部最大最小值，不是全局的最大最小值。3.排序归一化，不管原来的特征取值是什么样的，将特征按大小排序，根据特征所对应的序给予一个新的值。</p>
</li>
<li><p>离散化<br>在上面介绍过连续值的取值空间可能是无穷的，为了便于表示和在模型中处理，需要对连续值特征进行离散化处理。常用的离散化方法包括等值划分和等量划分。等值划分是将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0，1)，[1，2)，…，[9，10)。等量划分是根据样本总数进行均分，每段等量个样本划分为1段。例如距离特征，取值范围［0，3000000］，现在需要切分成10段，如果按照等比例划分的话，会发现绝大部分样本都在第1段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。</p>
</li>
<li><p>缺省值处理<br>有些特征可能因为无法采样或者没有观测值而缺失，例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺省值。缺省值如何赋予，也有很多种方法。例如单独表示，众数，平均值等。</p>
</li>
</ul>
<h3 id="6-2-特征降维"><a href="#6-2-特征降维" class="headerlink" title="6.2 特征降维"></a>6.2 特征降维</h3><p>在介绍特征降维之前，先介绍下特征升维。在机器学习中，有一个VC维理论。根据VC维理论，VC维越高，打散能力越强，可容许的模型复杂度越高。在低维不可分的数据，映射到高维是可分。可以想想，给你一堆物品，人脑是如何对这些物品进行分类，依然是找出这些物品的一些特征，例如：颜色，形状，大小，触感等等，然后根据这些特征对物品做以归类，这其实就是一个先升维，后划分的过程。比如我们人脑识别香蕉。可能首先我们发现香蕉是黄色的。这是在颜色这个维度的一个切分。但是很多东西都是黄色的啊，例如哈密瓜。那么怎么区分香蕉和哈密瓜呢？我们发现香蕉形状是弯曲的。而哈密瓜是圆形的，那么我们就可以用形状来把香蕉和哈密瓜划分开了，即引入一个新维度：形状，来区分。这就是一个从“颜色”一维特征升维到二维特征的例子。</p>
<p>那问题来了，既然升维后模型能力能变强，那么是不是特征维度越高越好呢？为什么要进行特征降维&amp;特征选择？主要是出于如下考虑：1. 特征维数越高，模型越容易过拟合，此时更复杂的模型就不好用。2. 相互独立的特征维数越高，在模型不变的情况下，在测试集上达到相同的效果表现所需要的训练样本的数目就越大。 3. 特征数量增加带来的训练、测试以及存储的开销都会增大。4.在某些模型中，例如基于距离计算的模型KMeans，KNN等模型，在进行距离计算时，维度过高会影响精度和性能。5.可视化分析的需要。在低维的情况下，例如二维，三维，我们可以把数据绘制出来，可视化地看到数据。当维度增高时，就难以绘制出来了。在机器学习中，有一个非常经典的维度灾难的概念。用来描述当空间维度增加时，分析和组织高维空间，因体积指数增加而遇到各种问题场景。例如，100个平均分布的点能把一个单位区间以每个点距离不超过0.01采样；而当维度增加到10后，如果以相邻点距离不超过0.01小方格采样单位超一单位超正方体，则需要10^20 个采样点。</p>
<p>正是由于高维特征有如上描述的各种各样的问题，所以我们需要进行特征降维和特征选择等工作。特征降维常用的算法有PCA，LDA等。特征降维的目标是将高维空间中的数据集映射到低维空间数据，同时尽可能少地丢失信息，或者降维后的数据点尽可能地容易被区分</p>
<ul>
<li><p>PCA算法<br>通过协方差矩阵的特征值分解能够得到数据的主成分，以二维特征为例，两个特征之间可能存在线性关系（例如运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是发现这种特征之间的线性关系，并去除。</p>
</li>
<li><p>LDA算法<br>考虑label，降维后的数据点尽可能地容易被区分</p>
</li>
</ul>
<h3 id="6-3-特征选择"><a href="#6-3-特征选择" class="headerlink" title="6.3 特征选择"></a>6.3 特征选择</h3><p>特征选择的目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。</p>
<p>特征选择的一般过程如下图所示：<br><img src="http://omu7tit09.bkt.clouddn.com/15049336032351.png" alt=""><br>主要分为产生过程，评估过程，停止条件和验证过程。</p>
<h4 id="6-3-1-特征选择-产生过程和生成特征子集方法"><a href="#6-3-1-特征选择-产生过程和生成特征子集方法" class="headerlink" title="6.3.1 特征选择-产生过程和生成特征子集方法"></a>6.3.1 特征选择-产生过程和生成特征子集方法</h4><blockquote>
<p>完全搜索(Complete)</p>
</blockquote>
<ul>
<li>广度优先搜索( Breadth First Search )：广度优先遍历特征子空间。枚举所有组合，穷举搜索，实用性不高。</li>
<li>分支限界搜索( Branch and Bound )：穷举基础上加入分支限界。例如：剪掉某些不可能搜索出比当前最优解更优的分支。</li>
<li>其他，如定向搜索 (Beam Search )，最优优先搜索 ( Best First Search )等</li>
</ul>
<blockquote>
<p>启发式搜索(Heuristic)</p>
</blockquote>
<ul>
<li>序列前向选择( SFS ， Sequential Forward Selection )：从空集开始，每次加入一个选最优。</li>
<li>序列后向选择( SBS ， Sequential Backward Selection )：从全集开始，每次减少一个选最优。</li>
<li>增L去R选择算法 ( LRS ， Plus-L Minus-R Selection )：从空集开始，每次加入L个，减去R个，选最优（L&gt;R)或者从全集开始，每次减去R个，增加L个，选最优(L&lt;R)。</li>
</ul>
<p>其他如双向搜索( BDS ， Bidirectional Search )，序列浮动选择( Sequential Floating Selection )等</p>
<blockquote>
<p>随机搜索(Random)</p>
</blockquote>
<ul>
<li>随机产生序列选择算法(RGSS， Random Generation plus Sequential Selection)</li>
<li>随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。</li>
<li>模拟退火算法( SA， Simulated Annealing )：以一定的概率来接受一个比当前解要差的解，而且这个概率随着时间推移逐渐降低</li>
<li>遗传算法( GA， Genetic Algorithms )：通过交叉、突变等操作繁殖出下一代特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。</li>
</ul>
<p>随机算法共同缺点:依赖随机因素，有实验结果难重现。</p>
<h4 id="6-3-2-特征选择－有效性分析"><a href="#6-3-2-特征选择－有效性分析" class="headerlink" title="6.3.2 特征选择－有效性分析"></a>6.3.2 特征选择－有效性分析</h4><p>对特征的有效性进行分析，得到各个特征的特征权重，根据是否与模型有关可以分为1.与模型相关特征权重，使用所有的特征数据训练出来模型，看在模型中各个特征的权重，由于需要训练出模型，模型相关的权重与此次学习所用的模型比较相关。不同的模型有不同的模型权重衡量方法。例如线性模型中，特征的权重系数等。2.与模型无关特征权重。主要分析特征与label的相关性，这样的分析是与这次学习所使用的模型无关的。与模型无关特征权重分析方法包括(1)交叉熵，(2)Information Gain，(3)Odds ratio，(4)互信息，(5)KL散度等</p>
<h2 id="七、特征监控"><a href="#七、特征监控" class="headerlink" title="七、特征监控"></a>七、特征监控</h2><p>在机器学习任务中，特征非常重要。</p>
<ul>
<li>个人经验，80%的效果由特征带来。下图是随着特征数的增加，最终模型预测值与实际值的相关系数变化。</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049337889760.png" alt=""></p>
<ul>
<li>对于重要的特征进行监控与有效性分析，了解模型所用的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防止灾难性结果。需要建立特征有效性的长效监控机制</li>
</ul>
<p>我们对关键特征进行了监控，下面特征监控界面的一个截图。通过监控我们发现有一个特征的覆盖率每天都在下降，与特征数据提供方联系之后，发现特征数据提供方的数据源存在着问题，在修复问题之后，该特征恢复正常并且覆盖率有了较大提升。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049338172365.png" alt=""></p>
<p>在发现特征出现异常时，我们会及时采取措施，对服务进行降级处理，并联系特征数据的提供方尽快修复。对于特征数据生成过程中缺乏监控的情况也会督促做好监控，在源头解决问题。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 特征处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（39）：实例详解机器学习如何解决问题]]></title>
      <url>/2017/09/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8839%EF%BC%89%EF%BC%9A%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>随着大数据时代的到来，机器学习成为解决问题的一种重要且关键的工具。不管是工业界还是学术界，机器学习都是一个炙手可热的方向，但是学术界和工业界对机器学习的研究各有侧重，学术界侧重于对机器学习理论的研究，工业界侧重于如何用机器学习来解决实际问题。我们结合美团在机器学习上的实践，进行一个实战（InAction）系列的介绍（带“机器学习InAction系列”标签的文章），介绍机器学习在解决工业界问题的实战中所需的基本技术、经验和技巧。本文主要结合实际问题，概要地介绍机器学习解决实际问题的整个流程，包括对问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节；另外几篇则会对这些关键环节进行更深入地介绍。</p>
<a id="more"></a>
<p>下文分为</p>
<ul>
<li>1）机器学习的概述，</li>
<li>2）对问题建模，</li>
<li>3）准备训练数据，</li>
<li>4）抽取特征，</li>
<li>5）训练模型，</li>
<li>6）优化模型，</li>
<li>7）总结 </li>
</ul>
<p>共7个章节进行介绍。</p>
<h2 id="二、机器学习的概述："><a href="#二、机器学习的概述：" class="headerlink" title="二、机器学习的概述："></a>二、机器学习的概述：</h2><h3 id="2-1-什么是机器学习？"><a href="#2-1-什么是机器学习？" class="headerlink" title="2.1 什么是机器学习？"></a>2.1 什么是机器学习？</h3><p>随着机器学习在实际工业领域中不断获得应用，这个词已经被赋予了各种不同含义。在本文中的“机器学习”含义与wikipedia上的解释比较契合，如下：<br>Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data.</p>
<p>机器学习可以分为无监督学习（unsupervised learning）和有监督学习（supervised learning），在工业界中，有监督学习是更常见和更有价值的方式，下文中主要以这种方式展开介绍。如下图中所示，有监督的机器学习在解决实际问题时，有两个流程，一个是离线训练流程（蓝色箭头），包含数据筛选和清洗、特征抽取、模型训练和优化模型等环节；另一个流程则是应用流程（绿色箭头），对需要预估的数据，抽取特征，应用离线训练得到的模型进行预估，获得预估值作用在实际产品中。在这两个流程中，离线训练是最有技术挑战的工作（在线预估流程很多工作可以复用离线训练流程的工作），所以下文主要介绍离线训练流程。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049318784960.png" alt=""></p>
<h3 id="2-2-什么是模型？"><a href="#2-2-什么是模型？" class="headerlink" title="2.2 什么是模型？"></a>2.2 什么是模型？</h3><p>模型，是机器学习中的一个重要概念，简单的讲，指特征空间到输出空间的映射；一般由模型的假设函数和参数w组成（下面公式就是Logistic Regression模型的一种表达，在训练模型的章节做稍详细的解释）；一个模型的假设空间（hypothesis space），指给定模型所有可能w对应的输出空间组成的集合。工业界常用的模型有Logistic Regression（简称LR）、Gradient Boosting Decision Tree（简称GBDT）、Support Vector Machine（简称SVM）、Deep Neural Network（简称DNN）等。<br><img src="http://omu7tit09.bkt.clouddn.com/15049319158292.jpg" alt=""></p>
<p>模型训练就是基于训练数据，获得一组参数w，使得特定目标最优，即获得了特征空间到输出空间的最优映射，具体怎么实现，见训练模型章节。</p>
<h3 id="2-3-为什么要用机器学习解决问题？"><a href="#2-3-为什么要用机器学习解决问题？" class="headerlink" title="2.3 为什么要用机器学习解决问题？"></a>2.3 为什么要用机器学习解决问题？</h3><ul>
<li>目前处于大数据时代，到处都有成T成P的数据，简单规则处理难以发挥这些数据的价值；</li>
<li>廉价的高性能计算，使得基于大规模数据的学习时间和代价降低；</li>
<li>廉价的大规模存储，使得能够更快地和代价更小地处理大规模数据；</li>
<li>存在大量高价值的问题，使得花大量精力用机器学习解决问题后，能获得丰厚收益。</li>
</ul>
<h3 id="2-4-机器学习应该用于解决什么问题？"><a href="#2-4-机器学习应该用于解决什么问题？" class="headerlink" title="2.4 机器学习应该用于解决什么问题？"></a>2.4 机器学习应该用于解决什么问题？</h3><ul>
<li>目标问题需要价值巨大，因为机器学习解决问题有一定的代价；</li>
<li>目标问题有大量数据可用，有大量数据才能使机器学习比较好地解决问题（相对于简单规则或人工）；</li>
<li>目标问题由多种因素（特征）决定，机器学习解决问题的优势才能体现（相对于简单规则或人工）；</li>
<li>目标问题需要持续优化，因为机器学习可以基于数据自我学习和迭代，持续地发挥价值。</li>
</ul>
<h2 id="三、对问题建模"><a href="#三、对问题建模" class="headerlink" title="三、对问题建模"></a>三、对问题建模</h2><p>本文以DEAL（团购单）交易额预估问题为例（就是预估一个给定DEAL一段时间内卖了多少钱），介绍使用机器学习如何解决问题。首先需要：</p>
<ul>
<li>收集问题的资料，理解问题，成为这个问题的专家；</li>
<li>拆解问题，简化问题，将问题转化机器可预估的问题。</li>
</ul>
<p>深入理解和分析DEAL交易额后，可以将它分解为如下图的几个问题：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049320386400.png" alt=""></p>
<h3 id="3-1-单个模型？多个模型？如何来选择？"><a href="#3-1-单个模型？多个模型？如何来选择？" class="headerlink" title="3.1 单个模型？多个模型？如何来选择？"></a>3.1 单个模型？多个模型？如何来选择？</h3><p>按照上图进行拆解后，预估DEAL交易额就有2种可能模式，一种是直接预估交易额；另一种是预估各子问题，如建立一个用户数模型和建立一个访购率模型（访问这个DEAL的用户会购买的单子数），再基于这些子问题的预估值计算交易额。</p>
<p>不同方式有不同优缺点，具体如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-09-09 下午12.41.37.png" alt="屏幕快照 2017-09-09 下午12.41.37"></p>
<p>选择哪种模式？</p>
<ul>
<li>1）问题可预估的难度，难度大，则考虑用多模型；</li>
<li>2）问题本身的重要性，问题很重要，则考虑用多模型；</li>
<li>3）多个模型的关系是否明确，关系明确，则可以用多模型。</li>
</ul>
<p>如果采用多模型，如何融合？</p>
<ul>
<li>可以根据问题的特点和要求进行线性融合，或进行复杂的融合。以本文问题为例，至少可以有如下两种：</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049321442593.png" alt=""></p>
<h3 id="3-2-模型选择"><a href="#3-2-模型选择" class="headerlink" title="3.2 模型选择"></a>3.2 模型选择</h3><p>对于DEAL交易额这个问题，我们认为直接预估难度很大，希望拆成子问题进行预估，即多模型模式。那样就需要建立用户数模型和访购率模型，因为机器学习解决问题的方式类似，下文只以访购率模型为例。要解决访购率问题，首先要选择模型，我们有如下的一些考虑：</p>
<blockquote>
<p>主要考虑</p>
</blockquote>
<ul>
<li>1）选择与业务目标一致的模型；</li>
<li>2）选择与训练数据和特征相符的模型。</li>
</ul>
<p>训练数据少，High Level特征多，则使用“复杂”的非线性模型（流行的GBDT、Random Forest等）；</p>
<p>训练数据很大量，Low Level特征多，则使用“简单”的线性模型（流行的LR、Linear-SVM等）。</p>
<blockquote>
<p>补充考虑</p>
</blockquote>
<p>1）当前模型是否被工业界广泛使用；<br>2）当前模型是否有比较成熟的开源工具包（公司内或公司外）；<br>3）当前工具包能够的处理数据量能否满足要求；<br>4）自己对当前模型理论是否了解，是否之前用过该模型解决问题。</p>
<p>为实际问题选择模型，需要转化问题的业务目标为模型评价目标，转化模型评价目标为模型优化目标；根据业务的不同目标，选择合适的模型，具体关系如下：<br><img src="http://omu7tit09.bkt.clouddn.com/15049322585629.png" alt=""></p>
<p>通常来讲，预估真实数值（回归）、大小顺序（排序）、目标所在的正确区间（分类）的难度从大到小，根据应用所需，尽可能选择难度小的目标进行。对于访购率预估的应用目标来说，我们至少需要知道大小顺序或真实数值，所以我们可以选择Area Under Curve（AUC）或Mean Absolute Error（MAE）作为评估目标，以Maximum likelihood为模型损失函数（即优化目标）。综上所述，我们选择spark版本 GBDT或LR，主要基于如下考虑：</p>
<p>1）可以解决排序或回归问题；<br>2）我们自己实现了算法，经常使用，效果很好；<br>3）支持海量数据；<br>4）工业界广泛使用。</p>
<h2 id="四、准备训练数据"><a href="#四、准备训练数据" class="headerlink" title="四、准备训练数据"></a>四、准备训练数据</h2><p>深入理解问题，针对问题选择了相应的模型后，接下来则需要准备数据；数据是机器学习解决问题的根本，数据选择不对，则问题不可能被解决，所以准备训练数据需要格外的小心和注意：</p>
<h3 id="4-1-注意点："><a href="#4-1-注意点：" class="headerlink" title="4.1 注意点："></a>4.1 注意点：</h3><p>待解决问题的数据本身的分布尽量一致；<br>训练集/测试集分布与线上预测环境的数据分布尽可能一致，这里的分布是指（x,y）的分布，不仅仅是y的分布；<br>y数据噪音尽可能小，尽量剔除y有噪音的数据；<br>非必要不做采样，采样常常可能使实际数据分布发生变化，但是如果数据太大无法训练或者正负比例严重失调（如超过100:1）,则需要采样解决。</p>
<h3 id="4-2-常见问题及解决办法"><a href="#4-2-常见问题及解决办法" class="headerlink" title="4.2 常见问题及解决办法"></a>4.2 常见问题及解决办法</h3><p>待解决问题的数据分布不一致：<br>1）访购率问题中DEAL数据可能差异很大，如美食DEAL和酒店DEAL的影响因素或表现很不一致，需要做特别处理；要么对数据提前归一化，要么将分布不一致因素作为特征，要么对各类别DEAL单独训练模型。<br>数据分布变化了：<br>1）用半年前的数据训练模型，用来预测当前数据，因为数据分布随着时间可能变化了，效果可能很差。尽量用近期的数据训练，来预测当前数据，历史的数据可以做降权用到模型，或做transfer learning。<br>y数据有噪音：<br>1）在建立CTR模型时，将用户没有看到的Item作为负例，这些Item是因为用户没有看到才没有被点击，不一定是用户不喜欢而没有被点击，所以这些Item是有噪音的。可以采用一些简单规则，剔除这些噪音负例，如采用skip-above思想，即用户点过的Item之上，没有点过的Item作为负例（假设用户是从上往下浏览Item）。<br>采样方法有偏，没有覆盖整个集合：<br>1）访购率问题中，如果只取只有一个门店的DEAL进行预估，则对于多门店的DEAL无法很好预估。应该保证一个门店的和多个门店的DEAL数据都有；<br>2）无客观数据的二分类问题，用规则来获得正/负例，规则对正/负例的覆盖不全面。应该随机抽样数据，进行人工标注，以确保抽样数据和实际数据分布一致。</p>
<h3 id="4-3-访购率问题的训练数据"><a href="#4-3-访购率问题的训练数据" class="headerlink" title="4.3 访购率问题的训练数据"></a>4.3 访购率问题的训练数据</h3><p>收集N个月的DEAL数据（x）及相应访购率（y）；<br>收集最近N个月，剔除节假日等非常规时间 （保持分布一致）；<br>只收集在线时长&gt;T 且 访问用户数 &gt; U的DEAL （减少y的噪音）；<br>考虑DEAL销量生命周期 （保持分布一致）；<br>考虑不同城市、不同商圈、不同品类的差别 （保持分布一致）。</p>
<h2 id="五、抽取特征"><a href="#五、抽取特征" class="headerlink" title="五、抽取特征"></a>五、抽取特征</h2><p>完成数据筛选和清洗后，就需要对数据抽取特征，就是完成输入空间到特征空间的转换（见下图）。针对线性模型或非线性模型需要进行不同特征抽取，线性模型需要更多特征抽取工作和技巧，而非线性模型对特征抽取要求相对较低。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049324768552.png" alt=""></p>
<p>通常，特征可以分为High Level与Low Level，High Level指含义比较泛的特征，Low Level指含义比较特定的特征，举例来说：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">DEAL A1属于POIA，人均50以下，访购率高；</div><div class="line">DEAL A2属于POIA，人均50以上，访购率高；</div><div class="line">DEAL B1属于POIB，人均50以下，访购率高；</div><div class="line">DEAL B2属于POIB，人均50以上，访购率底；</div></pre></td></tr></table></figure>
<p>基于上面的数据，可以抽到两种特征，POI（门店）或人均消费；POI特征则是Low Level特征，人均消费则是High Level特征；假设模型通过学习，获得如下预估：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">如果DEALx 属于POIA（Low Level feature），访购率高；</div><div class="line">如果DEALx 人均50以下（High Level feature），访购率高。</div></pre></td></tr></table></figure>
<p>所以，总体上，Low Level 比较有针对性，单个特征覆盖面小（含有这个特征的数据不多），特征数量（维度）很大。High Level比较泛化，单个特征覆盖面大（含有这个特征的数据很多），特征数量（维度）不大。长尾样本的预测值主要受High Level特征影响。高频样本的预测值主要受Low Level特征影响。</p>
<p>对于访购率问题，有大量的High Level或Low Level的特征，其中一些展示在下图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049325864156.png" alt=""></p>
<p>非线性模型的特征</p>
<ul>
<li>1）可以主要使用High Level特征，因为计算复杂度大，所以特征维度不宜太高；</li>
<li>2）通过High Level非线性映射可以比较好地拟合目标。</li>
</ul>
<p>线性模型的特征</p>
<ul>
<li>1）特征体系要尽可能全面，High Level和Low Level都要有；</li>
<li>2）可以将High Level转换Low Level，以提升模型的拟合能力。</li>
</ul>
<h3 id="5-1-特征归一化"><a href="#5-1-特征归一化" class="headerlink" title="5.1 特征归一化"></a>5.1 特征归一化</h3><p>特征抽取后，如果不同特征的取值范围相差很大，最好对特征进行归一化，以取得更好的效果，常见的归一化方式如下：</p>
<ul>
<li><p>Rescaling：<br>归一化到[0,1] 或 [-1，1]，用类似方式：<br><img src="http://omu7tit09.bkt.clouddn.com/15049326747797.jpg" alt=""></p>
</li>
<li><p>Standardization：<br>设为x分布的均值，为x分布的标准差；<br><img src="http://omu7tit09.bkt.clouddn.com/15049327022087.jpg" alt=""></p>
</li>
<li><p>Scaling to unit length：<br>归一化到单位长度向量<br><img src="http://omu7tit09.bkt.clouddn.com/15049327222282.jpg" alt=""></p>
</li>
</ul>
<h3 id="5-2-特征选择"><a href="#5-2-特征选择" class="headerlink" title="5.2 特征选择"></a>5.2 特征选择</h3><p>特征抽取和归一化之后，如果发现特征太多，导致模型无法训练，或很容易导致模型过拟合，则需要对特征进行选择，挑选有价值的特征。</p>
<ul>
<li><p>Filter：<br>假设特征子集对模型预估的影响互相独立，选择一个特征子集，分析该子集和数据Label的关系，如果存在某种正相关，则认为该特征子集有效。衡量特征子集和数据Label关系的算法有很多，如Chi-square，Information Gain。</p>
</li>
<li><p>Wrapper：<br>选择一个特征子集加入原有特征集合，用模型进行训练，比较子集加入前后的效果，如果效果变好，则认为该特征子集有效，否则认为无效。</p>
</li>
<li><p>Embedded：<br>将特征选择和模型训练结合起来，如在损失函数中加入L1 Norm ，L2 Norm。</p>
</li>
</ul>
<h2 id="六、训练模型"><a href="#六、训练模型" class="headerlink" title="六、训练模型"></a>六、训练模型</h2><p>完成特征抽取和处理后，就可以开始模型训练了，下文以简单且常用的Logistic Regression模型（下称LR模型）为例，进行简单介绍。</p>
<p>设有m个（x,y）训练数据，其中x为特征向量，y为label，；w为模型中参数向量，即模型训练中需要学习的对象。<br>所谓训练模型，就是选定假说函数和损失函数，基于已有训练数据（x,y），不断调整w，使得损失函数最优，相应的w就是最终学习结果，也就得到相应的模型。</p>
<h3 id="6-1-模型函数"><a href="#6-1-模型函数" class="headerlink" title="6.1 模型函数"></a>6.1 模型函数</h3><p>假说函数，即假设x和y存在一种函数关系：<br><img src="http://omu7tit09.bkt.clouddn.com/15049328059857.jpg" alt=""></p>
<p>损失函数，基于上述假设函数，构建模型损失函数（优化目标），在LR中通常以（x,y）的最大似然估计为目标：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049328618956.jpg" alt=""></p>
<h3 id="6-2-优化算法"><a href="#6-2-优化算法" class="headerlink" title="6.2 优化算法"></a>6.2 优化算法</h3><ul>
<li>梯度下降（Gradient Descent）<br>即w沿着损失函数的负梯度方向进行调整，示意图见下图，的梯度即一阶导数（见下式），梯度下降有多种类型，如随机梯度下降或批量梯度下降。<br><img src="http://omu7tit09.bkt.clouddn.com/15049328995925.jpg" alt=""><br>随机梯度下降（Stochastic Gradient Descent），每一步随机选择一个样本，计算相应的梯度，并完成w的更新，如下式，<br><img src="http://omu7tit09.bkt.clouddn.com/15049329100270.jpg" alt=""></li>
</ul>
<p>批量梯度下降（Batch Gradient Descent）,每一步都计算训练数据中的所有样本对应的梯度，w沿着这个梯度方向迭代，即<br><img src="http://omu7tit09.bkt.clouddn.com/15049329245601.jpg" alt=""></p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049329425541.png" alt=""></p>
<ul>
<li>牛顿法（Newton’s Method）</li>
</ul>
<p>牛顿法的基本思想是在极小点附近通过对目标函数做二阶Taylor展开，进而找到L(w)的极小点的估计值。形象地讲，在wk处做切线，该切线与L(w)=0的交点即为下一个迭代点wk+1（示意图如下）。w的更新公式如下，其中目标函数的二阶偏导数，即为大名鼎鼎的Hessian矩阵。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049329695345.jpg" alt=""></p>
<p>拟牛顿法（Quasi-Newton Methods）：计算目标函数的二阶偏导数，难度较大，更为复杂的是目标函数的Hessian矩阵无法保持正定；不用二阶偏导数而构造出可以近似Hessian矩阵的逆的正定对称阵，从而在”拟牛顿”的条件下优化目标函数。</p>
<p>BFGS： 使用BFGS公式对H(w)进行近似，内存中需要放H(w),内存需要O(m2)级别；</p>
<p>L-BFGS：存储有限次数（如k次）的更新矩阵，用这些更新矩阵生成新的H(w),内存降至O(m)级别；</p>
<p>OWLQN: 如果在目标函数中引入L1正则化，需要引入虚梯度来解决目标函数不可导问题，OWLQN就是用来解决这个问题。<br><img src="http://omu7tit09.bkt.clouddn.com/15049330065987.png" alt=""></p>
<ul>
<li>Coordinate Descent<br>对于w，每次迭代，固定其他维度不变，只对其一个维度进行搜索，确定最优下降方向（示意图如下），公式表达如下：</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049330264923.jpg" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/15049330405131.jpg" alt=""></p>
<h2 id="七、优化模型"><a href="#七、优化模型" class="headerlink" title="七、优化模型"></a>七、优化模型</h2><p>经过上文提到的数据筛选和清洗、特征设计和选择、模型训练，就得到了一个模型，但是如果发现效果不好？怎么办？</p>
<p>【首先】<br>反思目标是否可预估，数据和特征是否存在bug。</p>
<p>【然后】<br>分析一下模型是Overfitting还是Underfitting，从数据、特征和模型等环节做针对性优化。</p>
<h3 id="7-1-Underfitting-amp-Overfitting"><a href="#7-1-Underfitting-amp-Overfitting" class="headerlink" title="7.1 Underfitting &amp; Overfitting"></a>7.1 Underfitting &amp; Overfitting</h3><p>所谓Underfitting，即模型没有学到数据内在关系，如下图左一所示，产生分类面不能很好的区分X和O两类数据；产生的深层原因，就是模型假设空间太小或者模型假设空间偏离。<br>所谓Overfitting，即模型过渡拟合了训练数据的内在关系，如下图右一所示，产生分类面过好地区分X和O两类数据，而真实分类面可能并不是这样，以至于在非训练数据上表现不好；产生的深层原因，是巨大的模型假设空间与稀疏的数据之间的矛盾。<br><img src="http://omu7tit09.bkt.clouddn.com/15049330908787.png" alt=""></p>
<p>在实战中，可以基于模型在训练集和测试集上的表现来确定当前模型到底是Underfitting还是Overfitting，判断方式如下表：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-09-09 下午12.58.26.png" alt="屏幕快照 2017-09-09 下午12.58.26"></p>
<h3 id="7-2-怎么解决Underfitting和Overfitting问题？"><a href="#7-2-怎么解决Underfitting和Overfitting问题？" class="headerlink" title="7.2 怎么解决Underfitting和Overfitting问题？"></a>7.2 怎么解决Underfitting和Overfitting问题？</h3><p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-09-09 下午12.59.02.png" alt="屏幕快照 2017-09-09 下午12.59.02"></p>
<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>综上所述，机器学习解决问题涉及到问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节，有如下要点：</p>
<ul>
<li><p>理解业务，分解业务目标，规划模型可预估的路线图。</p>
</li>
<li><p>数据：y数据尽可能真实客观；训练集/测试集分布与线上应用环境的数据分布尽可能一致。</p>
</li>
<li><p>特征：利用Domain Knowledge进行特征抽取和选择；针对不同类型的模型设计不同的特征。</p>
</li>
<li><p>模型：针对不同业务目标、不同数据和特征，选择不同的模型；如果模型不符合预期，一定检查一下数据、特征、模型等处理环节是否有bug；考虑模型Underfitting和Qverfitting，针对性地优化。</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（38）：外卖订单量预测异常报警模型实践]]></title>
      <url>/2017/09/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8838%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E5%BC%82%E5%B8%B8%E6%8A%A5%E8%AD%A6%E6%A8%A1%E5%9E%8B%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>外卖业务的快速发展对系统稳定性提出了更高的要求，每一次订单量大盘的异常波动，都需要做出及时的应对，以保证系统的整体稳定性。如何做出较为准确的波动预警，显得尤为重要。</p>
<a id="more"></a>
<p>从时间上看，外卖订单量时间序列有两个明显的特征（如下图所示）：</p>
<p>周期性。每天订单量的变化趋势都大致相同，午高峰和晚高峰订单量集中。<br>实时性。当天的订单量可能会受天气等因素影响，呈现整体的上涨或下降。<br>订单量波动预警，初期外卖订单中心使用的是当前时刻和前一时刻订单量比较，超过一定阈值就报警的方式，误报率和漏报率都比较大。后期将业务数据上传到美团点评的服务治理平台，使用该平台下的基线报警模型进行监控报警。基线数据模型考虑到了订单量时间序列的周期性特征，但是忽略了实时性特征，在实际使用中误报率依然很高，大量的误报漏报导致RD对于报警已经麻木，出现问题时不能及时响应，因此，急需一种新的异常检测模型，提高报警的准确率。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049303473058.png" alt=""></p>
<h2 id="二、异常检测的定义"><a href="#二、异常检测的定义" class="headerlink" title="二、异常检测的定义"></a>二、异常检测的定义</h2><p>异常，意为“异于正常”。异常检测，就是从一组数据中寻找那些和期望数据不同的数据。监控数据都是和时间相关的，每一个监控指标只有和时间组合一起才有其具体的含义。按照时间顺序，将监控指标组成一个序列，我们就得到了监控指标的时间序列。</p>
<p>基于预测的异常检测模型如下图所示，xt是真实数据，通过预测器得到预测数据，然后xt和pt分别作为比较器的输入，最终得到输出yt。yt是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。<br><img src="http://omu7tit09.bkt.clouddn.com/15049303904786.png" alt=""></p>
<p>异常检测主要有两种策略：</p>
<ul>
<li>异常驱动的异常检测（敏感性）：宁愿误报，也不能错过任何一个异常，这适用于非常重要的检测。简单概括，就是“宁可错杀一千，不能放过一个”。</li>
<li>预算驱动的异常检测（准确性）：这种策略的异常检测，从字面理解就是只有定量的一些预算去处理这些报警，那么只能当一定是某种问题时，才能将报警发送出来。</li>
</ul>
<p>这两种策略不可兼容的。对于检测模型的改善，可以从两个方面入手，一是预测器的优化，二是比较器的优化。我们从这两个方面描述模型的改善。</p>
<h2 id="三、预测器设计"><a href="#三、预测器设计" class="headerlink" title="三、预测器设计"></a>三、预测器设计</h2><p>预测器，就是用一批历史数据预测当前的数据。使用的历史数据集大小，以及使用的预测算法都会影响最终的预测效果。</p>
<p>外卖订单量具有明显的周期性，同时相邻时刻的订单量数据也有很强的相关性，我们的目标，就是使用上面说的相关数据预测出当前的订单量。下面，我们分析几种常用的预测器实现。</p>
<h3 id="3-1-同比环比预测器"><a href="#3-1-同比环比预测器" class="headerlink" title="3.1 同比环比预测器"></a>3.1 同比环比预测器</h3><p>同比环比是比较常用的异常检测方式，它是将当前时刻数据和前一时刻数据（环比）或者前一天同一时刻数据（同比）比较，超过一定阈值即认为该点异常。如果用图2.1模型来表示，那么预测器就可以表示为用当前时刻前一时刻或者前一天同一时刻数据作为当前时刻的预测数据。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049304701237.png" alt=""></p>
<p>假如需要预测图中黄色数据，那么环比使用图中的蓝色数据作为预测黄点的源数据，同比使用图中红色数据作为预测黄点的源数据。</p>
<h3 id="3-2-基线预测器"><a href="#3-2-基线预测器" class="headerlink" title="3.2 基线预测器"></a>3.2 基线预测器</h3><p>同比环比使用历史上的单点数据来预测当前数据，误差比较大。t时刻的监控数据，与<br>$t-1,t-2,…$时刻的监控数据存在相关性。同时，与$t-k,t-2k,…$时刻的数据也存在相关性（k为周期），如果能利用上这些相关数据对t时刻进行预测，预测结果的误差将会更小。</p>
<p>比较常用的方式是对历史数据求平均，然后过滤噪声，可以得到一个平滑的曲线（基线），使用基线数据来预测当前时刻的数据。该方法预测t时刻数据（图中黄色数据）使用到的历史数据如下图所示（图中红色数据）：<br><img src="http://omu7tit09.bkt.clouddn.com/15049305406119.png" alt=""></p>
<p>基线数据预测器广泛应用在业务大盘监控中，预测效果如图3.3所示。从图中可以看出，基线比较平滑，在低峰期预测效果比较好，但是在外卖的午高峰和晚高峰预测误差比较大。<br><img src="http://omu7tit09.bkt.clouddn.com/15049305780074.png" alt=""></p>
<h3 id="3-3-Holt-Winters预测器"><a href="#3-3-Holt-Winters预测器" class="headerlink" title="3.3 Holt-Winters预测器"></a>3.3 Holt-Winters预测器</h3><p>同比环比预测到基线数据预测，使用的相关数据变多，预测的效果也较好。但是基线数据预测器只使用了周期相关的历史数据，没有使用上同周期相邻时刻的历史数据，相邻时刻的历史数据对于当前时刻的预测影响是比较大的。如外卖订单量，某天天气不好，很多用户不愿意出门，那么当天的外卖的订单量就会呈现整体的上涨，这种整体上涨趋势只能从同一周期相邻时刻的历史数据中预测出来。如图3.4所示，预测图中黄色数据，如果使用上图中所有的红色数据，那么预测效果会更好。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049306111501.png" alt=""></p>
<p>本文使用了Holt-Winters来实现这一目标。</p>
<p>Holt-Winters是三次指数滑动平均算法，它将时间序列数据分为三部分：残差数据a(t)，趋势性数据b(t)，季节性数据s(t)。使用Holt-Winters预测t时刻数据，需要t时刻前包含多个周期的历史数据。相关链接：Exponential smoothing、Holt-Winters seasonal method。</p>
<p>各部分的迭代计算公式（周期为k）：<br><img src="http://omu7tit09.bkt.clouddn.com/15049307018880.png" alt=""></p>
<p>如图所示，(a)显示了某一段时间内外卖订单的原始提单监控数据（分钟统计量，周期为1天），图(b)显示了其Holt-Winters的分解图（四幅图分别对应原始数据、残差数据分量、趋势数据分量、周期数据分量）。将订单量时间序列分解为残差数据a(t)，趋势数据b(t)，周期数据s(t)后，就可以使用下面的公式预测未来不同时刻时刻的订单量，其中h表示未来时刻距离当前时刻的跨度。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049307371760.png" alt=""></p>
<p>外卖订单量，是按分钟统计的离散时间序列，所以如果需要预测下一分钟的订单量，令h=1。<br><img src="http://omu7tit09.bkt.clouddn.com/15049307830931.png" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/15049307692132.png" alt=""></p>
<h3 id="3-4-外卖报警模型中的预测器"><a href="#3-4-外卖报警模型中的预测器" class="headerlink" title="3.4 外卖报警模型中的预测器"></a>3.4 外卖报警模型中的预测器</h3><p>在外卖订单量异常检测中，使用Holt-Winters预测器实时预测下一分钟订单量，每次需要至少5天以上的订单量数据才能有较好的预测效果，数据量要求比较大。</p>
<p>在实际的异常检测模型中，我们对Holt-Winters预测器进行了简化。预测器的趋势数据表示的是时间序列的总体变化趋势，如果以天为周期看待外卖的订单量时间序列，是没有明显的趋势性的，图3.5(b)的分解图也证明了这一点。因此，我们可以去掉其中的趋势数据部分。</p>
<p>各部分的迭代公式简化为(3-1)：<br><img src="http://omu7tit09.bkt.clouddn.com/15049308308252.png" alt=""></p>
<p>预测值：<br><img src="http://omu7tit09.bkt.clouddn.com/15049308423699.png" alt=""></p>
<p>h越大，预测值Yhat[t+h] 的误差也就越大。实时的订单流监控，令h=1，每当有新的监控数据时，更新输入序列，然后预测下一分钟数据。<br><img src="http://omu7tit09.bkt.clouddn.com/15049308574294.png" alt=""></p>
<p>Holt-Winters每一次预测都需要大量的输入数据序列。从上面模型的简化公式可以看出，对残差数据a(t)的预测是对序列(a(t-m),a(t-m+1),…a(t-2),a(t-1))的一次指数滑动平均，对周期数据s(t)的预测是对序列（s(t-mk) ,s(t-(m-1)k),…s(t-k)）的一次滑动平均，大量的输入数据是用于周期数据s(t)的计算。</p>
<p>a(t)和s(t)是互相关联的迭代计算过程，如果从周期性角度看公式(3-1)，可以发现：计算当前周期内的a(t)时，使用的是上一周期计算出来的s(t-k)，当前周期计算出的s(t)是用于下一周期a(t+k)的计算。为了将算法应用到线上的实时预测，我们可以将Holt-Winters算法拆分为两个独立的计算过程：</p>
<ul>
<li>定时任务计算序列的周期数s(t)。</li>
<li>对残差序列做实时预测。</li>
</ul>
<p>下面就分别从这两个步骤介绍外卖报警模型中的预测器实现。</p>
<h4 id="3-4-1-计算序列的周期性数据"><a href="#3-4-1-计算序列的周期性数据" class="headerlink" title="3.4.1 计算序列的周期性数据"></a>3.4.1 计算序列的周期性数据</h4><p>时间序列的周期性数据不需要实时计算，按周期性更新即可，如外卖订单大盘监控，s(t)只需要每天更新一次即可。对于s(t)的计算，可以有多种方法，可以使用上面提到的Holt-Winters按公式(3-1)计算出时间序列的周期性数据（如图3.5b所示），或直接使用前一天的监控数据作为当天的周期数据（这两种方式都需要对输入序列进行预处理，保证算法的输入序列不含有异常数据）。也可以用上面3.2节提到的，将历史数据做平均求出基线作为序列的周期性数据。</p>
<p>目前外卖订单中心报警模型采用的是Holt-Winters计算周期数据的方式。在将该模型推广到外卖其他业务线监控时，使用了计算基线数据作为周期数据的方式，这里简单对比一下两种方式的优劣。</p>
<blockquote>
<p>使用Holt-Winters算法计算周期数据</p>
</blockquote>
<p>优点：如果序列中含有周期性的陡增陡降点，Holt-Winters计算出的周期数据中会保留这些陡增陡降趋势，因此可以准确的预测出这些趋势，不会产生误报。比如外卖订单的提单数据，在每天的某个时刻都有一个定期陡降，使用该方式可以正确的预测出下降的趋势。如图3.6所示，蓝色线是真实数据，棕色线是预测数据，在该时刻，棕色线准确的预测出了下降点。</p>
<p>缺点：需要对输入数据进行预处理，去除异常数据。如果输入序列中含有异常数据，使用Holt-Winters时可能会把这些异常数据计算到周期数据中，影响下一周期的预测从而产生误报（Holt-Winters理论上也只是滑动平均的过程，因此如果输入数据中含有比较大的异常数据时，存在这种可能性，实际应用中订单的报警模型也出现过这种误报）。<br><img src="http://omu7tit09.bkt.clouddn.com/15049309740692.png" alt=""></p>
<blockquote>
<p>历史数据平均求基线</p>
</blockquote>
<p>优点：计算出的周期数据比较平滑，不需要对输入序列进行预处理，计算过程中可以自动屏蔽掉异常数据的影响，计算过程简单，如图3.3所示的基线数据。<br>缺点：周期数据比较平滑，不会出现陡增陡降点，因此对于周期性出现的陡增陡降不能很好的预测，出现误报。比如外卖活动的大盘（如图3.7所示，红线是真实数据，黑线是预测数据），提前下单优惠在每天某个时刻会出现周期性陡降，使用该方式会出现误报。<br><img src="http://omu7tit09.bkt.clouddn.com/15049309933978.png" alt=""></p>
<p>两种求周期数据的方式各有优劣，可以根据各自的监控数据特点选择合适的计算方式。如果监控数据中含有大量的周期性的陡增陡降点，那么推荐使用方式1，可以避免在这些时间点的误报。如果监控数据比较平滑，陡增陡降点很少，那么推荐方式2，计算简单的同时，也能避免因输入数据预处理不好而造成的意料之外的误报。</p>
<h4 id="3-4-2-残差数据实时预测"><a href="#3-4-2-残差数据实时预测" class="headerlink" title="3.4.2 残差数据实时预测"></a>3.4.2 残差数据实时预测</h4><p>计算出周期数据后，下一个目标就是对残差数据的预测。使用下面的公式，实际监控数据与周期数据相减得到残差数据，对残差数据做一次滑动平均，预测出下一刻的残差，将该时刻的残差、周期数据相加即可得到该时刻的预测数据。残差序列的长度设为60，即可以得到比较准确的预测效果。<br><img src="http://omu7tit09.bkt.clouddn.com/15049310383778.png" alt=""></p>
<p>对于实时预测，使用的是当天的周期数据和前60分钟数据。最终的预测结果如图3.8(a)(b)所示，其中蓝色线是真实数据，红色线是预测数据。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049310554367.png" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/15049310659176.png" alt=""></p>
<h2 id="四、比较器设计"><a href="#四、比较器设计" class="headerlink" title="四、比较器设计"></a>四、比较器设计</h2><p>预测器预测出当前时刻订单量的预测值后，还需要与真实值比较来判断当前时刻订单量是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在订单模型的报警检测中没有使用这种方式，而是使用了两个串联的Filter（如图4.1所示），只有当两个Fliter都认为该点异常时，才进行报警，下面简单介绍一下两个Filter的实现。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049310861803.png" alt=""></p>
<p>离散度Filter：根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度Filter利用了这一特性，取连续15分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于e1序列方差的某个倍数，我们就认为该点可能是异常点。</p>
<p>阈值Filter：根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度Filter进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值Filter。阈值Filter设计了一个分段阈值函数y=f(x)，对于实际值x和预测值p，只有当|x-p|&gt;f(x)时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049311196340.png" alt=""></p>
<h2 id="五、报警模型最终效果"><a href="#五、报警模型最终效果" class="headerlink" title="五、报警模型最终效果"></a>五、报警模型最终效果</h2><p>最终的外卖订单异常报警模型结构图如图5.1所示，每天会有定时Job从ETL中统计出最近10天的历史订单量，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前订单量。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049311452338.png" alt=""></p>
<p>新的报警模型上线后，外卖订单量的异常检测的漏报率和误报率都有显著的提升，上线半年以来，对于每一次的异常都能准确的检测出来，漏报率近乎为0。误报率在通常情况下限制在了每周0~3次误报。</p>
<p>报警模型目前应用在外卖订单量的异常检测中，同时推广到了外卖业务的其他各种大盘监控中，取得了不错的效果。在报警模型上线后，我们发现并解决了一些系统隐患点，如：</p>
<ul>
<li>点评侧外卖提单量在每天定时有一个下降尖刺，经过排查是因为客户端冷启动短时间内大量的请求，导致SLB性能达到瓶颈，从而导致接口成功率下降。</li>
<li>点评侧外卖订单取消量经常会有尖刺，经过排查发现是由于在支付时，需要进行跨机房的账号转换，专线网络抖动时造成接口超时。</li>
<li>外卖订单量在每天某些时刻都有陡降趋势，经过排查，是因为这些点大量商家开始休息导致的。</li>
</ul>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>将机器学习中的预测算法运用到外卖订单的异常检测中，极大的提高了异常检测的准确性和敏感性，提升了系统稳定运维的效率。该报警模型也有很广泛的应用场景，美团点评的各个业务线的监控数据，绝大多数都是含有明显周期性的时间序列，本文提出的模型都能运用到这些监控数据的异常检测中。</p>
<p>当然，模型还有进一步完善的空间，如：</p>
<ul>
<li>历史数据的预处理优化。在进行周期数据计算时，对于输入序列的预处理，如果能够排除绝大部分的异常数据，那么最终检测的误报率将会进一步的降低。</li>
<li>在不会产生持续误报的情况下替换有异常的实时数据。对于当前数据的预测，利用的都是前60分钟的真实数据，但是这些数据可能本身就存在异常数据，那么就存在一种情况，当出现异常时，真实数据开始下跌，预测数据紧接着也会下跌（如图3.8b所示）。这种情况有时候可能满足需求（比如只在异常开始的时候进行报警，异常持续时间内不再报警，防止报警太多造成的信息轰炸），有时候可能不满足需求（比如要求预测数据不跟随异常变化而变化，这种情况可以应用在故障期间的损失统计中）。如果需要预测值不随异常变化而变化，一种可能的方法是，当检测到当前数据是异常数据时，将预测数据替换当前的真实数据，作为下一时刻预测器的输入，这样可以防止异常数据对于下一时刻预测值的影响。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 异常检测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（37）：外卖O2O的用户画像实践]]></title>
      <url>/2017/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8837%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96O2O%E7%9A%84%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AE%9E%E8%B7%B5/</url>
      <content type="html"><![CDATA[<p>美团外卖经过3年的飞速发展，品类已经从单一的外卖扩展到了美食、夜宵、鲜花、商超等多个品类。用户群体也从早期的学生为主扩展到学生、白领、社区以及商旅，甚至包括在KTV等娱乐场所消费的人群。随着供给和消费人群的多样化，如何在供给和用户之间做一个对接，就是用户画像的一个基础工作。所谓千人千面，画像需要刻画不同人群的消费习惯和消费偏好。</p>
<a id="more"></a>
<p>外卖O2O和传统的电商存在一些差异。可以简单总结为如下几点：</p>
<p>1）新事物，快速发展：这意味很多用户对外卖的认知较少，对平台上的新品类缺乏了解，对自身的需求也没有充分意识。平台需要去发现用户的消费意愿，以便对用户的消费进行引导。</p>
<p>2）高频：外卖是个典型的高频O2O应用。一方面消费频次高，用户生命周期相对好判定；另一方面消费单价较低，用户决策时间短、随意性大。</p>
<p>3）场景驱动：场景是特定的时间、地点和人物的组合下的特定的消费意图。不同的时间、地点，不同类型的用户的消费意图会有差异。例如白领在写字楼中午的订单一般是工作餐，通常在营养、品质上有一定的要求，且单价不能太高；而到了周末晚上的订单大多是夜宵，追求口味且价格弹性较大。场景辨识越细致，越能了解用户的消费意图，运营效果就越好。</p>
<p>4）用户消费的地理位置相对固定，结合地理位置判断用户的消费意图是外卖的一个特点。</p>
<h2 id="一、外卖产品运营对画像技术的要求"><a href="#一、外卖产品运营对画像技术的要求" class="headerlink" title="一、外卖产品运营对画像技术的要求"></a>一、外卖产品运营对画像技术的要求</h2><p>如下图所示，我们大致可以把一个产品的运营分为用户获取和用户拓展两个阶段。在用户获取阶段，用户因为自然原因或一些营销事件（例如广告、社交媒体传播）产生对外卖的注意，进而产生了兴趣，并在合适的时机下完成首购，从而成为外卖新客。在这一阶段，运营的重点是提高效率，通过一些个性化的营销和广告手段，吸引到真正有潜在需求的用户，并刺激其转化。在用户完成转化后，接下来的运营重点是拓展用户价值。这里有两个问题：第一是提升用户价值，具体而言就是提升用户的单均价和消费频次，从而提升用户的LTV（life-time value)。基本手段包括交叉销售（新品类的推荐）、向上销售（优质高价供给的推荐）以及重复购买（优惠、红包刺激重复下单以及优质供给的推荐带来下单频次的提升）；第二个问题是用户的留存，通过提升用户总体体验以及在用户有流失倾向时通过促销和优惠将用户留在外卖平台。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049176529900.png" alt=""></p>
<p>所以用户所处的体验阶段不同，运营的侧重点也需要有所不同。而用户画像作为运营的支撑技术，需要提供相应的用户刻画以满足运营需求。根据上图的营销链条，从支撑运营的角度，除去提供常规的用户基础属性（例如年龄、性别、职业、婚育状况等）以及用户偏好之外，还需要考虑这么几个问题：1）什么样的用户会成为外卖平台的顾客（新客识别）；2）用户所处生命周期的判断，用户是否可能从平台流失（流失预警）；3）用户处于什么样的消费场景（场景识别）。后面“外卖O2O的用户画像实践”一节中，我们会介绍针对这三个问题的一些实践。</p>
<h2 id="二、外卖画像系统架构"><a href="#二、外卖画像系统架构" class="headerlink" title="二、外卖画像系统架构"></a>二、外卖画像系统架构</h2><p>下图是我们画像服务的架构：数据源包括基础日志、商家数据和订单数据。数据完成处理后存放在一系列主题表中，再导入kv存储，给下游业务端提供在线服务。同时我们会对整个业务流程实施监控。主要分为两部分，第一部分是对数据处理流程的监控，利用用内部自研的数据治理平台，监控每天各主题表产生的时间、数据量以及数据分布是否有异常。第二部分是对服务的监控。目前画像系统支持的下游服务包括：广告、排序、运营等系统。<br><img src="http://omu7tit09.bkt.clouddn.com/15049176981394.png" alt=""></p>
<h2 id="三、外卖O2O的用户画像实践"><a href="#三、外卖O2O的用户画像实践" class="headerlink" title="三、外卖O2O的用户画像实践"></a>三、外卖O2O的用户画像实践</h2><h3 id="3-1-新客运营"><a href="#3-1-新客运营" class="headerlink" title="3.1 新客运营"></a>3.1 新客运营</h3><p>新客运营主要需要回答下列三个问题：</p>
<p>1）新客在哪里？</p>
<p>2）新客的偏好如何？</p>
<p>3）新客的消费力如何？</p>
<p>回答这三个问题是比较困难的，因为相对于老客而言，新客的行为记录非常少或者几乎没有。这就需要我们通过一些技术手段作出推断。例如：新客的潜在转化概率，受到新客的人口属性（职业、年龄等）、所处地域（需求的因素）、周围人群（同样反映需求）以及是否有充足供给等因素的影响；而对于新客的偏好和消费力，从新客在到店场景下的消费行为可以做出推测。另外用户的工作和居住地点也能反映他的消费能力。<br>对新客的预测大量依赖他在到店场景下的行为，而用户的到店行为对于外卖是比较稀疏的，大多数的用户是在少数几个类别上有过一些消费行为。这就意味着我们需要考虑选择什么样的统计量描述：是消费单价，总消费价格，消费品类等等。然后通过大量的试验来验证特征的显著性。另外由于数据比较稀疏，需要考虑合适的平滑处理。</p>
<p>我们在做高潜新客挖掘时，融入了多方特征，通过特征的组合最终作出一个效果比较好的预测模型。我们能够找到一些高转化率的用户，其转化率比普通用户高若干倍。通过对高潜用户有针对性的营销，可以极大提高营销效率。</p>
<h3 id="3-2-流失预测"><a href="#3-2-流失预测" class="headerlink" title="3.2 流失预测"></a>3.2 流失预测</h3><p>新客来了之后，接下来需要把他留在这个平台上，尽量延长生命周期。营销领域关于用户留存的两个基本观点是（引自菲利普.科特勒 《营销管理》）：</p>
<p>获取一个新顾客的成本是维系现有顾客成本的5倍！</p>
<p>如果将顾客流失率降低5%，公司利润将增加25%~85%</p>
<p>用户流失的原因通常包括：竞对的吸引；体验问题；需求变化。我们借助机器学习的方法，构建用户的描述特征，并借助这些特征来预测用户未来流失的概率。这里有两种做法: 第一种是预测用户未来若干天是否会下单这一事件发生的概率。这是典型的概率回归问题，可以选择逻辑回归、决策树等算法拟合给定观测下事件发生的概率；第二种是借助于生存模型，例如COX-PH模型，做流失的风险预测。下图左边是概率回归的模型，用户未来T天内是否有下单做为类别标记y，然后估计在观察到特征X的情况下y的后验概率P(y|X)。右边是用COX模型的例子，我们会根据用户在未来T天是否下单给样本一个类别，即观测时长记为T。假设用户的下单的距今时长t&lt;T，将t作为生存时长t’；否则将生存时长t’记为T。这样一个样本由三部分构成：样本的类别(flag)，生存时长(t’)以及特征列表。通过生存模型虽然无法显式得到P(t’|X)的概率，但其协变量部分实际反映了用户流失的风险大小。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049177296058.png" alt=""></p>
<p>生存模型中，βTx反映了用户流失的风险，同时也和用户下次订单的时间间隔成正相关。下面的箱线图中，横轴为βTx，纵轴为用户下单时间的间隔。<br><img src="http://omu7tit09.bkt.clouddn.com/15049177439027.png" alt=""></p>
<p>我们做了COX模型和概率回归模型的对比。在预测用户XX天内是否会下单上面，两者有相近的性能。</p>
<p>美团外卖通过使用了用户流失预警模型，显著降低了用户留存的运营成本。</p>
<h3 id="3-3-场景运营"><a href="#3-3-场景运营" class="headerlink" title="3.3 场景运营"></a>3.3 场景运营</h3><p>拓展用户的体验，最重要的一点是要理解用户下单的场景。了解用户的订餐场景有助于基于场景的用户运营。对于场景运营而言，通常需要经过如下三个步骤：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049177875464.png" alt=""><br>场景可以从时间、地点、订单三个维度描述。比如说工作日的下午茶，周末的家庭聚餐，夜里在家点夜宵等等。其中重要的一点是用户订单地址的分析。通过区分用户的订单地址是写字楼、学校或是社区，再结合订单时间、订单内容，可以对用户的下单场景做到大致的了解。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049178093195.png" alt=""></p>
<p>上图是我们订单地址分析的流程。根据订单系统中的用户订单地址文本，基于自然语言处理技术对地址文本分析，可以得到地址的主干名称（指去掉了楼宇、门牌号的地址主干部分）和地址的类型（写字楼、住宅小区等）。在此基础上通过一些地图数据辅助从而判断出最终的地址类型。<br>另外我们还做了合并订单的识别，即识别一个订单是一个人下单还是拼单。把拼单信息、地址分析以及时间结合在一起，我们可以预测用户的消费场景，进而基于场景做交叉销售和向上销售。</p>
<h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>外卖的营销特征，跟其他行业的主要区别在于：</p>
<p>外卖是一个高频的业务。由于用户的消费频次高，用户生命周期的特征体现较显著。运营可以基于用户所处生命周期的阶段制定营销目标，例如用户完成首购后的频次提升、成熟用户的价值提升、衰退用户的挽留以及流失用户的召回等。因此用户的生命周期是一个基础画像，配合用户基本属性、偏好、消费能力、流失预测等其他画像，通过精准的产品推荐或者价格策略实现运营目标。</p>
<p>用户的消费受到时间、地点等场景因素驱动。因此需要对用户在不同的时间、地点下消费行为的差异做深入了解，归纳不同场景下用户需求的差异，针对场景制定相应的营销策略，提升用户活跃度。</p>
<p>另外由于外卖是一个新鲜的事物，在用户对一些新品类和新产品缺乏认知的情况下，需要通过技术手段识别用户的潜在需求，进行精准营销。例如哪些用户可能会对小龙虾、鲜花、蛋糕这样的相对低频、高价值的产品产生购买。可以采用的技术手段包括用户分群、对已产生消费的用户做look-alike扩展、迁移学习等。</p>
<p>同时我们在制作外卖的用户画像时还面临如下挑战：</p>
<p>1）数据多样性，存在大量非结构化数据例如用户地址、菜品名称等。需要用到自然语言处理技术，同时结合其他数据进行分析。</p>
<p>2）相对于综合电商而言，外卖是个相对单一的品类，用户在外卖上的行为不足以全方位地描述用户的基本属性。因此需要和用户在其他场合的消费行为做融合。</p>
<p>3）外卖单价相对较低，用户消费的决策时间短、随意性强。不像传统电商用户在决策前有大量的浏览行为可以用于捕捉用户单次的需求。因此更需要结合用户画像分析用户的历史兴趣、以及用户的消费场景，在消费前对用户做适当的引导、推荐。</p>
<p>面临这些挑战，需要用户画像团队更细致的数据处理、融合多方数据源，同时发展出新的方法论，才能更好地支持外卖业务发展的需要。而外卖的上述挑战，又分别和一些垂直领域电商类似，经验上存在可以相互借鉴之处。因此，外卖的用户画像的实践和经验累积，必将对整个电商领域的大数据应用作出新的贡献！</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 用户画像 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（36）：GBDT算法原理深入解析]]></title>
      <url>/2017/09/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8836%EF%BC%89%EF%BC%9AGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>梯度提升（Gradient boosting）是一种用于回归、分类和排序任务的机器学习技术[1]，属于Boosting算法族的一部分。Boosting是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是“三个臭皮匠顶个诸葛亮”的道理。梯度提升同其他boosting方法一样，通过集成（ensemble）多个弱学习器，通常是决策树，来构建最终的预测模型。</p>
<a id="more"></a>
<p>Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足。Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务[2]，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。</p>
<p>基于梯度提升算法的学习器叫做GBM(Gradient Boosting Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是if-then规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。现在主流的GBDT算法实现中这些方法基本上都有实现，因此GBDT算法的超参数还是比较多的，应用过程中需要精心调参，并用交叉验证的方法选择最佳参数。</p>
<p>本文对GBDT算法原理进行介绍，从机器学习的关键元素出发，一步一步推导出GBDT算法背后的理论基础，读者可以从这个过程中了解到GBDT算法的来龙去脉。对于该算法的工程实现，本文也有较好的指导意义，实际上对机器学习关键概念元素的区分对应了软件工程中的“开放封闭原则”的思想，基于此思想的实现将会具有很好的模块独立性和扩展性。</p>
<h2 id="一、机器学习的关键元素"><a href="#一、机器学习的关键元素" class="headerlink" title="一、机器学习的关键元素"></a>一、机器学习的关键元素</h2><p>先复习下监督学习的关键概念：模型（model）、参数（parameters）、目标函数（objective function）</p>
<p>模型就是所要学习的条件概率分布或者决策函数，它决定了在给定特征向量x时如何预测出目标y。定义$x_i\in R^d$为训练集中的第$i$个训练样本，则线性模型（linear model）可以表示为：$\hat{y}=\sum_jw_jx_{ij }$。模型预测的分数$\hat{y_i}$在不同的任务中有不同的解释。例如在逻辑回归任务中，$1/(1+exp(-\hat{y}_i))$表示模型预测为正例的概率；而在排序学习任务中，$\hat{y_i}$表示排序分。</p>
<p>参数就是我们要从数据中学习得到的内容。模型通常是由一个参数向量决定的函数。例如，线性模型的参数可以表示为：$\Theta=\{w_j|j=1,\cdots,d\}$</p>
<p>目标函数通常定义为如下形式：</p>
<script type="math/tex; mode=display">Obj(\Theta)=L(\Theta)+\Omega(\Theta)</script><p>其中，$L(\Theta)$是损失函数，用来衡量模型拟合训练数据的好坏程度；$\Omega(\Theta)$称之为正则项，用来衡量学习到的模型的复杂度。训练集上的损失（Loss）定义为：$L=\sum_{i=1}^n l(y_i, \hat{y}_i)$。常用的损失函数有平方损失（square loss）：$l(y_i, \hat{y}_i)=(y_i - \hat{y}_i)^2$；Logistic损失： $l(y_i, \hat{y}_i)=y_i ln(1+e^{y_i}) + (1-y_i)ln(1+e^{\hat{y}_i})$。常用的正则项有L1范数$\Omega(w)=\lambda \Vert w \Vert_1$和L2范数$\Omega(w)=\lambda \Vert w \Vert_2$。Ridge regression就是指使用平方损失和L2范数正则项的线性回归模型；Lasso regression就是指使用平方损失和L1范数正则项的线性回归模型；逻辑回归（Logistic regression）指使用logistic损失和L2范数或L1范数正则项的线性模型。</p>
<p>目标函数之所以定义为损失函数和正则项两部分，是为了尽可能平衡模型的偏差和方差（Bias Variance Trade-off）。最小化目标函数意味着同时最小化损失函数和正则项，损失函数最小化表明模型能够较好的拟合训练数据，一般也预示着模型能够较好地拟合真实数据（groud true）；另一方面，对正则项的优化鼓励算法学习到较简单的模型，简单模型一般在测试样本上的预测结果比较稳定、方差较小（奥坎姆剃刀原则）。也就是说，优化损失函数尽量使模型走出欠拟合的状态，优化正则项尽量使模型避免过拟合。</p>
<p>从概念上区分模型、参数和目标函数给学习算法的工程实现带来了益处，使得机器学习的各个组成部分之间耦合尽量松散。</p>
<h2 id="二、加法模型"><a href="#二、加法模型" class="headerlink" title="二、加法模型"></a>二、加法模型</h2><p>GBDT算法可以看成是由K棵树组成的加法模型：</p>
<script type="math/tex; mode=display">\hat{y}_i=\sum_{k=1}^K f_k(x_i), f_k \in F \tag 0</script><p>其中$F$为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta=\{f_1,f_2, \cdots, f_K \}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合。</p>
<p>上述加法模型的目标函数定义为：$Obj=\sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$，其中表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。</p>
<p>如何来学习加法模型呢？</p>
<p>解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： </p>
<script type="math/tex; mode=display">\begin{split}
\hat{y}_i^0 &= 0 \\
\hat{y}_i^1 &= f_1(x_i) = \hat{y}_i^0 + f_1(x_i) \\
\hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \hat{y}_i^1 + f_2(x_i) \\
& \cdots \\
\hat{y}_i^t &= \sum_{k=1}^t f_k(x_i) = \hat{y}_i^{t-1} + f_t(x_i) \\
\end{split}</script><p>那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。<br>在第$t$步，模型对$x_i$的预测为：$\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为：</p>
<script type="math/tex; mode=display">\begin{split}
Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=i}^t \Omega(f_i) \\
&=  \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \Omega(f_t) + constant
\end{split}\tag{1}</script><p>举例说明，假设损失函数为平方损失（square loss），则目标函数为：</p>
<script type="math/tex; mode=display">\begin{split}
Obj^{(t)} &= \sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 + \Omega(f_t) + constant \\
&= \sum_{i=1}^n \left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right] + \Omega(f_t) + constant
\end{split}\tag{2}</script><p>其中$(\hat{y}_i^{t-1} - y_i)$，称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。</p>
<blockquote>
<p>泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在$a$点处$n+1$次可导，那么对于这个区间上的任意$x$都有：</p>
<script type="math/tex; mode=display">\displaystyle f(x)=\sum _{n=0}^{N}\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)</script><p>，其中的多项式称为函数在$a$处的泰勒展开式，$R_n(x)$是泰勒公式的余项且是$(x-a)^n$的高阶无穷小。——维基百科</p>
</blockquote>
<p>根据泰勒公式把函数$f(x+\Delta x)$在点处二阶展开，可得到如下等式： </p>
<script type="math/tex; mode=display">f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \tag 3</script><p>由等式(1)可知，目标函数是关于变量$\hat{y}_i^{t-1} + f_t(x_i)$若把变量$\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\Delta x$，则等式(1)可转化为： </p>
<script type="math/tex; mode=display">Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) + constant \tag 4</script><p>其中，$g_i$定义为损失函数的一阶导数，即$g_i=\partial_{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\partial_{\hat{y}^{t-1}}^2l(y_i,\hat{y}^{t-1})$。 假设损失函数为平方损失函数，则$g_i=\partial_{\hat{y}^{t-1}}(\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)$，$h_i=\partial_{\hat{y}^{t-1}}^2(\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： </p>
<script type="math/tex; mode=display">Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) \tag 5</script><p>由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。</p>
<h2 id="二、GBDT算法"><a href="#二、GBDT算法" class="headerlink" title="二、GBDT算法"></a>二、GBDT算法</h2><p>一颗生成好的决策树，假设其叶子节点个数为$T$，该决策树是由所有叶子节点对应的值组成的向量$w \in R^T$，以及一个把特征向量映射到叶子节点索引（Index）的函数$q:R^d \to \{1,2,\cdots,T\}$组成的。因此，决策树可以定义为$f_t(x)=w_{q(x)}$。</p>
<p>决策树的复杂度可以由正则项$\Omega(f_t)=\gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2$来定义，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。</p>
<p>定义集合$I_j=\{ i \vert q(x_i)=j \}$为所有被划分到叶子节点的训练样本的集合。等式(5)可以根据树的叶子节点重新组织为T个独立的二次函数的和： </p>
<script type="math/tex; mode=display">\begin{split}
Obj^{(t)} &\approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) \\
&= \sum_{i=1}^n \left[ g_iw_{q(x_i)} + \frac12h_iw_{q(x_i)}^2 \right] + \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 \\
&= \sum_{j=1}^T \left[(\sum_{i \in I_j}g_i)w_j + \frac12(\sum_{i \in I_j}h_i + \lambda)w_j^2 \right] + \gamma T
\end{split}\tag 6</script><p>定义$G_j=\sum_{i \in I_j}g_i$，$H_j=\sum_{i \in I_j}h_i$，则等式(6)可写为：</p>
<script type="math/tex; mode=display">Obj^{(t)} = \sum_{j=1}^T \left[G_iw_j + \frac12(H_i + \lambda)w_j^2 \right] + \gamma T</script><p>假设树的结构是固定的，即函数$q(x)$确定，令函数$Obj^{(t)}$的一阶导数等于0，即可求得叶子节点对应的值为：$w_j^*=-\frac{G_j}{H_j+\lambda} \tag 7$此时，目标函数的值为</p>
<script type="math/tex; mode=display">Obj = -\frac12 \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag 8</script><p>综上，为了便于理解，单颗决策树的学习过程可以大致描述为： </p>
<ol>
<li>枚举所有可能的树结构$q$ </li>
<li>用等式(8)为每个$q$计算其对应的分数$Obj$，分数越小说明对应的树结构越好。</li>
<li>根据上一步的结果，找到最佳的树结构，用等式(7)为树的每个叶子节点计算预测值</li>
</ol>
<p>然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。 </p>
<ol>
<li>从深度为0的树开始，对每个叶节点枚举所有的可用特征 </li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） </li>
<li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 </li>
<li>回到第1步，递归执行到满足特定条件为止</li>
</ol>
<p>在上述算法的第二步，样本排序的时间复杂度为$O(nlogn)$，假设共用K个特征，那么生成一颗深度为K的树的时间复杂度为$O(dKnlogn)$。具体实现可以进一步优化计算复杂度，比如可以缓存每个特征的排序结果等。</p>
<p>如何计算每次分裂的收益呢？假设当前节点记为,分裂之后左孩子节点记为，右孩子节点记为，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：$Gain=Obj_C-Obj_L-Obj_R$，具体地，根据等式(8)可得：</p>
<script type="math/tex; mode=display">Gain=\frac12 \left[ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \tag 9</script><p>其中，$-\gamma$项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。等式(9)还可以用来计算输入特征的相对重要程度，具体见下一节</p>
<p>最后，总结一下GBDT的学习算法： </p>
<ol>
<li>算法每次迭代生成一颗新的决策树 </li>
<li>在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$</li>
<li>通过贪心策略生成新的决策树，通过等式(7)计算每个叶节点对应的预测值 </li>
<li>把新生成的决策树$f_t(x)$添加到模型中：$\hat{y}_i^t = \hat{y}_i^{t-1} + f_t(x_i)$</li>
</ol>
<p>通常在第四步，我们把模型更新公式替换为：$\hat{y}_i^t = \hat{y}_i^{t-1} + \epsilon f_t(x_i)$，其中$\epsilon$称之为步长或者学习率。增加因子的目的是为了避免模型过拟合。</p>
<h2 id="三、特征重要度"><a href="#三、特征重要度" class="headerlink" title="三、特征重要度"></a>三、特征重要度</h2><p>集成学习因具有预测精度高的优势而受到广泛关注，尤其是使用决策树作为基学习器的集成学习算法。树的集成算法的著名代码有随机森林和GBDT。随机森林具有很好的抵抗过拟合的特性，并且参数（决策树的个数）对预测性能的影响较小，调参比较容易，一般设置一个比较大的数。GBDT具有很优美的理论基础，一般而言性能更有优势。</p>
<p>基于树的集成算法还有一个很好的特性，就是模型训练结束后可以输出模型所使用的特征的相对重要度，便于我们选择特征，理解哪些因素是对预测有关键影响，这在某些领域（如生物信息学、神经系统科学等）特别重要。本文主要介绍基于树的集成算法如何计算各特征的相对重要度。</p>
<h3 id="3-1-优势"><a href="#3-1-优势" class="headerlink" title="3.1 优势"></a>3.1 优势</h3><ul>
<li>使用不同类型的数据时，不需要做特征标准化/归一化</li>
<li>可以很容易平衡运行时效率和精度；比如，使用boosted tree作为在线预测的模型可以在机器资源紧张的时候截断参与预测的树的数量从而提高预测效率</li>
<li>学习模型可以输出特征的相对重要程度，可以作为一种特征选择的方法</li>
<li>模型可解释性好</li>
<li>对数据字段缺失不敏感</li>
<li>能够自动做多组特征间的interaction，具有很好的非性线性</li>
</ul>
<h3 id="3-2-特征重要度的计算"><a href="#3-2-特征重要度的计算" class="headerlink" title="3.2 特征重要度的计算"></a>3.2 特征重要度的计算</h3><p>Friedman在GBM的论文中提出的方法：</p>
<p>特征$j$的全局重要度通过特征$j$在单颗树中的重要度的平均值来衡量：</p>
<script type="math/tex; mode=display">\hat{J_{j}^2}=\frac1M \sum_{m=1}^M\hat{J_{j}^2}(T_m)</script><p>其中，M是树的数量。特征$j$在单颗树中的重要度的如下：</p>
<script type="math/tex; mode=display">\hat{J_{j}^2}(T)=\sum\limits_{t=1}^{L-1} \hat{i_{t}^2} 1(v_{t}=j)</script><p>其中，$L$为树的叶子节点数量，$L-1$即为树的非叶子节点数量（构建的树都是具有左右孩子的二叉树），是和节点$t$相关联的特征，$\hat{i_t^2}$是节点分裂之后平方损失的减少值。</p>
<h3 id="3-3-实现代码"><a href="#3-3-实现代码" class="headerlink" title="3.3 实现代码"></a>3.3 实现代码</h3><p>为了更好的理解特征重要度的计算方法，下面给出scikit-learn工具包中的实现，代码移除了一些不相关的部分。</p>
<p>下面的代码来自于GradientBoostingClassifier对象的feature_importances属性的计算方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_importances_</span><span class="params">(self)</span>:</span></div><div class="line">    total_sum = np.zeros((self.n_features, ), dtype=np.float64)</div><div class="line">    <span class="keyword">for</span> tree <span class="keyword">in</span> self.estimators_:</div><div class="line">        total_sum += tree.feature_importances_ </div><div class="line">    importances = total_sum / len(self.estimators_)</div><div class="line">    <span class="keyword">return</span> importances</div></pre></td></tr></table></figure>
<p>其中，self.estimators_是算法构建出的决策树的数量，tree.feature_importances_ 是单棵树的特征重要度向量，其计算方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">cpdef compute_feature_importances(self, normalize=True):</div><div class="line">    &quot;&quot;&quot;Computes the importance of each feature (aka variable).&quot;&quot;&quot;</div><div class="line">    while node != end_node:</div><div class="line">        if node.left_child != _TREE_LEAF:</div><div class="line">            # ... and node.right_child != _TREE_LEAF:</div><div class="line">            left = &amp;nodes[node.left_child]</div><div class="line">            right = &amp;nodes[node.right_child]</div><div class="line">            importance_data[node.feature] += (</div><div class="line">                node.weighted_n_node_samples * node.impurity -</div><div class="line">                left.weighted_n_node_samples * left.impurity -</div><div class="line">                right.weighted_n_node_samples * right.impurity)</div><div class="line">        node += 1</div><div class="line">    importances /= nodes[0].weighted_n_node_samples</div><div class="line">    return importances</div></pre></td></tr></table></figure>
<p>上面的代码关键点是两个：</p>
<p>第一点：weighted_n_node_samples : array of int, shape [node_count]<br>        weighted_n_node_samples[i] holds the weighted number of training samples reaching node i.</p>
<p>第二点：impurity : array of double, shape [node_count]<br>        impurity[i] holds the impurity (i.e., the value of the splitting criterion) at node i.</p>
<p>当然上面的代码经过了简化，保留了核心思想。计算所有的非叶子节点在分裂时加权不纯度的减少，减少得越多说明特征越重要</p>
<p>不纯度的减少实际上就是该节点此次分裂的收益，因此我们也可以这样理解，节点分裂时收益越大，该节点对应的特征的重要度越高。关于收益的定义就是上一节中等式(9)的定义。</p>
<p>参考资料<br>[1] <a href="https://www.wikiwand.com/en/Gradient_boosting" target="_blank" rel="noopener">Gradient Boosting</a> 的更多内容<br>[2] <a href="http://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">XGBoost</a>是一个优秀的GBDT开源软件库，有多种语言接口<br>[3] <a href="https://github.com/cheng-li/pyramid" target="_blank" rel="noopener">Pyramid</a>是一个基于Java语言的机器学习库，里面也有GBDT算法的介绍和实现<br>[4] Friedman的论文<a href="http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">《Greedy function approximation: a gradient boosting machine》</a>是比较早的GBDT算法文献，但是比较晦涩难懂，不适合初学者，高阶选手可以进一步学习<br>[5] <a href="http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">“A Gentle Introduction to Gradient Boosting”</a>是关于Gradient Boosting的一个通俗易懂的解释，比较适合初学者或者是已经对GBDT算法原理印象不深的从业者<br>[6] 关于GBDT算法调参的经验和技巧可以参考这两篇博文：<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">《GBM调参指南》</a>、<br><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener">《XGBoost调参指南》</a>，作者使用的算法实现工具来自于著名的Python机器学习工具scikit-learn<br>[7] GBDT算法在搜索引擎排序中的应用可以查看这篇论文<a href="http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf" target="_blank" rel="noopener">《Web-Search Ranking with Initialized Gradient Boosted Regression Trees 》</a>，这篇论文提出了一个非常有意思的方法，用一个已经训练好的随机森林模型作为GBDT算法的初始化，再用GBDT算法优化最终的模型，取得了很好的效果</p>
<p>[1] <a href="https://pdfs.semanticscholar.org/156e/3c979e7bc25381fdd0614d1bab60b7aa5dfd.pdf" target="_blank" rel="noopener">Feature Selection for Ranking using Boosted Trees </a><br>[2] <a href="http://alicezheng.org/papers/gbfs.pdf" target="_blank" rel="noopener">Gradient Boosted Feature Selection</a><br>[3] <a href="http://www.jmlr.org/papers/volume10/tuv09a/tuv09a.pdf" target="_blank" rel="noopener">Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination</a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> GBDT </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（35）：使用Sklearn进行集成学习（实践）]]></title>
      <url>/2017/09/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8835%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AE%9E%E8%B7%B5%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>jjj<br><a id="more"></a><br>hhh</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 集成学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（34）：使用Sklearn进行集成学习（理论）]]></title>
      <url>/2017/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8834%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>很多人在竞赛（Kaggle，天池等）或工程实践中使用了集成学习（例如，RF、GTB等），确实也取得了不错的效果，在保证准确度的同时也提升了模型防止过拟合的能力。但是，我们真的用对了集成学习吗？</p>
<a id="more"></a>
<p>sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。</p>
<p>我认为，学会调参是进行集成学习工作的前提。然而，第一次遇到这些算法和模型时，肯定会被其丰富的参数所吓到，要知道，教材上教的伪代码可没这么多参数啊！！！没关系，暂时，我们只要记住一句话：参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！</p>
<p>本篇博文将详细阐述模型参数背后的理论知识，在下篇博文中，我们将对最热门的两个模型Random Forrest和Gradient Tree Boosting（含分类和回归，所以共4个模型）进行具体的参数讲解。如果你实在无法静下心来学习理论，你也可以在下篇博文中找到最直接的调参指导，虽然我不赞同这么做。</p>
<h2 id="二、集成学习是什么？"><a href="#二、集成学习是什么？" class="headerlink" title="二、集成学习是什么？"></a>二、集成学习是什么？</h2><p>我们还是花一点时间来说明一下集成学习是什么，如果对此有一定基础的同学可以跳过本节。简单来说，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。</p>
<p>目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文《Ensemble Learning》对这三种集成学习框架有了明确的定义，概括如下：</p>
<ul>
<li>bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果：</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15043255665954.jpg" alt=""></p>
<ul>
<li>boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果：</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15043256075450.jpg" alt=""></p>
<ul>
<li>stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测：</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/15043256303505.jpg" alt=""><br>有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。</p>
<h2 id="三、偏差和方差"><a href="#三、偏差和方差" class="headerlink" title="三、偏差和方差"></a>三、偏差和方差</h2><p>广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15043259439732.png" alt=""></p>
<h3 id="3-1-模型的偏差和方差"><a href="#3-1-模型的偏差和方差" class="headerlink" title="3.1 模型的偏差和方差"></a>3.1 模型的偏差和方差</h3><p>模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。</p>
<p>定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。</p>
<p>研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。</p>
<p>我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。</p>
<p>在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15043259867980.png" alt=""></p>
<h3 id="3-2-bagging的偏差和方差"><a href="#3-2-bagging的偏差和方差" class="headerlink" title="3.2 bagging的偏差和方差"></a>3.2 bagging的偏差和方差</h3><p>对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：<br><img src="http://omu7tit09.bkt.clouddn.com/15043260804255.png" alt=""></p>
<p>根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</p>
<p>Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。</p>
<h3 id="3-3-boosting的偏差和方差"><a href="#3-3-boosting的偏差和方差" class="headerlink" title="3.3 boosting的偏差和方差"></a>3.3 boosting的偏差和方差</h3><p>对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：<br><img src="http://omu7tit09.bkt.clouddn.com/15043263221887.png" alt=""></p>
<p>通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。</p>
<p>因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h3 id="3-4-模型的独立性"><a href="#3-4-模型的独立性" class="headerlink" title="3.4 模型的独立性"></a>3.4 模型的独立性</h3><p>聪明的读者这时肯定要问了，如何衡量基模型的独立性？我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢？</p>
<p>我们讨论模型的随机性时，抽样是针对于样本的整体。而bagging中的抽样是针对于训练集（整体的子集），所以并不能称其为对整体的独立随机抽样。那么到底bagging中基模型的相关性体现在哪呢？在知乎问答《为什么说bagging是减少variance，而boosting是减少bias?》中请教用户“过拟合”后，我总结bagging的抽样为两个过程：</p>
<ul>
<li>样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样</li>
<li>子抽样：从整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量</li>
</ul>
<p>假若在子抽样的过程中，两个基模型抽取的输入随机变量有一定的重合，那么这两个基模型对整体样本的抽样将不再独立，这时基模型之间便具有了相关性。</p>
<h3 id="3-5-小结"><a href="#3-5-小结" class="headerlink" title="3.5 小结"></a>3.5 小结</h3><p>还记得调参的目标吗：模型在训练集上的准确度和防止过拟合能力的大和谐！为此，我们目前做了一些什么工作呢？</p>
<ul>
<li>使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力</li>
<li>对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低</li>
<li>对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低</li>
<li>整体模型的偏差和方差与基模型的偏差和方差息息相关</li>
</ul>
<p>这下总算有点开朗了，那些让我们抓狂的参数，现在可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。</p>
<h2 id="四、Gradient-Boosting"><a href="#四、Gradient-Boosting" class="headerlink" title="四、Gradient Boosting"></a>四、Gradient Boosting</h2><p>对基于Gradient Boosting框架的模型的进行调试时，我们会遇到一个重要的概念：损失函数。在本节中，我们将把损失函数的“今生来世”讲个清楚！</p>
<p>基于boosting框架的整体模型可以用线性组成式来描述，其中$h<a href="x">i</a>$为基模型与其权值的乘积：<img src="http://omu7tit09.bkt.clouddn.com/15049164023539.png" alt=""></p>
<p>根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：<img src="http://omu7tit09.bkt.clouddn.com/15049164187673.png" alt=""></p>
<p>这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使$F<a href="x">i</a>$逼近真实值y。</p>
<h3 id="4-1-拟合残差"><a href="#4-1-拟合残差" class="headerlink" title="4.1 拟合残差"></a>4.1 拟合残差</h3><p>使F<a href="x">i</a>逼近真实值，其实就是使h<a href="x">i</a>逼近真实值和上一轮迭代的预测值F<a href="x">i-1</a>之差，即残差（y-F<a href="x">i-1</a>）。最直接的做法是构建基模型来拟合残差，在博文《GBDT（MART） 迭代决策树入门教程 | 简介》中，作者举了一个生动的例子来说明通过基模型拟合残差，最终达到整体模型F(x)逼近真实值。</p>
<p>研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：<br><img src="http://omu7tit09.bkt.clouddn.com/15049165747607.png" alt=""></p>
<p>也就是说，若$F<a href="x">i-1</a>$加上拟合了反向梯度的$h<a href="x">i</a>$得到$F<a href="x">i</a>$，该值可能将导致平方差损失函数降低，预测的准确度提高！这显然不是巧合，但是研究者们野心更大，希望能够创造出一种对任意损失函数都可行的训练方法，那么仅仅拟合残差是不恰当的了。</p>
<h3 id="4-2-拟合反向梯度"><a href="#4-2-拟合反向梯度" class="headerlink" title="4.2 拟合反向梯度"></a>4.2 拟合反向梯度</h3><h4 id="4-2-1-契机：引入任意损失函数"><a href="#4-2-1-契机：引入任意损失函数" class="headerlink" title="4.2.1 契机：引入任意损失函数"></a>4.2.1 契机：引入任意损失函数</h4><p>引入任意损失函数后，我们可以定义整体模型的迭代式如下：<br><img src="http://omu7tit09.bkt.clouddn.com/15049166219029.png" alt=""></p>
<p>在这里，损失函数被定义为泛函。</p>
<h4 id="4-2-2-难题一：任意损失函数的最优化"><a href="#4-2-2-难题一：任意损失函数的最优化" class="headerlink" title="4.2.2 难题一：任意损失函数的最优化"></a>4.2.2 难题一：任意损失函数的最优化</h4><p>对任意损失函数（且是泛函）的最优化是困难的。我们需要打破思维的枷锁，将整体损失函数L’定义为n元普通函数（n为样本容量），损失函数L定义为2元普通函数（记住！！！这里的损失函数不再是泛函！！！）：<br><img src="http://omu7tit09.bkt.clouddn.com/15049167322659.png" alt=""></p>
<p>我们不妨使用梯度最速下降法来解决整体损失函数L’最小化的问题，先求整体损失函数的反向梯度：<br><img src="http://omu7tit09.bkt.clouddn.com/15049167506559.png" alt=""></p>
<p>假设已知样本x的当前预测值为$F<a href="x">i-1</a>$，下一步将预测值按照反向梯度，依照步长为$r[i]$，进行更新：<br><img src="http://omu7tit09.bkt.clouddn.com/15049167761742.png" alt=""></p>
<p>步长r[i]不是固定值，而是设计为：<br><img src="http://omu7tit09.bkt.clouddn.com/15049167933072.png" alt=""></p>
<h4 id="4-2-3-难题二：无法对测试样本计算反向梯度"><a href="#4-2-3-难题二：无法对测试样本计算反向梯度" class="headerlink" title="4.2.3 难题二：无法对测试样本计算反向梯度"></a>4.2.3 难题二：无法对测试样本计算反向梯度</h4><p>问题又来了，由于测试样本中y是未知的，所以无法求反向梯度。这正是Gradient Boosting框架中的基模型闪亮登场的时刻！在第i轮迭代中，我们创建训练集如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049168107346.png" alt=""></p>
<p>也就是说，让基模型拟合反向梯度函数，这样我们就可以做到只输入x这一个参数，就可求出其对应的反向梯度了（当然，通过基模型预测出来的反向梯度并不是准确的，这也提供了泛化整体模型的机会）。</p>
<p>综上，假设第i轮迭代中，根据新训练集训练出来的基模型为$f<a href="x">i</a>$，那么最终的迭代公式为：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049168358384.png" alt=""></p>
<h3 id="4-3-常见的损失函数"><a href="#4-3-常见的损失函数" class="headerlink" title="4.3 常见的损失函数"></a>4.3 常见的损失函数</h3><p>ls：最小均方回归中用到的损失函数。在之前我们已经谈到，从拟合残差的角度来说，残差即是该损失函数的反向梯度值（所以又称反向梯度为伪残差）。不同的是，从拟合残差的角度来说，步长是无意义的。该损失函数是sklearn中Gradient Tree Boosting回归模型默认的损失函数。</p>
<p>deviance：逻辑回归中用到的损失函数。熟悉逻辑回归的读者肯定还记得，逻辑回归本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以logistic函数表达。所以，如果该损失函数可用在多类别的分类问题上，故其是sklearn中Gradient Tree Boosting分类模型默认的损失函数。</p>
<p>exponential：指数损失函数，表达式为：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169121290.png" alt=""></p>
<p>对该损失函数求反向梯度得：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169232006.png" alt=""></p>
<p>这时，在第i轮迭代中，新训练集如下：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169392577.png" alt=""><br>脑袋里有什么东西浮出水面了吧？让我们看看Adaboost算法中，第i轮迭代中第j个样本权值的更新公式：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169515007.png" alt=""><br>样本的权值什么时候会用到呢？计算第i轮损失函数的时候会用到：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169634827.png" alt=""><br>让我们再回过头来，看看使用指数损失函数的Gradient Boosting计算第i轮损失函数：<br><img src="http://omu7tit09.bkt.clouddn.com/15049169776362.png" alt=""></p>
<p>天呐，两个公式就差了一个对权值的归一项。这并不是巧合，当损失函数是指数损失时，Gradient Boosting相当于二分类的Adaboost算法。是的，指数损失仅能用于二分类的情况。</p>
<h3 id="4-4-步子太大容易扯着蛋：缩减"><a href="#4-4-步子太大容易扯着蛋：缩减" class="headerlink" title="4.4 步子太大容易扯着蛋：缩减"></a>4.4 步子太大容易扯着蛋：缩减</h3><p>缩减也是一个相对显见的概念，也就是说使用Gradient Boosting时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。将缩减代入迭代公式：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15049170105760.png" alt=""></p>
<p>缩减需要配合基模型数一起使用，当缩减率v降低时，基模型数要配合增大，这样才能提高模型的准确度。</p>
<h3 id="4-5-初始模型"><a href="#4-5-初始模型" class="headerlink" title="4.5 初始模型"></a>4.5 初始模型</h3><p>还有一个不那么起眼的问题，初始模型$F<a href="x">0</a>$是什么呢？如果没有定义初始模型，整体模型的迭代式一刻都无法进行！所以，我们定义初始模型为：<br><img src="http://omu7tit09.bkt.clouddn.com/15049170394638.png" alt=""></p>
<p>根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。对所有的样本来说，根据初始模型预测出来的值都一样。</p>
<h3 id="4-5-Gradient-Tree-Boosting"><a href="#4-5-Gradient-Tree-Boosting" class="headerlink" title="4.5 Gradient Tree Boosting"></a>4.5 Gradient Tree Boosting</h3><p>终于到了备受欢迎的Gradient Tree Boosting模型了！但是，可讲的却已经不多了。我们已经知道了该模型的基模型是树模型，并且可以通过对特征的随机抽样进一步减少整体模型的方差。我们可以在维基百科的Gradient Boosting词条中找到其伪代码实现。</p>
<h3 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a>4.6 小结</h3><p>到此，读者应当很清楚Gradient Boosting中的损失函数有什么意义了。要说偏差描述了模型在训练集准确度，则损失函数则是描述该准确度的间接量纲。也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！</p>
<h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>　　磨刀不误砍柴功，我们花了这么多时间来学习必要的理论，我强调一次：必要的理论！集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度和防止过拟合的能力达到协调，从而达到在样本总体上的最佳准确度。有了本文的理论知识铺垫，在下篇中，我们将对Random Forest和Gradient Tree Boosting中的每个参数进行详细阐述，同时也有一些小试验证明我们的结论。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Scikit-Learn </tag>
            
            <tag> 集成学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（33）：特征处理（Feature Processing）]]></title>
      <url>/2017/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8833%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%EF%BC%88Feature%20Processing%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>特征工程（Feature Engineering）经常被说为机器学习中的black art，这里面包含了很多不可言说的方面。怎么处理好特征，最重要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方。这篇文章我之所以命名为特征处理（Feature Processing），是因为这里面要介绍的东西只是特征工程中的一小部分。这部分比较基础，比较容易说，所以由此开始。</p>
<a id="more"></a>
<p>单个原始特征（或称为变量）通常属于以下几类之一：</p>
<ul>
<li>连续（continuous）特征；</li>
<li>无序类别（categorical）特征；</li>
<li>有序类别（ordinal）特征。</li>
</ul>
<p>本文中我主要介绍针对单个特征的处理方法，虽然也会附带介绍基础的特征组合方法。同时处理多个特征，以及更复杂的特征处理方法介绍，以后我再另外细说。下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。</p>
<h2 id="一、初级篇"><a href="#一、初级篇" class="headerlink" title="一、初级篇"></a>一、初级篇</h2><p>这节要讲的处理技术，应该刚接触机器学习不久的同学都会知道。</p>
<h3 id="1-1-连续特征"><a href="#1-1-连续特征" class="headerlink" title="1.1 连续特征"></a>1.1 连续特征</h3><h3 id="1-2-无序特征"><a href="#1-2-无序特征" class="headerlink" title="1.2 无序特征"></a>1.2 无序特征</h3><p>可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>color取值</th>
<th>向量表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>red</td>
<td>(1, 0, 0)</td>
<td></td>
</tr>
<tr>
<td>green</td>
<td>(0, 1, 0)</td>
</tr>
<tr>
<td>blue</td>
<td>(0, 0, 1)</td>
</tr>
</tbody>
</table>
</div>
<p>这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。</p>
<p>机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。</p>
<p>既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>color取值</th>
<th>向量表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>red</td>
<td>(1, 0)</td>
<td></td>
</tr>
<tr>
<td>green</td>
<td>(0, 1)</td>
</tr>
<tr>
<td>blue</td>
<td>(0, 0)</td>
</tr>
</tbody>
</table>
</div>
<p>这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。</p>
<h3 id="1-3-有序特征"><a href="#1-3-有序特征" class="headerlink" title="1.3 有序特征"></a>1.3 有序特征</h3><p>有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad &lt; normal &lt; good。</p>
<p>当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。</p>
<p>当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>status取值</th>
<th>向量表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>bad</td>
<td>(1, 0, 0)</td>
<td></td>
</tr>
<tr>
<td>normal</td>
<td>(1, 1, 0)</td>
</tr>
<tr>
<td>good</td>
<td>(1, 1, 1)</td>
</tr>
</tbody>
</table>
</div>
<p>上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。</p>
<h2 id="二、中级篇"><a href="#二、中级篇" class="headerlink" title="二、中级篇"></a>二、中级篇</h2><p>最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。</p>
<p>以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y：</p>
<script type="math/tex; mode=display">y=w^Tx</script><p>但大部分实际情况下，yy与xx都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果yy与xx的实际关系如下图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15041455130654.png" alt=""></p>
<p>那么直接把xx扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。</p>
<p>对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。</p>
<h3 id="2-1-方法一：离散化"><a href="#2-1-方法一：离散化" class="headerlink" title="2.1 方法一：离散化"></a>2.1 方法一：离散化</h3><p>最常用的转化方式是对xx做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。</p>
<p>离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系。</p>
<p>比如取离散点{0.5,1.5,2.5}，通过判断xx属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始值xx</th>
<th>离散化后的值</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.1</td>
<td>(1, 0, 0, 0)</td>
</tr>
<tr>
<td>1.3</td>
<td>(0, 1, 0, 0)</td>
</tr>
<tr>
<td>3.2</td>
<td>(0, 0, 0, 1)</td>
</tr>
<tr>
<td>5.8</td>
<td>(0, 0, 0, 1)</td>
</tr>
</tbody>
</table>
</div>
<p>离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法：</p>
<blockquote>
<p>等距离离散：</p>
</blockquote>
<p>顾名思义，就是离散点选取等距点。我们上面对xx取离散点{0.5,1.5,2.5}就是一种等距离散，见下图。图中垂直的灰线代表离散点。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15041459283636.png" alt=""></p>
<blockquote>
<p>等样本点离散</p>
</blockquote>
<p>选取的离散点保证落在每段里的样本点数量大致相同，见下图。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15041459715963.png" alt=""></p>
<blockquote>
<p>画图观察趋势</p>
</blockquote>
<p>以xx为横坐标，yy为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把xx离散化为长度为3的向量。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15041460055828.png" alt=""></p>
<p>上面介绍的这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的xx到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。</p>
<p>针对上面这种硬离散的一种改进就是使用软离散，也就是在离散时考虑到xx与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。有兴趣的同学可以去ESL1这本书中找点感觉。</p>
<h3 id="2-2-函数变换"><a href="#2-2-函数变换" class="headerlink" title="2.2 函数变换"></a>2.2 函数变换</h3><p>函数变换直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>常用非线性函数f(x)</th>
<th>x的取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>$x^α; α∈(−∞,+∞)$</td>
<td>$(−∞,+∞)$</td>
</tr>
<tr>
<td>$log(x)$</td>
<td>$(0,+∞)$</td>
</tr>
<tr>
<td>$log(\frac{x}{1−x})$</td>
<td>$(0,1)$</td>
</tr>
</tbody>
</table>
</div>
<p>这个方法操作起来很简单，但记得对新加入的特征做归一化。</p>
<p>对于我们前面的问题，只要把$x^2$，$x^3$也作为特征加入即可，因为实际上y就是x的一个三次多项式。</p>
<h2 id="三、高级篇"><a href="#三、高级篇" class="headerlink" title="三、高级篇"></a>三、高级篇</h2><h3 id="3-1-笛卡尔乘积"><a href="#3-1-笛卡尔乘积" class="headerlink" title="3.1 笛卡尔乘积"></a>3.1 笛卡尔乘积</h3><p>我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。下面的矩阵表达方式更清楚地说明了这种组合。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>X</th>
<th>on</th>
<th>off</th>
</tr>
</thead>
<tbody>
<tr>
<td>red</td>
<td></td>
</tr>
<tr>
<td>green</td>
<td></td>
</tr>
<tr>
<td>blue</td>
</tr>
</tbody>
</table>
</div>
<p>对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。</p>
<h3 id="3-2-离散化续篇"><a href="#3-2-离散化续篇" class="headerlink" title="3.2 离散化续篇"></a>3.2 离散化续篇</h3><p>在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是画图观察趋势。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。</p>
<p>机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法</p>
<p>决策树离散化方法通常也是每次离散化一个连续特征，做法如下：</p>
<p>单独用此特征和目标值yy训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。</p>
<p>这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。</p>
<h3 id="3-3-核方法"><a href="#3-3-核方法" class="headerlink" title="3.3 核方法"></a>3.3 核方法</h3><p>核方法经常作为线性模型的一种推广出现。以线性回归模型为例，它对应的核方法如下：</p>
<script type="math/tex; mode=display">f_\theta (x)=\sum_{i=1}^n\theta _iK(x,x_i)</script><p>其中$\{x_i\}^n_{i=1}$为训练样本点，$K(x_i，x_j)$为核函数，比如常用的高斯核函数为：</p>
<script type="math/tex; mode=display">K(x_i，x_j)=exp(-\frac{||x_i-x_j||_2^2}{2h^2})</script><p>如果我们把上面模型里的${K(x,x_i)}^n_{i=1}$看成特征，而$θ$看成模型参数的话，上面的模型仍旧是个线性模型。所以可以认为核方法只是特征函数变换的一种方式。</p>
<p>当然，如果把核函数$K(x_i,x_j)$看成一种相似度的话，那上面的模型就是kNN模型了，或者叫做加权平均模型也可以。</p>
<p>因为核方法在预测时也要用到训练样本点，耗内存且计算量大，所以在数据量较大的实际问题中用的并不多。</p>
<p>到此，我已经介绍了不少针对单个特征的处理方法。这些处理方法很难说哪个好哪个不好。有些问题这个好，有些问题那个好，也没什么绝招能直接判断出哪种方法能适合哪些问题。唯一的招就是：Experiment a lot!</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>Trevor Hastie et al. The Elements of Statistical Learning, 2001. ↩</li>
<li>Douglas Aberdeen et al. The Learning Behind Gmail Priority Inbox, 2010. ↩</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征工程 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（32）：MapReduce执行流程详解]]></title>
      <url>/2017/07/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8832%EF%BC%89%EF%BC%9AMapReduce%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3/</url>
      <content type="html"><![CDATA[<h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>MapReduce最早是由Google提出的分布式数据处理模型，随后受到了业内的广泛关注，并被大量应用到各种商业场景中。比如：</p>
<a id="more"></a>
<ul>
<li>搜索：网页爬取、倒排索引、PageRank。</li>
<li>Web访问日志分析：分析和挖掘用户在web上的访问、购物行为特征，实现个性化推荐；分析用户访问行为。</li>
<li>文本统计分析：比如莫言小说的WordCount、词频TFIDF分析；学术论文、专利文献的引用分析和统计；维基百科数据分析等。</li>
<li>海量数据挖掘：非结构化数据、时空数据、图像数据的挖掘。</li>
<li>机器学习：监督学习、无监督学习、分类算法如决策树、SVM等。</li>
<li>自然语言处理：基于大数据的训练和预测；基于语料库构建单词同现矩阵，频繁项集数据挖掘、重复文档检测等。</li>
<li>广告推荐：用户点击（CTR)和购买行为（CVR）预测。</li>
</ul>
<p>一个Map/Reduce作业（job）通常会把输入的数据（input file）且分为若干个独立的数据块（splits），然后由map任务（task）以完全并行的方式处理它们。Map/Reduce框架会对map的输出做一个Shuffle操作，Suffle操作后的结果会输入给reduce任务。整个Map/Reduce框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15032152319495.png" alt=""></p>
<p>Map/Reduce计算集群由一个单独的JobTracker（master）和每个集群节点一个TaskTracker（slave）共同组成。JobTracker负责调度构成一个作业的所有任务，这些任务会被分派到不同的TaskTracker上去执行，JobTRacker会去监控它们的执行、重新执行已经失败的任务。而TaskTracker仅负责执行由JobTracker指派的任务。<br><img src="http://omu7tit09.bkt.clouddn.com/15032154071740.png" alt=""></p>
<p>本文将按照map/reduce执行流程中各个任务的时间顺序详细叙述map/reduce的各个任务模块，包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。下图是一个不错的执行流程图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15032154926693.jpg" alt=""></p>
<h2 id="二、作业的提交与监控"><a href="#二、作业的提交与监控" class="headerlink" title="二、作业的提交与监控"></a>二、作业的提交与监控</h2><p>JobClient是用户提交的作业与JobTracker交互的主要接口。<br><img src="http://omu7tit09.bkt.clouddn.com/15032157622568.jpg" alt=""></p>
<p>JobClient提交作业的过程如下：</p>
<ol>
<li>map/reduce程序通过runJob()方法新建一个JobClient实例;</li>
<li>向JobTracker请求一个新jobID，通过JobTracker的getNewJobId()获取；</li>
<li>检查作业输入输出说明。如果没有指定输出目录或者输出目录已经存在，作业将不会被提交，map/reduce程序； 输入作业划分split，如果划分无法计算（如：输入路径不存在），作业将不会被提交，错误返回给map/reduce程序。</li>
<li>将运行作业所需要的资源（作业的jar文件、配置文件、计算所得的输入划分）复制到一个以作业ID命名的目录中；</li>
<li>通过调用JobTracker的submitJob()方法，告诉JobTracker作业准备提交；</li>
<li>JobTracker将提交的作业放到一个内部队列中，交由作业调度器进行调度，并对其进行初始化。</li>
<li>创建Map任务、Reduce任务：一个split对应一个map，有多少split就有多少map; Reduce任务的数量由JobConf的mapred.reduce.tasks属性决定</li>
<li>TaskTracker执行一个简单的循环，定期发送心跳（heartbeat）给JobTracker</li>
</ol>
<h2 id="三、执行流程"><a href="#三、执行流程" class="headerlink" title="三、执行流程"></a>三、执行流程</h2><h3 id="3-1-Input-file"><a href="#3-1-Input-file" class="headerlink" title="3.1 Input file"></a>3.1 Input file</h3><p>Input file是map/reduce任务的原始数据，一般存储在HDFS上。应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop的 job client提交作业（jar包/可执行程序等）和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。</p>
<h4 id="3-1-1-InputFormat"><a href="#3-1-1-InputFormat" class="headerlink" title="3.1.1 InputFormat"></a>3.1.1 InputFormat</h4><p>InputFormat为Map/Reduce作业输入的细节规范。Map/Reduce框架根据作业的InputFormat来：</p>
<ol>
<li>检查作业输入的正确性，如格式等。</li>
<li>把输入文件切分成多个逻辑InputSplit实例， 一个InputSplit将会被分配给一个独立的Map任务。</li>
<li>提供RecordReader实现，这个RecordReader从逻辑InputSplit中获得输入记录（”K-V对”），这些记录将由Map任务处理。</li>
</ol>
<p>InputFormat有如下几种:<br><img src="http://omu7tit09.bkt.clouddn.com/15032163887568.png" alt=""></p>
<blockquote>
<p>TextInputFormat:</p>
</blockquote>
<p>TextInputFormat是默认的INputFormat，输入文件中的每一行就是一个记录，Key是这一行的byte offset，而value是这一行的内容。如果一个作业的Inputformat是TextInputFormat，并且框架检测到输入文件的后缀是 .gz 和 .lzo，就会使用对应的CompressionCodec自动解压缩这些文件。但是需要注意，上述带后缀的压缩文件不会被切分，并且整个压缩文件会分给一个mapper来处理。</p>
<blockquote>
<p>KeyValueTextInputFormat</p>
</blockquote>
<p>输入文件中每一行就是一个记录，第一个分隔符字符切分每行。在分隔符字符之前的内容为Key，在之后的为Value。分隔符变量通过key.value.separator.in.input.line变量设置，默认为(\t)字符。</p>
<blockquote>
<p>NLineInputFormat</p>
</blockquote>
<p>与TextInputFormat一样，但每个数据块必须保证有且只有Ｎ行，mapred.line.input.format.linespermap属性，默认为1。</p>
<blockquote>
<p>SequenceFileInputFormat</p>
</blockquote>
<p>一个用来读取字符流数据的InputFormat，为用户自定义的。字符流数据是Hadoop自定义的压缩的二进制数据格式。它用来优化从一个MapReduce任务的输出到另一个MapReduce任务的输入之间的数据传输过程。</p>
<h3 id="3-2-输入分片（Input-files）"><a href="#3-2-输入分片（Input-files）" class="headerlink" title="3.2 输入分片（Input files）"></a>3.2 输入分片（Input files）</h3><p>InputSplit是一个单独的Map任务需要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。通常一个split就是一个block，这样做的好处是使得Map任务可以在存储有当前数据的节点上运行本地的任务，而不需要通过网络进行跨节点的任务调度。</p>
<p>可以通过设置mapred.min.split.size， mapred.max.split.size, block.size来控制拆分的大小。如果mapred.min.split.size大于block size，则会将两个block合成到一个split，这样有部分block数据需要通过网络读取；如果mapred.max.split.size小于block size，则会将一个block拆成多个split，增加了Map任务数。</p>
<p>在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">输入文件大小                10M     65M     127M</div><div class="line">分割后的InputSplit大小      10M     64M,1M     64M，63M</div></pre></td></tr></table></figure>
<p>在Map任务开始前，会先获取文件在HDFS上的路径和block信息，然后根据splitSize对文件进行切分（splitSize = computeSplitSize(blockSize, minSize, maxSize) ），默认splitSize 就等于blockSize的默认值（64m）。</p>
<p>假设现在我们有两个文本文件，作为我们例子的输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">File 1 内容：</div><div class="line">My name is Tony</div><div class="line">My company is pivotal</div><div class="line"></div><div class="line">File 2 内容：</div><div class="line">My name is Lisa</div><div class="line">My company is EMC</div></pre></td></tr></table></figure>
<h3 id="3-2-Map阶段"><a href="#3-2-Map阶段" class="headerlink" title="3.2 Map阶段"></a>3.2 Map阶段</h3><p>Map是一类将输入记录集转换为中间格式记录集的独立任务，主要是读取InputSplit的每一个Key,Value对并进行处理。</p>
<p>首先我们的输入就是两个文件， 默认情况下就是两个split, 对应前面图中的split 0, split 1</p>
<p>两个split 默认会分给两个Mapper来处理， WordCount例子相当地暴力， 这一步里面就是直接把文件内容分解为单词和 1 （注意， 不是具体数量， 就是数字1）其中的单词就是我们的主健，也称为Key, 后面的数字就是对应的值，也称为value.</p>
<p>那么对应两个Mapper的输出就是：</p>
<blockquote>
<p>split 0</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">My       1</div><div class="line">name    1</div><div class="line">is         1</div><div class="line">Tony     1</div><div class="line">My          1</div><div class="line">company     1</div><div class="line">is       1</div><div class="line">Pivotal   1</div></pre></td></tr></table></figure>
<blockquote>
<p>split 1</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">My       1</div><div class="line">name    1</div><div class="line">is       1</div><div class="line">Lisa     1</div><div class="line">My       1</div><div class="line">company  1</div><div class="line">is       1</div><div class="line">EMC   　　1</div></pre></td></tr></table></figure>
<h3 id="3-3-Shuffle阶段"><a href="#3-3-Shuffle阶段" class="headerlink" title="3.3 Shuffle阶段"></a>3.3 Shuffle阶段</h3><p>将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</p>
<h4 id="3-3-1-Partition"><a href="#3-3-1-Partition" class="headerlink" title="3.3.1 Partition"></a>3.3.1 Partition</h4><p>Partition 是什么？ Partition 就是分区。</p>
<p>为什么要分区？ 因为有时候会有多个Reducer, Partition就是提前对输入进行处理， 根据将来的Reducer进行分区. 到时候Reducer处理的时候， 只需要处理分给自己的数据就可以了。 </p>
<p>如何分区？ 主要的分区方法就是按照Key 的不同，把数据分开，其中很重要的一点就是要保证Key的唯一性， 因为将来做Reduce的时候有可能是在不同的节点上做的， 如果一个Key同时存在于两个节点上， Reduce的结果就会出问题， 所以很常见的Partition方法就是哈希。</p>
<p>结合我们的例子， 我们这里假设有两个Reducer, 前面两个split 做完Partition的结果就会如下：</p>
<blockquote>
<p>split 0</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company　1</div><div class="line">is     　1</div><div class="line">is　　　　1</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">My　　   1</div><div class="line">My　　　　1</div><div class="line">name　　1</div><div class="line">Pivotal   1</div><div class="line">Tony　　  1</div></pre></td></tr></table></figure>
<blockquote>
<p>split 1</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company 1</div><div class="line">is 　　　1</div><div class="line">is      1</div><div class="line">EMC　　　1</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">My　　   1</div><div class="line">My       1</div><div class="line">name　　 1</div><div class="line">Lisa     1</div></pre></td></tr></table></figure>
<p>其中Partition 1 将来是准备给Reducer 1 处理的， Partition 2 是给Reducer 2 的</p>
<p>这里我们可以看到， Partition 只是把所有的条目按照Key 分了一下区， 没有其他任何处理， 每个区里面的Key 都不会出现在另外一个区里面。</p>
<h4 id="3-3-2-Sort"><a href="#3-3-2-Sort" class="headerlink" title="3.3.2 Sort"></a>3.3.2 Sort</h4><p>Sort 就是排序喽， 其实这个过程在我来看并不是必须的， 完全可以交给客户自己的程序来处理。 那为什么还要排序呢？ 可能是写MapReduce的大牛们想，“大部分reduce 程序应该都希望输入的是已经按Key排序好的数据， 如果是这样， 那我们就干脆顺手帮你做掉啦， 请叫我雷锋！”  ……好吧， 你是雷锋.</p>
<p>那么我们假设对前面的数据再进行排序， 结果如下：</p>
<blockquote>
<p>split 0</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company　1</div><div class="line">is     　1</div><div class="line">is　　　　1</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">My　　   1</div><div class="line">My　　　　1</div><div class="line">name　　1</div><div class="line">Pivotal   1</div><div class="line">Tony　　  1</div></pre></td></tr></table></figure>
<blockquote>
<p>split 1</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company 1</div><div class="line">EMC　　 1</div><div class="line">is 　　　1</div><div class="line">is      1</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">Lisa　　 1</div><div class="line">My　　   1</div><div class="line">My       1</div><div class="line">name　　 1</div></pre></td></tr></table></figure>
<p>这里可以看到， 每个partition里面的条目都按照Key的顺序做了排序</p>
<h4 id="3-3-3-Combine"><a href="#3-3-3-Combine" class="headerlink" title="3.3.3 Combine"></a>3.3.3 Combine</h4><p>什么是Combine呢？ Combine 其实可以理解为一个mini Reduce 过程， 它发生在前面Map的输出结果之后， 目的就是在结果送到Reducer之前先对其进行一次计算， 以减少文件的大小， 方便后面的传输。 但这步也不是必须的。</p>
<p>按照前面的输出， 执行Combine:</p>
<blockquote>
<p>split 0</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company　1</div><div class="line">is     　2</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">My　　   2</div><div class="line">name　　1</div><div class="line">Pivotal   1</div><div class="line">Tony　　  1</div></pre></td></tr></table></figure>
<blockquote>
<p>split 1</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company 1</div><div class="line">EMC　　 1</div><div class="line">is 　　　2</div><div class="line"></div><div class="line">Partition 2:</div><div class="line">Lisa　　 1</div><div class="line">My　　   2</div><div class="line">name　　 1</div></pre></td></tr></table></figure>
<p>我们可以看到， 针对前面的输出结果， 我们已经局部地统计了is 和My的出现频率， 减少了输出文件的大小。</p>
<h4 id="3-3-4-copy"><a href="#3-3-4-copy" class="headerlink" title="3.3.4 copy"></a>3.3.4 copy</h4><p>下面就要准备把输出结果传送给Reducer了。 这个阶段被称为Copy, 但事实上雷子认为叫他Download更为合适， 因为实现的时候， 是通过http的方式， 由Reducer节点向各个mapper节点下载属于自己分区的数据。</p>
<p>那么根据前面的Partition, 下载完的结果如下：</p>
<p>Reducer 节点 1 共包含两个文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company　1</div><div class="line">is     　2</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Partition 1:</div><div class="line">company　　1</div><div class="line">EMC　　　　1</div><div class="line">is　　　　2</div></pre></td></tr></table></figure>
<p>Reducer 节点 2 也是两个文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> Partition 2:</div><div class="line">My　　   2</div><div class="line">name　　1</div><div class="line">Pivotal   1</div><div class="line">Tony　　  1</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Partition 2:</div><div class="line">Lisa　　 1</div><div class="line">My　　   2</div><div class="line">name　　 1</div></pre></td></tr></table></figure>
<p>这里可以看到， 通过Copy, 相同Partition 的数据落到了同一个节点上。</p>
<h4 id="3-3-5-merge"><a href="#3-3-5-merge" class="headerlink" title="3.3.5 merge"></a>3.3.5 merge</h4><p>如上一步所示， 此时Reducer得到的文件是从不同Mapper那里下载到的， 需要对他们进行合并为一个文件， 所以下面这一步就是Merge, 结果如下：</p>
<blockquote>
<p>Reducer 节点 1</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">company　1</div><div class="line">company  1</div><div class="line">EMC　　  1</div><div class="line">is     　2</div><div class="line">is　　　　2</div></pre></td></tr></table></figure>
<blockquote>
<p>Reducer 节点 2</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Lisa　　1</div><div class="line">My　　   2</div><div class="line">My　　　　2</div><div class="line">name　　1</div><div class="line">name　　1</div><div class="line">Pivotal   1</div><div class="line">Tony　　  1</div></pre></td></tr></table></figure>
<h3 id="3-4-Redeuce阶段"><a href="#3-4-Redeuce阶段" class="headerlink" title="3.4 Redeuce阶段"></a>3.4 Redeuce阶段</h3><p>reduce阶段对数据进行归约处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15032285242095.jpg" alt=""></p>
<h2 id="四、实例说明"><a href="#四、实例说明" class="headerlink" title="四、实例说明"></a>四、实例说明</h2><p>下面将以WordCount为例，解释MapReduce各个阶段的概念。 假设存在一个文本a.txt，文本内每行是一个数字，我们要统计每个数字出现的次数。文本内的数字称为Word，数字出现的次数称为Count。如果MaxCompute Mapreduce完成这一功能，需要经历下图描述的几个步骤：<br><img src="http://omu7tit09.bkt.clouddn.com/15032286457705.jpg" alt=""></p>
<ol>
<li>首先对文本进行分片，将每片内的数据作为单个Map Worker的输入；</li>
<li>Map处理输入，每获取一个数字，将数字的Count设置为1，并将此<word, count="">对输出，此时以Word作为输出数据的Key；<br>在Shuffle阶段前期，首先对每个Map Worker的输出，按照Key值，即Word值排序。排序后进行Combine操作，即将Key值(Word值)相同的Count累加，构成一个新的<word, count="">对。此过程被称为合并排序；</word,></word,></li>
<li>在Shuffle阶段后期，数据被发送到Reduce端。Reduce Worker收到数据后依赖Key值再次对数据排序；</li>
<li>每个Reduce Worker对数据进行处理时，采用与Combiner相同的逻辑，将Key值(Word值)相同的Count累加，得到输出结果；</li>
</ol>
<p>分布式相关<br>mr 方案解决矩阵相乘的代码；<br>hadoop原理，shuffle如何排序，map如何切割数据，如何处理数据倾斜，join的mr代码</p>
<p>MR的shuffle过程？内存不够时涉及大文件排序如何处理？ 答：先hash到不同文件中，每个文件排序，然后每个文件读取行，类似归并排序的思路？</p>
<p>Hadoop,Spark,storm下面的产品，原理，适用场景</p>
<p>spark跟hadoop的区别答：spark有RDD机制，写内存，相对hadoop适合迭代运算</p>
<p>如何用hadoop实现k-means<br>简单介绍 MapReduce 原理，有没有看过源码，说说 Map 阶段怎么实现的,<br>MapReduce 实现统计出现次数最多的前 100 个访问 IP.<br>MapReduce 实现统计不重复用户 ID,MapReduce 实现两个数据集求交集。<br>HBase 行健怎么设计,spark 性能一般优化方法,spark streaming 和 storm 区别.给了一张笔试题， 10 道选择，一道大题。选择题是 java 基础知识，大题一个有三问：根据场景写出 Hive 建表语句； Hsql 从表中查询；<br>用MapReduce写好友推荐，在一堆单词里面找出现次数最多的k个<br>用分布式的方法做采样怎么保证采样结果完全符合预期？<br>后面又问了Hadoop,Spark,storm下面的产品，原理，适用场景，<br>写一个 Hadoop 版本的 wordcount。<br>K-means能否分布式实现？ 答：因为本身是迭代式算法，所以只能半分布式实现，即在计算类的均值、每个样本点属于哪个类的时候<br>还有怎么解决mapreduce数据倾斜</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（31）：在线最优化求解（online Optimization）]]></title>
      <url>/2017/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8831%EF%BC%89%EF%BC%9A%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%EF%BC%88online%20Optimization%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>最优化求解问题可能是我们在工作中遇到的最多的一类问题了：从已有的数据中提炼出最适合的模型参数，从而对未知的数据进行预测。当我们面对高维高数据量的场景时，常见的批量处理的方式已经显得力不从心，需要有在线处理的方法来解决此类问题。本文以模型的稀疏性作为主线，逐一介绍几个在线最优化求解算法，并进行推导，力求讲清楚算法的来龙去脉，以及不同算法之间的区别和联系，达到融会贯通。在各个算法原理介绍之后，都给出该算法的工程实现伪代码，可以用于实际工作的参考。</p>
<a id="more"></a>
<h2 id="一、动机与目的"><a href="#一、动机与目的" class="headerlink" title="一、动机与目的"></a>一、动机与目的</h2><p>在实际工作中，无论是工程师、项目经理、产品同学都会经常讨论一类话题：“从线上对比的效果来看，某某特征或因素对xx产品的最终效果有很大的影响。”这类话题本质上说的是通过已有的数据反映出某些特定的因素对结果有很强的正（或负）相关性。而如何定量计算这种相关性？如何得到一套模型参数能够使得效果达到最好？这就是最优化计算要做的事情。</p>
<p>举一类典型点的例子：在推荐和广告计算中，我们经常会需要对某些值进行预测，例如在一条推荐或广告在曝光之前先预测用户是否会点击（CTR预估），或者是否会由此产生某些转换（RPM预估）。这列问题可以表示为：针对一个输入$X=[x_1,x_2,……x_N]\in R^N$，通过某个函数$h(X)$计算（预测）输出$y\in R$。根据$y$值为连续的还是离散的，预测问题被划分成回归问题（Regression）和分类问题（Classification）。而利用已有的样本数据$\{(X_j,y_j) | j=1,2,3….,M\}$训练$h(X)$的过程往往转换成一个最优化求解的过程。</p>
<p>无论是线性回归（Linear Regression）、逻辑回归（Logistic Regression）、支持向量机（SVM）、深度学习（Deep Learning）中，最优化求解都是基本的步骤。常见的梯度下降、牛顿法、拟牛顿法等属于批量处理的方法（Batch），每次更新都需要对已经训练过的样本重新训练一遍。当我们面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效，因此需要在线处理的方法来解决相同的问题。关于在线最优化问题（Online Optimization）的论文比较多，注意查找阅读费时费力，那么本文就以高维高数据量的应用场景中比较看重的稀疏性作为主线，来介绍一些在线最优化的方法。</p>
<p>本文的预期读者大概有如下几类：</p>
<ol>
<li>具有很深的机器学习经验和背景的高阶人员：就拿这篇文章当做一个关于在线最优化算法的回归材料好了，如有错误和不足欢迎指正。</li>
<li>具有一定机器学习经验的中级读者：可以将本文作为一个理论资料进行阅读，略过“预备知识”部分，直接进入主题，将之前对于在线最优化算法的理解串联起来，希望对将来的工作提供帮助。</li>
<li>对机器学习有认识但是时间经验较少的初级读者：从预备知识看起，注意理解相关概念和方法，从而达到融会贯通的目的。</li>
<li>仅仅对算法的工程实现感兴趣的读者：大致浏览下预备知识的2.3节，了解我们要讨论什么，然后直奔各算法的算法逻辑（伪代码），照着实现就好了。</li>
<li>高富帅和白富美：只需要知道本文讨论的是一堆好用的求最优解的方法，可以用于分类回归预测的一系列问题，然后吩咐工程师去实践就好了。还可以拿这篇文章嘲笑机器学习的屌丝：看你们弄些啥，累死累活的，挣那么几个钢镚。</li>
</ol>
<h2 id="二、预备知识"><a href="#二、预备知识" class="headerlink" title="二、预备知识"></a>二、预备知识</h2><h3 id="2-1-凸函数"><a href="#2-1-凸函数" class="headerlink" title="2.1 凸函数"></a>2.1 凸函数</h3><p>如果$f(X)$是定义在N为向量空间上的实值函数，对于在$f(X)$的定义域$C$上的任意两个点$X_1$和$X_2$，以及任意$[0,1]$之间的值$t$都有：</p>
<script type="math/tex; mode=display">f(tX_1+(1-t)X_2)≤tf(X_1)+(1-t)f(X_2) \ \ \ ∀𝑋_1,𝑋_2∈𝐶, \ \  0≤𝑡≤1</script><p>则$f(X)$是严格凸函数（Strict Convex），如图一所示，（a）为严格凸函数，（b）为凸函数。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午12.13.26.png" alt="屏幕快照 2017-07-27 上午12.13.26"></p>
<h3 id="2-2-拉格朗日乘数法及KKT条件"><a href="#2-2-拉格朗日乘数法及KKT条件" class="headerlink" title="2.2 拉格朗日乘数法及KKT条件"></a>2.2 拉格朗日乘数法及KKT条件</h3><p>通常我们需要求解的最优化问题有如下三类：</p>
<p>1.无约束优化问题：</p>
<script type="math/tex; mode=display">X=arg\underset{X}{min} \  f(X)</script><p>含义是求解$X$，使得目标函数$f(X)$最小；<br>2.有等式约束的优化问题：</p>
<script type="math/tex; mode=display">X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n</script><p>含义是在n个等式约束$h_k(X)=0$的条件下，求解$X$，使得目标函数$f(X)$最小；<br>3.有不等式约束的优化问题：</p>
<script type="math/tex; mode=display">X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n \\ g_l(X)≤0；l=1,2.....m</script><p>含义是在n个等式约束$h_k(X)$以及m各不等式约束$g_l(X)$的条件下，求解$X$使得目标函数$f(X)$最小。</p>
<p>针对无约束最优化问题，通常做法就是对$f(X)$求导，并令$\frac{\partial}{\partial X}f(X)=0$，求解可以得到最优值。如果$f(X)$为凸函数，则可以保证结果为全局最优解。</p>
<p>针对有等式约束的最优化问题，采用拉格朗日乘数法（Lagrange Multiplier）进行求解：通过拉格朗日系数$A=[a_1,a_2…a_n]^T\in R^n$把等式约束和目标函数组合成为一个式子，对该式子进行最优化求解：</p>
<script type="math/tex; mode=display">X=arg\underset{X}{min}\ [f(X)+A^TH(X)]</script><p>其中,$H(X)=[h_1(X),h_2(X)…h_n(X)]^T\in R^n$，相当于将有等式约束的最优化问题转换成了无约束最优化求解问题，解决方法依旧是对$f(X)+A^TH(X)$的各个参数$(X,A)$求偏导，并令其为0，联立等式求解。</p>
<p>针对有不等式约束的最优化问题，常用的方法是KKT条件（Karush-Kuhn-Tucker Conditions）：同样地，把所有的不等式约束、等是约束和目标函数全部写为一个式子：</p>
<script type="math/tex; mode=display">L(X,A,B)=f(X)+A^TH(X)+B^TG(X)</script><p>KKT条件是说最优值必须满足以下条件：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial X}L(X,A,B)=0 \\ H(X)=0 \\ B^TG(X)=0</script><p>其中，$B=[b_1,b_2…b_m]^T\in R^m，G(X)=[g_1(X),g_2(X)…g_m(X)]^T \in R^m$。KKT条件是求解最优值$X^*$的必要条件，要使其成为充分必要条件，还需要$f(X)$为凸函数才行。</p>
<p>在KKT条件中，$B^TG(X)=0$这个条件最有趣，因为$g_l(X)≤0$,如果要满足这个等式，需要$b_l=0$或者$g_l(X)=0$。在我们后面的推导中会用到这个性质。</p>
<h3 id="2-3-从Batch到Online"><a href="#2-3-从Batch到Online" class="headerlink" title="2.3 从Batch到Online"></a>2.3 从Batch到Online</h3><p>我们面对的最优化问题都是无约束的最优化问题（有约束最优化问题可以利用拉格朗日乘数法或KKT条件转换成无约束最优化问题），因此我们通常可以将它们描述成：</p>
<script type="math/tex; mode=display">W=𝑎𝑟𝑔\underset{w}{𝑚𝑖𝑛}\ l(𝑊, 𝑍) \\ 𝑍 = \{(𝑋_𝑗,𝑦_𝑗)|𝑗 = 1,2,...𝑀\}\\𝑦_𝑗 =h(𝑊,𝑋_𝑗)</script><p>这里$Z$为观测样本集合（训练集）；$X_j$是第$j$条阉割版的特征向量；$y_j=h(W,X_j)$为预测值；$h(W,X_j)$为特征向量到预测值的映射函数；$l (𝑊,𝑍) $为最优化求解的目标函数，也称作损失函数，损失函数通常可以分解为各样本损失函数的累加，即$l(𝑊,𝑍) =\sum_{j=1}^{M}l(𝑊,𝑍_𝑗)$;$W$为特征权重，也就是我们需要求解的参数。以线性回归和逻辑回归为例，它们的映射函数和损失函数分别为：<br> <img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午9.44.34.png" alt="屏幕快照 2017-07-27 上午9.44.34"><br>在2.1中我们给出了无约束最优化问题解析解的求法。而在我们实际的数值计算中，通常做法是随机给定一个初始的$W^{(0)}$，通过迭代，在每次迭代中计算损失函数在当前$W^{(t)}$的下降方向，并更新$W$,直到损失函数稳定在最小的点。例如著名的梯度下降法（Gradient Descent）就是通过计算损失函数的在当前$W^{(t)}$处的梯度（Gradient），以梯度$\nabla_Wl(W^{(t)},Z)$的反方向作为下降方向更新$W$，如果损失函数是一个非平滑的凸函数（Non-Smooth Convex），在不可导处用次梯度（Subgradient）方向的反方向作为下降方向。算法如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午9.52.55.png" alt="屏幕快照 2017-07-27 上午9.52.55"><br>GD是一种批量处理的方式（Batch），每次更新$W$的时候都要扫描所有的样本以计算一个全局的梯度$\nabla_Wl(W,Z)$</p>
<p>考虑另一种权重更新策略：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午9.54.35.png" alt="屏幕快照 2017-07-27 上午9.54.35"><br>在算法2中，每次迭代仅仅根据单个样本更新权重$W$，这种算法称作随机梯度下降（SGD，Stochastic Gradient Descent）</p>
<p>与 GD 每次扫所有的样本以计算一个全局的梯度相比，SGD 则每次只针对一 个观测到的样本进行更新。通常情况下，SGD 能够比 GD“更快”地令𝑊逼近最优值。当样 本数𝑀特别大的时候，SGD 的优势更加明显，并且由于 SGD 针对观测到的“一条”样本更新 𝑊，很适合进行增量计算，实现梯度下降的 Online 模式(OGD, Online Gradient Descent)。</p>
<h3 id="2-4-正则化"><a href="#2-4-正则化" class="headerlink" title="2.4 正则化"></a>2.4 正则化</h3><p>正则化(Regularization)的意义本质上是为了避免训练得到的模型过度拟合(overfitting) 训练数据。我们用图 2 来说明什么是过拟合。图 2 是一个局部加权线性回归(Locally weighted linear regression)的训练结果，当学习度为 1 时，相当于进行线性回归，这时候模型与训练样本以及实际曲线拟合得都不够好，模型处于 欠拟合(underfitting)状态;当学习度逐渐增加到 4 的过程中，模型逐渐与实际曲线吻合; 随着学习度继续增加，越来越多的样本直接落到模型曲线上(模型拟合训练数据)，但是模 型却与实际曲线相差越来越大，出现了过拟合。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午9.58.07.png" alt="屏幕快照 2017-07-27 上午9.58.07"><br>过拟合体现出来的现象就是特征权重𝑊的各个维度的绝对值非常大:一些大正数，一些大负数。这种模型虽然能够很好匹配样本(如图 2 中 Degree = 20 的情况)，但是对新样本做 预测的时候会使得预测值与真实值相差很远。</p>
<p>为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重𝑊的限制， 限制它的模不要太大，如果用𝜓(𝑊)表示特征权重𝑊的一种求模计算，那么(2-3-1)转换成:</p>
<p>为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重$𝑊$的限制， 限制它的模不要太大，如果用$𝜓(𝑊)$表示特征权重$𝑊$的一种求模计算，那么(2-3-1)转换成:</p>
<script type="math/tex; mode=display">W=arg \underset {W}{min }l(W,Z) \\ s.t. 𝜓(𝑊)<𝛿</script><p>这个时候我们面对的是一个有约束的最优化问题。根据 KKT 条件，我们知道当选取合适的系数𝜆，那么这个有约束最优化问题等价于如下无约束最优化问题:</p>
<script type="math/tex; mode=display">W=arg\underset{W}{min}[l(W,Z)+𝜆𝜓(𝑊)]</script><p>其中$𝜓(𝑊)$ 称作正则化因子(Regularizer)，是一个关于$𝑊$求模的函数，我们常用正则化因子有L1和L2正则化:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.02.31.png" alt="屏幕快照 2017-07-27 上午10.02.31"><br>不管是使用L1还是L2正则化，其基本原理都是一样的，即在最小化损失函数$l(𝑊,𝑍)$的同时，还要考虑$𝑊$的模带来的贡献，从而避免$𝑊$的维度上取一些绝对值很大的值。</p>
<p>$L_1$和$L_2$正则化的区别主要有两个:</p>
<ol>
<li>L1 正则化在0处不可导，而 L2 正则化可导。好 在无论是 L1 还是 L2 正则化本身都是凸函数，因此在计算 L1 正则化的梯度方向的可以采用次梯度代替;</li>
<li>在 Batch 模式下，L1 正则化通常产生更加稀疏(Sparse)的模型，𝑊的更 多维度为 0，这些为 0 的维度就代表了不是很相关的维度，从而起到了特征选择(Feature Selection)的目的。</li>
</ol>
<p>我们对稀疏性(Sparsity)比较感兴趣。除了特征选择的作用以外，稀疏性可以使得预测计算的复杂度降低。那么为什么 L1 正则化会产生这种稀疏性?通常可以根据文献[9]中的理解，如图 3 所示:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.08.52.png" alt="屏幕快照 2017-07-27 上午10.08.52"></p>
<p>假如特征维度𝑁 = 2，那么 L1 正则化引入的约束条件是𝑊只能取转置方形中的值(图3-(a)中黑色方框内)，L2正则化对应的是一个圆形区域(图3-(b)中黑色圆形区 域)，图 3 中绿色部分代表损失函数的等高线，等高线与约束区域的首次相交的地方就是最优解。可以看到，由于 L1 正则化的约束区域与坐标轴相交的地方都有“角”出现，等高线更容易在这个“角”上与约束区域相交，导致另一个维度上的权重值为 0;而 L2 正则化则 没有这种性质，因为没有“角”，等高线在坐标轴上与约束区域相交的概率大为减小。这样 从直观上就解释了 L1 正则化更容易产生稀疏性的原因。</p>
<p>那么在Online模式下呢，不同于 Batch，Online 中每次𝑊的更新并不是沿着全局梯度进行下降，而是沿着某个样本的产生的梯度方向进行下降，整个寻优过程变得像是一个“随机” 查找的过程(SGD 中 Stochastic 的来历)，这样 Online 最优化求解即使采用 L1 正则化的方式， 也很难产生稀疏解。后面介绍的各个在线最优化求解算法中，稀疏性是一个主要的追求目标。</p>
<h2 id="三、在线最优化求解算法"><a href="#三、在线最优化求解算法" class="headerlink" title="三、在线最优化求解算法"></a>三、在线最优化求解算法</h2><p>在前面我们做了一些热身，下面将针对在线最优化求解介绍一些算法。在 2.4 中介绍了 L1 正则化在 Online 模式下也不能产生较好的稀疏性，而稀疏性对于高维特征向量以及大数 据集又特别的重要。因此，我们沿着 升模型稀疏性的主线进行算法介绍。</p>
<h3 id="3-1-TG"><a href="#3-1-TG" class="headerlink" title="3.1 TG"></a>3.1 TG</h3><p>为了得到稀疏的特征权重𝑊，最简单粗暴的方式就是设定一个阈值，当𝑊的某维度上系 数小于这个阈值时将其设置为 0(称作简单截断)。这种方法实现起来很简单，也容易理解。 但实际中(尤其在 OGD 里面)𝑊的某个系数比较小可能是因为该维度训练不足引起的，简单进行截断会造成这部分特征的丢失。</p>
<p>截断梯度法(TG, Truncated Gradient)是由 John Langford，Lihong Li 和 Tong Zhang 在 2009 年 出[10]，实际上是对简单截断的一种改进。下面首先 述一下 L1 正则化和简单截断的方 法，然后我们再来看 TG 对简单截断的改进以及这三种方法在特定条件下的转化。</p>
<h4 id="3-1-1-L1正则化法"><a href="#3-1-1-L1正则化法" class="headerlink" title="3.1.1 L1正则化法"></a>3.1.1 L1正则化法</h4><p>由于L1正则项在0处不可导，往往会造成平滑的凸优化问题变成非平滑凸优化问题，因此在每次迭代中采用次梯度计算L1正则项的梯度。权重更新方式为：</p>
<script type="math/tex; mode=display">𝑊^{(𝑡+1)} = 𝑊^𝑡 −𝜂^{(𝑡)}𝐺^{(t)} −𝜂^{(𝑡)}𝜆sgn(𝑊^{(t)})</script><p>注意，这里$𝜆∈R$是一个标量，且$𝜆≥ 0$，为 L1 正则化参数;$sgn(𝑣)$为符号函数，如果$𝑉=[𝑣_1,𝑣_2,…,𝑣_𝑁] ∈R^𝑁$是一个向量，$𝑣_𝑖$是向量的一个维度，那么有$sgn(𝑉)=[sgn(𝑣_1) , sgn(𝑣_2) , … , sgn(𝑣_𝑁)∈R^𝑁$;$𝜂^{(𝑡)}$为学习率，通常将其设置成$1\sqrt{t}$的函数; $𝐺^{(t)}= 𝛻_𝑊l(𝑊^{(𝑡)},𝑍^{(𝑡)})$ 代表了第𝑡次迭代中损失函数的梯度，由于 OGD 每次仅根据观测到的一个样本进行权重更新，因此也不再使用区分样本的下标$𝑗$。</p>
<h4 id="3-1-2-简单截断法"><a href="#3-1-2-简单截断法" class="headerlink" title="3.1.2 简单截断法"></a>3.1.2 简单截断法</h4><p>以𝑘为窗口，当𝑡/𝑘不为整数时采用标准的 SGD 进行迭代，当𝑡/𝑘为整数时，采用如下权重更新方式:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.25.07.png" alt="屏幕快照 2017-07-27 上午10.25.07"><br>注意，这里面$𝜃∈R$是一个标量，且$𝜃≥0$;如果$𝑉=[𝑣_1, 𝑣_2, … , 𝑣_𝑁] ∈ R^𝑁$是一个向量，$𝑣_𝑖$是向量的一个维度，那么有$𝑇_0(𝑉,𝜃) = [𝑇_0(𝑣_1,𝜃) ,𝑇_0(𝑣_2,𝜃) ,…,𝑇_0(𝑣_𝑁,𝜃)] ∈ R^𝑁$。</p>
<h4 id="3-1-3-截断梯度法（TG）"><a href="#3-1-3-截断梯度法（TG）" class="headerlink" title="3.1.3 截断梯度法（TG）"></a>3.1.3 截断梯度法（TG）</h4><p>上述的简单截断法被 TG 的作者形容为 too aggressive，因此 TG 在此基础上进行了改进，同样是采用截断的方式，但是比较不那么粗暴。采用相同的方式表示为:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.34.07.png" alt="屏幕快照 2017-07-27 上午10.34.07"></p>
<p>其中$\lambda^{(t)} \in R$且$\lambda ^{(t)}≥0$。TG同样是以k为窗口，每𝑘步进行一次截断。当$𝑡/𝑘$不为整数时$𝜆(𝑡) = 0$，当$𝑡/𝑘$为整数时$𝜆(𝑡) = 𝑘𝜆$。从公式(3-1-3)可以看出，𝜆和𝜃决定了𝑊的稀疏程度，这 两个值越大，则稀疏性越强。尤其令𝜆 = 𝜃时，只需要通过调节一个参数就能控制稀疏性。</p>
<p>根据公式（3-1-3），我们很容易写出TG的算法逻辑：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.38.01.png" alt="屏幕快照 2017-07-27 上午10.38.01"></p>
<h4 id="3-1-4-TG与简单截断以及L1正则化的关系"><a href="#3-1-4-TG与简单截断以及L1正则化的关系" class="headerlink" title="3.1.4 TG与简单截断以及L1正则化的关系"></a>3.1.4 TG与简单截断以及L1正则化的关系</h4><p>简单截断和截断梯度的区别在于采用了不同的截断公式$𝑇_0$和$𝑇_1$，如图 4 所示。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.39.14.png" alt="屏幕快照 2017-07-27 上午10.39.14"><br>为了清晰地进行比较，我们将公式(3-1-3)进行改写， 述特征权重每个维度的更新方式:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.44.51.png" alt="屏幕快照 2017-07-27 上午10.44.51"><br>如果令$\lambda_{TG}^{(t)}=\theta$，截断公式$Trnc(w,\lambda_{TG}^{(t)},\theta)$变成：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.51.48.png" alt="屏幕快照 2017-07-27 上午10.51.48"><br>此时 TG 退化成简单截断法。<br>如果令$\theta =∞$，截断公式$Trnc(w,\lambda_{TG}^{(t)},\theta)$变成：</p>
<script type="math/tex; mode=display">
Trnc\left(w,\lambda_{TG}^{\left(t\right)},\infty\right)=\left\{\begin{array}{l}
    0\ \  if\ |w|\le\lambda_{TG}^{\left(t\right)}\\
    w-\lambda_{TG}^{\left(t\right)}sgn \left(w\right)\  \ otherwise\\
\end{array}\right.</script><p>如果再令𝑘 = 1，那么特征权重维度更新公式变成<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.57.02.png" alt="屏幕快照 2017-07-27 上午10.57.02"></p>
<p>此时 TG 退化成 L1 正则化法。</p>
<h3 id="3-2-FOBOS"><a href="#3-2-FOBOS" class="headerlink" title="3.2 FOBOS"></a>3.2 FOBOS</h3><h4 id="3-2-1-FOBOS算法原理"><a href="#3-2-1-FOBOS算法原理" class="headerlink" title="3.2.1 FOBOS算法原理"></a>3.2.1 FOBOS算法原理</h4><p>前向后向切分(FOBOS, Forward-Backward Splitting)是由 John Duchi 和 Yoram Singer   出的[11]。从全称上来看，该方法应该叫 FOBAS，但是由于一开始作者管这种方法叫 FOLOS (Forward Looking Subgradients)，为了减少读者的困扰，作者干脆只修改一个字母，叫 FOBOS。<br>在 FOBOS 中，将权重的更新分为两个步骤:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午10.58.47.png" alt="屏幕快照 2017-07-27 上午10.58.47"><br>前一个步骤实际上是一个标准的梯度下降步骤，后一个步骤可以理解为对梯度下降的结果进行微调。<br>观察第二个步骤，发现对𝑊的微调也分为两部分:(1) 前一部分保证微调发生在梯度下 降结果的附近;(2)后一部分则用于处理正则化，产生稀疏性。</p>
<p>如果将(3-2-1)中的两个步骤合二为一，即将$𝑊^{(𝑡 +\frac{1}{2})}$的计算代入$𝑊^{(𝑡 +1)}$中，有:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.00.51.png" alt="屏幕快照 2017-07-27 上午11.00.51"><br>令$𝐹(𝑊) =\frac{1}{2}||𝑊 − 𝑊^{(𝑡)} + 𝜂^{(𝑡)}𝐺^{(𝑡)}||^2 + 𝜂^{(𝑡+\frac{1}{2})} Ψ(𝑊)$如果$W^{(t+1)}$存在一个最优解，那么可以推断0向量一定属于$F(W)$的次梯度集合：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.07.41.png" alt="屏幕快照 2017-07-27 上午11.07.41"><br>由于$W^{(t+1)}=arg\underset{W}{min}F(W)$那么有：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.08.31.png" alt="屏幕快照 2017-07-27 上午11.08.31"><br>上式实际上给出了FOBOS中权重更新的另一种形式：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.10.06.png" alt="屏幕快照 2017-07-27 上午11.10.06"><br>我们这里可以看到，$W^{(t+1)}$不仅仅与迭代前的状态$W^{(t)}$有关，而且与迭代后的$Ψ(𝑊^{(𝑡+1)})$有关。可能这就是FOBOS名称的由来。</p>
<h4 id="3-2-2-L1-FOBOS"><a href="#3-2-2-L1-FOBOS" class="headerlink" title="3.2.2 L1-FOBOS"></a>3.2.2 L1-FOBOS</h4><p>关于 FOBOS 的收敛性和 Regret 就不在此讨论了，详情可参见论文[11]。这里我们来看 看 FOBOS 如何在 L1 正则化下取得比较好的稀疏性。</p>
<p>在 L1 正则化下，有$Ψ(𝑊)=𝜆||W||_1$.为了简化描述，用向量$𝑉 = [𝑣_1,𝑣_2 …𝑣_𝑁] ∈ R^𝑁$来表示$W^{(t+\frac{1}{2})}$，用标量$\bar{𝜆} ∈ R$来表示$𝜂^{(𝑡+\frac{1}{2})} 𝜆$，并将公式（3-2-1）等号右边按维度展开：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.49.57.png" alt="屏幕快照 2017-07-27 上午11.49.57"><br>我们可以看到，在上式求和公式中的每一项都是大于等于0的，所以公式（3-2-2）可以拆解成对特征权重$W$的每一维度单独求解：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.51.59.png" alt="屏幕快照 2017-07-27 上午11.51.59"><br>首先，假设$𝑤^∗$是$minimize_{w_i}(\frac{1}{2}(w_i-v_i)^2+\bar{\lambda }|w_i|)$的最优解，则有$𝑤^*𝑣_i ≥ 0$，这是因为:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 上午11.56.20.png" alt="屏幕快照 2017-07-27 上午11.56.20"><br>既然有$𝑤_𝑖∗𝑣_𝑖 ≥ 0$，那么我们分两种情况$𝑣_𝑖 ≥ 0$和$𝑣_𝑖 &lt; 0$来讨论:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午12.03.34.png" alt="屏幕快照 2017-07-27 下午12.03.34"><br>综合上面的分析，可以得到在 FOBOS 在 L1 正则化条件下，特征权重的各个维度更新的方式为:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午12.04.37.png" alt="屏幕快照 2017-07-27 下午12.04.37"><br>其中，$𝑔_𝑖^{(𝑡)}$为梯度$𝐺^{(𝑡)}$ 在维度𝑖上的取值。<br>根据公式(3-2-3)，我们很容易就可以设计出 L1-FOBOS 的算法逻辑:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午12.05.26.png" alt="屏幕快照 2017-07-27 下午12.05.26"></p>
<h4 id="3-2-3-L1-FOBOS与TG的关系"><a href="#3-2-3-L1-FOBOS与TG的关系" class="headerlink" title="3.2.3 L1-FOBOS与TG的关系"></a>3.2.3 L1-FOBOS与TG的关系</h4><p>从公式(3-2-3)可以看出，L1-FOBOS 在每次更新𝑊的时候，对𝑊的每个维度都会进行判定，当满足$𝑤_𝑖^{𝑡} − 𝜂^{(𝑡)}𝑔_𝑖^{(𝑡)} − 𝜂^{(𝑡+\frac{1}{2})}𝜆 ≤ 0$ 时对该维度进行“截断”，令$𝑤_𝑖^{(𝑡+1)} = 0$。那么我们怎么去理解这个判定条件呢?如果我们把判定条件写成$𝑤_𝑖^{𝑡} − 𝜂^{(𝑡)}𝑔_𝑖^{(𝑡)} ≤𝜂^{(𝑡+\frac{1}{2})}𝜆 $，那么这个含 义就很清晰了:当一条样本产生的梯度不足以令对应维度上的权重值发生足够大的变化$𝜂^{(𝑡+\frac{1}{2})}𝜆 $，认为在本次更新过程中该维度不够重要，应当令其权重为 0。</p>
<p>对于 L1-FOBOS 特征权重的各个维度更新公式(3-2-3)，也可以写作如下形式:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午12.08.48.png" alt="屏幕快照 2017-07-27 下午12.08.48"><br>比较上式与TG的特征权重维度更新公式(3-1-4)，我们发现如果令$𝜃 = ∞，𝑘 = 1，𝜆^{(t)}_{TG}(𝑡) = 𝜂^{(𝑡+\frac{1}{2})}𝜆$,L1-FOBOS 与 TG 完全一致。我们可以认为 L1-FOBOS 是 TG 在特定条件下的特殊形式。</p>
<h3 id="3-3-RDA"><a href="#3-3-RDA" class="headerlink" title="3.3 RDA"></a>3.3 RDA</h3><h4 id="3-3-1-RDA算法原理"><a href="#3-3-1-RDA算法原理" class="headerlink" title="3.3.1 RDA算法原理"></a>3.3.1 RDA算法原理</h4><p>不论怎样，简单截断、TG、FOBOS 都还是建立在 SGD 的基础之上的，属于梯度下降类 型的方法，这类型方法的优点就是精度比较高，并且 TG、FOBOS 也都能在稀疏性上得到  升。但是有些其它类型的算法，例如 RDA，是从另一个方面来求解 Online Optimization 并且 更有效地 升了特征权重的稀疏性。</p>
<p>正则对偶平均(RDA, Regularized Dual Averaging)是微软十年的研究成果，RDA 是 Simple Dual Averaging Scheme 一个扩展，由 Lin Xiao 发表于 2010 年。<br>在 RDA 中，特征权重的更新策略为:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.29.43.png" alt="屏幕快照 2017-07-27 下午2.29.43"></p>
<p>其中， $&lt;𝐺^{(𝑟)}, 𝑊&gt;$ 表示梯度$𝐺^{(𝑟)}$对𝑊的积分平均值(积分中值);$Ψ(𝑊)$为正则项;$h(𝑊)$为一个辅助的严格凸函数; $\{𝛽^{(𝑡)}|t ≥ 1\}$ 是一个非负且非自减序列。</p>
<p>本质上，公式（3-3-1）中包含了3个部分：</p>
<ol>
<li>线性函数$\frac{1}{t}\sum _{r=1}^t<g^{(t)},w>$包含了之前所有梯度（或次梯度）的平均值（dual average）；</g^{(t)},w></li>
<li>正则项$Ψ(𝑊)$</li>
<li>额外正则项$\frac{\beta ^{(t)}}{t}h(W)$，它是一个严格凸函数</li>
</ol>
<h4 id="3-3-2-L1-RDA"><a href="#3-3-2-L1-RDA" class="headerlink" title="3.3.2 L1-RDA"></a>3.3.2 L1-RDA</h4><p>我们下面来看看在 L1 正则化下，RDA 中的特征权重更新具有什么样的形式以及如何产 生稀疏性。</p>
<p>令$𝛹(𝑊) = 𝜆||𝑊||_1$，并且由于$h(W)$是一个关于𝑊的严格凸函数，不妨令$h(W)=\frac{1}{2}||W||_2^2$， 此外，将非负非自减序列$\{𝛽^{(𝑡)}|t≥1\}$定义为$𝛽^{(𝑡)}=𝛾\sqrt{𝑡}$，将L1正则化代入公式(3-3-1)有:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.39.59.png" alt="屏幕快照 2017-07-27 下午2.39.59"></p>
<p>针对特征权重的各个维度将其拆解成N个独立的标量最小化问题：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.41.03.png" alt="屏幕快照 2017-07-27 下午2.41.03"></p>
<p>这里$\lambda&gt;0,\frac{\gamma}{\sqrt{t}}&gt;0;\bar{g_t}^{(t)}=\frac{1}{t}\sum_{r=1}^tg_i^{(r)}$；公式（3-3-3）就是一个无约束的非平滑最优化问题。</p>
<p>其中第2项$\lambda|w_i|$在$w_i=0$处不可导。假设$w_i^<em>$是其最优解，并且定义$𝜉\in \partial |w_i^</em>|$为$|w_i|$在$w_i^*$的次导数，那么有：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.45.33.png" alt="屏幕快照 2017-07-27 下午2.45.33"><br>如果对公式(3-3-3)求导(求次导数)并等于 0，则有:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.46.03.png" alt="屏幕快照 2017-07-27 下午2.46.03"><br>由于$\lambda&gt;0$，我们针对（3-3-5）分三种情况$|\bar{g_i}^{(t)}|&lt;\lambda,|\bar{g_i}^{(t)}|&gt;\lambda和|\bar{g_i}^{(t)}|&lt;-\lambda$来进行讨论：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.49.35.png" alt="屏幕快照 2017-07-27 下午2.49.35"></p>
<p>综合上面的分析，可以得到L1-RDA特征权重的各个维度更新方式为：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.50.17.png" alt="屏幕快照 2017-07-27 下午2.50.17"><br>这里我们发现，当某个维度上累积梯度平均值的绝对值$|\bar{g_i}^{(t)}|$小于阈值$\lambda$的时候，改为度权重将被置0，特征权重的稀疏性由此产生。<br>根据公式（3-3-6）,可以设计出L1-RDA的算法逻辑：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午2.52.16.png" alt="屏幕快照 2017-07-27 下午2.52.16"></p>
<h4 id="3-3-3-L1-RDA与L1-FOBOS的比较"><a href="#3-3-3-L1-RDA与L1-FOBOS的比较" class="headerlink" title="3.3.3 L1-RDA与L1-FOBOS的比较"></a>3.3.3 L1-RDA与L1-FOBOS的比较</h4><p>在3.2.3中我们看到了L1-FOBOS实际上是TG的一种特殊形式，在L1-FOBOS中，进行“截断”的判定条件是$|w_i^{(t)}-𝜂^{(𝑡)}𝑔_i^{(𝑡)}|≤\lambda_{TG}^{(t)}=𝜂^{(𝑡+\frac{1}{2})}𝜆$，通常会定义𝜂为与$\frac{1}{\sqrt{t}}$正相关的函数$(𝜂 = Θ(\frac{1}{\sqrt{t}})$ ，因此L1-FOBOS的截断阈值为$ Θ(\frac{1}{\sqrt{t}}) \lambda$随着t的增加，这个阈值会逐渐降低。</p>
<p>相比较而言，从3-3-6可以看出，L1-RDA的截断阈值为$\lambda$，是一个常数，并不随着$t$而变化，因此可以认为$L1-RDA$比$L1-FOBOS$在截断判定上更加aggressive，这种性质使得L1-RDA更容易产生稀疏性；此外，RDA中判定对象是梯度的累加均值$\bar{g_i}^{(t)}$，不同于TG或L1-FOBOS中针对单词梯度计算的结果进行判定，避免了由于某些维度由于训练不足导致截断的问题。并且通过调节$\lambda$一个参数，很容易在精度和稀疏性上进行权衡。</p>
<h3 id="3-4-FTRL"><a href="#3-4-FTRL" class="headerlink" title="3.4 FTRL"></a>3.4 FTRL</h3><p>在 3.3.3 中我们从原理上定性比较了 L1-FOBOS 和 L1-RDA 在稀疏性上的表现。有实验证 明，L1-FOBOS 这一类基于梯度下降的方法有比较高的精度，但是 L1-RDA 却能在损失一定精 度的情况下产生更好的稀疏性。那么这两者的优点能不能在一个算法上体现出来?这就是 FTRL 要解决的问题。</p>
<p>FTRL(Follow the Regularized Leader)是由 Google 的 H. Brendan McMahan 在 2010 年  出的[14]，后来在 2011 年发表了一篇关于 FTRL 和 AOGD、FOBOS、RDA 比较的论文[15]，2013 年又和 Gary Holt, D. Sculley, Michael Young 等人发表了一篇关于 FTRL 工程化实现的论文[16]。</p>
<p>本节我们首先从形式上将 L1-FOBOS 和 L1-RDA 进行统一，然后介绍从这种统一形式产生 FTRL 算法，以及介绍 FTRL 算法工程化实现的算法逻辑。</p>
<h4 id="3-4-1-L1-FOBOS和L1-RDA在形式上的统一"><a href="#3-4-1-L1-FOBOS和L1-RDA在形式上的统一" class="headerlink" title="3.4.1 L1-FOBOS和L1-RDA在形式上的统一"></a>3.4.1 L1-FOBOS和L1-RDA在形式上的统一</h4><p>L1-FOBOS在形式上，每次迭代都可以表示为（这里我们令$𝜂^{(𝑡+\frac{1}{2})}= 𝜂^{(𝑡)}= Θ(\frac{1}{\sqrt{𝑡}})$是一个随着t变化的非增正序列)：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.04.49.png" alt="屏幕快照 2017-07-27 下午3.04.49"><br>把两个公式合并到一起，有：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.05.29.png" alt="屏幕快照 2017-07-27 下午3.05.29"><br>通过这个公式很难直接求出$W^{(t+1)}$的解析解，但是我们可以按照维度将其分解为N个独立的最优化步骤：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.06.29.png" alt="屏幕快照 2017-07-27 下午3.06.29"><br>由于$\frac{𝜂^{(t)}}{2}(g_i^{(t)})^2+w_i^{(t)}g_i^{(t)}$与变量$w_i$无关，因此上式可以等价于：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.10.41.png" alt="屏幕快照 2017-07-27 下午3.10.41"><br>再将这N个独立最优化子步骤合并，那么L1-FOBOS可以写作：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.12.31.png" alt="屏幕快照 2017-07-27 下午3.12.31"><br>而对于L1-RDA的公式（3-3-2）,我们可以写作：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.13.10.png" alt="屏幕快照 2017-07-27 下午3.13.10"><br>这里$G^{(1:t)}=\sum^t_{s=1}G^{(s)}$，如果令$\sigma^{(s)}=\frac{1}{𝜂^{(𝑠)}}-\frac{1}{𝜂^{(𝑠-1)}},𝜎^{(1:𝑡)}=\frac{1}{𝜂^{(𝑡)}} $上面两个式子可以写作：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.16.59.png" alt="屏幕快照 2017-07-27 下午3.16.59"><br>需要注意，与论文[15]中的Table 1不同，我们并没有将L1-FOBOS也写成累加梯度的形式。</p>
<p>比较（3-4-1）和（3-4-2）这两个公式，可以看出L1-FOBOS和L1-RDA的区别在于：</p>
<ol>
<li>前者计算的是累加梯度以及L1正则项只考虑当前模的贡献，而后者采用了累加的处理方式；</li>
<li>前者的第三项限制W的变化不能离已迭代过的解太远，而后者则限制W不能离0点太远。</li>
</ol>
<h4 id="3-4-2-FTRL算法原理"><a href="#3-4-2-FTRL算法原理" class="headerlink" title="3.4.2 FTRL算法原理"></a>3.4.2 FTRL算法原理</h4><p>FTRL综合考虑了FOBOS和RDA对于正则项和W限制的区别，其特征权重的更新公式为：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.20.56.png" alt="屏幕快照 2017-07-27 下午3.20.56"></p>
<p>注意，公式（3-4-3）中出现了L2正则项$\frac{1}{2}||W||^2_2$在论文[15]的公式中并没有这一项，但是在其2013年发表的FTRL工程化实现的论文[16]却使用了L2正则项。事实上该项的引入并不影响FTRL的稀疏性，后面的推导会显示这一点。L2正则项的引入仅仅相当于对最优化过程多了一个约束，使得结果求解结果更加平滑。</p>
<p>公式（3-4-3）看上去很复杂，更新权重貌似非常困难的样子，不妨将其进行改写，将最后一项展开，等价于求下面这样一个最优化问题<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.26.37.png" alt="屏幕快照 2017-07-27 下午3.26.37"><br>由于$\frac{1}{2}\sum_{s=1}^t\sigma^{(s)}||W^{(s)}||_2^2$相对于W来说是一个常数，并且令$Z^{(t)}=G^{(1:t)}- \sum_{s=1}^t\sigma^{(s)}W^{(s)}$，上式等价于：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.29.21.png" alt="屏幕快照 2017-07-27 下午3.29.21"><br>针对特征权重的各个维度将其拆解成N个独立的标量最小化问题：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.30.01.png" alt="屏幕快照 2017-07-27 下午3.30.01"><br>到这里，我们遇到了与（3-3-3）类似的问题，用相同的分析方法可以得到：<img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.30.47.png" alt="屏幕快照 2017-07-27 下午3.30.47"></p>
<p>从（3-4-4）可以看出，引入L2正则化并没有对FTRL结果的稀疏性产生任何影响。</p>
<h4 id="3-4-3-Per-Coordinate-Learning-Rates"><a href="#3-4-3-Per-Coordinate-Learning-Rates" class="headerlink" title="3.4.3 Per-Coordinate Learning Rates"></a>3.4.3 Per-Coordinate Learning Rates</h4><p>前面介绍了FTRL的基本推导，但是这里还有一个问题是一直没有被讨论到的：关于学习率$𝜂^{(𝑡)}$的选择和计算。事实上在FTRL中，每个维度上的学习率都是单独考虑的（Per-Coordinate Learning Rates）。</p>
<p>在一个标准的OGD里面使用的是一个全局的学习率策略$𝜂^{(𝑡)} =\frac{ 1}{\sqrt{𝑡}}$，这个策略保证了学习率是一个正的非增长序列，对于每一个特征维度都是一样的。</p>
<p>考虑特征维度的变化率：如果特征1比特征2的变化更快，那么在维度1上的学习率应该下降地更快。我们很容易就可以想到可以用某个维度上的梯度分量来反映这种变化率。在FTRL中，维度i上的学习率是这样计算的：</p>
<script type="math/tex; mode=display">𝜂_𝑖^{(𝑡)}=\frac{𝛼}{𝛽 + \sqrt{\sum_𝑡^{𝑠 = 1} (𝑔_𝑖^{( 𝑠 )})^ 2}}</script><p>由于$\sigma^{(1:t)}=\frac{1}{𝜂^{(𝑡)}}$，所以公式（3-4-4）中</p>
<script type="math/tex; mode=display">\sum_{s=1}^t=\frac{1}{𝜂_i^{(𝑡)}}=\frac{\beta+\sqrt{\sum_t^{s= 1} (g_i ^{(s)})^2}}{a}</script><p>，这里的$\alpha和\beta$是需要输入的参数，(3-4-4)中学习率写成累加的形式，是为了方便理解后面FTRL的迭代计算逻辑。</p>
<h4 id="3-4-4-FTRL算法逻辑"><a href="#3-4-4-FTRL算法逻辑" class="headerlink" title="3.4.4 FTRL算法逻辑"></a>3.4.4 FTRL算法逻辑</h4><p>到现在为止，我们已经得到了 FTRL 的特征权重维度的更新方法(公式(3-4-4))，每个特征维度的学习率计算方法(公式(3-4-5))，那么很容易写出FTRL的算法逻辑（这里是根据(3-4-4) 和(3-4-5)写的 L1&amp;L2-FTRL 求解最优化的算法逻辑，而论文[16]中 Algorithm 1 给出的是 L1&amp;L2-FTRL 针对 Logistic Regression 的算法逻辑):<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.43.42.png" alt="屏幕快照 2017-07-27 下午3.43.42"><br>FTRL 里面的 4 个参数需要针对具体的问题进行设置，指导性的意见参考论文[16]。</p>
<h2 id="四、结束语"><a href="#四、结束语" class="headerlink" title="四、结束语"></a>四、结束语</h2><p>本文作为在线最优化算法的整理和总结，沿着稀疏性的主线，先后介绍了简单截断法、TG、FOBOS、RDA以及FTRL。从类型上看，简单截断法、TG、FOBOS属于同一类，都是梯度下降类的算法，并且TG在特定条件可以转换成简单截断法和FOBOS；RDA属于简单对偶平均的扩展应用；FTRL可以视作RDA和FOBOS的结合，同时具备二者的有点。目前来看，RDA和FTRL是最好的系数模型Online Learning 的算法。</p>
<p>谈到高维高数据量的最优化求解，不可避免的要涉及到并行计算的问题。作者之前有篇 博客[4]讨论了 batch 模式下的并行逻辑回归，其实只要修改损失函数，就可以用于其它问题 的最优化求解。另外，对于 Online 下，文献[17]给出了一种很直观的方法:<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-27 下午3.46.45.png" alt="屏幕快照 2017-07-27 下午3.46.45"><br>对于 Online 模式的并行化计算，一方面可以参考 ParallelSGD 的思路，另一方面也可以 借鉴 batch 模式下对高维向量点乘以及梯度分量并行计算的思路。总之，在理解算法原理的 基础上将计算步骤进行拆解，使得各节点能独自无关地完成计算最后汇总结果即可。</p>
<p>最后，需要指出的是相关论文里面使用的数学符号不尽相同，并且有的论文里面也存在 一定的笔误，但是并不影响我们对其的理解。在本文中尽量采用了统一风格和含义的符号、 变量等，因此在与参考文献中的公式对比的时候会稍有出入。另外，由于笔者的水平有限， 行文中存在的错误难以避免，欢迎大家指正、拍砖。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Convex function. <a href="http://en.wikipedia.org/wiki/Convex_function" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Convex_function</a><br>[2] Lagrange multiplier. <a href="http://en.wikipedia.org/wiki/Lagrange_multiplier" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Lagrange_multiplier</a><br>[3] Karush–Kuhn–Tucker conditions. <a href="http://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions</a><br>[4] 冯扬. 并行逻辑回归. <a href="http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html</a><br>[5] Gradient. <a href="http://sv.wikipedia.org/wiki/Gradient" target="_blank" rel="noopener">http://sv.wikipedia.org/wiki/Gradient</a><br>[6] Subgradient. <a href="http://sv.wikipedia.org/wiki/Subgradient" target="_blank" rel="noopener">http://sv.wikipedia.org/wiki/Subgradient</a><br>[7] Andrew Ng. CS229 Lecture notes. <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a><br>[8] Stochastic Gradient Descent. <a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Stochastic_gradient_descent</a><br>[9] T. Hastie, R. Tibshirani &amp; J. Friedman. The Elements of Statistical Learning, Second Edition: Data Mining,<br>Inference, and Prediction. Springer Series in Statistics. Springer, 2009<br>[10] John Langford, Lihong Li &amp; Tong Zhang. Sparse Online Learning via Truncated Gradient. Journal of Machine<br>Learning Research, 2009<br>[11] John Duchi &amp; Yoram Singer. Efficient Online and Batch Learning using Forward Backward Splitting. Journal of<br>Machine Learning Research, 2009<br>[12] Lin Xiao. Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization. Journal of<br>Machine Learning Research, 2010<br>[13] Convex Set. <a href="http://en.wikipedia.org/wiki/Convex_set" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Convex_set</a><br>[14] H. Brendan McMahan &amp; M Streter. Adaptive Bound Optimization for Online Convex Optimization. In COLT,<br>2010<br>[15] H. Brendan McMahan. Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1<br>Regularization. In AISTATS, 2011<br>[16] H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd<br>Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica, Ad Click Prediction: a View from the Trenches. In ACM SIGKDD, 2013<br>[17] Martin A. Zinkevich, Markus Weimer, Alex Smola &amp; Lihong Li. Parallelized Stochastic Gradient Descent. In NIPS 2010</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> online learning </tag>
            
            <tag> FTRL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（30）：Scikit-Learn总结]]></title>
      <url>/2017/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8830%EF%BC%89%EF%BC%9AScikit-Learn%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p><a href="http://ff120.github.io/2017/05/14/机器学习专题/机器学习_Scikit-Learn使用技巧/" target="_blank" rel="noopener">http://ff120.github.io/2017/05/14/机器学习专题/机器学习_Scikit-Learn使用技巧/</a><br>Scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，了解这个工具包对于机器学习算法的整个流程会有一个整体的了解。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。</p>
<a id="more"></a>
<h2 id="一、性能评价指标"><a href="#一、性能评价指标" class="headerlink" title="一、性能评价指标"></a>一、性能评价指标</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># 计算均方误差</div><div class="line">from sklearn import metrics</div><div class="line">import numpy as np</div><div class="line">rmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))</div><div class="line"></div><div class="line"># 计算准确率</div><div class="line">acc = metrics.accuracy_score(y_test,y_pred)</div><div class="line"></div><div class="line">#混淆矩阵</div><div class="line">cm = metrics.confusion_matrix(y_test,y_pred)</div><div class="line"></div><div class="line"># classification_report</div><div class="line">cr = metrics.classification_report(y_true,y_pred)</div><div class="line"></div><div class="line"># ROC AUC 曲线</div><div class="line">from sklearn.metrics import roc_curve,auc</div></pre></td></tr></table></figure>
<h2 id="二、数据集划分"><a href="#二、数据集划分" class="headerlink" title="二、数据集划分"></a>二、数据集划分</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">from sklearn import cross_validation</div><div class="line">X_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.3,random_state=0)</div><div class="line"></div><div class="line"># K折</div><div class="line">from sklearn.cross_validation import KFold</div><div class="line">kf = KFold(n_samples, n_folds=2)</div><div class="line">for train, test in kf:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line"></div><div class="line"># 保证不同的类别之间的均衡，这里需要用到标签labels</div><div class="line">from sklearn.cross_validation import StratifiedKFold</div><div class="line">labels = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</div><div class="line">skf = StratifiedKFold(labels, 3)</div><div class="line">for train, test in skf:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line">    </div><div class="line"># 留一交叉验证</div><div class="line">from sklearn.cross_validation import LeaveOneOut</div><div class="line">loo = LeaveOneOut(n_samples)</div><div class="line">for train, test in loo:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line"></div><div class="line"># 留P交叉验证</div><div class="line">from sklearn.cross_validation import LeavePOut</div><div class="line">lpo = LeavePOut(n_samples, p=2)</div><div class="line">for train, test in lpo:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line">    </div><div class="line">    </div><div class="line"># 按照额外提供的标签留一交叉验证,常用的情况是按照时间序列</div><div class="line">from sklearn.cross_validation import LeaveOneLabelOut</div><div class="line">labels = [1, 1,1, 2, 2]</div><div class="line">lolo = LeaveOneLabelOut(labels)</div><div class="line">for train, test in lolo:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line">    </div><div class="line"># 按照额外提供的标签留P交叉验证</div><div class="line">from sklearn.cross_validation import LeavePLabelOut</div><div class="line">labels = [1, 1, 2, 2, 3, 3,3]</div><div class="line">lplo = LeavePLabelOut(labels, p=2)</div><div class="line">for train, test in lplo:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div><div class="line"></div><div class="line"># 随机分组</div><div class="line">from sklearn.cross_validation import ShuffleSplit</div><div class="line">ss = ShuffleSplit(16, n_iter=3, test_size=0.25,random_state=0)</div><div class="line">for train_index, test_index in ss:</div><div class="line">    print(&quot;%s %s&quot; % (train_index, test_index))</div><div class="line"></div><div class="line"># 考虑类别均衡的随机分组</div><div class="line">from sklearn.cross_validation import StratifiedShuffleSplit</div><div class="line">import numpy as np</div><div class="line">X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])</div><div class="line">y = np.array([0, 0, 1, 1])</div><div class="line">sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)</div><div class="line">for train, test in sss:</div><div class="line">    print(&quot;%s %s&quot; % (train, test))</div></pre></td></tr></table></figure>
<h2 id="三、特征选择"><a href="#三、特征选择" class="headerlink" title="三、特征选择"></a>三、特征选择</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"># 基于方差的特征选择</div><div class="line">from sklearn import feature_selection</div><div class="line">vt = feature_selection.VarianceThreshold(threshold=&apos;&apos;)</div><div class="line">vt.fit(X_train)</div><div class="line">X_train_transformed = vt.transform(X_train)</div><div class="line">X_test_transformed = vt.transform(X_test)</div><div class="line"></div><div class="line"># 按照某种排序规则 选择前K个特征</div><div class="line"># 除了使用系统定义好的函数f_classif，还可以自己定义函数</div><div class="line">sk = SelectKBest(feature_selection.f_classif,k=100)</div><div class="line">sk.fit(X_train,y_train)</div><div class="line">X_train_transformed = sk.transform(X_train)</div><div class="line">X_test_transformed = sk.transform(X_test)</div><div class="line"></div><div class="line"># 递归特征消除</div><div class="line">rfecv = RFECV(estimator=svc, step=step, cv=StratifiedKFold(y, n_folds = n_folds),scoring=&apos;accuracy&apos;)</div><div class="line">rfecv.fit(X_train, y_train)</div><div class="line">X_train_transformed = rfecv.transform(X_train)</div><div class="line">X_test_transformed = rfecv.transform(y_train)</div><div class="line"></div><div class="line"># 使用L1做特征选择</div><div class="line">from sklearn.svm import LinearSVC</div><div class="line">lsvc = LinearSVC(C=1, penalty=&quot;l1&quot;, dual=False)</div><div class="line">lsvc.fit(X_train,y_train)</div><div class="line">X_train_transformed = lsvc.transform(X_train)</div><div class="line">X_test_transformed = lsvc.transform(y_train)</div><div class="line"></div><div class="line"># 基于树的特征选择</div><div class="line">from sklearn.ensemble import ExtraTreesClassifier</div><div class="line">etc = ExtraTreesClassifier()</div><div class="line">etc.fit(X_train, y_train)</div><div class="line">X_train_transformed = etc.transform(X_train)</div><div class="line">X_test_transformed = etc.transform(X_test)</div><div class="line"></div><div class="line"># 基于线性判别分析做特征选择</div><div class="line">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</div><div class="line">lda = LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;,shrinkage=&apos;auto&apos;)</div><div class="line">lda.fit(X_train, y_train)</div><div class="line">X_train_transformed = lda.transform(X_train)</div><div class="line">X_test_transformed = lda.transform(X_test)</div></pre></td></tr></table></figure>
<blockquote>
<p>基于方差的特征选择</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from sklearn.feature_selection import VarianceThreshold</div><div class="line">X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]</div><div class="line">sel = VarianceThreshold(threshold=(.8 * (1 - .8)))</div><div class="line">X2 = sel.fit_transform(X)</div><div class="line">X2</div><div class="line">Out[5]: </div><div class="line">array([[0, 1],</div><div class="line">       [1, 0],</div><div class="line">       [0, 0],</div><div class="line">       [1, 1],</div><div class="line">       [1, 0],</div><div class="line">       [1, 1]])</div></pre></td></tr></table></figure>
<p>可以看到，方差小于0.16的只有第一维特征，所以X2保留下来的是原来的第二维和第三维特征。这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p>
<blockquote>
<p>Univariate feature selection （单变量特征选择）</p>
</blockquote>
<p>主要使用统计的方法计算各个统计值，再根据一定的阈值筛选出符合要求的特征，去掉不符合要求的特征。</p>
<p>主要的统计方法有</p>
<ul>
<li>F值分类： f_classif </li>
<li>值回归：f_regression</li>
<li>卡方统计：chi2 (适用于非负特征值和稀疏特征值)</li>
</ul>
<p>主要的选择策略</p>
<ul>
<li>选择排名前K的特征：SelectKbest</li>
<li>选择前百分之几的特征：SelectPercentile</li>
<li>SelectFpr：Select features based on a false positive rate test.</li>
<li>SelectFdr：Select features based on an estimated false discovery rate.</li>
<li>SelectFwe：Select features based on family-wise error rate.</li>
<li>GenericUnivariateSelect：Univariate feature selector with configurable mode.</li>
</ul>
<p>其中</p>
<ul>
<li>false positive rate：FP / (FP + TP) 假设类别为0，1；记0为negative,1为positive, FPR就是实际的类别是0，但是分类器错误的预测为1的个数 与 分类器预测的类别为1的样本的总数（包括正确的预测为1和错误的预测为1） 的比值。</li>
<li>estimated false discovery rate: 错误的拒绝原假设的概率；</li>
<li>family-wise error rate: 至少有一个检验犯第一类错误的概率；假设检验的两类错误： &gt; - 第一类错误：原假设是正确的，但是却被拒绝了。(用α表示） &gt; - 第二类错误：原假设是错误的，但是却被接受了。(用β表示)</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> Scikit-Learn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（29）：Sparsity and Some Basics of L1 Regularization]]></title>
      <url>/2017/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8829%EF%BC%89%EF%BC%9ASparsity%20and%20Some%20Basics%20of%20L1%20Regularization/</url>
      <content type="html"><![CDATA[<p>转载自<a href="http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/#ed61992b37932e208ae114be75e42a3e6dc34cb3" target="_blank" rel="noopener">pluskid的个人博客</a><br>Sparsity 是当今机器学习领域中的一个重要话题。John Lafferty 和 Larry Wasserman 在 2006 年的一篇<a href="http://www3.stat.sinica.edu.tw/statistica/J16N2/editorial3.pdf" target="_blank" rel="noopener">评论</a>中提到：</p>
<a id="more"></a>
<blockquote>
<p>Some current challenges … are high dimensional data, sparsity, semi-supervised learning, the relation between computation and risk, and structured prediction.</p>
<blockquote>
<p>—John Lafferty and Larry Wasserman. Challenges in statistical machine learning. Statistica Sinica. Volume 16, Number 2, pp. 307-323, 2006.</p>
</blockquote>
</blockquote>
<p>Sparsity 的最重要的“客户”大概要属 high dimensional data 了吧。现在的机器学习问题中，具有非常高维度的数据随处可见。例如，在文档或图片分类中常用的 <a href="https://www.wikiwand.com/en/Bag-of-words_model" target="_blank" rel="noopener">bag of words</a> 模型里，如果词典的大小是一百万，那么每个文档将由一百万维的向量来表示。高维度带来的的一个问题就是计算量：在一百万维的空间中，即使计算向量的内积这样的基本操作也会是非常费力的。不过，如果向量是稀疏的的话（事实上在 bag of words 模型中文档向量通常都是非常稀疏的），例如两个向量分别只有$L_1$和$L_2$个非零元素，那么计算内积可以只使用$min(L_1,L_2 )$次乘法完成。因此稀疏性对于解决高维度数据的计算量问题是非常有效的。</p>
<p>当然高维度带来的问题不止是在计算量上。例如在许多生物相关的问题中，数据的维度非常高，但是由于收集数据需要昂贵的实验，因此可用的训练数据却相当少，这样的问题通常称为“small , large  problem”—我们一般用  表示数据点的个数，用  表示变量的个数，即数据维度。当$p&gt;&gt;n$的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。<br>因为如果用上所有变量的话，$p$越大，通常会导致模型越复杂，但是反过来$n$又很小，于是就会出现很严重的 overfitting 问题。例如，最简单的线性回归模型：</p>
<script type="math/tex; mode=display">f(X)=\sum_{j=1}^p</script><p>使用square loss来学习的话，就变成最小化如下的问题：</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2=\frac{1}{n}||y-Xw||^2</script><p>这里$X=(x_1,······,x_n)^T \in R^{n \times p}$是数据矩阵，而$y=(y_1,······,y_n)^T$是由标签组成的列向量。该问题具有解析解$\hat{w}=(X^TX)^{-1}X^Ty$然而，如果$p&gt;n$的话，矩阵$X^TX$将会不是满秩的，而这个解也没法算出来。捉着更确切地说，将会有无穷多个解。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解随机选一个的话，很可能并不是很好地解，总而言之，我们过拟合了。</p>
<p>解决 overfitting 最常用的办法就是 regularization ，例如著名的 ridge regression 就是添加一个  $\ell_2 $regularizer ：</p>
<script type="math/tex; mode=display">J_R(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||^2</script><p>直观地看，添加这个regularizer会使得模型的解偏向于norm较小的w。从凸优化的角度来说，最小化上面这个$J(w)$等价于如下问题：</p>
<script type="math/tex; mode=display">\underset{w}{min}\frac{1}{n}||y-Xw||^2</script><p>其中$C$和$\lambda$对应的是个常数。也就是说，也就是说，我们通过限制$w$的norm的大小实现了对模型空间的限制，从而在一定程度上（取决于$\lambda$的大小）避免了overfitting。不过ridge regression并不具有产生稀疏解的能力，得到的系数$w$仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。</p>
<p>不过，特别是在像生物或者医学等通常需要和人交互的领域，稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。比如说，一个病如果依赖于 5 个变量的话，将会更易于医生理解、描述和总结规律，但是如果依赖于 5000 个变量的话，基本上就超出人肉可处理的范围了。</p>
<p>在这里引入稀疏性的方法是用$L_1$regularization 代替 $L_2$regularization，得到如下的目标函数：</p>
<script type="math/tex; mode=display">J_L(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||_1</script><p>该问题通常被称为LASSO（least absolute shrinkage and selection operator）。LASSO 仍然是一个 convex optimization 问题，不过不再具有解析解。它的优良性质是能产生稀疏性，导致$w$中许多项变成零。</p>
<p>可是，为什么它能产生稀疏性呢？这也是一直让我挺感兴趣的一个问题，事实上在之前申请学校的时候一次电话面试中我也被问到了这个问题。我当时的回答是背后的理论我并不是很清楚，但是我知道一个直观上的理解。下面我们就先来看一下这个直观上的理解。</p>
<p>首先，和 ridge regression 类似，上面形式的 LASSO 问题也等价于如下形式：</p>
<script type="math/tex; mode=display">\underset {w}{min }\frac{1}{n}||y-Xw||^2, \ \ \ s.t. ||w||_1≤C</script><p>也就是说，我们将模型空间限制在$w$的一个 $\ell_1$-ball中。为了便于可视化，我们考虑两维的情况，在 $(w^1,w^2)$平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为$C$的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解。如图 所示：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-24 下午5.47.44.png" alt="屏幕快照 2017-07-24 下午5.47.44"></p>
<p>可以看到，$\ell_1-ball$与$\ell_2-ball$的不同就在于他和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有$w^1=0$  ，而更高维的时候（想象一下三维的 $\ell_1 $-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。</p>
<p>相比之下，$\ell_2 $-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么$\ell_1$  regularization 能产生稀疏性，而$\ell_2$regularization 不行的原因了。</p>
<p>不过，如果只限于 intuitive 的解释的话，就不那么好玩了，但是背后完整的理论又不是那么容易能够搞清楚的，既然这次的标题是 Basics ，我们就先来看一个简单的特殊情况好了。</p>
<p>接下来我们考虑 orthonormal design 的情况：$\frac{1}{n}X^TX=I$，然后看看LASSO的解具体是什么样子。注意orthonormal design 实际上是要求特征之间相互正交。这可以通过对数据进行 <a href="https://www.wikiwand.com/en/Principal_component_analysis" target="_blank" rel="noopener">PCA</a>以及模长 normalize 来实现。</p>
<p>注意到LASSO 的目标函数是 convex 的，根据 <a href="https://www.wikiwand.com/en/Karush–Kuhn–Tucker_conditions" target="_blank" rel="noopener">KKT</a> 条件，在最优解的地方要求 gradient  。不过这里有一点小问题：$\ell_1$ -norm 不是光滑的，不存在 gradient ，所以我们需要用一点 <a href="https://www.wikiwand.com/zh-hans/次导数" target="_blank" rel="noopener">subgradient</a> 的东西。</p>
<p>定义：（subgradient，subdifferential）.对于在$p$维欧式空间中的凸开子集$U$上定义的实值函数$f:U \rightarrow R$，一个向量$p$维向量$v$称为$f$在一点$x_0 \in U$处的subgradient，如果对于任意$x \in U$，满足</p>
<script type="math/tex; mode=display">f(x)-f(x_0)≥v·(x-x_0)</script><p>由在点$x_0$处的所有subgradient所组成的集合称为$x_0$处的subdifferential，记为$\partial f(x_0)$</p>
<p>注意 subgradient 和 subdifferential 只是对凸函数定义的。例如一维的情况，$f(x)=|x|$，在$x=0$处的subdifferential 就是$[-1,+1]$这个区间（集合）。注意在$f$的 gradient 存在的点，subdifferential 将是由 gradient 构成的一个单点集合。这样就将 gradient 的概念加以推广了。这个推广有一个很好的性质。</p>
<p>性质（CONDITION GLOBAL MINIMIZER）.点$x_0$是凸函数$f$的一个全局最小值点，当且仅当$0\in \partial f(x_0)$</p>
<p>证明很简单，将$0\in \partial f(x_0)$带入定义的那个式子就可以得到。有了这个工具之后，就可以对 LASSO 的最优解进行分析了。在此之前，我们先看一下原始的 least square 问题的最优解现在变成了什么样子，由于 orthonormal design ，我们有</p>
<script type="math/tex; mode=display">\hat{w}=\frac{1}{n}X^Ty</script><p>然后我们再来看LASSO，假设$\bar{w}=(\bar{w}^1,······，\bar{w}^p)^T$是$J_L(w)$的全局最优值点。考虑第$j$个变量$\bar{w}^j$，有两种情况。</p>
<p>第一种情况：gradient存在，此时$\bar{w}^j≠0$</p>
<p>由于gradient在最小值点必须等于零，我们有</p>
<script type="math/tex; mode=display">\frac{\partial J_L(w)}{\partial w_j}|_{\bar{w}_j}=0</script><p>亦即</p>
<script type="math/tex; mode=display">-\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda sign(\bar{w}^j)=0</script><p>根据orthonormal design性质以及least square问题在orthonormal design时的解$\hat{w}^j$化简得到</p>
<script type="math/tex; mode=display">\bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)</script><p>从这个式子也可以明显地看出$\bar{w}^j$和$\hat{w}^j$是同号的，于是$sign(\bar{w}^j)=sign(\hat{w}^j)$所以上面的式子变为</p>
<script type="math/tex; mode=display">\bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})</script><p>再用一次$sign(\bar{w}^j)=sign(\hat{w}^j)$，两边同时乘以$sign(\bar{w}^j)$，可以得到</p>
<script type="math/tex; mode=display">|\hat{w}^j|-\frac{\lambda}{2}=|\bar{w}^j|≥0</script><p>于是刚才的式子可以进一步写成</p>
<script type="math/tex; mode=display">\bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+</script><p>这里$(x)_+=max \{x,0\}$表示$x$ 的正部。</p>
<p>第二种情况：gradient不存在，此时$\bar{w}^j=0$根据subgradient在最小值点出的性质，此时有：</p>
<script type="math/tex; mode=display">0=\bar{w}^j\in \partial {J_L(\bar{w})}=\{-\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda e:e \in [-1,1]\}</script><p>亦即存在$e_0 \in [-1,1]$使得</p>
<script type="math/tex; mode=display">0=2\bar{w}^j-2\hat{w}^j+\lambda e_0</script><p>于是</p>
<script type="math/tex; mode=display">|\hat{w}^j|=\frac{\lambda}{2}|e_0|≤\frac{\lambda}{2}</script><p>又因为$\bar{w}^j=0$，所以这个时候式子也可以统一为</p>
<script type="math/tex; mode=display">\bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+</script><p>的形式。 </p>
<p>如此一来，在 orthonormal design 的情况下，LASSO 的最优解就可以写为</p>
<script type="math/tex; mode=display">\bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+</script><p>可以用图形象地表达出来。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-24 下午7.12.33.png" alt="屏幕快照 2017-07-24 下午7.12.33"></p>
<p>图上画了原始的 least square 解，LASSO 的解以及 ridge regression 的解，用上面同样的方法（不过由于 ridge regularizer 是 smooth 的，所以过程却简单得多）可以得知 ridge regression 的解是如下形式</p>
<script type="math/tex; mode=display">\frac{n}{1+n\lambda}\hat{w}^j</script><p>可以认为ridge regression 只是做了一个全局缩放，而 LASSO 则是做了一个 soft thresholding ：将绝对值小于$\frac{\lambda}{2}$的那些系数直接变成零了，这也就更加令人信服地解释了 LASSO 为何能够产生稀疏解了。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> L1正则 </tag>
            
            <tag> 稀疏性 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（28）：L1、L2正则化]]></title>
      <url>/2017/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8828%EF%BC%89%EF%BC%9AL1%E3%80%81L2%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>之前讨论了机器学习中的偏差-方差权衡。机器学习里的损失函数（代价函数）可以用来描述模型与真模型（ground truth）之间的差距，因此可以解决“偏差”的问题。但是仅有损失函数，我们无法解决方差的问题，因而会有过拟合风险。</p>
<a id="more"></a>
<p>这次我们讨论损失函数的反面——正则项，看看L1正则项和L2正则项是如何使机器学习模型避免过拟合的。</p>
<p>我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。</p>
<p>过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。</p>
<h2 id="一、从经验风险最小化到结构经验最小化"><a href="#一、从经验风险最小化到结构经验最小化" class="headerlink" title="一、从经验风险最小化到结构经验最小化"></a>一、从经验风险最小化到结构经验最小化</h2><p>经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题：</p>
<script type="math/tex; mode=display">
\underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}</script><p>当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p>
<p>但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为：</p>
<script type="math/tex; mode=display">
R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。<br>结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题：</p>
<script type="math/tex; mode=display">
\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。</p>
<p>其中正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下：</p>
<script type="math/tex; mode=display">
\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。</p>
<h2 id="二、范数与正则项"><a href="#二、范数与正则项" class="headerlink" title="二、范数与正则项"></a>二、范数与正则项</h2><p>在线性代数、函数分析等数学分支中，范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。对于零向量，另其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，而Euclidean距离也是一种范数。</p>
<p>范数满足通常意义上长度的三个基本性质：</p>
<ul>
<li>非负性：$||\vec{x}||≥0$</li>
<li>齐次性：$||c·\vec{x}||=|c|||\vec{x}||$</li>
<li>三角不等式：$||\vec{x}+\vec{y}||≤||\vec{x}||+||\vec{y}||$</li>
</ul>
<p>在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证（当 c=0 时）。</p>
<p>因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是机器学习的学习目标——参数向量。</p>
<p>范数的一般化定义：设$p≥1$的实数，$p-norm$定义为：</p>
<script type="math/tex; mode=display">||x||_p:=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}  \ \ \  \ （1）</script><p>机器学习中有几个常用的范数，分别是：</p>
<ul>
<li>$L_1-$范数：$||x⃗ ||=∑^d_{i=1}| x_i|$;</li>
<li>$L_2-$范数：$||x⃗ ||_2=(∑^d_{i=1}x^2_i)^{1/2}$；</li>
<li>$L_p-$范数：$||x||_p=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}$</li>
<li>$L_∞-$范数：$||x⃗ ||_∞=lim_{p→+∞}(∑^d_{i=1}x^p_i)^{1/p}$。</li>
</ul>
<p>当p=1时，我们称之为taxicab Norm，也叫Manhattan Norm。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的l1范数。其表示某个向量中所有元素绝对值的和。</p>
<p>而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance。也即我们要讨论的l2范数。 </p>
<p>而当p=0时，因其不再满足三角不等性，严格的说此时p已不算是范数了，但很多人仍然称之为l0范数。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用。</p>
<p>下图给出了一个Lp球的形状随着P的减少的可视化图。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008845934277.png" alt=""></p>
<p>在机器学习中，如果使用了$||\vec{w}||_p作为正则项$，则我们说，该机器学习任务引入了$L_p-$正则项。</p>
<h3 id="2-1-L-0-与-L-1-正则项（LASSO-regularizer）"><a href="#2-1-L-0-与-L-1-正则项（LASSO-regularizer）" class="headerlink" title="2.1 $L_0$与$L_1-$正则项（LASSO regularizer）"></a>2.1 $L_0$与$L_1-$正则项（LASSO regularizer）</h3><p>在机器学习里，最简单的学习算法可能是所谓的线性回归模型</p>
<script type="math/tex; mode=display">F(x⃗ ;w⃗ ,b)=∑_{i=1}^nw_i⋅x_i+b</script><p>我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。</p>
<p>于是，我们得到了避免过拟合的第一个思路：使尽可能多的参数为零。为此，最直观地我们可以引入$L_0$-范数。令</p>
<script type="math/tex; mode=display">Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_0\frac{∥w⃗ ∥_0}{n},ℓ_0>0</script><p>这意味着，我们希望绝大多数$\vec{w}$的分量为零。</p>
<p>通过引入 $L_0-$正则项，我们实际上引入了一种「惩罚」机制，即：若要增加模型复杂度以加强模型的表达能力降低损失函数，则每次使得一个参数非零，则引入 $ℓ_0$ 的惩罚系数。也就是说，如果使得一个参数非零得到的收益（损失函数上的收益）不足 $ℓ_0$；那么增加这样的复杂度是得不偿失的。</p>
<p>通过引入$L_0-$正则项，我们可以使模型稀疏化且易于解释，并且在某种意义上实现了「特征选择」。这看起来很美好，但是 $L_0-$正则项也有绕不过去坎：</p>
<ul>
<li>非连续</li>
<li>非凸</li>
<li>不可求导</li>
</ul>
<p>因此，$L_0$正则项虽好，但是求解这样的最优化问题，难以在多项式时间内找到有效解（NP-Hard 问题）。于是，我们转而考虑 L0L0-范数最紧的凸放松（tightest convex relaxation）：L1-范数。令</p>
<script type="math/tex; mode=display">Ω(F(x⃗ ;w⃗ ))\overset{def}{=}ℓ_1\frac{∥w⃗ ∥_1}{n},ℓ_1>0</script><p>我们来看一下参数更新的过程，有哪些变化。考虑目标函数</p>
<script type="math/tex; mode=display">Obj(F)=L(F)+γ⋅ℓ_1\frac{∥w⃗ ∥_1}{n}</script><p>对参数$w_i$求偏导数</p>
<script type="math/tex; mode=display">\frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_1}{n}sgn(w_i)</script><p>因此参数更新的过程为</p>
<script type="math/tex; mode=display">w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_1}{n}sgn(w_i)</script><p>因为$η\frac{γℓ_1}{n}&gt;0$所以多出的项$η\frac{γℓ_1}{n}sgn(w_i)$使得$w_i→0$，实现稀疏化。</p>
<h3 id="2-2-L-2-正则项（Ridge-Regularizer）"><a href="#2-2-L-2-正则项（Ridge-Regularizer）" class="headerlink" title="2.2 $L_2$正则项（Ridge Regularizer）"></a>2.2 $L_2$正则项（Ridge Regularizer）</h3><p>让我们回过头，考虑多项式模型，它的一般形式为：</p>
<script type="math/tex; mode=display">F=∑_{i=1}^nw_i⋅x^i+b</script><p>我们注意到，当多项式模型过拟合时，函数曲线倾向于靠近噪声点。这意味着，函数曲线会在噪声点之间来回扭曲跳跃。这也就是说，在某些局部，函数曲线的切线斜率会非常高（函数导数的绝对值非常大）。对于多项式模型来说，函数导数的绝对值，实际上就是多项式系数的一个线性加和。这也就是说，过拟合的多项式模型，它的参数的绝对值会非常大（至少某几个参数分量的绝对值非常大）。因此，如果我们有办法使得这些参数的值，比较稠密均匀地集中在0附近，就能有效地避免过拟合。</p>
<p>于是我们引入$L_2-$正则项，令</p>
<script type="math/tex; mode=display">Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_2\frac{∥w⃗ ∥_2 }{2n},ℓ_2>0</script><p>因此有目标函数</p>
<script type="math/tex; mode=display">Obj(F)=L(F)+γ⋅ℓ_2\frac{∥w⃗ ∥_2 }{2n}</script><p>对参数$w_i$求偏导数，有</p>
<script type="math/tex; mode=display">\frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_2}{n}w_i</script><p>再有参数更新</p>
<script type="math/tex; mode=display">w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_2}{n}w_i=(1−η\frac{γℓ_2}n)w_i−η\frac{∂L}{∂w_i}</script><p>考虑到$η\frac{γℓ_2}{n}&gt;0$，因此，引入$L_2-$正则项之后，相当于衰减了（decay）参数的权重，使参数趋近于0。</p>
<h3 id="2-3-L-1-正则项与-L-2-正则项的区别"><a href="#2-3-L-1-正则项与-L-2-正则项的区别" class="headerlink" title="2.3 $L_1-$正则项与$L_2-$正则项的区别"></a>2.3 $L_1-$正则项与$L_2-$正则项的区别</h3><p>现在，我们考虑这样一个问题：为什么使用$L-1-$正则项，会倾向于使得参数稀疏化；而使用$L_2-$正则项，会倾向于使得参数稠密地接近于0？</p>
<p>这里引用一张来自周志华老师的著作，《机器学习》（西瓜书）里的插图，尝试解释这个问题。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008845171281.png" alt=""></p>
<p>为了简便起见，我们只考虑模型有两个参数$w_1$和$w_2$的情形。</p>
<p>在图中，我们有三组等值线，位于同一条等值线上的$w_1$与$w_2$映射到相同的平方损失项、$L_1-$范数和$L_2-$范数。并且，对于三组等值线来说，当$(w_1,w_2)$沿着等值线法线方向，向外扩张，则对应的值增大；反之，若沿着法线向内收缩，则对应的值减小。</p>
<p>因此，对于目标函数$Obj(F)$来说，实际上是要在正则项的等值线与损失函数的等值线中寻找一个交点，使得二者的和最小。</p>
<p>对于$L_1-$正则项来说，因为$L_1-$正则项是一组菱形，这些交点容易落在坐标轴上。因此，另一个参数的值在这个交点上就是0，从而实现了稀疏化。</p>
<p>对于 $L_2-$正则项来说，因为 $L_2-$正则项的等值线是一组圆形。所以，这些交点可能落在整个平面的任意位置。所以它不能实现「稀疏化」。但是，另一方面，由于 $(w_1,w_2)$ 落在圆上，所以它们的值会比较接近。这就是为什么 $L_2-$正则项可以使得参数在零附近稠密而平滑。</p>
<h2 id="三、贝叶斯先验"><a href="#三、贝叶斯先验" class="headerlink" title="三、贝叶斯先验"></a>三、贝叶斯先验</h2><p>从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布。</p>
<h3 id="3-1-Linear-Regression"><a href="#3-1-Linear-Regression" class="headerlink" title="3.1 Linear Regression"></a>3.1 Linear Regression</h3><p>我们先看下最原始的线性回归：<br><img src="http://omu7tit09.bkt.clouddn.com/15008831241254.png" alt=""></p>
<script type="math/tex; mode=display">\begin{align*}
 & p(\epsilon^{(i)})  = \frac{1}{\sqrt{2\pi}\delta}exp\left(  -\frac{(\epsilon^{(i)})^2}{2\delta^2} \right)\\
 \Rightarrow & p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)
\end{align*}</script><p>由最大似然估计（MLE）：</p>
<script type="math/tex; mode=display">\begin{align*}
L(w) & = p(\vec{y}|X;w)\\
& = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)\\
& = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)
\end{align*}</script><p>取对数：</p>
<script type="math/tex; mode=display">\begin{align*}
l(w) & = \log L(w)\\
& =\log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})}{2\delta^2}  \right)\\
& = \sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2}  \right)\\
& = m \log \frac{1}{\sqrt{2\pi}\delta} - \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2
\end{align*}</script><p>即</p>
<script type="math/tex; mode=display">w_{MLE} = \arg \underset{w}{\min} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2</script><p>这就导出了我们最原始的$least-squares$损失函数，但这是在我们对参数$w$没有加入任何先验分布的情况下。在数据维度很高的情况下，我们的模型参数很多，模型复杂度高，容易发生过拟合。</p>
<p>比如我们常说的 “small n, large p problem”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当  的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity.<br><img src="http://omu7tit09.bkt.clouddn.com/15008831886499.png" alt=""></p>
<p>这个时候，我们可以对参数$w$引入先验分布，降低模型复杂度。</p>
<h3 id="3-2-Ridge-Regression"><a href="#3-2-Ridge-Regression" class="headerlink" title="3.2 Ridge Regression"></a>3.2 Ridge Regression</h3><p>Ridge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。</p>
<p>我们对参数 w 引入协方差为 $\alpha$ 的零均值高斯先验。<br><img src="http://omu7tit09.bkt.clouddn.com/15008832011388.jpg" alt=""></p>
<p>取对数：<br><img src="http://omu7tit09.bkt.clouddn.com/15008832214560.jpg" alt=""></p>
<p>等价于：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008832817463.jpg" alt=""><br>这不就是Ridge Regression吗？<br><img src="http://omu7tit09.bkt.clouddn.com/15008832999831.png" alt=""><br>看我们得到的参数，在零附近是不是很密集，老实说 ridge regression 并不具有产生稀疏解的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的计算量。</p>
<blockquote>
<p>Typically ridge or ℓ2 penalties are <strong>much better</strong> for minimizing prediction error rather than ℓ1 penalties. The reason for this is that when two predictors are highly correlated, ℓ1 regularizer will simply pick one of the two predictors. In contrast, the ℓ2 regularizer will keep both of them and jointly shrink the corresponding coefficients a little bit. Thus, while the ℓ1 penalty can certainly reduce overfitting, you may also experience a loss in predictive power.</p>
</blockquote>
<p>那现在我们知道了，对参数引入 高斯先验 等价于L2正则化。</p>
<h3 id="3-3-LASSO"><a href="#3-3-LASSO" class="headerlink" title="3.3 LASSO"></a>3.3 LASSO</h3><p>LASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0。</p>
<p>在Ridge Regression中，我们对 w 引入了高斯分布，那么拉普拉斯分布(Laplace distribution)呢？</p>
<p>注：LASSO - least absolute shrinkage and selection operator.<br><img src="http://omu7tit09.bkt.clouddn.com/15008835165358.png" alt=""></p>
<p>我们看下拉普拉斯分布长啥样：<br><img src="http://omu7tit09.bkt.clouddn.com/15008835341132.jpg" alt=""></p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008835458006.png" alt=""><br>关于拉普拉斯和正态分布的渊源，大家可以参见 正态分布的前世今生。<br>重复之前的推导过程我们很容易得到：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008835638203.jpg" alt=""><br>该问题通常被称为 LASSO (least absolute shrinkage and selection operator) 。LASSO 仍然是一个 convex optimization 问题，不具有解析解。它的优良性质是能产生稀疏性，导致 w 中许多项变成零。</p>
<p>再次总结下，对参数引入 拉普拉斯先验 等价于 L1正则化。</p>
<h3 id="3-4-Elastic-Net"><a href="#3-4-Elastic-Net" class="headerlink" title="3.4 Elastic Net"></a>3.4 Elastic Net</h3><p>然而LASSO虽然可以做variable selection，但是不consistent啊，而且当n很小时至多只能选出n个变量；而且不能做group selection。</p>
<p>可能有同学会想，既然 L1和 L2正则各自都有自己的优势，那我们能不能将他们 combine 起来？</p>
<p>于是有了在L1和L2 penalty之间做个权重就是elastic net</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008836000503.png" alt=""><br>因为lasso在解决之前提到的“small n, large p problem”存在一定缺陷。<br><img src="http://omu7tit09.bkt.clouddn.com/15008836178996.png" alt=""></p>
<p>得到结果：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008836391269.jpg" alt=""></p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15008836498017.png" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/15008836608354.png" alt=""></p>
<p>此外针对不consistent有了adaptive lasso，针对不能做group selection有了group lasso, 在graphical models里有了graphical lasso。然后有人说unbiasedness, sparsity and continuity这三条都满足多好，于是有了MCP和SCAD同时满足这三条性质。还有很多penalized regression的方法。</p>
<h3 id="3-5-总结"><a href="#3-5-总结" class="headerlink" title="3.5 总结"></a>3.5 总结</h3><p><img src="http://omu7tit09.bkt.clouddn.com/15008840329778.png" alt=""></p>
<p>正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。</p>
<p>这篇文章从理论推导讲到算法实现。除了高斯先验、拉普拉斯先验，还讲了其他先验：<br>Lazy Sparse Stochastic Gradient Descent for Regularized Mutlinomial Logistic Regression</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> L1正则 </tag>
            
            <tag> L2正则 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（27）：Isolation Forest]]></title>
      <url>/2017/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8827%EF%BC%89%EF%BC%9AIsolation%20Forest/</url>
      <content type="html"><![CDATA[<blockquote>
<p>“An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.” — D. M. Hawkins, Identification of Outliers, Chapman and Hall, 1980.</p>
</blockquote>
<a id="more"></a>
<p>异常检测 (anomaly detection)，或者又被称为“离群点检测” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设：</p>
<ul>
<li>1）异常数据跟样本中大多数数据不太一样。</li>
<li>2）异常数据在整体数据样本中占比比较小。</li>
</ul>
<p>为了刻画异常数据的“不一样”，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度。而 Isolation Forest (Liu et al. 2011) 的想法要巧妙一些，它尝试直接去刻画数据的“疏离”(isolation)程度，而不借助其他量化指标。Isolation Forest 因为简单、高效，在学术界和工业界都有着不错的名声。</p>
<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>我们先用一个简单的例子来说明 Isolation Forest 的基本想法。假设现在有一组一维数据（如下图所示），我们要对这组数据进行随机切分，希望可以把点 A 和点 B 单独切分出来。具体的，我们先在最大值和最小值之间随机选择一个值 x，然后按照 <x 和="">=x 可以把数据分成左右两组。然后，在这两组数据中分别重复这个步骤，直到数据不可再分。显然，点 B 跟其他数据比较疏离，可能用很少的次数就可以把它切分出来；点 A 跟其他数据点聚在一起，可能需要更多的次数才能把它切分出来。<br><img src="http://orsw4brg1.bkt.clouddn.com/v2-dc4248661ea4a274f671c096c0f1eef4_b.jpg" alt=""></x></p>
<p>我们把数据从一维扩展到两维。同样的，我们沿着两个坐标轴进行随机切分，尝试把下图中的点A’和点B’分别切分出来。我们先随机选择一个特征维度，在这个特征的最大值和最小值之间随机选择一个值，按照跟特征值的大小关系将数据进行左右切分。然后，在左右两组数据中，我们重复上述步骤，再随机的按某个特征维度的取值把数据进行细分，直到无法细分，即：只剩下一个数据点，或者剩下的数据全部相同。跟先前的例子类似，直观上，点B’跟其他数据点比较疏离，可能只需要很少的几次操作就可以将它细分出来；点A’需要的切分次数可能会更多一些。<br><img src="http://orsw4brg1.bkt.clouddn.com/iforest2.jpg" alt=""><br>按照先前提到的关于“异常”的两个假设，一般情况下，在上面的例子中，点B和点B’ 由于跟其他数据隔的比较远，会被认为是异常数据，而点A和点A’ 会被认为是正常数据。直观上，异常数据由于跟其他数据点较为疏离，可能需要较少几次切分就可以将它们单独划分出来，而正常数据恰恰相反。这其实正是 Isolation Forest（IF）的核心概念。IF采用二叉树去对数据进行切分，数据点在二叉树中所处的深度反应了该条数据的“疏离”程度。整个算法大致可以分为两步：</p>
<ul>
<li>训练：抽取多个样本，构建多棵二叉树（Isolation Tree，即 iTree）；</li>
<li>预测：综合多棵二叉树的结果，计算每个数据点的异常分值。</li>
</ul>
<blockquote>
<p>训练：</p>
</blockquote>
<p>构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件：</p>
<ul>
<li>1）数据不可再分，即：只包含一条数据，或者全部数据相同。</li>
<li>2）二叉树达到限定的最大深度。</li>
</ul>
<p><img src="http://orsw4brg1.bkt.clouddn.com/431880-20150905115956889-1354964997.jpg" alt=""></p>
<blockquote>
<p>预测：</p>
</blockquote>
<p>计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径长度（也可以叫深度）。具体的，先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size，则数据 x 在这棵 iTree 上的路径长度 h(x)，可以用下面这个公式计算：</p>
<script type="math/tex; mode=display">h(x)=e+C(T.size)</script><p>公式中，$e$表示数据$x$从$iTree$的根节点到叶节点过程中经过的边的数目，$C(T.size)$ 可以认为是一个修正值，它表示在一棵用 $T.size$ 条样本数据构建的二叉树的平均路径长度。一般的，$C(n)$的计算公式如下：</p>
<script type="math/tex; mode=display">C(n)=2H(n-1)-\frac{2(n-1)}{n}</script><p>其中，$H(n-1)$可用$ln(n-1)+0.5772156649$估算，这里的常数是欧拉常数。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果：</p>
<script type="math/tex; mode=display">Score(x)=2^{-\frac{E[h(x)]}{C(\psi)}}</script><p>公式中，$E(h(x)) $表示数据 x 在多棵 iTree 的路径长度的均值，$\psi$表示单棵 iTree 的训练样本的样本数，$C(\psi)$ 表示用$\psi$条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。</p>
<p>从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。</p>
<h2 id="二、算法特点"><a href="#二、算法特点" class="headerlink" title="二、算法特点"></a>二、算法特点</h2><p>在论文中，也比较了其它的常用异常挖掘的算法。比如常用的统计方法，基于分类的方法，和基于聚类的方法，这些传统算法通常是对正常的数据构建一个模型，然后把不符合这个模型的数据，认为是异常数据。而且，这些模型通常为正常数据作优化，而不是为异常数据作优化。而iForest可以显示地找出异常数据，而不用对正常的数据构建模型。</p>
<p>由于异常数据的两个特征（少且不同： few and different）：异常数据只占很少量;异常数据特征值和正常数据差别很大。</p>
<p>异常数据的这两个特征，确定了算法的理论基础。因此，构建二叉树型结构的时候，异常数据离根更近，而正常数据离根更远，更深。算法为了效率考虑，也限定了树的深度：ceil(log2(n))，这个值近似于树的平均深度，因为只需要关心那些低于平均高度的数据点，而不需要树进行完全生成。</p>
<p>算法只需要两个参数：树的多少与采样的多少。实验发现，在100颗树的时候，路径的长度就已经覆盖得比较好了，因此选100颗也就够了。采样，是为了更好的将正常数据和异常数据分离开来。有别于其它模型，采样数据越多，反面会降低iForest识别异常数据的能力。因为，通常使用256个样本，这也是scikit-learn实现时默认使用的采样数。</p>
<p>由于算法只需要采样数据256条样本，并且对树的深度也有限制，因此，算法对内存要求很低，且处理速度很快，其时间复杂度也是线性的。</p>
<p>不像其它算法，需要计算距离或者密度来寻找异常数据，iForest算法可以很好的处理高维数据和大数据，并且也可以作为在线预测。假设采样为256条，结点最大为511个，假设一个节点占b字节，共使用t颗树，那么需要的内存只有511tb字节，基本上只需要几M到几十M的内存就够了。数据还显示，预测287,748条数据只花了7.6秒。</p>
<p>另外，iForest既能发现群异常数据，也能发现散点异常数据。同时也能处理训练数据中不包含异常数据的情况。</p>
<h2 id="三、代码示例"><a href="#三、代码示例" class="headerlink" title="三、代码示例"></a>三、代码示例</h2> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"> print(__doc__)</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</div><div class="line"></div><div class="line">rng = np.random.RandomState(<span class="number">42</span>)</div><div class="line"></div><div class="line"><span class="comment"># Generate train data</span></div><div class="line">X = <span class="number">0.3</span> * rng.randn(<span class="number">100</span>, <span class="number">2</span>)</div><div class="line">X_train = np.r_[X + <span class="number">2</span>, X - <span class="number">2</span>]</div><div class="line"><span class="comment"># Generate some regular novel observations</span></div><div class="line">X = <span class="number">0.3</span> * rng.randn(<span class="number">20</span>, <span class="number">2</span>)</div><div class="line">X_test = np.r_[X + <span class="number">2</span>, X - <span class="number">2</span>]</div><div class="line"><span class="comment"># Generate some abnormal novel observations</span></div><div class="line">X_outliers = rng.uniform(low=<span class="number">-4</span>, high=<span class="number">4</span>, size=(<span class="number">20</span>, <span class="number">2</span>))</div><div class="line"></div><div class="line"><span class="comment"># fit the model</span></div><div class="line">clf = IsolationForest(max_samples=<span class="number">100</span>, random_state=rng)</div><div class="line">clf.fit(X_train)</div><div class="line">y_pred_train = clf.predict(X_train)</div><div class="line">y_pred_test = clf.predict(X_test)</div><div class="line">y_pred_outliers = clf.predict(X_outliers)</div><div class="line"></div><div class="line"><span class="comment"># plot the line, the samples, and the nearest vectors to the plane</span></div><div class="line">xx, yy = np.meshgrid(np.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">50</span>), np.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">50</span>))</div><div class="line">Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])</div><div class="line">Z = Z.reshape(xx.shape)</div><div class="line"></div><div class="line">plt.title(<span class="string">"IsolationForest"</span>)</div><div class="line">plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)</div><div class="line"></div><div class="line">b1 = plt.scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], c=<span class="string">'white'</span>,</div><div class="line">                 s=<span class="number">20</span>, edgecolor=<span class="string">'k'</span>)</div><div class="line">b2 = plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=<span class="string">'green'</span>,</div><div class="line">                 s=<span class="number">20</span>, edgecolor=<span class="string">'k'</span>)</div><div class="line">c = plt.scatter(X_outliers[:, <span class="number">0</span>], X_outliers[:, <span class="number">1</span>], c=<span class="string">'red'</span>,</div><div class="line">                s=<span class="number">20</span>, edgecolor=<span class="string">'k'</span>)</div><div class="line">plt.axis(<span class="string">'tight'</span>)</div><div class="line">plt.xlim((<span class="number">-5</span>, <span class="number">5</span>))</div><div class="line">plt.ylim((<span class="number">-5</span>, <span class="number">5</span>))</div><div class="line">plt.legend([b1, b2, c],</div><div class="line">           [<span class="string">"training observations"</span>,</div><div class="line">            <span class="string">"new regular observations"</span>, <span class="string">"new abnormal observations"</span>],</div><div class="line">           loc=<span class="string">"upper left"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p> <img src="http://orsw4brg1.bkt.clouddn.com/%E4%B8%8B111%E8%BD%BD.png" alt=""><br> 算法基本上不需要配置参数就可以直接使用，通常就以下几个(参数明显比随机森林简单)：</p>
<ul>
<li>n_estimators: 默认为100，配置iTree树的多少</li>
<li>max_samples: 默认为265，配置采样大小</li>
<li>max_features: 默认为全部特征，对高维数据，可以只选取部分特征</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> iForest </tag>
            
            <tag> 异常检测算法 </tag>
            
            <tag> 集成算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（26）：因子分解机（FM）与场感知分解机（FFM）]]></title>
      <url>/2017/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8826%EF%BC%89%EF%BC%9A%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FM%EF%BC%89%E4%B8%8E%E5%9C%BA%E6%84%9F%E7%9F%A5%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FFM%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>本文转载自<a href="https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团点评技术团队</a></p>
<p>FM和FFM模型是最近几年提出的模型，凭借其在数据量比较打并且特征稀疏的情况下，忍让能够得到优秀的性能和效果，屡次在各大公司举办的CTR预估比赛中获得不错的战绩。</p>
<a id="more"></a>
<p>在计算广告领域，点击率CTR（click-through rate）和转化率CVR（conversion rate）是衡量广告流量的两个关键指标。准确的估计CTR、CVR对于提高流量的价值，增加广告收入有重要的指导作用。预估CTR、CVR，业界常用的方法由人工特征工程+LR（Logistic Regression）、GBDT（Gradient Boosting Decision Tree）+LR、FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型。在这些模型中，FM和FFM近年来表现突出，分别在Criteo和Avazu举办的CTR预测竞赛中夺得冠军。</p>
<p>本文基于对FFM模型的深度调研和使用经验，从原理、实现和应用几个方面对FFM进行探讨，希望能够从原理上解释FFM模型在点击率预估上取得优秀效果的原因。因为FFM是在FM的基础上改进得来的，所以，我们首先引入FM模型。</p>
<h2 id="一、FM（因子分解机）"><a href="#一、FM（因子分解机）" class="headerlink" title="一、FM（因子分解机）"></a>一、FM（因子分解机）</h2><h3 id="1-1-FM的原理及推导"><a href="#1-1-FM的原理及推导" class="headerlink" title="1.1 FM的原理及推导"></a>1.1 FM的原理及推导</h3><p>因子分解机（Factorization Machine，简称FM），又称分解机。是由德国康斯坦茨大学的Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，先了解一下在实际场景中，稀疏数据是怎样产生的。</p>
<p>假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。元数据如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Clicked?</th>
<th>Country</th>
<th>Day</th>
<th>Ad_type</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>USA</td>
<td>26/11/15</td>
<td>Movie</td>
</tr>
<tr>
<td>0</td>
<td>China</td>
<td>1/7/14</td>
<td>Game</td>
</tr>
<tr>
<td>1</td>
<td>China</td>
<td>19/2/15</td>
<td>Game</td>
</tr>
</tbody>
</table>
</div>
<p>“Clicked？”是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。    </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Clicked?</th>
<th>Country=USA</th>
<th>Country=China</th>
<th>Day=26/11/15</th>
<th>Day=1/7/14</th>
<th>Day=19/2/15</th>
<th>Ad_type=Movie</th>
<th>Ad_type=Game</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>由上表可以看出，经过One-Hot编码之后，大部分样本数据特征是比较稀疏的。上面的样例中，每个样本有7维特征，但平均仅有3维特征具有非零值。实际上，这种情况并不是此例独有的，在真实应用场景中这种情况普遍存在。例如，CTR/CVR预测时，用户的性别、职业、教育水平、品类偏好、商品的品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的末级品类约有550个，采用One-Hot编码生成550个数值特征，但每个样本的这550个特征，有且仅有一个是有效的（非零）。由此可见，数据稀疏性是实际问题中不可避免的挑战。</p>
<p>One-Hot编码的另一个特点就是导致特征空间大。例如，商品品类有550维特征，一个categorical特征转换为550维数值特征，特征空间剧增。</p>
<p>同时通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。如：“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。</p>
<p>表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：</p>
<ul>
<li>1）人工特征工程（数据分析＋人工构造）；</li>
<li>2）通过模型做组合特征的学习（深度学习方法、FM/FFM方法）</li>
</ul>
<p>本章主要讨论FM和FFM用来学习特征之间的关联。多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征 $x_i$ 和 $x_j$ 的组合采用 $x_i$ 表示，即 $x_i$ 和 $x_j$ 都非零时，组合特征 $x_ix_j$ 才有意义。从对比的角度，本文只讨论二阶多项式模型。模型的表达式如下：</p>
<script type="math/tex; mode=display">y(x)=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nw_{ij}x_ix_j</script><p>其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值，$w_0、w_i、w_{ij}$是模型的参数。</p>
<p>从这个公式可以看出，组合特征的参数一共有$\frac{n(n-1)}{2}$个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，回归模型的参数$w$的学习结果就是从训练样本中计算充分统计量（凡是符合指数族分布的模型都具有此性质），而在这里交叉项的每一个参数$w_{ij}$的学习过程需要大量的$x_i$、$x_j$同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“$x_i$和$x_j$都非零”的样本数就会更少。训练样本不充分，学到的参数$w_{ij}$就不是充分统计量结果，导致参数$w_{ij}$不准确，而这会严重影响模型预测的效果（performance）和稳定性。</p>
<p>那么，如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路。在Model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量点积就是矩阵中user对item的打分。<img src="http://omu7tit09.bkt.clouddn.com/14999608253323.png" alt=""></p>
<p>类似地，所有二次项参数 $w_{ij}$可以组成一个对称阵 $W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为 $W=V^TV$，$V$ 的第$ j$列便是第 $j$ 维特征的隐向量。换句话说，每个参数 $w_{ij}=⟨v_i,v_j⟩$，这就是FM模型的核心思想。因此，FM的模型方程为（本文不讨论FM的高阶形式）</p>
<script type="math/tex; mode=display">y(x)=w_0+\sum _{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n⟨vi,vj⟩x_ix_j \  \  \ \   \  \ ···（2）</script><p>其中，$v_i$是第i维特征的隐向量，$⟨⋅,⋅⟩$代表向量点积，计算公式为</p>
<script type="math/tex; mode=display">⟨v_i,v_j⟩=\sum_{f=1}^kv_{i,f}·v_{j,f}</script><p>隐向量的长度为$k(k&lt;&lt;n)$，包含k个描述特征的因子。<br>具体解读一下这个公式</p>
<ul>
<li>线性模型+交叉项：直观地看FM模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。</li>
<li>交叉项系数 → 隐向量内积：由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数$w_{ij}$（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积$⟨v_i,v_j⟩$表示$w_{ij}$。具体的，FM求解过程中的做法是：对每一个特征分量$x_i$引入隐向量$v_i＝(v_{i,1},v_{i,2},⋯,v_{i,k})$，利用$v_iv^T_j$内积结果对交叉项的系数$w_{ij}$进行估计，公式表示：$ŵ_{ij}=v_iv^T_j$</li>
</ul>
<p>根据上式，二次项的参数数量减少为$kn$个，远少于多项式模型的参数数量。</p>
<p>此外，参数因子化表示后，使得$x_hx_i$的参数与$x_ix_j$的参数不再相互独立。这样我们就可以在样本系数的情况下相对合理地估计FM模型交叉项的参数。具体地：</p>
<script type="math/tex; mode=display">⟨v_h,v_i⟩=\sum_{f=1}^k v_{h,f}·v_{i,f}</script><script type="math/tex; mode=display">⟨v_i,v_j⟩=\sum_{f=1}^k v_{i,f}·v_{j,f}</script><p>$x_hx_i$与$x_ix_j$的系数分别为$⟨v_h,v_i⟩$和$⟨v_i,v_j⟩$，它们之间有共同项$v_i$，也就是说，所有包含$x_i$的非零组合特征（存在某个$j≠i$,使得$x_ix_j≠0$）的样本都可以用来学习隐向量$v_i$，这在很大程度上避免了数据系数行造成参数估计不准确的影响。而在多项式模型中，$w_{hi}$和$w_{ij}$是相互独立的。</p>
<p>显而易见，公式(2)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge、Cross-Entropy损失来求解分类问题。当然，在进行二元分类时，FM的输出需要经过Sigmoid变换，这与Logistic回归是一样的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>FM应用场景</th>
<th>损失函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>回归</td>
<td>均方误差（MSE）损失</td>
<td>Mean Square Error，与平方误差类似</td>
</tr>
<tr>
<td>二类分类</td>
<td>Hinge/Cross-Entopy损失</td>
<td>分类时，结果需要做sigmoid变换</td>
</tr>
<tr>
<td>排序</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>直观上看，FM的复杂度是$O(kn^2)$，但是，通过下面的等价转换，可以将FM的二次项化简，其复杂度可以优化到$O(kn)$，即：</p>
<script type="math/tex; mode=display">\sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_i,x_j=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2-\sum_{i=1}^nv_{i,f}^2x_i^2]</script><p>下面给出详细推导：</p>
<script type="math/tex; mode=display">\sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j \\ =\frac{1}{2}\sum_{i=1}^n\sum_{f=1}^n⟨v_i,v_j⟩x_ix_j-\frac{1}{2}\sum_{i=1}^n⟨v_i,v_i⟩x_ix_i\\=\frac{1}{2}(\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j-\sum_{i=1}^n\sum_{f=1}^kv_{i,f}v_{i,f}x_ix_i)\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)·(\sum_{j=1}^nv_{j,f}x_j)-\sum_{i=1}^nv_{i,f}^2x_i^2]\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2- \sum_{i=1}^nv_{i,f}^2x_i^2]</script><p>解读第一步到第二部，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角和上三角相等，有下式成立：</p>
<script type="math/tex; mode=display">A=\frac{1}{2}(2A+B)-\frac{1}{2}B</script><p>其中，</p>
<script type="math/tex; mode=display">A=\sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j,B=\sum_{i=1}^n⟨v_i,v_j⟩x_ix_i</script><p>如果用随机梯度下降（SGD）法学系模型参数。那么模型各个参数的梯度如下：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta}y\left(x\right)=\left\{\begin{array}{l}
    1,\ \ if\ \theta\ is\ w_0\left(\textrm{常数项}\right)\\
    x_i,\ if\ \theta\ is\ w_i\left(\textrm{线性项}\right)\\
    x_i\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j-v_{i,f}x_{i}^{2},\ if\ \theta\ is\ v_{i,f}\left(\textrm{交叉项}\right)\\
\end{array}\right.</script><p>其中，$v_{j,f}$是隐向量$v_j$的第f个元素。</p>
<p>由于$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$只与f有关，在参数迭代过程中，只需要计算第一次所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$，就能够方便地得到所有$v_{i,f}$的梯度。显然，计算所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$的复杂度是$O(kn)$；已知$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$时，计算每个参数梯度的复杂度是$O(n)$；得到梯度后，更新每个参数的复杂度是$O(1)$；模型参数一共有$nk+n+1$个。因此，FM参数训练的时间复杂度为$O(kn )$</p>
<h3 id="1-2-FM的优势"><a href="#1-2-FM的优势" class="headerlink" title="1.2 FM的优势"></a>1.2 FM的优势</h3><p>综上可知，FM算法可以再线性时间内完成模型训练，以及对新样本作出预测，所以说FM是一个非常高效的模型。FM模型的核心作用可以概括为以下三个：</p>
<ul>
<li>1）FM降低了交叉项参数学习不充分的影响：one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数$w_ij$用对应特征隐向量的内积表示，即$⟨v_i,v_j⟩$。这样参数学习由之前学习交叉项参数$w_{ij}$的过程，转变为学习$n$个单特征对应k维隐向量的过程。很明显，单特征参数（k维隐向量$v_i$）的学习要比交叉项参数$w_{ij}$学习的更加充分。示例说明：<br>假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>共现交叉特征</th>
<th>样本数</th>
<th>注</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt;女性，汽车&gt;</td>
<td>500</td>
<td>同时出现&lt;女性，汽车&gt;的样本数</td>
</tr>
<tr>
<td>&lt;女性，化妆品&gt;</td>
<td>1000</td>
<td>同时出现&lt;女性，化妆品&gt;的样本数</td>
</tr>
<tr>
<td>&lt;男性，汽车&gt;</td>
<td>1500</td>
<td>同时出现&lt;男性，汽车&gt;的样本数</td>
</tr>
<tr>
<td>&lt;男性，化妆品&gt;</td>
<td>0</td>
<td>样本中无此特征组合项</td>
</tr>
</tbody>
</table>
</div>
<p>&lt;女性，汽车&gt;的含义是女性看汽车广告。可以看到，但特征对应的样本数远大于组合特征对应的样本数。训练时，但特征参数相比交叉项特征参数会学习地更充分。因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。</p>
<ul>
<li><p>2）FM提升了模型预估能力。依然看上面的示例，样本中没有没有&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果yoga多项式模型来建模，对应的交叉项参数$w_{男性，化妆品}$是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。<br>FM模型是否能得到交叉项参数$w_{男性，化妆品}$呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为$w_{男性，化妆品}=<v_{男性}，v_{化妆品}>$，即用男性特征隐向量$v_{男性}$和化妆品特征隐向量$v_{化妆品}$的内积表示交叉项参数$w_{男性，化妆品}$由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用$<v_{男性}，v_{化妆品}>$得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估呢不给力。</v_{男性}，v_{化妆品}></v_{男性}，v_{化妆品}></p>
</li>
<li><p>3）FM提升了参数学习效率：这个显而易见，参数个数由$(n2+n+1)(n2+n+1)$变为$(nk+n+1)(nk+n+1)$个，模型训练复杂度也由$O(mn^2)$变为$O(mnk)$。mm为训练样本数。对于训练样本和特征数而言，都是线性复杂度。此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。</p>
<p>最后一句话总结，FM最大特点和优势：<strong>FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力</strong>。</p>
<p>与其他模型相比，它的优势如下：</p>
<ul>
<li>FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等；</li>
<li>相比SVM的二阶多项式核而言，FM在样本稀疏的情况下是有优势的；而且，FM的训练/预测复杂度是线性的，而二项多项式核SVM需要计算核矩阵，核矩阵复杂度就是N平方。</li>
<li>相比MF而言，我们把MF中每一项的rating分改写为 $r_{ui}∼β_u+γ_i+x^T_uy_i$，从公式(2)中可以看出，这相当于只有两类特征 $u$ 和$ i$ 的FM模型。对于FM而言，我们可以加任意多的特征，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM，在此不再赘述。</li>
</ul>
</li>
</ul>
<h2 id="二、FFM（场感知分解机器）"><a href="#二、FFM（场感知分解机器）" class="headerlink" title="二、FFM（场感知分解机器）"></a>二、FFM（场感知分解机器）</h2><h3 id="2-1-FFM的原理及推导"><a href="#2-1-FFM的原理及推导" class="headerlink" title="2.1 FFM的原理及推导"></a>2.1 FFM的原理及推导</h3><p>场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自Yu-Chin Juan(阮毓钦，毕业于中国台湾大学，现在美国Criteo工作)与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。通过引入field的概念，FFM把相同性质的特征归于同一个field。以上面的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。在FFM中，每一维特征 $x_i$，针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_{i,f_j}$。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 nn 个特征属于 ff 个field，那么FFM的二次项有 nfnf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。</p>
<script type="math/tex; mode=display">y(x)=w_0+∑_{i=1}^nw_ix_i+∑_{i=1}^n∑_{j=i+1}^n⟨v_{i,fj},v_{j,f_i}⟩x_ix_j</script><p>其中，$f_j$是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二次参数有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其复杂度为$O(kn^2)$。</p>
<p>下面以一个例子简单说明FFM的特征组合方式。输入记录如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>User</th>
<th>Movie</th>
<th><span class="Apple-tab-span" style="white-space:pre"></span>Genre</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>YuChin</td>
<td><span class="Apple-tab-span" style="white-space:pre"></span>3Idiots</td>
<td><span class="Apple-tab-span" style="white-space:pre"></span>Comedy, Drama</td>
<td>$9.99</td>
</tr>
</tbody>
</table>
</div>
<p>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Field name</th>
<th>Field index</th>
<th>Feature name</th>
<th>Feature index</th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>1</td>
<td><span class="Apple-tab-span" style="white-space:pre"></span>User=YuChin</td>
<td>1</td>
</tr>
<tr>
<td>Movie</td>
<td>2</td>
<td>Movie=3Idiots</td>
<td>2</td>
</tr>
<tr>
<td>Genre</td>
<td>3</td>
<td>Genre=Comedy</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Genre=Drama</td>
<td>4</td>
</tr>
<tr>
<td>Price</td>
<td>4</td>
<td>Price</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>那么，FFM的组合特征有10项，如下图所示。<img src="http://omu7tit09.bkt.clouddn.com/15000038125272.png" alt=""></p>
<p>其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有$C^2_n=\frac{n(n−1)}{2}$个。</p>
<h3 id="2-2-FFM的应用"><a href="#2-2-FFM的应用" class="headerlink" title="2.2 FFM的应用"></a>2.2 FFM的应用</h3><p>在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。</p>
<p>CTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：用户相关的特征、商品相关的特征、以及用户-商品匹配特征。用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。</p>
<p>为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。</p>
<p>CTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><span class="Apple-tab-span" style="white-space:pre"></span>#(field)</th>
<th>#(feature)</th>
<th>AUC</th>
<th>Logloss</th>
</tr>
</thead>
<tbody>
<tr>
<td>站内CTR</td>
<td>39</td>
<td>2456</td>
<td>0.77</td>
<td>0.38</td>
</tr>
<tr>
<td>站内CVR</td>
<td>67</td>
<td>2441</td>
<td>0.92</td>
<td>0.13</td>
</tr>
</tbody>
</table>
</div>
<p>由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。</p>
<p>在训练FFM的过程中，有许多小细节值得特别关注。</p>
<p>第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.normpa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。</p>
<p>第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1][0,1] 是非常必要的。</p>
<p>第三，省略零值特征。从FFM模型的表达式可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p>
<h3 id="2-3-FFM实现"><a href="#2-3-FFM实现" class="headerlink" title="2.3 FFM实现"></a>2.3 FFM实现</h3><p>Yu-Chin Juan实现了一个C++版的FFM模型，源码可从Github下载[10]。这个版本的FFM省略了常数项和一次项，模型方程如下。</p>
<script type="math/tex; mode=display">ϕ(w,x)=∑_{j1,j2∈C_2}⟨w_{j_1,f_2},w_{j_2,f_1}⟩x_{j_1}x_{j_2}</script><p>其中，$C_2$是非零特征的二元组合，$j_1$是特征，属于field $f_1$，$w_{j_1,f_2}$是特征 $j_1$对field $f_2$ 的隐向量。此FFM模型采用logistic loss作为损失函数，和L2惩罚项，因此只能用于二元分类问题。</p>
<script type="math/tex; mode=display">\underset{w}{min}∑_{i=1}^Llog(1+exp{−y_iϕ(w,x_i)})+\frac{λ}{2}‖w‖2</script><p>其中，$y_i∈{−1,1}$是第 i个样本的label，L是训练样本数量，λ 是惩罚项系数。模型采用SGD优化，优化流程如下。<img src="http://omu7tit09.bkt.clouddn.com/15000056384443.png" alt=""><br>参考 Algorithm1, 下面简单解释一下FFM的SGD优化过程。<br>算法的输入 tr、va、pa 分别是训练样本集、验证样本集和训练参数设置。</p>
<ol>
<li>根据样本特征数量（tr.ntr.n）、field的个数（tr.mtr.m）和训练参数（papa），生成初始化模型，即随机生成模型的参数；</li>
<li>如果归一化参数 pa.normpa.norm 为真，计算训练和验证样本的归一化系数，样本i的归一化系数为<script type="math/tex; mode=display">R[i]=\frac{1}{||X[i]||}</script></li>
<li>对每一轮迭代，如果随机更新参数 pa.randpa.rand 为真，随机打乱训练样本的顺序；</li>
<li>对每一个训练样本，执行如下操作:<ul>
<li>计算每一个样本的FFM项，即公式中的输出 $ϕ$；</li>
<li>计算每一个样本的训练误差，如算法所示，这里采用的是交叉熵损失函数 $log(1+eϕ)$；  </li>
<li>利用单个样本的损失函数计算梯度 $gΦ$，再根据梯度更新模型参数；</li>
</ul>
</li>
</ol>
<ul>
<li><ol>
<li>对每一个验证样本，计算样本的FFM输出，计算验证误差；</li>
</ol>
</li>
<li><ol>
<li>重复步骤3~5，直到迭代结束或验证误差达到最小。</li>
</ol>
</li>
</ul>
<p>在SGD寻优时，代码采用了一些小技巧，对于提升计算效率是非常有效的。</p>
<p>第一，梯度分步计算。采用SGD训练FFM模型时，只采用单个样本的损失函数来计算模型参数的梯度。</p>
<script type="math/tex; mode=display">L=L_{err}+L_{reg}=log(1+exp\{−y_iϕ(w,x_i)\})+\frac{λ}{2}‖w‖^2</script><script type="math/tex; mode=display">\frac{∂L}{∂w}=\frac{∂L_{err}}{∂ϕ}\frac{∂ϕ}{∂w}+\frac{∂L_{reg}}{∂w}</script><p>上面的公式表明，$\frac{∂L_{err}}{∂ϕ}$与具体的模型参数无关。因此，每次更新模型时，只需计算一次，之后直接调用$\frac{∂L_{err}}{∂ϕ}$的值即可。对于更新 $nfk$个模型参数，这种方式能够极大提升运算效率。</p>
<p>第二，自适应学习率。此版本的FFM实现没有采用常用的指数递减的学习率更新策略，而是利用 $nfk$ 个浮点数的临时空间，自适应地更新学习率。学习率是参考AdaGrad算法计算的[11]，按如下方式更新</p>
<script type="math/tex; mode=display">w_{j_1,j_2}^ {'}=w_{j_1,f_2}-\frac{η}{\sqrt{1+\sum_t(g^t_{w_{j_1,f_2}})^2}}·g_{w_{j_1,f_2}}</script><p>其中，$w_{j_1,f_2}$是特征 $j_1$ 对field $f_2$ 隐向量的一个元素，元素下标未标出；$g_{w_{j_1,f_2}}$是损失函数对参数 $w_{j_1,f_2}$的梯度；$g^t_{w_{j_1,f_2}}$是第 t 次迭代的梯度；η是初始学习率。可以看出，随着迭代的进行，每个参数的历史梯度会慢慢累加，导致每个参数的学习率逐渐减小。另外，每个参数的学习率更新速度是不同的，与其历史梯度有关，根据AdaGrad的特点，对于样本比较稀疏的特征，学习率高于样本比较密集的特征，因此每个参数既可以比较快速达到最优，也不会导致验证误差出现很大的震荡。</p>
<p>第三，OpenMP多核并行计算。OpenMP是用于共享内存并行系统的多处理器程序设计的编译方案，便于移植和多核扩展[12]。FFM的源码采用了OpenMP的API，对参数训练过程SGD进行了多线程扩展，支持多线程编译。因此，OpenMP技术极大地提高了FFM的训练效率和多核CPU的利用率。在训练模型时，输入的训练参数ns_threads指定了线程数量，一般设定为CPU的核心数，便于完全利用CPU资源。</p>
<p>第四，SSE3指令并行编程。SSE3全称为数据流单指令多数据扩展指令集3，是CPU对数据层并行的关键指令，主要用于多媒体和游戏的应用程序中。SSE3指令采用128位的寄存器，同时操作4个单精度浮点数或整数。SSE3指令的功能非常类似于向量运算。例如，a 和 b 采用SSE3指令相加（a 和 b 分别包含4个数据），其功能是 a 中的4个元素与 b 中4个元素对应相加，得到4个相加后的值。采用SSE3指令后，向量运算的速度更加快捷，这对包含大量向量运算的FFM模型是非常有利的。</p>
<p>除了上面的技巧之外，FFM的实现中还有很多调优技巧需要探索。例如，代码是按field和特征的编号申请参数空间的，如果选取了非连续或过大的编号，就会造成大量的内存浪费；在每个样本中加入值为1的新特征，相当于引入了因子化的一次项，避免了缺少一次项带来的模型偏差等。</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>本文主要介绍了FFM的思路来源和理论原理，并结合源码说明FFM的实际应用和一些小细节。从理论上分析，FFM的参数因子化方式具有一些显著的优势，特别适合处理样本稀疏性问题，且确保了较好的性能；从应用结果来看，站内CTR/CVR预估采用FFM是非常合理的，各项指标都说明了FFM在点击率预估方面的卓越表现。当然，FFM不一定适用于所有场景且具有超越其他模型的性能，合适的应用场景才能成就FFM的“威名”。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="http://blog.csdn.net/lilyth_lilyth/article/details/48032119" target="_blank" rel="noopener">http://blog.csdn.net/lilyth_lilyth/article/details/48032119</a></li>
<li><a href="http://www.cnblogs.com/Matrix_Yao/p/4773221.html" target="_blank" rel="noopener">http://www.cnblogs.com/Matrix_Yao/p/4773221.html</a></li>
<li><a href="http://www.herbrich.me/papers/adclicksfacebook.pdf" target="_blank" rel="noopener">http://www.herbrich.me/papers/adclicksfacebook.pdf</a></li>
<li><a href="https://www.kaggle.com/c/criteo-display-ad-challenge" target="_blank" rel="noopener">https://www.kaggle.com/c/criteo-display-ad-challenge</a></li>
<li><a href="https://www.kaggle.com/c/avazu-ctr-prediction" target="_blank" rel="noopener">https://www.kaggle.com/c/avazu-ctr-prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Demand-side_platform" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Demand-side_platform</a></li>
<li><a href="http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf" target="_blank" rel="noopener">http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf</a></li>
<li><a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf</a></li>
<li><a href="http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf" target="_blank" rel="noopener">http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf</a></li>
<li><a href="https://github.com/guestwalk/libffm" target="_blank" rel="noopener">https://github.com/guestwalk/libffm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad</a></li>
<li><a href="http://openmp.org/wp/openmp-specifications/" target="_blank" rel="noopener">http://openmp.org/wp/openmp-specifications/</a></li>
<li><a href="http://blog.csdn.net/gengshenghong/article/details/7008704" target="_blank" rel="noopener">http://blog.csdn.net/gengshenghong/article/details/7008704</a></li>
<li><a href="https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf" target="_blank" rel="noopener">https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> FM </tag>
            
            <tag> FFM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法]]></title>
      <url>/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="一、最速下降法"><a href="#一、最速下降法" class="headerlink" title="一、最速下降法"></a>一、最速下降法</h2><p>最速下降法，又称为梯度下降法，是无约束最优化领域中最简单的算法，单独就这种算法来看，属于早就“过时”了的一种算法。但是，它的理念是其他某些算法的组成部分，或者说是在其他算法中，也有最速下降法的影子。它是一种迭代算法，每一步需要求解目标函数的梯度向量。</p>
<a id="more"></a>
<p>假设$f(x)$是$R^n$上具有一阶连续偏导的函数。要求解的无约束最优化问题是：</p>
<script type="math/tex; mode=display">\underset {x\in R^n}{min} \ f(x)</script><p>梯度下降法是一种迭代算法。选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。</p>
<p>由于$f(x)$具有一阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开：</p>
<script type="math/tex; mode=display">f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})</script><p>这里，$g_k=g(x^{(k)})=∇f(x^{(k)})$为$f(x)$在$x^{(k)}$的梯度。</p>
<p>求出第$k+1$次迭代值$x^{(k+1)}$:</p>
<script type="math/tex; mode=display">x^{(k+1)}\leftarrow x^{(k)}+\lambda_kp_k</script><p>其中$p_k$是搜索方向，取负梯度方向$p_k=-∇f(x^{(k)})$,$\lambda _k$是步长，由一维搜索确定，即$\lambda_k$使得</p>
<script type="math/tex; mode=display">f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k)</script><p>算法步骤如下：</p>
<p>输入：目标函数$f(x)$，梯度函数$g(x)=∇f(x)$，计算精度$\xi$;</p>
<p>输出：$f(x)$的极小点$x^*$</p>
<ol>
<li>取初始值$x^{(0)}\in R^{n}$，置$k=0$</li>
<li>计算 $f(x^{(k)})$</li>
<li>计算梯度$g_k=g(x^{(k)})$，当$||g_k||＜\xi$时，停止迭代，令$x^*=x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\lambda_k$，使<script type="math/tex; mode=display">f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k)</script></li>
<li>置$x^{(k+1 )}=x^{(k)}+\lambda _kp_k$，计算$f(x^{(k+1)})$当$||f(x^{(k+1)})-f(x^{(k)})||＜\xi$或$||x^{(k=1)}-x^{(k)}||＜\xi$，停止迭代，令$x^*=x^{(k+1)}$</li>
<li>否则，置$k=k+1$，转到步骤3。</li>
</ol>
<p>当目标函数是凸函数时，梯度下降法的解释全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。</p>
<h2 id="二、牛顿法"><a href="#二、牛顿法" class="headerlink" title="二、牛顿法"></a>二、牛顿法</h2><p>考虑如下无约束的极小化问题</p>
<script type="math/tex; mode=display">\underset{X}{min}\ f(x)</script><p>其中$X=(x_1,x_2,x_3，···，x_N)^T \in R^N$这里我们假定$f$为凸函数，且两阶连续可微。记$x^*$为目标函数的极小值。</p>
<p>为了简单起见，首先考虑$N=1$的简单情形（此时目标函数$f(X)$变为$f(x)$）。牛顿法的基本思想是：在现有极小值估计值的附近对$f(x)$作二阶泰勒展开，进而找极小点的下一个估计值。设$x_k$为当前的极小点估计值，则</p>
<script type="math/tex; mode=display">f(x) = f(x_k)+ f^{'}(x_k)(x-x_k)+\frac{1}{2}f^{''}(x_k)(x-x_k)^2</script><p>表示$f(x)$在$x_k$附近的二阶泰勒展开式（略去了关于$x-x_k$的高阶项）。由于求得是最值，由极值必要条件可知，$f(x)$应该满足<script type="math/tex">f^{'}(x)=0</script>，即</p>
<script type="math/tex; mode=display">f^{'}(x_k)+f^{''}(x_k)(x-x_k)^2</script><p>从而求得</p>
<script type="math/tex; mode=display">x=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}</script><p>于是，若给定初始值$x_0$，则可以构造如下的迭代格式</p>
<script type="math/tex; mode=display">x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}</script><p>于是，若给定初始值$x_0$，则可以构造如下的迭代格式</p>
<script type="math/tex; mode=display">x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}} , \  k=0,1,···</script><p>产生序列$\{x_k\}$来逼近$f(x)$的极小点。在一定条件下$\{x_k\}$可以收敛到$f(x)$的极小点。</p>
<p>对于$N&gt;1$的情形，二阶泰勒展开式可以做推广，此时</p>
<script type="math/tex; mode=display">f(X)=f(X_k)+∇f(X_k)\ ·\ (X-X_k)+\frac{1}{2}· (X-X_k)^T·∇^2f(X_k)·(X-X_k)</script><p>其中$∇f$为$f$的梯度向量，$∇^2f$为海森矩阵，其定义分别为</p>
<script type="math/tex; mode=display">
\nabla f=\left[\begin{array}{c}
    \frac{\partial f}{\partial x_1}\\
    \frac{\partial f}{\partial x_2}\\
    ···\\
    \frac{\partial f}{\partial x_N}\\
\end{array}\right],\\nabla f^2=\left[\begin{matrix}
    \frac{\partial^2f}{\partial x_{1}^{2}}&        \frac{\partial^2f}{\partial x_1\partial x_2}&        ···&        \frac{\partial^2f}{\partial x_1\partial x_N}\\
    \frac{\partial^2f}{\partial x_2\partial x_1}&        \frac{\partial^2f}{\partial x_{2}^{2}}&        ···&        \frac{\partial^2f}{\partial x_2\partial x_N}\\
    ···&        ···&        ···&        ···\\
    \frac{\partial^2f}{\partial x_N\partial x_1}&        \frac{\partial^2f}{\partial x_N\partial x_2}&        ···&        \frac{\partial^2f}{\partial x_{N}^{2}}\\
\end{matrix}\right]</script><p>注意，$∇f$和$∇^2f$中的元素均为关于$X$的函数，以下分别将其简记为$g$和$H$。特别地，若$f$的混合偏导数可交换次序(即对$\forall\ i,j$，成立$\frac{\partial^2f}{\partial x_i\partial x_j}=\frac{\partial^2f}{\partial x_j\partial x_i}$)，则海森矩阵$H$为对称矩阵，而$∇f(X_k)$和$∇^2f(X_k )$则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$g_k$和$H_k$（这里字母g表示gradient，H表示Hessian）</p>
<p>同样地，由于是求极小点，极值必要条件要求它为$f(X)$的驻点，即</p>
<script type="math/tex; mode=display">∇f(X)=0</script><p>亦即对二阶泰勒展开作用一个梯度算子</p>
<script type="math/tex; mode=display">g_k+H_k·(X-X_k)=0</script><p>进一步，若矩阵$H_k$非奇异，则可解得</p>
<script type="math/tex; mode=display">X=X_k-H_k^{-1}·g_k</script><p>于是，若给定初始值$X_0$，则同样可以构造出迭代格式</p>
<script type="math/tex; mode=display">X_{k+1}=X_k-H^{-1}_k·g_k</script><p>这就是原始的牛顿迭代法，其迭代格式中的搜索方向$d_k=-H^{-1}_k·g_k$称为牛顿方向。下面给出牛顿法的完整算法描述：</p>
<ol>
<li>给定初值$X_0$和精度阀值$\xi$，并令$k:=0$</li>
<li>计算$g_k$和$H_k$</li>
<li>若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$</li>
<li>计算新的迭代点$X_{k+1}:=X_k+d_k$</li>
<li>令k:=k+1，转至步2</li>
</ol>
<p>当目标函数是二次函数时，由于二次泰勒展开函数与原目标函数不是近似而是完全相同的二次式，海森矩阵退化成一个常数矩阵，从任一初始点出发，秩序一步迭代即可达到$f(X)$的极小点$X^*$，因此牛顿法是一种具有二次收敛性的算法。对于非二次函数，若函数的二次形性态较强，或迭代点已进入极小点的领域，则其收敛速度也是很快的，这是牛顿法的主要优点。</p>
<p>但原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$f(X_{k=1})&gt;f(X_k )$的情况，这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$\{X_k\}$的发散而导致计算失败。</p>
<p>为了消除这个弊病，人们提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍然采用$d_k$，但每次迭代需沿此方向作一维搜索（line search），寻求最优的步长因子$\lambda _k $，即</p>
<script type="math/tex; mode=display">\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)</script><p>下面给出阻尼牛顿法的完整算法描述：</p>
<ol>
<li>给定初值$X_0$和精度阀值$\xi$，并令$k:=0$</li>
<li>计算$g_k$和$H_k$</li>
<li>若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$</li>
<li>利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，计算新的迭代点$X_{k+1}:=X_k+d_k$</li>
<li>令k:=k+1，转至步2</li>
</ol>
<p>至此完成了牛顿法的算法介绍，接下来对其做个小结：</p>
<p>牛顿法是梯度下降法的进一步发展，梯度下降法利用目标函数的一阶偏导数信息、以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向加快收敛，它具二阶收敛速度。但牛顿法主要存在以下两个缺点：</p>
<ol>
<li>对目标函数有较严格的要求。函数必须具有连续的一、二阶偏导数，海森矩阵必须正定。</li>
<li>极端相当复杂，除需要计算梯度以外，还需要计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$N$的平方比增加，当$N$很大时这个问题更加突出。</li>
</ol>
<h2 id="三、拟牛顿法"><a href="#三、拟牛顿法" class="headerlink" title="三、拟牛顿法"></a>三、拟牛顿法</h2><p>牛顿法虽然收敛速度快，但是计算过程中需要计算目标函数的二阶偏导数，计算复杂度较大。而且有时目标函数的海森矩阵无法保持正定，从而使牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海森矩阵或者海森矩阵的逆的正定对称阵，在拟牛顿的条件下优化目标函数。不同的构造方法就产生了不同的拟牛顿法。</p>
<p>也有人把“拟牛顿法”翻译成“准牛顿法”，其实都是表示“类似于牛顿法”的意思，因此只是对算法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。</p>
<p>在介绍具体的拟牛顿法之前，我们先推到一个拟牛顿条件，或者叫拟牛顿方程，还有的叫做割线条件。因为对海森矩阵（或海森矩阵的逆）做近似总不能随便近似，也需要理论指导，而拟牛顿条件则是用来提供理论指导的，它指出了用来近似的矩阵应该满足的条件。</p>
<p>为明确起见，下文中用$B$表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$H^{-1}$的近似，即$B≈H,D≈H^{-1}$</p>
<h3 id="3-1-拟牛顿条件"><a href="#3-1-拟牛顿条件" class="headerlink" title="3.1 拟牛顿条件"></a>3.1 拟牛顿条件</h3><p>设经过$k+1$次迭代后得到$X_{k+1}$，此时将目标函数$f(X)$在$X_{k+1}$附近作泰勒展开，取二阶近似，得到</p>
<script type="math/tex; mode=display">f(X)≈ f(X_{k+1})+∇f(X_{k+1})\ ·\ (X-X_{k+1})+\frac{1}{2}· (X-X_{k+1})^T·∇^2f(X_{k+1})·(X-X_{k+1})</script><p>在两边同时作用一个梯度算子$∇$，可得</p>
<script type="math/tex; mode=display">∇f(X)≈∇f（X_{k+1}）+H_{k+1}·(X-X_{k+ 1})</script><p>取$X=X_k$并整理，可得</p>
<script type="math/tex; mode=display">g_{k+1}-g_k≈H_{k+1}·(X_{k+1}-X_k)</script><p>若引入记号$s_k=X_{k+1}， y_k=g_{k+1}-g_k$则可以改写成</p>
<script type="math/tex; mode=display">y_k≈H_{k+1}·s_k</script><p>或者</p>
<script type="math/tex; mode=display">s_k≈H^{-1}_{k+1}·y_k</script><p>这就是所谓的拟牛顿条件，它对迭代过程中的海森矩阵$H_{k+1}$作约束，因此，对$H_{k+1}$做近似的$B_{k+1}$，以及对$H_{k+1}^{-1}$做近似的$D_{k+1 }$可以将</p>
<script type="math/tex; mode=display">y_k≈H_{k+1}·s_k</script><p>或者</p>
<script type="math/tex; mode=display">s_k≈H^{-1}_{k+1}·y_k</script><p>作为指导。</p>
<h3 id="3-2-DFP算法"><a href="#3-2-DFP算法" class="headerlink" title="3.2 DFP算法"></a>3.2 DFP算法</h3><p>DFP算法是以William C.Davidon、Roger Fletcher、Michael J.D.Powell三个人的名字的首字母命名的，它由Davidon于1959年首先提出，是最早的拟牛顿法。该算法的核心是：通过迭代的方法，对$H_{k+1}^{-1}$做近似，迭代格式为</p>
<script type="math/tex; mode=display">D_{k+1}=D_k+\Delta D_k , k=0,1,2,···</script><p>其中的$D_0$通常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta D_k$如何构造。</p>
<p>注意，我们猜想$\Delta D_k$可能与$s_k,y_k$和$D_k$发生关联。这里，我们采用“待定法”，即首先将$\Delta D_k$待定城某种形式，然后结合拟牛顿条件来进行推导。</p>
<p>那将$\Delta D_k$待定成什么形式呢？说起来比较tricky，我们将其待定为</p>
<script type="math/tex; mode=display">\Delta D_k=\alpha uu^T+\beta vv^T</script><p>其中$\alpha$和$\beta$为待定向量。从形式上看，这种待定公式至少保证了矩阵$\Delta D_k$的对称性（因为$uu^T$和$vv^T$均为对称矩阵）</p>
<p>将其代入迭代式，并结合拟牛顿指导条件，可得</p>
<script type="math/tex; mode=display">s_k=D_ky_k+\alpha uu^Ty_k+\beta vv^Ty_k</script><p>将其改写一下</p>
<script type="math/tex; mode=display">s_k=D_ky_k+u(\alpha u^Ty_k)+v(\beta v^Ty_k)\\=D_ky_k+(\alpha u^Ty_k)u+(\beta v^Ty_k)v</script><p>括号中为两个数，既然是数，我们不妨作如下简单赋值</p>
<script type="math/tex; mode=display">\alpha u^Ty_k=1 ，\  \beta v^Ty_k=-1$$即$$\alpha=\frac{1}{u^Ty_k},\beta=-\frac{1}{v^Ty_k}</script><p>其中向量$u,v$仍有待确定。</p>
<p>我们把$s_k=D_ky_k+u-v$写作</p>
<script type="math/tex; mode=display">u-v=s_k-D_ky_k</script><p>要上式成立，不妨直接取</p>
<script type="math/tex; mode=display">u=s_k,v=D_ky_k</script><p>代入求$\alpha$和$\beta$的式子，便得到</p>
<script type="math/tex; mode=display">\alpha=\frac{1}{s^T_ky_k},\beta=\frac{1}{(D_ky_k)^Ty_k}=-\frac{1}{y^T_kD_ky_k}</script><p>其中第二个式子用到了$D_k$的对称性。至此，我们已经将校正矩阵$\Delta D_k$构造出来了，我们就可以得到</p>
<script type="math/tex; mode=display">\Delta D_k=\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}</script><p>综上，我们给出DFP算法的一个完整的算法描述。</p>
<ol>
<li>给定初值$X_0$和精度阀值$\xi$，并令$k:=0$</li>
<li>确定搜索方向$d_k=-D^{-1}_k·g_k$</li>
<li>利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$</li>
<li>若$||g_{k=1}||&lt;\xi$，则算法结束</li>
<li>计算$y_k=g_{k+1}-g_k$</li>
<li>计算<script type="math/tex; mode=display">D_{k+1}=D_k+\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}</script></li>
<li>令$k:=k+1$转至步骤2.</li>
</ol>
<h3 id="3-3-BFGS算法"><a href="#3-3-BFGS算法" class="headerlink" title="3.3 BFGS算法"></a>3.3 BFGS算法</h3><p>BFGS算法是以其发明者Broyden、Fletcher、Goldfarb和Shanno四个人的名字的首字母命名的。与DFP算法相比，BFGS算法性能更加。目前它已成为求解无约束非线性优化问题最常用的方法之一。BFGS算法已有较完善的局部收敛理论，对其全局收敛的研究也取得了重要成果。</p>
<p>BFGS算法中核心公式的推导过程和DFP完全类似，只是互换了其中$s_k$和$y_k$的位置。需要注意的是，BFGS算法是直接逼近海森矩阵，即$B_k≈H_k$,仍采用迭代方法，设迭代格式为</p>
<script type="math/tex; mode=display">B_{k+1}=B_k+\Delta B_k , k=0,1,2,···</script><p>其中的$B_0$也常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta B_k$如何构造，同样，将其待定为</p>
<script type="math/tex; mode=display">\Delta B_k=\alpha uu^T+\beta vv^T</script><p>将其代入上式，并结合指导条件$y_k≈H_{k+1}·s_k$，可得</p>
<script type="math/tex; mode=display">y_k=B_ks_k+(au^Ts_k)u+(\beta v^Ts_k)v</script><p>通过令$au^Ts_k=1,\beta v^Ts_k=-1$,以及</p>
<script type="math/tex; mode=display">u=y_k,v=B_ks_k$$可以算得
$$\alpha=\frac{1}{y^T_ks_k},\beta = -\frac{1}{s^T_kB_ks_k}</script><p>综上，便得到了如下的校正矩阵$\Delta B_k$的公式</p>
<script type="math/tex; mode=display">\Delta B_k=\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k}</script><p>好了，现在把矩阵$\Delta B_k$和$\Delta D_k$拿出来对比一下，除了你将$D$换成$B$外，就是把$s_k$和$y_k$互换了一下位置。</p>
<p>最后，给出BFGS算法的一个完整算法描述：</p>
<ol>
<li>给定初值$X_0$和精度阀值$\xi$，并令$k:=0$</li>
<li>确定搜索方向$d_k=-B^{-1}_k·g_k$</li>
<li>利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$</li>
<li>若$||g_{k=1}||&lt;\xi$，则算法结束</li>
<li>计算$y_k=g_{k+1}-g_k$</li>
<li>计算<script type="math/tex; mode=display">B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k}</script></li>
<li>令$k:=k+1$转至步骤2.</li>
</ol>
<h3 id="3-4-L-BFGS算法"><a href="#3-4-L-BFGS算法" class="headerlink" title="3.4 L-BFGS算法"></a>3.4 L-BFGS算法</h3>]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 损失函数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（24）：机器学习中的损失函数]]></title>
      <url>/2017/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8824%EF%BC%89%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>损失函数（loss function）是用来估量模型的预测值f(x)与真实值$Y$不一致的程度，它是一个非负实数值函数，通常使用$L(Y,f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子：</p>
<script type="math/tex; mode=display">\theta^* = argmin_\theta \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i;\theta))+\lambda Φ(θ)</script><a id="more"></a>
<p>前面的均值函数表示的是经验风险函数，$L$代表的是损失函数，后面的$Φ$是正则化项（regularizer）或者叫惩罚项（penalty term）,它可以是$L_1$，也可以是$L_2$等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的$\theta$值。下面列出集中常见的损失函数。</p>
<h2 id="一、对数损失函数（逻辑回归）"><a href="#一、对数损失函数（逻辑回归）" class="headerlink" title="一、对数损失函数（逻辑回归）"></a>一、对数损失函数（逻辑回归）</h2><p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即$max F(y, f(x)) —&gt; min -F(y, f(x))$)。从损失函数的视角来看，它就成了log损失函数了。</p>
<p>Log损失函数的标准形式：</p>
<script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script><p>刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数$L(Y.P(Y|X))$表达的是样本在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以$logP(Y|X)$也会达到最大值，因此在前面加上负号之后，最大化$P(Y|X)$就等价于最小化$L$了。</p>
<p>logistic回归的$P(y|x)$表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：</p>
<script type="math/tex; mode=display">
P\left(Y=y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\left\{\begin{array}{l}
    h_{\theta}\left(x^{\left(i\right)}\right)=\frac{1}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=1\\
    1-h_{\theta}\left(x^{\left(i\right)}\right)=\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=0\\
\end{array}\right.</script><p>将上面的公式合并在一起，可得到第$i$个样本正确预测的概率：</p>
<script type="math/tex; mode=display">P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)}))^{y(i)}·(1-h_\theta(x^{(i)}))^{1-y(i)}</script><p>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为：</p>
<script type="math/tex; mode=display">
P\left(Y\ | \ X;\theta\right)=\prod_{i=1}^N{\left(\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}\right)}</script><p>将上式代入到对数损失函数中，得到最终的损失函数为：</p>
<script type="math/tex; mode=display">J(\theta) = -\frac{1}{N}\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}</script><p>之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。</p>
<h2 id="二、平方损失函数（最小二乘法，Ordinary-Least-Squares）"><a href="#二、平方损失函数（最小二乘法，Ordinary-Least-Squares）" class="headerlink" title="二、平方损失函数（最小二乘法，Ordinary Least Squares）"></a>二、平方损失函数（最小二乘法，Ordinary Least Squares）</h2><p>最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便；</li>
<li>欧氏距离是一种很好的相似性度量标准；</li>
<li>在不同的表示域变换后特征性质不变。</li>
</ul>
<p>平方损失（Square loss）的标准形式如下：<script type="math/tex">L(Y,f(X))=(Y-f(x))^2</script>当样本个数为n时，此时的损失函数变为：$$L(Y,f(X))=\sum_{i=1}^n(Y-f(X))^2$$$Y-f(X)$表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p>而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：</p>
<script type="math/tex; mode=display">MSE=\frac{1}{N}\sum_{i=1}^N(\tilde{Y_i}-Y_i)^2</script><p>上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。</p>
<h2 id="三、指数损失函数（Adaboost）"><a href="#三、指数损失函数（Adaboost）" class="headerlink" title="三、指数损失函数（Adaboost）"></a>三、指数损失函数（Adaboost）</h2><p>学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到$f_m(x)$:</p>
<script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+a_mG_m(x)</script><p>Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数$a$和G：</p>
<script type="math/tex; mode=display">arg\underset{a,G}{min}=\sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+aG(x_i))]</script><p>而指数损失函数(exp-loss）的标准形式如下:</p>
<script type="math/tex; mode=display">L(y,f(x))=exp[-yf(x)]</script><p>可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为：</p>
<script type="math/tex; mode=display">L(y,f(x))=\frac{1}{N}\sum_{i=1}^nexp[-y_if(x_i)]</script><h2 id="四、Hinge损失函数（SVM）"><a href="#四、Hinge损失函数（SVM）" class="headerlink" title="四、Hinge损失函数（SVM）"></a>四、Hinge损失函数（SVM）</h2><h3 id="4-1-Hinge损失函数（SVM）"><a href="#4-1-Hinge损失函数（SVM）" class="headerlink" title="4.1 Hinge损失函数（SVM）"></a>4.1 Hinge损失函数（SVM）</h3><p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数：</p>
<script type="math/tex; mode=display">\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2</script><p>目标函数的第一项是经验损失或经验风险，函数</p>
<script type="math/tex; mode=display">L(y·(w·x+b))=[1-y(w·x+b)]_+</script><p>称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数：</p>
<script type="math/tex; mode=display">
\left[z\right]_+=\left\{\begin{array}{l}
    z\ ,\ z>0\\
    0\ ,\ z\le 0\\
\end{array}\right.</script><p>这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。</p>
<p>接下来证明线性支持向量机原始最优化问题：</p>
<script type="math/tex; mode=display">
\underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}</script><script type="math/tex; mode=display">
s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N</script><script type="math/tex; mode=display">
\xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N</script><p>等价于最优化问题</p>
<script type="math/tex; mode=display">\underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2</script><p>先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作</p>
<script type="math/tex; mode=display">\underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2</script><p>若取$\lambda =\frac{1}{2C}$则</p>
<script type="math/tex; mode=display">\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)</script><p>与原始最优化问题等价。</p>
<p>合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。</p>
<p>图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-08 下午6.11.53.png" alt="屏幕快照 2017-07-08 下午6.11.53"><br>图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求</p>
<h3 id="4-2-逻辑斯谛回归和SVM的损失函数对比"><a href="#4-2-逻辑斯谛回归和SVM的损失函数对比" class="headerlink" title="4.2 逻辑斯谛回归和SVM的损失函数对比"></a>4.2 逻辑斯谛回归和SVM的损失函数对比</h3><p>我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：<img src="http://omu7tit09.bkt.clouddn.com/14995092441592.png" alt="">其中 $g(z)=(1+exp(−z))^{−1}$<br>可以将两者统一起来:</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14995093179927.png" alt=""><br><img src="http://omu7tit09.bkt.clouddn.com/14995093274307.png" alt=""><br>这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。</p>
<p>svm考虑局部（支持向量），而logistic回归考虑全局，就像大学里的辅导员和教师间的区别。</p>
<p>辅导员关心的是挂科边缘的人，常常找他们谈话，告诫他们一定得好好学习，不要浪费大好青春，挂科了会拿不到毕业证、学位证等等，相反，对于那些相对优秀或者良好的学生，他们却很少去问，因为辅导员相信他们一定会按部就班的做好分内的事；而大学里的教师却不是这样的，他们关心的是班里的整体情况，大家是不是基本都理解了，平均分怎么样，至于某个人的分数是59还是61，他们倒不是很在意。</p>
<p>总结：</p>
<ol>
<li>LR采用log损失，SVM采用合页损失。</li>
</ol>
<ul>
<li>LR对异常值敏感，SVM对异常值不敏感。</li>
<li>在训练集较小时，SVM较适用，而LR需要较多的样本。</li>
<li>LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。</li>
<li>对非线性问题的处理方式不同，LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过kernel。</li>
<li>svm 更多的属于非参数模型，而logistic regression 是参数模型，本质不同。其区别就可以参考参数模型和非参模型的区别</li>
</ul>
<p>那怎么根据特征数量和样本量来选择SVM和LR模型呢？Andrew NG的课程中给出了以下建议：</p>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。(LR和不带核函数的SVM比较类似。)</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 损失函数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（23）：TF-IDF与余弦相似度]]></title>
      <url>/2017/07/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8823%EF%BC%89%EF%BC%9ATF-IDF%E4%B8%8E%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
      <content type="html"><![CDATA[<p>TF-IDF(term frequency=inverse document frequency)是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常备搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。</p>
<a id="more"></a>
<h2 id="一、原理"><a href="#一、原理" class="headerlink" title="一、原理"></a>一、原理</h2><p>设想现在我们正在阅读新闻，如何最快速的了解新闻的主旨？毫无疑问——关键词。TF-IDF就具有这样的能力：提取关键词。</p>
<h3 id="1-1-TF"><a href="#1-1-TF" class="headerlink" title="1.1 TF"></a>1.1 TF</h3><p>假设一个词在一篇文章中出现的次数越多，那么它就越”紧扣主题”。以本文为例，我们可以统计词频(TF)，不难发现“TF-IDF”,“应用”、“原理”是出现频率很高的词，后文称keywords。这符合我们的假设，但是有些词却出现的次数更多，如：的、是、有等。这类词语没有明确意义，我们称为停顿词(Stopwords)。</p>
<p>如果单纯按照词频算关键词，你会发现几乎所有的文章都是stopwords的词频最高。换句话说，像这种”万金油”，是没有区分度的词语，不能很好的起到将文章分类的作用。</p>
<p>此外，抛开停用词，如果该文档中的几个词出现的频率一样，也不意味着，作为关键词，它们的重要性是一致的。比如这篇文档中，“TF-IDF”、“意义”、“文档”这三个词的词频出现的次数一样多，但因为“意义”是很常见的词，相对而言，“TF-IDF”、“文档”不那么常见。即使它们的词频一样，我们也有理由认为，“TF-IDF”和“文档”的重要性大于“意义”，也就是使，在关键词排序上，“TF-IDF”和“文档”也应该排在“意义”的前面。</p>
<p>所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。这时就需要祭出逆文档频率(IDF)来解决词语权重的问题。</p>
<h2 id="1-2-IDF"><a href="#1-2-IDF" class="headerlink" title="1.2 IDF"></a>1.2 IDF</h2><p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。</p>
<p>知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</p>
<h2 id="1-3-公式化表达"><a href="#1-3-公式化表达" class="headerlink" title="1.3 公式化表达"></a>1.3 公式化表达</h2><p>对于在某一特定文件里的词语$t_i$来说，它的重要性可表示为：</p>
<script type="math/tex; mode=display">TF_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}}</script><p>以上式子中$n_{i,j}$是该词在文件$d_{j}$中的出现次数而分母则是在文件$d_j$中所有字词的出现次数之和。</p>
<p>逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到：</p>
<script type="math/tex; mode=display">IDF_i=log\frac{|D|}{|{j:t_i\in d_j}|}</script><p>其中</p>
<ul>
<li>$|D|$：语料库中的文件总数</li>
<li>$|{j:t_i\in d_j}|$：包含词语$t_i$的文件数目（即$n_{i,j}≠0的文件数目$）如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$1+|{j:t_i\in d_j}|$</li>
</ul>
<p>然后</p>
<script type="math/tex; mode=display">TF-IDF = TF_{i,j}\times IDF _i</script><p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。</p>
<h2 id="1-4-应用"><a href="#1-4-应用" class="headerlink" title="1.4 应用"></a>1.4 应用</h2><p>我们通过Google搜索结果数为例，将含有中文“的”结果数15.8亿作为整个语料库大小，计算一些关键词和停用词的TF-IDF值。为了计算简便，假设全文分词后一共500词，则结果如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-07 上午9.19.11.png" alt="屏幕快照 2017-07-07 上午9.19.11"></p>
<p>TF-IDF的优点是计算简单，利于理解，性价比极高。但是它也有缺陷，首先单纯依据文章中的TF来衡量重要性，忽略了位置信息。如段首，句首一般权重更高；其次，有的文章可能关键词只出现1-2次，但可能通篇都是围绕其进行阐述和解释，所以单纯靠TF仍然不能解决所有的情况。</p>
<h2 id="二、余弦相似度"><a href="#二、余弦相似度" class="headerlink" title="二、余弦相似度"></a>二、余弦相似度</h2><p>余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。</p>
<p>注意这上下界对任何维度的向量空间中都适用，而且余弦相似性最常用于高维正空间。例如在信息检索中，每个词项被赋予不同的维度，而一个文档由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度。</p>
<h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p>两个向量间的余弦值可以通过使用欧几里得点积公式求出：</p>
<script type="math/tex; mode=display">a·b=|a|·|b|\ cos \theta</script><p>给定两个属性向量$A$和$B$，其余相似性$\theta$由点积和向量长度给出，如下所示：</p>
<script type="math/tex; mode=display">similarity = cos(\theta)=\frac{A·B}{|A||B|}=\frac{\sum_{i=1}^nA_i\times B_i}{\sqrt{\sum_{i=1}^n(A_i)^2}\times \sqrt{\sum_{i=1}^n(B_i)^2}}</script><p>这里的$A_i$和$B_i$分别代表向量$A$和$B$的各分量。</p>
<p>给出的相似性范围从-1到1：-1意味着两个向量指向的方向正好截然相反，1表示它们的指向是完全相同的，0通常表示它们之间是独立的，而在这之间的值则表示中间的相似性或相异性。</p>
<p>对于文本匹配，属性向量A 和B 通常是文档中的词频向量。余弦相似性，可以被看作是在比较过程中把文件长度正规化的方法。</p>
<p>在信息检索的情况下，由于一个词的频率（TF-IDF权）不能为负数，所以这两个文档的余弦相似性范围从0到1。并且，两个词的频率向量之间的角度不能大于90°。</p>
<p>由此，我们就得到了”找出相似文章”的一种算法：</p>
<ul>
<li>1）使用TF-IDF算法，找出两篇文章的关键词；</li>
<li>2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；</li>
<li>3）生成两篇文章各自的词频向量；</li>
<li>4）计算两个向量的余弦相似度，值越大就表示越相似。</li>
</ul>
<p>“余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TF-IDF </tag>
            
            <tag> 余弦相似度 </tag>
            
            <tag> 文档检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（22）：主成分分析]]></title>
      <url>/2017/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8822%EF%BC%89%EF%BC%9A%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。</p>
<a id="more"></a>
<h2 id="一、数据的向量表示及降维问题"><a href="#一、数据的向量表示及降维问题" class="headerlink" title="一、数据的向量表示及降维问题"></a>一、数据的向量表示及降维问题</h2><p>一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：</p>
<blockquote>
<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p>
</blockquote>
<p>其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：</p>
<script type="math/tex; mode=display">(500,240,25,13,2312.15)^𝖳</script><p>注意这里用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时会省略转置符号，但我们说到向量默认都是指列向量。</p>
<p>我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p>
<p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p>
<p>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。</p>
<p>当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p>
<p>这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p>
<p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p>
<p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p>
<h2 id="二、向量的表示及基变换"><a href="#二、向量的表示及基变换" class="headerlink" title="二、向量的表示及基变换"></a>二、向量的表示及基变换</h2><p>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p>
<h3 id="2-1-内积与投影"><a href="#2-1-内积与投影" class="headerlink" title="2.1 内积与投影"></a>2.1 内积与投影</h3><p>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p>
<script type="math/tex; mode=display">(a_1,a_2,⋯,a_n)^𝖳⋅(b_1,b_2,⋯,b_n)𝖳=a_1b_1+a_2b_2+⋯+a_nb_n</script><p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。</p>
<p>假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则$A=(x_1,y_1)$，$B=(x_2,y_2)$。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：<br><img src="http://omu7tit09.bkt.clouddn.com/14992470618968.png" alt=""><br>现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为$|A|cos(a)|A|cos(a)$，其中$|A|=\sqrt{x^2_1+y^2_1}$是向量A的模，也就是A线段的标量长度。</p>
<p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。</p>
<p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p>
<script type="math/tex; mode=display">A⋅B=|A||B|cos(a)</script><p>A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让$|B|=1$，那么就变成了：</p>
<script type="math/tex; mode=display">A⋅B=|A|cos(a)</script><p>也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。</p>
<h3 id="2-2-基"><a href="#2-2-基" class="headerlink" title="2.2 基"></a>2.2 基</h3><p>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：<br><img src="http://omu7tit09.bkt.clouddn.com/14992473264569.png" alt=""></p>
<p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p>
<p>不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p>
<p>更正式的说，向量(x,y)实际上表示线性组合：</p>
<script type="math/tex; mode=display">x(1,0)^T+y(0,1)^T</script><p>不难证明所有二维向量都可以表示为这样的线性组合。此处（1，0）和（0，1）叫做二维空间中的一组基。<br><img src="http://omu7tit09.bkt.clouddn.com/14992474414800.png" alt=""></p>
<p>所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p>
<p>我们之所以默认选择$(1,0)$和$(0,1)$为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p>
<p>例如，$(1,1)$和$(-1,1)$也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$和$-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}$。</p>
<p>现在，我们想获得$(3,2)$在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},−\frac{1}{\sqrt{2}}$。下图给出了新的基以及(3,2)在新基上坐标值的示意图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14992476980092.png" alt=""></p>
<p>另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p>
<h3 id="2-3-基变换的矩阵表示"><a href="#2-3-基变换的矩阵表示" class="headerlink" title="2.3 基变换的矩阵表示"></a>2.3 基变换的矩阵表示</h3><p>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将$(3,2)$变换为新基上的坐标，就是用$(3,2)$与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
\end{matrix}\right]\left[\begin{array}{c}
    3\\
    2\\
\end{array}\right]=\left[\begin{array}{c}
    \frac{5}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}\\
\end{array}\right]</script><p>其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    \frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
    -\frac{1}{\sqrt{2}}&        \frac{1}{\sqrt{2}}\\
\end{matrix}\right]\left[\begin{matrix}
    1&        2&        3\\
    1&        2&        3\\
\end{matrix}\right]=\left[\begin{matrix}
    \frac{2}{\sqrt{2}}&        \frac{4}{\sqrt{2}}&        \frac{6}{\sqrt{2}}\\
    0&        0&        0\\
\end{matrix}\right]</script><p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p>
<p>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。</p>
<p>数学表示为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
    p_1\\
    p_2\\
    ···\\
    p_r\\
\end{array}\right]\left[\begin{matrix}
    a_1&        a_2&        ···&        a_M\\
\end{matrix}\right]=\left[\begin{matrix}
    p_1a_1&        p_1a_2&        ···&        p_1a_M\\
    p_2a_1&        p_2a_2&        ···&        p_2a_M\\
    ···&        ···&        ···&        ···\\
    p_ra_1&        p_ra_2&        ···&        p_ra_M\\
\end{matrix}\right]</script><p>其中$p_i$是一个行向量，表示第$i$个基，$a_j$是一个列向量，表示第$j$个原始数据记录。</p>
<p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p>
<p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p>
<h2 id="三、协方差矩阵及优化目标"><a href="#三、协方差矩阵及优化目标" class="headerlink" title="三、协方差矩阵及优化目标"></a>三、协方差矩阵及优化目标</h2><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p>
<p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p>
<p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    1&        1&        2&        4&        2\\
    1&        3&        3&        4&        4\\
\end{matrix}\right]</script><p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。</p>
<p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    -1&        -1&        0&        2&        0\\
    -2&        0&        0&        1&        1\\
\end{matrix}\right]</script><p>我们可以看下五条数据在平面直角坐标系内的样子：<br><img src="http://omu7tit09.bkt.clouddn.com/14992501725051.png" alt=""></p>
<p>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p>
<p>通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。</p>
<p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p>
<p>以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p>
<p>下面，我们用数学方法表述这个问题。</p>
<h3 id="3-1-方差"><a href="#3-1-方差" class="headerlink" title="3.1 方差"></a>3.1 方差</h3><p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p>
<script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum^m_{i=1}(a_i-\mu)^2</script><p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum^m_{i=1}a_i^2</script><p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<h3 id="3-2-协方差"><a href="#3-2-协方差" class="headerlink" title="3.2 协方差"></a>3.2 协方差</h3><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p>
<p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p>
<p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p>
<script type="math/tex; mode=display">Cov(a,b) = \frac{1}{m}\sum_{i=1}^ma_ib_i</script><p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p>
<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>
<p>至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</p>
<h3 id="3-3-协方差矩阵"><a href="#3-3-协方差矩阵" class="headerlink" title="3.3 协方差矩阵"></a>3.3 协方差矩阵</h3><p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p>
<p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p>
<p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p>
<script type="math/tex; mode=display">
X=\left[\begin{matrix}
    a_1&        a_1&        ···&        a_m\\
    b_1&        b_2&        ···&        b_m\\
\end{matrix}\right]</script><p>然后我们用X乘以X的转置，并乘上系数$1/m$：</p>
<script type="math/tex; mode=display">\frac{1}{m}XX^T = \left[\begin{matrix}
    \frac{1}{m}\sum_{i=1}^ma_i^2&        \frac{1}{m}\sum_{i=1}^ma_ib_i\\
    \frac{1}{m}\sum_{i=1}^ma_ib_i&        \frac{1}{m}\sum_{i=1}^mb_i^2\\
\end{matrix}\right]</script><p>奇迹出现了！这个对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p>
<p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设￥$C=\frac{1}{m}XX^𝖳$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。</p>
<h3 id="3-4-协方差矩阵对角化"><a href="#3-4-协方差矩阵对角化" class="headerlink" title="3.4 协方差矩阵对角化"></a>3.4 协方差矩阵对角化</h3><p>根据上述推导，我们发现要达到优化条件，等价于将协方差对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p>
<p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设$Y=PX$，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p>
<script type="math/tex; mode=display">D = \frac{1}{m}YY^T\\ =\frac{1}{m}(PX)(PX)^T\\ =\frac{1}{m}PXX^TP\\=PCP^T</script><p>现在事情很明白了，我们要找的$P$不是别的，而是能让原始协方差矩阵对角化的$P$。换句话说，优化目标变成了寻找一个矩阵$P$，满足$PCP^T$是一个对角矩阵，并且对角元素按从小到大依次排列，那么$P$的前$K$行就是要寻找的基，用$P$的前$K$行组成的矩阵乘以$X$就使得$X$从$N$维降到了$K$维并满足上述优化条件。</p>
<p>至此，我们离“发明”PCA还有仅一步之遥！</p>
<p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p>
<p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>
<ul>
<li>1）实对称矩阵不同特征值对应的特征向量必然正交。</li>
<li>2）设特征向量λλ重数为r，则必然存在r个线性无关的特征向量对应于λλ，因此可以将这r个特征向量单位正交化。</li>
</ul>
<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,⋯,e_n$，我们将其按列组成矩阵：</p>
<script type="math/tex; mode=display">E = (e_1\ e_2 \ ··· \ e_n)</script><p>则对协方差矩阵$C$有如下结论：</p>
<script type="math/tex; mode=display">E^TCE =
\varLambda\ =\left[\begin{matrix}
    \lambda_1&        &        &        \\
    &        \lambda_2&        &        \\
    &        &        ···&        \\
    &        &        &        \lambda_n\\
\end{matrix}\right]</script><p>其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p>
<p>到这里，我们发现我们已经找到了需要的矩阵P：P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照ΛΛ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p>
<p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p>
<h2 id="四、算法及实例"><a href="#四、算法及实例" class="headerlink" title="四、算法及实例"></a>四、算法及实例</h2><p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p>
<h3 id="4-1-PCA算法"><a href="#4-1-PCA算法" class="headerlink" title="4.1 PCA算法"></a>4.1 PCA算法</h3><p>总结一下PCA的算法步骤：<br>设有m条n维数据。</p>
<ul>
<li>1）将原始数据按列组成n行m列矩阵X</li>
<li>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li>
<li>3）求出协方差矩阵$C=\frac{1}{m}XX^T$</li>
<li>4）求出协方差矩阵的特征值及对应的特征向量</li>
<li>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前K行组成矩阵P</li>
<li>6）$Y=PX$即为降维到K维后的数据</li>
</ul>
<h3 id="4-2-实例"><a href="#4-2-实例" class="headerlink" title="4.2 实例"></a>4.2 实例</h3><p>这里以上文提到的</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    -1&        -1&        0&        2&        0\\
    -2&        0&        0&        1&        1\\
\end{matrix}\right]</script><p>为例，我们用PCA方法将这组二维数据降到一维。</p>
<p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p>
<script type="math/tex; mode=display">C=\frac{1}{5}
\left[\begin{matrix}
    -1&        -1&        0&        2&        0\\
    -2&        0&        0&        1&        1\\
\end{matrix}\right]
\left[\begin{matrix}
    -1&        -2\\
    -1&        0\\
    0&             0\\
    2&             1\\
    0&            1\\
\end{matrix}\right]=\left[\begin{matrix}
    \frac{6}{5}&        \frac{4}{5} \\    \frac{4}{5}&  
    \frac{6}{5} \\ 
\end{matrix}\right]</script><p>然后求其特征值和特征向量，具体求解方法不再详述。求解后特征值为：</p>
<script type="math/tex; mode=display">\lambda_1=2 , \lambda_2 =\frac{2}{5}</script><p>其对应的特征向量分别是：</p>
<script type="math/tex; mode=display">c_1\left[\begin{matrix}
            -2\\
    0\\
\end{matrix}\right],c_2\left[\begin{matrix}
            -1\\
    1\\
\end{matrix}\right]</script><p>其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数。那么标准化后的特征向量为：</p>
<script type="math/tex; mode=display">\left[\begin{matrix}
            \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}\\
\end{matrix}\right],\left[\begin{matrix}
            -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}\\
\end{matrix}\right]</script><p>因此我们的矩阵P是：</p>
<script type="math/tex; mode=display">P=\left[\begin{matrix}
            \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\
    -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
\end{matrix}\right]</script><p>可以验证协方差矩阵C的对角化：</p>
<script type="math/tex; mode=display">PCP^T=\left[\begin{matrix}
            \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\
    -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
\end{matrix}\right]\left[\begin{matrix}
    \frac{6}{5}&        \frac{4}{5} \\    \frac{4}{5}&  
    \frac{6}{5} \\ 
\end{matrix}\right]\left[\begin{matrix}
            \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\
    \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
\end{matrix}\right]=\left[\begin{matrix}
            2&0\\
    0&\frac{2}{5}\\
\end{matrix}\right]</script><p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p>
<script type="math/tex; mode=display">Y =\left[\begin{matrix}
            \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
\end{matrix}\right] \left[\begin{matrix}
            -1&-1&0&2&0\\
            -2&0&0&1&1\\
\end{matrix}\right]=\left[\begin{matrix}
            -\frac{3}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0&\frac{3}{\sqrt{2}}&-\frac{1}{2}\\
\end{matrix}\right]</script><p>降维投影结果如下图：<br><img src="http://omu7tit09.bkt.clouddn.com/14992688265732.png" alt=""></p>
<h2 id="五、理论意义"><a href="#五、理论意义" class="headerlink" title="五、理论意义"></a>五、理论意义</h2><p>PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。再看回顾一下PCA的效果。经过PCA处理后，二维数据投影到一维上可以有以下几种情况：<br><img src="http://omu7tit09.bkt.clouddn.com/14993039151739.png" alt=""></p>
<p>我们认为左图好，一方面是投影后方差最大，一方面是点到直线的距离平方和最小，而且直线过样本点的中心点。为什么右边的投影效果比较差？直觉是因为坐标轴之间相关，以至于去掉一个坐标轴，就会使得坐标点无法被单独一个坐标轴确定。</p>
<p>PCA得到的k个坐标轴实际上是k个特征向量，由于协方差矩阵对称，因此k个特征向量正交。</p>
<p>得到的新的样例矩阵$Y=PX$就是m个样例到k个特征向量的投影，也是这k个特征向量的线性组合。P中e之间是正交的。从矩阵乘法中可以看出，PCA所做的变换是将原始样本点（n维），投影到k个正交的坐标系中去，丢弃其他维度的信息。举个例子，假设宇宙是n维的（霍金说是11维的），我们得到银河系中每个星星的坐标（相对于银河系中心的n维向量），然而我们想用二维坐标去逼近这些样本点，假设算出来的协方差矩阵的特征向量分别是图中的水平和竖直方向，那么我们建议以银河系中心为原点的x和y坐标轴，所有的星星都投影到x和y上，得到下面的图片。然而我们丢弃了每个星星离我们的远近距离等信息。</p>
<h2 id="六、进一步讨论"><a href="#六、进一步讨论" class="headerlink" title="六、进一步讨论"></a>六、进一步讨论</h2><p>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>
<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>
<p>PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 </p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14993021431867.gif" alt="">上图中黑色点表示采样数据，排列成转盘的形状。容易想象，该数据的主元是$(P_1,P_2)$或是旋转角$\theta$。在这里PCA找出的主元将是$(P_1,P_2 )$。但是这显然不是最优和最简化的主元。$(P_1,P_2 )$之间存在着非线性的关系。根据先验的知识可知旋转角$\theta$是最优的主元（类比极坐标）。则在这种情况下，PCA就会失效。但是，如果加入先验的知识，对数据进行某种划归，就可以将数据转化为以$\theta$为线性的空间中。这类根据先验知识对数据预先进行非线性转换的方法就成为kernel-PCA，它扩展了PCA能够处理的问题的范围，又可以结合一些先验约束，是比较流行的方法。</p>
<p>有时数据的分布并不是满足高斯分布。如图表 5所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式</p>
<script type="math/tex; mode=display">P(y_1,y_2)=P(y_1)P(y_2 )</script><p>来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使$P(y_1,y_2)=0$。这一类方法被称为独立主元分解(ICA)。<br><img src="http://omu7tit09.bkt.clouddn.com/14993037124980.gif" alt=""><br>数据的分布并不满足高斯分布，呈明显的十字星状。这种情况下，方差最大的方向并不是最优主元方向。另外PCA还可以用于预测矩阵中缺失的元素。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> PCA </tag>
            
            <tag> 降维 </tag>
            
            <tag> 非监督学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（21）：SVD]]></title>
      <url>/2017/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8821%EF%BC%89%EF%BC%9ASVD%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%8F%8A%E5%85%B6%E6%84%8F%E4%B9%89/</url>
      <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>SVD实际上是数学专业内容，但它现在已经深入到不同的领域中。SVD的过程不是很好理解，因为它不够直观，但它对矩阵分解的效果却非常好。比如，Netflix（一个提供在线电影租赁的公司）曾经就悬赏100万美金，如果谁能提高他的电影推荐系统评分预测率10%的话。令人惊讶的是，这个目标充满了挑战，来自世界各地的团队运用了各种不同的技术。最终的获胜队伍“BellKor’s Pragmatic Chaos”采用的核心算法就是基于SVD。</p>
<a id="more"></a>
<p>SVD提供了一种非常便捷的矩阵分解方式，能够发现数据中十分有意思的潜在模式，在这篇文章中，我们将会提供对SVD集合上的理解和一些简单的应用实例。</p>
<h3 id="1-1-几何意义"><a href="#1-1-几何意义" class="headerlink" title="1.1 几何意义"></a>1.1 几何意义</h3><p>奇异值分解就是把一个线性变换分解成两个线性变换，一个线性变化代表旋转，另一个代表拉伸。</p>
<p>线性代数中最让人印象深刻的一点是，要将矩阵和空间中的线性变化视为同样的事物。比如对角矩阵$M$作用在任何一个向量上</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}
    3&        0\\
    0&        1\\
\end{matrix}\right]\left[\begin{array}{c}
    x\\
    y\\
\end{array}\right]=\left[\begin{array}{c}
    3x\\
    y\\
\end{array}\right]</script><p>其几何意义为在水平$x$方向上拉伸3倍，$y$方向保持不变的线性变换。换言之对角矩阵起到作用是将水平垂直网格作水平拉伸（或者反射后水平拉伸）的线性变换。<br><img src="http://omu7tit09.bkt.clouddn.com/14990677634208.jpg" alt=""><br>如果$M$不是对角矩阵。而是一个对称矩阵：</p>
<script type="math/tex; mode=display">
M=\left[\begin{matrix}
    2&        1\\
    1&        2\\
\end{matrix}\right]</script><p>那么我们也总能找到一组网格线，使得矩阵作用在该网格上仅仅表现为（反射）拉伸变换，而没有发生旋转变换。这个矩阵产生的变换效果如下图所示<br><img src="http://omu7tit09.bkt.clouddn.com/14992394690344.jpg" alt=""></p>
<p>考虑一下更一般的非对称矩阵</p>
<script type="math/tex; mode=display">
M=\left[\begin{matrix}
    1&        1\\
    0&        1\\
\end{matrix}\right]</script><p>很遗憾，此时我们再也找不到一组网格，使得矩阵作用在该网格之后只有拉伸变换（找不到背后的数学原因就是对一般非对称矩阵无法保证在实数域上可对角化）。我们退而求其次，找到一组网格，使得矩阵作用在该网格之后允许有拉伸变换和旋转变换，但要保证变换后的网格依旧互相垂直。这是可以做到的<img src="http://omu7tit09.bkt.clouddn.com/14992307483348.jpg" alt=""></p>
<p>下面我们就可以自然过渡到奇异值分解的引入。奇异值分解的几何含义为：对于任何的一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列保持两两正交。下面我们要说明的是，奇异值的几何含义为：这组变换后的新的向量序列的长度。<img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-05 下午1.04.29.png" alt="屏幕快照 2017-07-05 下午1.04.29"><br>当矩阵$M$作用在正交单位向量$v_1$和$v_2$上之后，得到$Mv_1$和$Mv_2$也是正交的。令$u_1$和$u_2$分别是$Mv_1$和$Mv_2$方向上的单位向量，即$Mv_1 = \sigma_1u_1$，$Mv_2 = \sigma _2 u_2$，写在一起就是$M[v_1,v_2] = [\sigma_1 u_1 \ \sigma_2 u_2]$，整理得到</p>
<script type="math/tex; mode=display">
M=M\left[v_1\ v_2\right]\left[\begin{array}{c}
    v_{1}^{T}\\
    v_{2}^{T}\\
\end{array}\right]=\left[\sigma_1u_1 \ \sigma_2u_2\right]\left[\begin{array}{c}
    v_{1}^{T}\\
    v_{2}^{T}\\
\end{array}\right]=\left[u_1\ u_2\right]\left[\begin{matrix}
    \sigma_1&        0\\
    0&        \sigma_2\\
\end{matrix}\right]\left[\begin{array}{c}
    v_{1}^{T}\\
    v_{2}^{T}\\
\end{array}\right]</script><p>这样就得到矩阵$M$的奇异值分解。奇异值$\sigma_1$和$\sigma_2$分别是$Mv_1$和$Mv_2$的长度。很容易可以把结论推广到一般$n$维的情况</p>
<h2 id="二、奇异值分解"><a href="#二、奇异值分解" class="headerlink" title="二、奇异值分解"></a>二、奇异值分解</h2><h3 id="2-1-特征值分解"><a href="#2-1-特征值分解" class="headerlink" title="2.1 特征值分解"></a>2.1 特征值分解</h3><p>如果方阵对某个向量只产生伸缩，而不产生旋转效果，那么这个向量就称为矩阵的特征向量，伸缩的比例就是对应的特征值。</p>
<script type="math/tex; mode=display">Ax=\lambda x</script><p><img src="http://omu7tit09.bkt.clouddn.com/14990727044929.png" alt=""></p>
<p>所以这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化（变化方向可能不止一个），如果我们想要描述好一个变换，那我们描述好这个变换主要的变化方向就好了。反过来看看之前特征值分解的式子，分解得到的$\Sigma$矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化的方向（从主要的变化到次要的变化排列）。</p>
<p>当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵变换。也就是之前受的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间敢很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。</p>
<p>学线性代数的时候，我们应该都学过这样一个定理：</p>
<blockquote>
<p>若A为n阶实对称阵（方阵），则存在由特征值组成的对角阵$\varLambda$和特征向量组成的正交阵$Q$，使得：</p>
<script type="math/tex; mode=display">A=Q\varLambda Q^T</script></blockquote>
<p>这就是我们所说的特征值分解（Eigenvalue decomposition: EVD）（$R^n → R^n$），而奇异值分解其实可以看做是特征值分解在任意矩阵$m\times n$上的推广形式($R^n →R^m$)。只有对方阵才有特征值的概念，所以对于任意的矩阵，我们引入了奇异值。</p>
<h3 id="2-2-奇异值分解"><a href="#2-2-奇异值分解" class="headerlink" title="2.2 奇异值分解"></a>2.2 奇异值分解</h3><p>上面的特征值分解是一个提取矩阵特征很不错的方法，当它只是对方阵而言的，在现实的世界中，我们看到的大部分都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个$N\times M$的矩阵就不可能是方阵！那么现在就来分析：对于任意的$m\times n$的矩阵，能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是SVD分解的精髓所在。</p>
<p>下面我们从特征值分解出发，导出奇异值分解。</p>
<p>首先我们注意到$A^T A$为$n$阶对称矩阵，我们可以对它做特征值分解。</p>
<script type="math/tex; mode=display">A^T A = VDV^T</script><p>这个时候我们可以得到一组正交基，$\{v_1,v_2,···v_n\}$：</p>
<script type="math/tex; mode=display">(A^T A)v_i=\lambda _i v_i</script><script type="math/tex; mode=display">
(Av_i,Av_j) = (Av_i)^T(Av_j) = v_i^TA^TAv_j
= v_i^T(\lambda_jv_j)
= \lambda_jv_i^Tv_j
= 0</script><p>由$r(A^T A)=r(A)=r$，这个时候我们得到了一组正交基，$\{Av_1,Av_2,···,Av_r\}$，先将其标准化，令：</p>
<script type="math/tex; mode=display">u_i = \frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda _i}}Av_i\Rightarrow Av_i = \sqrt {\lambda _i} u_i = \delta_iu_i</script><p>其中</p>
<script type="math/tex; mode=display">|Av_i|^2=(Av_i,Av_i)=\lambda_iv_i^Tv_i=\lambda_i \Rightarrow |Av_i| = \sqrt {\lambda _i}=\delta _i(奇异值)</script><p>将向量组$\{u_1,u_2,···,u_r\}$扩充为$R^m$中的标准正交基$\{u_1,u_2,···,u_r,···,u_m$，则：</p>
<script type="math/tex; mode=display">AV =A(v_1v_2···v_n) 
=(Av_1 \ Av_2 \ ···\ Av_r\  0 ··· \ 0)\\=(\delta _1u_1  \ \delta_2u_2 ··· \delta _r u_r \ 0 ··· \ 0=U\Sigma\\\Rightarrow A =U\Sigma V^T</script><p>我们可以从下图中直观的感受奇异值分解的矩阵相乘。<br><img src="http://omu7tit09.bkt.clouddn.com/14991419233249.png" alt=""><br>任意的矩阵$A$是可以分解成三个矩阵。其中$V$表示了原始域的标准正交基，$U$表示经过A变化后的$co-domain$的标准正交基，$\Sigma$表示了$V$中的向量与$U$中相对应向量之间的关系。</p>
<p>在很多情况下，前10%甚至1%的奇异值的和就占了全部分奇异值之和的99%以上了。也就是说，我们也可以用前$r$大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：</p>
<script type="math/tex; mode=display">A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}</script><p>$r$是一个远小于$m、n$的数，这样矩阵的乘法看起来像是下面的样子：<img src="http://omu7tit09.bkt.clouddn.com/14991423029453.png" alt=""><br>右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，$r$越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（早存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵:$U、\Sigma、V$就好了。</p>
<h2 id="三、应用实例"><a href="#三、应用实例" class="headerlink" title="三、应用实例"></a>三、应用实例</h2><h3 id="3-1-推荐系统"><a href="#3-1-推荐系统" class="headerlink" title="3.1 推荐系统"></a>3.1 推荐系统</h3><p>我们现在有一批高尔夫球手对九个不同hole的所需挥杆次数数据，我们希望基于这些数据建立模型，来预测选手对于某个给定hole的挥杆次数。（这个例子来自于： Singular Value Decomposition (SVD) Tutorial，强烈建议大家都去看看）<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-04 上午10.49.41.png" alt="屏幕快照 2017-07-04 上午10.49.41"></p>
<p>最简单的一个思路，我们队每个hole设立一个难度指标HoleDifficulty，对每位选手的能力也设立一个评价指标PlayAbility，实际的得分取决于这俩者的乘积：</p>
<script type="math/tex; mode=display">PredictedScore = HoleDifficulty · PlayerAbility</script><p>我们可以简单地把每位选手的Ability都设为1，那么：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-04 上午10.50.25.png" alt="屏幕快照 2017-07-04 上午10.50.25"><br>接着我们将HoleDifficulty 和 PlayAbility这两个向量标准化，可以得到如下的关系：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-04 上午10.51.26.png" alt="屏幕快照 2017-07-04 上午10.51.26"></p>
<p>好熟悉，这不就是传说中的SVD吗，这样就出来了。</p>
<p>这里面蕴含了一个非常有趣的思想，也是SVD这么有用的核心：</p>
<p>最开始高尔夫球员和Holes之间是没有直接联系的，我们通过feature把它们联系在一起：不同的Hole进洞难度是不一样的，每个球手对进度难度的把控也是不一样的，那么我们就可以通过进洞难度这个feature将它们联系在一起，将它们乘起来就得到了我们想要的挥杆次数。</p>
<p>这个思想很重要，对于我们理解LSI和SVD再推荐系统中的应用相当重要。</p>
<p>SVD分解其实就是利用隐藏的Feature建立起矩阵行与列之间的联系。</p>
<p>大家可能注意到，上面那个矩阵秩为1，所以我们很容易就能将其分解，但是在实际问题中我们就得依靠SVD分解，这个时候的隐藏特征往往也不止一个了。</p>
<p>我们将上面的数据稍作修改：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-04 上午10.57.19.png" alt="屏幕快照 2017-07-04 上午10.57.19"><br>进行奇异值分解，可以得到：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-04 上午10.57.51.png" alt="屏幕快照 2017-07-04 上午10.57.51"><br>隐藏特征的重要性是与其对应的奇异值大小成正比的，也就是奇异值越大，其所对应的隐藏特征也越重要。</p>
<p>我们将这个思想推广一下</p>
<p>在推荐系统中，用户和物品之间没有直接联系。但是我们可以通过feature把它们联系在一起。对于电影来说，这样的特征可以是：喜剧还是悲剧，是动作片还是爱情片。用户和这样的feature之间是有关系的，比如某个用户喜欢看爱情片，另外一个用户喜欢看动作片；物品和feature之间也是有关系的，比如某个电影是喜剧，某个电影是悲剧。那么通过和feature之间的联系，我们就找到了用户和物品之间的关系。<br><img src="http://omu7tit09.bkt.clouddn.com/14991374078248.png" alt=""></p>
<h3 id="3-2-数据压缩"><a href="#3-2-数据压缩" class="headerlink" title="3.2 数据压缩"></a>3.2 数据压缩</h3><p>矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称SVD分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。下面先尽量避开严格的数学符号推导，直观的从一张图片出发，让我们来看看奇异值代表什么意义。</p>
<p>这是女神上野树里（Ueno Juri）的一张照片，像素为$450\times 333$<img src="http://omu7tit09.bkt.clouddn.com/14991425195362.jpg" alt=""><br>我们都知道，图片实际上对应着一个矩阵，矩阵的大小就是像素大小，比如这张图对应的矩阵阶数就是450*333，矩阵上每个元素的数值对应着像素值。我们记这个像素矩阵为 AA 。<br>现在我们对矩阵 AA 进行奇异值分解。直观上，奇异值分解将矩阵分解成若干个秩一矩阵之和，用公式表示就是：</p>
<script type="math/tex; mode=display">A = \sigma_1u_1v_1^T+\sigma_2u_2v_2^T+···+\sigma_ru_rv_r^T</script><p>其中等式右边每一项前的系数$\sigma$就是奇异值，$u$和$v$分别表示列向量，秩一矩阵的意思是秩为1的矩阵。注意到每一项$uv^T$都是秩为1的矩阵。我们假定奇异值满足</p>
<script type="math/tex; mode=display">\sigma_1≥\sigma_2≥···≥\sigma_r＞0</script><p>（奇异值大于0是个重要的性质，但这里先别在意），如果不满足的话重新排列顺序即可，这无非是编号顺序的问题。</p>
<p>既然奇异值有从大到小排列的顺序，我们自然要问，如果只保留大的奇异值，舍去较小的奇异值，这样(1)式里的等式自然不再成立，那会得到怎样的矩阵——也就是图像？</p>
<p>令 $A_1=σ_1u_1v^T_1$ ，这只保留(1)中等式右边第一项，然后作图<img src="http://omu7tit09.bkt.clouddn.com/14991456788688.jpg" alt=""><br>结果就是完全看不清是啥···我们试着多增加几项进来：</p>
<script type="math/tex; mode=display">A_5=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_5μ_5v^T_5</script><p>再作图<img src="http://omu7tit09.bkt.clouddn.com/14991457633850.jpg" alt=""><br>隐约可以辨别这是短发伽椰子的脸……但还是很模糊，毕竟我们只取了5个奇异值而已。下面我们取20个奇异值试试，也就是(1)式等式右边取前20项构成 A20。<br>虽然还有些马赛克般的模糊，但我们总算能辨别出这是Juri酱的脸。当我们取到(1)式等式右边前50项时：<br><img src="http://omu7tit09.bkt.clouddn.com/14991457842537.jpg" alt=""><br>我们得到和原图差别不大的图像。也就是说当k从1不断增大时，A_k不断的逼近A。让我们回到公式</p>
<script type="math/tex; mode=display">A=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_rμ_rv^T_r</script><p>矩阵表示一个$450\times333$的矩阵，需要保存$450\times 333=149850$个元素的值。等式右边和分别是$450\times 1$和$333\times1$的向量，每一项有个元素。如果我们要存储很多高清的图片，而又受限于存储空间的限制，在尽可能保证图像可被识别的精度的前提下，我们可以保留奇异值较大的若干项，舍去奇异值较小的项即可。例如在上面的例子中，如果我们只保留奇异值分解的前50项，则需要存储的元素为，和存储原始矩阵相比，存储量仅为后者的26%。</p>
<p>奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵A都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重。</p>
<p>奇异值分解也可以高效地表示数据。例如，假设我们想传送下列图片，包含$15 \times 25 $个黑色或者白色的像素阵列<br><img src="http://omu7tit09.bkt.clouddn.com/14991461869150.jpg" alt=""><br>因为在图像中只有三种类型的列（如下）,它可以以更紧凑的形式被表示。<img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-05 下午1.43.30.png" alt="屏幕快照 2017-07-05 下午1.43.30"><br>就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息.<img src="http://omu7tit09.bkt.clouddn.com/14992334554512.gif" alt=""><br>如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。</p>
<script type="math/tex; mode=display">\sigma _1 =14.72 \  \sigma_2 =5.22  \sigma _3 =3.31</script><p>因此，矩阵可以如下表示</p>
<script type="math/tex; mode=display">M=u_1σ_1v_1^T+ u_2σ_2v_2^T + u_3σ_3 v_3^T</script><p>我们有三个包含15个元素的向量$v_i$，三个包含25个元素的向量$u_i$，以及三个奇异值$\sigma _i$，这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。在这种方式下，我们看到在矩阵中有三个线性独立的列，也就是说矩阵的秩是3。</p>
<h3 id="3-3-图像去噪"><a href="#3-3-图像去噪" class="headerlink" title="3.3 图像去噪"></a>3.3 图像去噪</h3><p>在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为0时，就可以去除图片中的噪声。如下是一张$25*15$的图像（本例来源于[1]）<br><img src="http://omu7tit09.bkt.clouddn.com/14991461869150.jpg" alt=""></p>
<p>但往往我们只能得到如下带有噪声的图像（和无噪声图像相比，下图的部分白格子中带有灰色）：<br><img src="http://omu7tit09.bkt.clouddn.com/14991462671332.jpg" alt=""></p>
<p>通过奇异值分解，我们发现矩阵的奇异值从大到小分别为：14.15，4.67，3.00，0.21，……，0.05。除了前3个奇异值较大以外，其余奇异值相比之下都很小。强行令这些小奇异值为0，然后只用前3个奇异值构造新的矩阵，得到<br><img src="http://omu7tit09.bkt.clouddn.com/14991463079178.jpg" alt=""><br>可以明显看出噪声减少了（白格子上灰白相间的图案减少了）。</p>
<h3 id="3-4-数据分析"><a href="#3-4-数据分析" class="headerlink" title="3.4 数据分析"></a>3.4 数据分析</h3><p>我们搜集的数据中总是存在噪声：无论采用的设备多精密，方法有多好，总是会存在一些误差的。如果你们还记得上文提到的，大的奇异值对应了矩阵中的主要信息的话，运用SVD进行数据分析，提取其中的主要部分的话，还是相当合理的。</p>
<p>作为例子，假如我们搜集的数据如下所示：<br><img src="http://omu7tit09.bkt.clouddn.com/14992328641320.gif" alt=""></p>
<p>我们将数据用矩阵的形式表示：<br><img src="http://omu7tit09.bkt.clouddn.com/14992328943042.jpg" alt=""><br>经过奇异值分解后，得到</p>
<script type="math/tex; mode=display">\sigma_1 = 6.04  \  \sigma _2 =0.22</script><p>由于第一个奇异值远比第二个要大，数据中有包含一些噪声，第二个奇异值在原始矩阵分解相对应的部分可以忽略。经过SVD分解后，保留了主要样本点如图所示<br><img src="http://omu7tit09.bkt.clouddn.com/14992329639516.gif" alt=""><br>就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息.</p>
<h3 id="3-5-潜在语义索引LSI"><a href="#3-5-潜在语义索引LSI" class="headerlink" title="3.5 潜在语义索引LSI"></a>3.5 潜在语义索引LSI</h3><p> 潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到：</p>
<blockquote>
<p> “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”</p>
</blockquote>
<p> 上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial：<br> <img src="http://omu7tit09.bkt.clouddn.com/14992368804491.png" alt=""></p>
<p>这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：<br><img src="http://omu7tit09.bkt.clouddn.com/14992369530915.png" alt=""><br>左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。</p>
<p>继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；</p>
<p>其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。</p>
<p>然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到：<br><img src="http://omu7tit09.bkt.clouddn.com/14992372618471.png" alt=""></p>
<p>在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。</p>
<h3 id="3-6-主成分分析"><a href="#3-6-主成分分析" class="headerlink" title="3.6 主成分分析"></a>3.6 主成分分析</h3><p>PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：<br><img src="http://omu7tit09.bkt.clouddn.com/14992353583255.png" alt=""></p>
<p>这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。</p>
<p>一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。</p>
<p> PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。</p>
<p> 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。</p>
<script type="math/tex; mode=display">
A_{m\times n}P_{n\times n}=\tilde{A}_{m\times n}</script><p>而将一个$m\times n$的矩阵$A$变换成一个$m\times r$的矩阵，这样就会使本来有$n$个feature的，变成了有$r$个feature了（r&lt;n），这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：</p>
<script type="math/tex; mode=display">
A_{m\times n}P_{n\times r}=\tilde{A}_{m\times r}</script><p>但是这个和SCD扯上关系的呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按照PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量。之前得到的SVD式子如下：</p>
<script type="math/tex; mode=display">A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}</script><p>在矩阵的两边同时乘上一个矩阵$V$，由于$V$是一个正交的矩阵，所以转置乘以$V$得到单位阵$I$，所以可以化成后面的式子：</p>
<script type="math/tex; mode=display">A_{m\times n}V_{r\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}V_{r\times n}=U_{m\times r} \Sigma_{r\times r}</script><p>将后面的式子与$A\times P $那个$m\times n$矩阵变换为$m\times r$的矩阵的式子对照看看，在这里，其实$V$就是$P$，也就是一个变化的向量。这里将一个$m\times n$的矩阵压缩到一个$m \ times r$的矩阵，也就是对列进行压缩。</p>
<p>如果我们想对行进行压缩（在PCA的观点下，对行进行业所可以理解为，讲一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉），同样我们写出一个通用的行压缩例子：</p>
<script type="math/tex; mode=display">
P_{r\times m}A_{m\times n}=\tilde{A}_{r\times n}</script><p>这样从一个$m$行的矩阵压缩到一个$r$行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以一个转置$U$得到：</p>
<script type="math/tex; mode=display">U^T_{r\times m}A_{m\times n}≈ U^T_{r\times m}U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}= \Sigma_{r\times r}V^T_{r\times n}</script><p> 这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SVD </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（8）：茶卡盐湖-青海湖之行]]></title>
      <url>/2017/06/26/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%888%EF%BC%89%EF%BC%9A%E8%8C%B6%E5%8D%A1%E7%9B%90%E6%B9%96-%E9%9D%92%E6%B5%B7%E6%B9%96%E4%B9%8B%E8%A1%8C/</url>
      <content type="html"><![CDATA[<blockquote>
<p>这是第一次去西北。<br>四天的行程，印象最深还是它的天，干净到不忍心去增添一点点其余的色彩，生怕破坏了它原本的样子。<br>这个废旧的火车头安详地躺在轨道上，依傍着茶卡盐湖，携带着机械复制时代内燃机车独有的刚毅在湖畔守候着，迎接着这个世纪的人，等待他们挥扬双手、露出笑容和它留下一张张合影。<br>置身这片广袤的土地，被它的空气包裹，只想安静地闭上眼睛，感受这个神奇世界的馈赠。</p>
</blockquote>
<a id="more"></a>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/FullSizeRender%202.jpg"></div><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/FullSizeRender%207.jpg"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/FullSizeRender%204.jpg"></div><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/IMG_3747.JPG"></div></div></div></div>
<blockquote>
<p>火车头底下铺着五十多米长的铁轨、垫着被岁月勾兑地五黑而厚重的木枕木，枕木中间均匀散落茶卡盐湖的结晶盐，像一道白色的光，顺着火车头，向后延展。<br>依旧是火车头，衣服的搭配和它看起来很契合，便在此处快门了好几十次。多么想把这轨道揉进苍茫的大地，它横在那里，丝毫没有感受到来自东方神域的呼喊，反而与四处来的人打成了一片。</p>
</blockquote>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/%E5%A4%A9%E7%A9%BA%E4%B9%8B%E9%95%9C%C2%B7%E7%AC%91%E9%9D%A5.jpg"></div><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/IMG_2012.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/IMG_3751.JPG"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/%E5%A4%A9%E7%A9%BA%E4%B9%8B%E9%95%9C%C2%B7%E9%93%81%E7%9A%AE%E8%BD%A6.jpg"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/FullSizeRender.jpg"></div></div></div></div>
<blockquote>
<p>那天风很疾，打在脸上，来不及躲闪。<br>坐在长凳子上，望着远处的山，它们似乎离自己很近，仅有的修饰就是顶上的那层雪，它不懂得如何变得招人喜欢，大自然给予它什么，它就接受什么样的。<br>我需要你冷静的表情。</p>
</blockquote>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3965.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://orsw4brg1.bkt.clouddn.com/%E5%A4%A9%E7%A9%BA%E4%B9%8B%E9%95%9C%C2%B7%E5%90%AC%E4%BA%91.jpg"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/FullSizeRender444.jpg"></div></div></div></div>
<blockquote>
<p>春天是什么样子的？<br>丹霞地貌，它的土是暗红色的。<br>阳光洒下来，透过你，穿过镜头，直抵我眼眶</p>
</blockquote>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_1906.JPG"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3660.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/IMG_3992.jpg"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/%E5%A4%A9%E7%A9%BA%E4%B9%8B%E9%95%9C%C2%B7%E6%B8%AF%E5%B2%9B%E5%A6%B9%E5%A6%B9.jpg"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://orsw4brg1.bkt.clouddn.com/IMG_3396.jpg"></div></div></div></div>
<blockquote>
<p>一大波风景照向你袭来</p>
</blockquote>
<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3493.JPG"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_4047.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_4052.JPG"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_1877.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3943.JPG"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3484.JPG"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_3464.JPG"></div><div class="group-picture-column" style="width: 50%;"><img src="http://omu7tit09.bkt.clouddn.com/IMG_1835.JPG"></div></div></div></div>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=139375&auto=1&height=66"></iframe>

]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 茶卡盐湖 </tag>
            
            <tag> 青海湖 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（20）：机器学习模型优化四要素]]></title>
      <url>/2017/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8820%EF%BC%89%EF%BC%9A%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E5%9B%9B%E8%A6%81%E7%B4%A0/</url>
      <content type="html"><![CDATA[<p>本文转载自<a href="http://tech.meituan.com/machine-learning-model-optimization.html" target="_blank" rel="noopener">美团点评技术团队博客</a>，该文以业界视角介绍了机器学习如何发挥其实际价值。作者胡淏，目前是美团算法工程师，毕业于哥伦比亚大学。先后在携程、支付宝、美团从事算法开发工作。了解风控、基因、旅游、即时物流相关问题的行业领先算法方案与流程。</p>
<a id="more"></a>
<h2 id="一、机器学习工程师的知识图谱"><a href="#一、机器学习工程师的知识图谱" class="headerlink" title="一、机器学习工程师的知识图谱"></a>一、机器学习工程师的知识图谱</h2><p><img src="http://orsw4brg1.bkt.clouddn.com/image_1bgvc1hi3led5tl1hb0n1i1hhm9.png" alt="image_1bgvc1hi3led5tl1hb0n1i1hh"><br>图1 机器学习工程师的知识图谱</p>
<p>上图列出了我认为一个成功的机器学习工程师需要关注和积累的点。机器学习实践中，我们平时都在积累自己的“弹药库”：分类、回归、无监督模型、Kaggle上特征变换的黑魔法、样本失衡的处理办法、缺失值填充……这些大概可以归类成模型和特征两个点。我们需要参考成熟的做法、论文，并自己实现，此外还需要多反思自己方法上是否还可以改进。如果模型和特征这俩个点都已经做的很好了，你就拥有了一张绿卡，能跨过在数据相关行业发挥模型技术价值的准入门槛。</p>
<p>在这个时候，比较关键的一步，就是搞笑的技术变现能力。</p>
<p>所谓高效，就是解决业务核心问题的专业能力。本文将描述这些专业能力，也就是模型优化的四个要素：模型、数据、特征、业务，还有更重要的，就是他们在模型项目中的优先级。</p>
<h2 id="二、模型项目推进的四要素"><a href="#二、模型项目推进的四要素" class="headerlink" title="二、模型项目推进的四要素"></a>二、模型项目推进的四要素</h2><p>项目推进过程中，四个要素相互之间的优先级大致是：业务&gt;特征&gt;数据&gt;模型。<br><img src="http://orsw4brg1.bkt.clouddn.com/image_1bgvc94uuh0c1o0sj9d1urb19d2m.png" alt="image_1bgvc94uuh0c1o0sj9d1urb19d2"><br>图2 四要素解决问题细分+优先级</p>
<h3 id="2-1-业务"><a href="#2-1-业务" class="headerlink" title="2.1 业务"></a>2.1 业务</h3><p>一个模型项目有好的技术选型、完备的特征体系、高质量的数据一定是很加分的，不过真正决定项目好与坏还有一个大前提，就是在这个项目的技术目标是否在解决当下核心业务问题。</p>
<p>业务问题包含两个方面：业务KPI和Deadline。举个例子，业务问题在两周之内降低目前手机丢失带来的支付宝销赃风险。这时如果你的方案是研发手机丢失的核心特征，比如改密是否合理，基本上就死的很惨，因为两周根本完不成，改密合理性也未必是模型优化好的切入点；反之，如果你的方案是和运营同学看bad case，梳理现阶段的作案通用手段，并通过分析上线一个简单模型或者业务规则的补丁，就明智很多。如果上线之后，案件量真掉下来了，就算你的方案准确率很糟糕、方法很low，但你解决了业务问题，这才是最重要的。</p>
<p>虽然业务目标很关键，不过一般讲，业务运营同学真的不太懂得如何和技术有效的沟通业务目标，比如：</p>
<ul>
<li>我们想做一个线下门店风险评级的项目，希望运营通过反作弊模型角度帮我们给门店打个分，这个分数包含的问题有：风险是怎么定义的、为什么要做风险评级、更大的业务目标是什么、怎么排期的、这个风险和我们反作弊模型之间的腋窝你是怎么看的？</li>
<li>做一个区域未来10min的配送时间预估模型。我们想通过运营的模型衡量在恶劣天气的时候每个区域的运力是否被击穿（业务现状和排期？运力被击穿可以扫下盲吗？运力击穿和配送时间之间是个什么业务逻辑、时间预估是刻画运力紧张度的最有效手段么？业务的关键场景是恶劣天气的话，我们仅仅训练恶劣天气场景的时间预估模型是否就好了？）</li>
</ul>
<p>为了保证整个技术项目没有做偏，项目一开始一定要和业务聊清楚三件事情：</p>
<ul>
<li><p><strong>业务核心问题、关键场景是什么。</strong></p>
</li>
<li><p><strong>如何评估该项目的成功，指标是什么。</strong></p>
</li>
<li><p><strong>通过项目输出什么关键信息给到业务，业务如何运营这个信息从而达到业务目标。</strong></p>
</li>
</ul>
<p>项目过程中，也要时刻回到业务，检查项目的健康度。</p>
<h3 id="2-2-数据与特征"><a href="#2-2-数据与特征" class="headerlink" title="2.2 数据与特征"></a>2.2 数据与特征</h3><p>要说正确的业务理解和切入，在为技术项目保驾护航，数据、特征便是一个模型项目性能方面的天花板。garbage in， garbage out 就在说这个问题。</p>
<p>这两天有位听众微信问我一个很难回答的问题，大概意思是，数据是特征拼起来构成的集合嘛，所以这不是两个要素。从逻辑上面讲，数据的确是一列一列的特征，不过数据与特征在概念层面是不同的：数据是已经采集的信息，特征是以兼容模型、最优化为目标对数据进行加工。就比如通过word2vec将非结构化数据结构化，就是将数据转化为特征的过程。</p>
<p>所以，我更认为特征工程是基于数据的一个非常精细、刻意的加工过程。从传统的特征转换、交互，到embedding、word2vec、高维分类变量数值化，最终目的都是更好的去利用现有的数据。之前有聊到的将推荐算法引入有监督学习模型优化中的做法，就是在把两个本不可用的高维ID类变量变成可用的数值变量。</p>
<p>观察到自己和童鞋们在特征工程中会遇到一些普遍问题，比如，特征设计不全面，没有耐心把现有特征做得细致……也整理出来一套方法论，仅供参考：</p>
<p><img src="http://orsw4brg1.bkt.clouddn.com/image_1bgvcqct213oq6ingm1v1c1eti13.png" alt="image_1bgvcqct213oq6ingm1v1c1eti13"><br>图3 变量体系、研发流程</p>
<p>在特征设计的时候，有两个点可以帮助我们把特征想的更全面：</p>
<ul>
<li><p><strong>现有的基础数据</strong></p>
</li>
<li><p><strong>业务“二维图”</strong></p>
</li>
</ul>
<p>这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子：</p>
<p>这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子：</p>
<p>外卖配送时间业务（维度甲：配送的环节，骑手到点、商家出餐、骑手配送、交付用户；维度乙：颗粒度，订单粒度、商家粒度、区域城市粒度；维度丙：配送类型，众包、自营……）。</p>
<p>反作弊变量体系（维度甲：作弊环节，登录、注册、实名、转账、交易、参与营销活动、改密……；维度乙：作弊介质，账户、设备、IP、WiFi、银行卡……）。</p>
<p>通过这些维度，你就可以展开一个“二维图”，把现有你可以想到的特征填上去，你一定会发现很多空白，比如下图，那么哪里还是特征设计的盲点就一目了然：</p>
<p><img src="http://orsw4brg1.bkt.clouddn.com/image_1bgvcvkkqo0q1kgt15qjdv54jm1g.png" alt="image_1bgvcvkkqo0q1kgt15qjdv54jm1g"><br>图4 账户维度在转账、红包方面的特征很少；没有考虑WiFi这个媒介；客满与事件数据没考虑</p>
<p>数据和特征决定了模型性能的天花板。deep learning当下在图像、语音、机器翻译、自动驾驶等领域非常火，但是 deep learning在生物信息、基因学这个领域就不是热词：这背后是因为在前者，我们已经知道数据从哪里来，怎么采集，这些数据带来的信息基本满足了模型做非常准确的识别；而后者，即便有了上亿个人体碱基构成的基因编码，技术选型还是不能长驱直入——超高的数据采集成本，人后天的行为数据的获取壁垒等一系列的问题，注定当下这个阶段在生物信息领域，人工智能能发出的声音很微弱，更大的舞台留给了生物学、临床医学、统计学。</p>
<h3 id="2-3-模型"><a href="#2-3-模型" class="headerlink" title="2.3 模型"></a>2.3 模型</h3><p><img src="http://orsw4brg1.bkt.clouddn.com/image_1bgvd7gs41k9e1g0s1d016dmdq1t.png" alt="image_1bgvd7gs41k9e1g0s1d016dmdq1t"><br>图5 满房开房的技术选型、特征工程roadmap</p>
<p>模型这件事儿，许多时候追求的不仅仅是准确率，通常还有业务这一层更大的约束。如果你在做一些需要强业务可解释的模型，比如定价和反作弊，那实在没必要上一个黑箱模型来为难业务。这时候，统计学习模型就很有用。</p>
<p>这种情况下，比拼性能的话，我觉得下面这个不等式通常成立：<code>Glmnet&gt;LASSO&gt;=Ridge&gt;LR/Logistic</code>。相比最基本的LR/Logistic，ridge通过正则化约束缓解了LR在过拟合方面的问题，lasso更是通过L1约束做类似变量选择的工作。</p>
<p>不过两个算法的痛点是很难决定最优的约束强度，Glmnet是Stanford给出的一套非常高效的解决方案。所以目前，我认为线性结构的模型，Glmnet的痛点是最少的，而且在R、Python、Spark上面都开源了。</p>
<p>如果我们开发复杂模型，通常成立第二个不等式 <code>RF（Random Forest，随机森林）&lt;= GBDT &lt;= XGBoost</code> 。拿数据说话，29个Kaggle公开的winner solution里面，17个使用了类似GBDT这样的Boosting框架，其次是 DNN（Deep Neural Network，深度神经网络），RF的做法在Kaggle里面非常少见。</p>
<p>RF和GBDT两个算法的雏形是CART（Classification And Regression Trees），由L Breiman和J Friedman两位作者在1984年合作推出。但是在90年代在发展模型集成思想the ensemble的时候，两位作者代表着两个至今也很主流的派系：stacking/ Bagging &amp; Boosting。</p>
<p>一种是把相互独立的CART（randomized variables，bootstrap samples）水平铺开，一种是深耕的Boosting，在拟合完整体后更有在局部长尾精细刻画的能力。同时，GBDT模型相比RF更加简单，内存占用小，这都是业界喜欢的性质。XGBoost在模型的轻量化和快速训练上又做了进一步的工作，也是目前我们比较喜欢尝试的模型。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 业务 </tag>
            
            <tag> 特征 </tag>
            
            <tag> 数据 </tag>
            
            <tag> 模型 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kaggle系列（4）：Rental Listing Inquiries（三）：XGBoost调参指南]]></title>
      <url>/2017/06/14/kaggle%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AXGBoost%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/</url>
      <content type="html"><![CDATA[<p>这篇文章翻译自<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener">Complete Guide to Parameter Tuning in XGBoost (with codes in Python)</a>，它详细介绍了XGBoost中参数的含义，然后在一个实例中对参数调整进行了实验。</p>
<a id="more"></a>
<h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><p>如果你的预测模型效果不怎么好，使用XGBoost吧。XGBoost已经成为许多数据科学家的终极武器了！这是一个内部实现高度复杂的算法，在处理各种不规范的数据时有足够强大的表现。</p>
<p>利用XGBoost建立模型很简单，但是因为它使用了很多参数，以致使用XGBoost来提升预测效果比较有难度。为提高模型的预测能力，调参是必须要做的一步。但仍然有很多现实的挑战——哪些参数是我们需要调整的？每个参数的最佳值又应该是多少呢？</p>
<p>这篇文章最适合刚刚接触XGBoost的人，在这篇文章中，我们将会介绍一些XGBoost相关的知识，同时了解一些XGBoost参数调整技艺。最后使用Python对一个数据集实践XGBoost。</p>
<h2 id="二、What-should-you-know"><a href="#二、What-should-you-know" class="headerlink" title="二、What should you know ?"></a>二、What should you know ?</h2><p>XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。因为我在前一篇文章，<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">基于Python的Gradient Boosting算法参数调整完全指南</a>，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。</p>
<p>特别鸣谢：我个人十分感谢Mr Sudalai Rajkumar (aka SRK)大神的支持，目前他在AV Rank中位列第二。如果没有他的帮助，就没有这篇文章。在他的帮助下，我们才能给无数的数据科学家指点迷津。给他一个大大的赞！</p>
<h2 id="三、Table-of-Contents"><a href="#三、Table-of-Contents" class="headerlink" title="三、Table of Contents"></a>三、Table of Contents</h2><h3 id="3-1-The-XGBoost-Advantage"><a href="#3-1-The-XGBoost-Advantage" class="headerlink" title="3.1 The XGBoost Advantage"></a>3.1 The XGBoost Advantage</h3><p>XGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势：</p>
<ul>
<li><p>正则化<br>标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。</p>
</li>
<li><p>并行处理<br>不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢？每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。<br>XGBoost 也支持Hadoop实现。</p>
</li>
<li><p>高度的灵活性<br>XGBoost 允许用户定义自定义优化目标和评价标准<br>它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</p>
</li>
<li><p>缺失值处理<br>XGBoost内置处理缺失值的规则。用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</p>
</li>
<li><p>剪枝<br>当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。<br>XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。<br>这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</p>
</li>
<li><p>内置交叉验证<br>XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而GBM使用网格搜索，只能检测有限个值。</p>
</li>
<li><p>在已有的模型基础上继续<br>XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。<br>sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。</p>
</li>
</ul>
<p>相信你已经对XGBoost强大的功能有了点概念。注意这是我自己总结出来的几点，你如果有更多的想法，尽管在下面评论指出，我会更新这个列表的！<br>你的胃口被我吊起来了吗？棒棒哒！如果你想更深入了解相关信息，可以参考下面这些文章：</p>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="noopener">XGBoost Guide - Introduce to Boosted Trees </a></p>
<p><a href="https://www.youtube.com/watch?v=X47SGnTMZIU" target="_blank" rel="noopener">Words from the Auther of XGBoost </a></p>
<h3 id="3-2-Understanding-XGBoost-Parameters"><a href="#3-2-Understanding-XGBoost-Parameters" class="headerlink" title="3.2 Understanding XGBoost Parameters"></a>3.2 Understanding XGBoost Parameters</h3><p>XGBoost的作者把所有参数分成了三类：</p>
<ul>
<li>通用参数：宏观函数控制。</li>
<li>Booster参数：控制每一步的Booster（tree/regression）</li>
<li>学习目标参数：控制训练目标的表现</li>
</ul>
<p>在这里我会类比GBM来讲解，所以作为一种基础知识，强烈推荐先阅读<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">这篇文章</a>。</p>
<h4 id="3-2-1-通用参数（General-Parameters）"><a href="#3-2-1-通用参数（General-Parameters）" class="headerlink" title="3.2.1 通用参数（General Parameters）"></a>3.2.1 通用参数（General Parameters）</h4><p>这些参数用来控制XGBoost的宏观功能。</p>
<ul>
<li><p><strong>booster[默认gbtree]</strong>：选择每次迭代的模型，有两种选择： </p>
<ul>
<li>gbtree：基于树的模型 </li>
<li>gbliner：线性模型</li>
</ul>
</li>
<li><p><strong>silent[默认0]</strong>：</p>
<ul>
<li>当这个参数值为1时，静默模式开启，不会输出任何信息。一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</li>
</ul>
</li>
<li><p><strong>nthread[默认值为最大可能的线程数]</strong>：</p>
<ul>
<li>这个参数用来进行多线程控制，应当输入系统的核数。如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。</li>
</ul>
</li>
</ul>
<p>还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。</p>
<h4 id="3-2-2-Booster参数（Booster-Parameters）"><a href="#3-2-2-Booster参数（Booster-Parameters）" class="headerlink" title="3.2.2 Booster参数（Booster Parameters）"></a>3.2.2 Booster参数（Booster Parameters）</h4><p>尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</p>
<ul>
<li><p><strong>eta[默认0.3]：</strong></p>
<ul>
<li>和GBM中的 learning rate 参数类似。通过减少每一步的权重，可以提高模型的鲁棒性。典型值为0.01-0.2。</li>
</ul>
</li>
<li><p><strong>min_child_weight[默认1]</strong>：</p>
<ul>
<li>决定最小叶子节点样本权重和。和GBM的min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</li>
</ul>
</li>
<li><p><strong>max_depth[默认6]：</strong></p>
<ul>
<li>和GBM中的参数相同，这个值为树的最大深度。这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。需要使用CV函数来进行调优。典型值：3-10</li>
</ul>
</li>
<li><p><strong>max_leaf_nodes：</strong></p>
<ul>
<li>树上最大的节点或叶子的数量。可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成 $n^2$ 个叶子。如果定义了这个参数，GBM会忽略max_depth参数。</li>
</ul>
</li>
<li><p><strong>gamma[默认0]：</strong></p>
<ul>
<li>在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</li>
</ul>
</li>
<li><p><strong>max_delta_step[默认0]：</strong></p>
<ul>
<li>这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。</li>
<li>通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。</li>
<li>这个参数一般用不到，但是你可以挖掘出来它更多的用处。</li>
</ul>
</li>
<li><p><strong>subsample[默认1]：</strong></p>
<ul>
<li>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。典型值：0.5-1。</li>
</ul>
</li>
<li><p><strong>colsample_bytree[默认1]：</strong></p>
<ul>
<li>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。典型值：0.5-1</li>
</ul>
</li>
<li><p><strong>colsample_bylevel[默认1]：</strong></p>
<ul>
<li>用来控制树的每一级的每一次分裂，对列数的采样的占比。我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</li>
</ul>
</li>
<li><p><strong>lambda[默认1]</strong></p>
<ul>
<li>权重的L2正则化项。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。</li>
</ul>
</li>
<li><p><strong>alpha[默认1]：</strong></p>
<ul>
<li>权重的L1正则化项。(和Lasso regression类似)。可以应用在很高维度的情况下，使得算法的速度更快。</li>
</ul>
</li>
<li><p><strong>scale_pos_weight[默认1]：</strong></p>
<ul>
<li>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。</li>
</ul>
</li>
</ul>
<h4 id="3-2-3-学习目标参数"><a href="#3-2-3-学习目标参数" class="headerlink" title="3.2.3 学习目标参数"></a>3.2.3 学习目标参数</h4><p>这个参数用来控制理想的优化目标和每一步结果的度量方法。</p>
<ul>
<li><strong>objective[默认reg:linear]：</strong>这个参数定义需要被最小化的损失函数。最常用的值有： <ul>
<li>binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。</li>
<li>multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。</li>
<li>multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</li>
</ul>
</li>
<li><strong>eval_metric[默认值取决于objective参数的取值]：</strong>对于有效数据的度量方法。对于回归问题，默认值是rmse，对于分类问题，默认值是error。典型值有： <ul>
<li>rmse 均方根误差( ∑Ni=1ϵ2N−−−−−−√ )</li>
<li>mae 平均绝对误差( ∑Ni=1|ϵ|N )</li>
<li>logloss 负对数似然函数值</li>
<li>error 二分类错误率(阈值为0.5)</li>
<li>merror 多分类错误率</li>
<li>mlogloss 多分类logloss损失函数</li>
<li>auc 曲线下面积</li>
</ul>
</li>
<li><strong>seed(默认0)：</strong>随机数的种子。设置它可以复现随机数据的结果，也可以用于调整参数</li>
</ul>
<p>如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是：</p>
<ul>
<li>eta -&gt;learning_rate </li>
<li>lambda-&gt;reg_lambda </li>
<li>alpha-&gt;reg_alpha </li>
</ul>
<p>你肯定在疑惑为啥咱们没有介绍和GBM中的<code>n_estimators</code>类似的参数。XGBClassifier中确实有一个类似的参数，但是，是在标准XGBoost实现中调用拟合函数时，把它作为<code>num_boosting_rounds</code>参数传入。 </p>
<p>XGBoost Guide 的一些部分是我强烈推荐大家阅读的，通过它可以对代码和参数有一个更好的了解：</p>
<p><a href="http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters" target="_blank" rel="noopener">XGBoost Parameters (official guide) </a></p>
<p><a href="https://github.com/dmlc/xgboost/tree/master/demo/guide-python" target="_blank" rel="noopener">XGBoost Demo Codes (xgboost GitHub repository)</a></p>
<p><a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html" target="_blank" rel="noopener">Python API Reference (official guide)</a></p>
<h3 id="3-3-Tuning-Parameters-with-Example"><a href="#3-3-Tuning-Parameters-with-Example" class="headerlink" title="3.3 Tuning Parameters (with Example)"></a>3.3 Tuning Parameters (with Example)</h3><ul>
<li>未完待续······</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> kaggle </tag>
            
            <tag> XGBoost </tag>
            
            <tag> 调参 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kaggle系列（3）：Rental Listing Inquiries（二）：XGBoost]]></title>
      <url>/2017/06/13/kaggle%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9AXGBoost/</url>
      <content type="html"><![CDATA[<p>上一节我们对数据集进行了初步的探索，并将其可视化，对数据有了初步的了解。这样我们有了之前数据探索的基础之后，就有了对其建模的基础feature，结合目标变量，即可进行模型训练了。我们使用交叉验证的方法来判断线下的实验结果，也就是把训练集分成两部分，一部分是训练集，用来训练分类器，另一部分是验证集，用来计算损失评估模型的好坏。</p>
<a id="more"></a>
<p>在Kaggle的希格斯子信号识别竞赛中，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。</p>
<p>今天，我们就先来跑一个XGBoost版的Base Model。先回顾一下XGBoost的原理吧：<a href="https://plushunter.github.io/2017/01/26/机器学习算法系列（8）：XgBoost/" target="_blank" rel="noopener">机器学习算法系列（8）：XgBoost</a></p>
<h2 id="一、-准备工作"><a href="#一、-准备工作" class="headerlink" title="一、 准备工作"></a>一、 准备工作</h2><p>首先我们导入需要的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> sys </div><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</div><div class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection,preprocessing,ensemble</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer,CountVectorizer</div></pre></td></tr></table></figure>
<p>其中一些包的用途会在之后具体用到的时候进行讲解。</p>
<p>导入我们的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">data_path = <span class="string">'../data/'</span></div><div class="line">train_file = data_path + <span class="string">"train.json"</span></div><div class="line">test_file = data_path +<span class="string">"test.json"</span></div><div class="line">train_df = pd.read_json(train_file)</div><div class="line">test_df = pd.read_json(test_file)</div><div class="line"><span class="keyword">print</span> train_df.shape</div><div class="line"><span class="keyword">print</span> test_df.shape</div><div class="line"></div><div class="line">(<span class="number">49352</span>, <span class="number">15</span>)</div><div class="line">(<span class="number">74659</span>, <span class="number">14</span>)</div></pre></td></tr></table></figure>
<p>查看一下前两行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_df.head(<span class="number">2</span>)</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%884.48.02.png" alt="屏幕快照 2017-06-19 下午4.48.02"><br><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%884.48.20.png" alt="屏幕快照 2017-06-19 下午4.48.20"></p>
<h2 id="二、特征构建"><a href="#二、特征构建" class="headerlink" title="二、特征构建"></a>二、特征构建</h2><p>我们不需要对数值型数据进行任何的预处理，所以首先建立一个数值型特征的列表，纳入features_to_use</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">features_to_use = [<span class="string">"bathrooms"</span>,<span class="string">"bedrooms"</span>,<span class="string">"latitude"</span>,<span class="string">"longitude"</span>,<span class="string">"price"</span>]</div></pre></td></tr></table></figure>
<p>现在让我们根据已有的一些特征来构建一些新的特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 照片数量(num_photos)</span></div><div class="line">train_df[<span class="string">'num_photos'</span>]=train_df[<span class="string">'photos'</span>].apply(len)</div><div class="line">test_df[<span class="string">'num_photos'</span>]=train_df[<span class="string">'photos'</span>].apply(len)</div><div class="line"></div><div class="line"><span class="comment"># 特征数量</span></div><div class="line">train_df[<span class="string">'num_features'</span>]=train_df[<span class="string">'features'</span>].apply(len)</div><div class="line">test_df[<span class="string">'num_features'</span>]=test_df[<span class="string">'features'</span>].apply(len)</div><div class="line"></div><div class="line"><span class="comment"># 描述词汇数量</span></div><div class="line">train_df[<span class="string">'num_description_words'</span>] = train_df[<span class="string">'description'</span>].apply(<span class="keyword">lambda</span> x: len(x.split(<span class="string">" "</span>)))</div><div class="line">test_df[<span class="string">'num_description_words'</span>] = test_df[<span class="string">'description'</span>].apply(<span class="keyword">lambda</span> x: len(x.split(<span class="string">" "</span>)))</div><div class="line"></div><div class="line"><span class="comment">#把创建的时间分解为多个特征        </span></div><div class="line">train_df[<span class="string">'created'</span>]=pd.to_datetime(train_df[<span class="string">'created'</span>])</div><div class="line">test_df[<span class="string">'created'</span>]=pd.to_datetime(test_df[<span class="string">'created'</span>])</div><div class="line"> </div><div class="line"><span class="comment">#让我们从时间中分解出一些特征，比如年，月，日，时</span></div><div class="line"><span class="comment">#年</span></div><div class="line">train_df[<span class="string">'created_year'</span>] = train_df[<span class="string">'created'</span>].dt.year</div><div class="line">test_df[<span class="string">'created_year'</span>] = test_df[<span class="string">'created'</span>].dt.year</div><div class="line"><span class="comment">#月</span></div><div class="line">train_df[<span class="string">'created_month'</span>] = train_df[<span class="string">'created'</span>].dt.month</div><div class="line">test_df[<span class="string">'created_month'</span>] = test_df[<span class="string">'created'</span>].dt.month</div><div class="line"><span class="comment">#日</span></div><div class="line">train_df[<span class="string">'created_day'</span>] = train_df[<span class="string">'created'</span>].dt.day</div><div class="line">test_df[<span class="string">'created_day'</span>] = test_df[<span class="string">'created'</span>].dt.day</div><div class="line"><span class="comment">#时</span></div><div class="line">train_df[<span class="string">'created_hour'</span>] = train_df[<span class="string">'created'</span>].dt.hour</div><div class="line">test_df[<span class="string">'created_hour'</span>] = test_df[<span class="string">'created'</span>].dt.hour</div><div class="line"></div><div class="line"><span class="comment">#把这些特征都放到所需特征列表中（上面已经创建，并加入了数值型特征） </span></div><div class="line">features_to_use.extend([<span class="string">"num_photos"</span>,<span class="string">"num_features"</span>,<span class="string">"num_description_words"</span>,<span class="string">"created_year"</span>,<span class="string">"created_month"</span>,<span class="string">"created_day"</span>,<span class="string">"created_hour"</span>,<span class="string">"listing_id"</span>])</div></pre></td></tr></table></figure>
<p>我们有四个分类型的特征：</p>
<ul>
<li>display_address</li>
<li>manager_id</li>
<li>building_id</li>
<li>street_address</li>
</ul>
<p>可以对它们分别进行特征编码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">categorical = [<span class="string">"display_address"</span>,<span class="string">"manager_id"</span>,<span class="string">'building_id'</span>,<span class="string">"street_address"</span>]</div><div class="line"><span class="keyword">for</span> f <span class="keyword">in</span> categorical:</div><div class="line">    <span class="keyword">if</span> train_df[f].dtype == <span class="string">'object'</span>:</div><div class="line">        lbl = preprocessing.LabelEncoder()</div><div class="line">        lbl.fit(list(train_df[f].values)+list(test_df[f].values))</div><div class="line">        train_df[f] = lbl.transform(list(train_df[f].values))</div><div class="line">        test_df[f] = lbl.transform(list(test_df[f].values))</div><div class="line">        features_to_use.append(f)</div></pre></td></tr></table></figure>
<p>还有一些字符串类型的特征，可以先把它们合并起来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">train_df[&quot;features&quot;] = train_df[&quot;features&quot;].apply(lambda x:&quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))</div><div class="line">print train_df[&apos;features&apos;].head(2)</div><div class="line">test_df[&apos;features&apos;] = test_df[&quot;features&quot;].apply(lambda x: &quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))</div><div class="line">print test_df[&apos;features&apos;].head(2)</div></pre></td></tr></table></figure>
<p>得到的字符串结果如下：</p>
<p>10000     Doorman Elevator Fitness_Center Cats_Allowed D…<br>100004    Laundry_In_Building Dishwasher Hardwood_Floors…</p>
<p>然后CountVectorizer类来计算TF-IDF权重</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tfidf = CountVectorizer(stop_words =&quot;english&quot;,max_features=200)</div><div class="line">tr_sparse = tfidf.fit_transform(train_df[&quot;features&quot;])</div><div class="line">te_sparse = tfidf.transform(test_df[&quot;features&quot;])</div></pre></td></tr></table></figure>
<p>这里我们需要提一点，对数据集进行特征变换时，必须同时对训练集和测试集进行操作。现在把这些处理过的特征放到一个集合中（横向合并）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_X = sparse.hstack([train_df[features_to_use],tr_sparse]).tocsr()</div><div class="line">test_X = sparse.hstack([test_df[features_to_use],te_sparse]).tocsr()</div></pre></td></tr></table></figure>
<p>然后把目标变量转换为0、1、2，如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">target_num_map = &#123;<span class="string">'high'</span>:<span class="number">0</span> , <span class="string">'medium'</span>:<span class="number">1</span> , <span class="string">'low'</span>:<span class="number">2</span>&#125;</div><div class="line">train_y = np.array(train_df[<span class="string">'interest_level'</span>].apply(<span class="keyword">lambda</span> x: target_num_map[x]))</div><div class="line"><span class="keyword">print</span> train_X.shape,test_X.shape</div><div class="line"></div><div class="line">(<span class="number">49352</span>, <span class="number">217</span>) (<span class="number">74659</span>, <span class="number">217</span>)</div></pre></td></tr></table></figure>
<p>可以看到，经过上面一系列的变量构造之后，其数量已经达到了217个。</p>
<p>接下来就可以进行建模啦。</p>
<h2 id="三、XGB建模"><a href="#三、XGB建模" class="headerlink" title="三、XGB建模"></a>三、XGB建模</h2><p>先写一个通用的XGB模型的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">runXGB</span><span class="params">(train_X,train_y,test_X,test_y=None,feature_names=None,seed_val=<span class="number">0</span>,num_rounds=<span class="number">1000</span>)</span>:</span></div><div class="line">    <span class="comment">#参数设定</span></div><div class="line">    param = &#123;&#125;</div><div class="line">    param[<span class="string">'objective'</span>] = <span class="string">'multi:softprob'</span><span class="comment">#多分类、输出概率值</span></div><div class="line">    param[<span class="string">'eta'</span>] = <span class="number">0.1</span><span class="comment">#学习率</span></div><div class="line">    param[<span class="string">'max_depth'</span>] = <span class="number">6</span><span class="comment">#最大深度，越大越容易过拟合</span></div><div class="line">    param[<span class="string">'silent'</span>] = <span class="number">1</span><span class="comment">#打印提示信息</span></div><div class="line">    param[<span class="string">'num_class'</span>] = <span class="number">3</span><span class="comment">#三个类别</span></div><div class="line">    param[<span class="string">'eval_metric'</span>]= <span class="string">"mlogloss"</span><span class="comment">#对数损失</span></div><div class="line">    param[<span class="string">'min_child_weight'</span>]=<span class="number">1</span><span class="comment">#停止条件，这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></div><div class="line">    param[<span class="string">'subsample'</span>] =<span class="number">0.7</span><span class="comment">#随机采样训练样本</span></div><div class="line">    param[<span class="string">'colsample_bytree'</span>] = <span class="number">0.7</span><span class="comment"># 生成树时进行的列采样</span></div><div class="line">    param[<span class="string">'seed'</span>] = seed_val<span class="comment">#随机数种子</span></div><div class="line">    num_rounds = num_rounds<span class="comment">#迭代次数</span></div><div class="line">    </div><div class="line">    plst = list(param.items())</div><div class="line">    xgtrain = xgb.DMatrix(train_X,label=train_y)</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> test_y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        xgtest = xgb.DMatrix(test_X,label=test_y)</div><div class="line">        watchlist = [(xgtrain,<span class="string">'train'</span>),(xgtest,<span class="string">'test'</span>)]</div><div class="line">        model = xgb.train(plst,xgtrain,num_rounds,watchlist,early_stopping_rounds=<span class="number">20</span>)</div><div class="line">      <span class="comment">#  early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        xgtest = xgb.DMatrix(test_X)</div><div class="line">        model = xgb.train(plst,xgtrain,num_rounds)</div><div class="line">    pred_test_y = model.predict(xgtest)</div><div class="line">    <span class="keyword">return</span> pred_test_y,model</div></pre></td></tr></table></figure>
<p>函数返回的是预测值和模型。</p>
<p>5折交叉验证将训练集划分为五份，其中的一份作为验证集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">cv_scores = []</div><div class="line">kf = model_selection.KFold(n_splits=<span class="number">5</span>,shuffle=<span class="keyword">True</span>,random_state=<span class="number">2016</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> dev_index,val_index <span class="keyword">in</span> kf.split(range(train_X.shape[<span class="number">0</span>])):</div><div class="line">    dev_X,val_X = train_X[dev_index,:],train_X[val_index,:]</div><div class="line">    dev_y,val_y = train_y[dev_index],train_y[val_index]</div><div class="line">    pred,model = runXGB(dev_X,dev_y,val_X,val_y)</div><div class="line">    cv_scores.append(log_loss(val_y,preds))</div><div class="line">    <span class="keyword">print</span> cv_scores</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">[0]	train-mlogloss:1.04135	test-mlogloss:1.04229</div><div class="line">Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.</div><div class="line"></div><div class="line">Will train until test-mlogloss hasn't improved in 20 rounds.</div><div class="line">[1]	train-mlogloss:0.989004	test-mlogloss:0.99087</div><div class="line">[2]	train-mlogloss:0.944233	test-mlogloss:0.947047</div><div class="line">[3]	train-mlogloss:0.90536	test-mlogloss:0.908933</div><div class="line">[4]	train-mlogloss:0.872054	test-mlogloss:0.876526</div><div class="line">[5]	train-mlogloss:0.841783	test-mlogloss:0.847383</div><div class="line">[6]	train-mlogloss:0.815921	test-mlogloss:0.822307</div><div class="line">[7]	train-mlogloss:0.793337	test-mlogloss:0.800476</div><div class="line">[8]	train-mlogloss:0.773562	test-mlogloss:0.781413</div><div class="line">[9]	train-mlogloss:0.754927	test-mlogloss:0.76381</div><div class="line">[10]	train-mlogloss:0.738299	test-mlogloss:0.747959</div><div class="line"></div><div class="line">······</div><div class="line">······</div><div class="line">[367]	train-mlogloss:0.348196	test-mlogloss:0.548011</div><div class="line">[368]	train-mlogloss:0.347768	test-mlogloss:0.547992</div><div class="line">[369]	train-mlogloss:0.347303	test-mlogloss:0.548021</div><div class="line">[370]	train-mlogloss:0.346807	test-mlogloss:0.548065</div><div class="line">[371]	train-mlogloss:0.346514	test-mlogloss:0.548079</div><div class="line">[372]	train-mlogloss:0.34615	test-mlogloss:0.548097</div><div class="line">[373]	train-mlogloss:0.345859	test-mlogloss:0.548111</div><div class="line">[374]	train-mlogloss:0.345377	test-mlogloss:0.548081</div><div class="line">[375]	train-mlogloss:0.344961	test-mlogloss:0.548068</div><div class="line">[376]	train-mlogloss:0.344493	test-mlogloss:0.548024</div><div class="line">[377]	train-mlogloss:0.344086	test-mlogloss:0.547975</div><div class="line">Stopping. Best iteration:</div><div class="line">[357]	train-mlogloss:0.352182	test-mlogloss:0.547867</div></pre></td></tr></table></figure>
<p>迭代357次之后，在训练集上的对数损失为0.352182，在验证集上的损失为0.5478。</p>
<p>然后在对测试集进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">preds,model=runXGB(train_X,train_y,test_X,num_rounds=<span class="number">400</span>)</div></pre></td></tr></table></figure>
<p>把结果按照比赛规定的格式写入csv文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">out_df = pd.DataFrame(preds)</div><div class="line">out_df.columns = [<span class="string">"high"</span>, <span class="string">"medium"</span>, <span class="string">"low"</span>]</div><div class="line">out_df[<span class="string">"listing_id"</span>] = test_df.listing_id.values</div><div class="line">out_df.to_csv(<span class="string">"xgb_starter2.csv"</span>, index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p>看一下最后的结果：<br><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-20%20%E4%B8%8A%E5%8D%8810.50.42.png" alt="屏幕快照 2017-06-20 上午10.50.42"><br>提交到kaggle上，这样我们整个建模的过程就完成了。</p>
<p>接下来两节中，我们重点讲一讲关于XGBoost的调参经验以及使用SK-learn计算TF-IDF。</p>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 特征工程 </tag>
            
            <tag> kaggle </tag>
            
            <tag> XGBoost </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kaggle系列（2）：Rental Listing Inquiries（一）：EDA]]></title>
      <url>/2017/06/13/kaggle%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AEDA/</url>
      <content type="html"><![CDATA[<h2 id="一、比赛简介"><a href="#一、比赛简介" class="headerlink" title="一、比赛简介"></a>一、比赛简介</h2><h3 id="1-1-比赛目的"><a href="#1-1-比赛目的" class="headerlink" title="1.1 比赛目的"></a>1.1 比赛目的</h3><p>这个kaggle比赛是由Sigma和RentHop两家公司共同推出的比赛。比赛的数据来自于RentHop的租房信息，大概的思路就是根据出租房的一系列特征，比如地理位置（经纬度、街道地址）、发布时间、房间设施（浴室、卧室数量）、描述信息、发布的图片信息、价格等来预测消费者对出租房的喜好程度。</p>
<p>这样可以帮助RentHop公司更好地处理欺诈事件，让房主和中介更加理解租客的需求与偏好，做出更加合理的决策。</p>
<a id="more"></a>
<h3 id="1-2-数据集"><a href="#1-2-数据集" class="headerlink" title="1.2 数据集"></a>1.2 数据集</h3><p>在这个比赛中，房源的数据来自于renthop网站，这些公寓都位于纽约市。其目的之前已经提到过了，就是基于一系列特征预测公寓房源的受欢迎程度，其目标变量是：<code>interest_level</code>，它是指从在网站上发布房源起始的时间内，房源的询问次数。</p>
<p>其中，比赛一共给了五个数据文件，分别是：</p>
<ul>
<li>train.json：训练集</li>
<li>test.json：测试集</li>
<li>sample_submission.csv：格式正确的提交示例</li>
<li>images_sample.zip：租房图片集（只抽取了100个图片集）</li>
<li>kaggle-renthop.7z：所有的租房图片集，一共有78.5GB的压缩文件。</li>
</ul>
<p>给出的特征的含义：</p>
<ul>
<li>bathrooms: 浴室的数量</li>
<li>bedrooms: 卧室的数量</li>
<li>building_id：</li>
<li>created：发布时间</li>
<li>description：一些描述</li>
<li>display_address：展出地址</li>
<li>features: 公寓的一些特征</li>
<li>latitude：纬度</li>
<li>listing_id</li>
<li>longitude：经度</li>
<li>manager_id：管理ID</li>
<li>photos: 租房图片集</li>
<li>price: 美元</li>
<li>street_address：街道地址</li>
<li>interest_level: 目标变量，受欢迎程度. 有三个类: ‘high’, ‘medium’, ‘low’</li>
</ul>
<h3 id="1-3-提交要求"><a href="#1-3-提交要求" class="headerlink" title="1.3 提交要求"></a>1.3 提交要求</h3><p>这个比赛使用的是多分类对数似然损失函数来评价模型。因为每一个房源都有一个对应的最准确的类别，对每一个房源，需要提交它属于每一类的概率值，它的计算公式如下：</p>
<script type="math/tex; mode=display">
\log loss=-\frac{1}{N}\sum_{i=1}^N{\sum_{j=1}^M{y_{ij}\log\left(p_{ij}\right)}}</script><p>其中$N$是测试集中的样本数量，$M$是类别的数量（3类：high、medium、low）,$log$是自然对数，$y_{ij}$表示样本$i$属于$j$类则为1，否则为0.$p_{ij}$表示样本$i$属于类别$j$的预测概率值。</p>
<p>一个样本的属于三个类别的预测可能性不需要加和为1，因为已经预先归一化了。为避免对数函数的极端情况，预测概率被替代为$max(min(p,1-10^{-15}),10^{-15})$</p>
<p>最后提交的文件为csv格式，它包含对每一类的预测概率值，行的顺序没有要求，文件必须要有一个表头，看起来像下面的示例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>listing_id</th>
<th>high</th>
<th>medium</th>
<th>low</th>
</tr>
</thead>
<tbody>
<tr>
<td>7065104</td>
<td>0.07743</td>
<td>0.23002</td>
<td>0.69254</td>
</tr>
<tr>
<td>7089035</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="二、Exploratory-Data-Analysis"><a href="#二、Exploratory-Data-Analysis" class="headerlink" title="二、Exploratory Data Analysis"></a>二、Exploratory Data Analysis</h2><p>在进行建模之前，我们都会对原始数据进行一些可视化探索，以便更快地熟悉数据，更有效进行之后的特征工程和建模。</p>
<p>我们先导入一些EDA过程中所需要的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">import</span> json</div><div class="line">color = sns.color_palette() <span class="comment"># 调色板</span></div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">pd.options.mode.chained_assignment = <span class="keyword">None</span> <span class="comment"># default = 'warn'</span></div></pre></td></tr></table></figure>
<p>其中numpy和pandas是数据分析处理中最流行的包，matplotlib和seaborn两个包用来绘制可视化图像，使用%matplotlib命令可以将matplotlib的图表直接嵌入到Notebook之中（%是魔术命令）。</p>
<h3 id="2-1-数据初探"><a href="#2-1-数据初探" class="headerlink" title="2.1 数据初探"></a>2.1 数据初探</h3><p>使用pandas打开训练集文件train.json，取前两行观测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_df = pd.read_json(<span class="string">'data/train.json'</span>)</div><div class="line">train_df.head(<span class="number">8</span>)</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%884.48.02.png" alt="屏幕快照 2017-06-19 下午4.48.02"><br><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%884.48.20.png" alt="屏幕快照 2017-06-19 下午4.48.20"></p>
<p>我们可以看到给定的数据中包含各种类型的特征，按照其特征可以分为以下几个类别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征类型</th>
<th>特征</th>
</tr>
</thead>
<tbody>
<tr>
<td>数值型</td>
<td>bathrooms、bedrooms、price</td>
</tr>
<tr>
<td>高势集类别</td>
<td>building_id、display_address、manager_id、street_address</td>
</tr>
<tr>
<td>时间型</td>
<td>created</td>
</tr>
<tr>
<td>地理位置型特征</td>
<td>longitude、latitude</td>
</tr>
<tr>
<td>文本</td>
<td>description</td>
</tr>
<tr>
<td>稀疏特征</td>
<td>features</td>
</tr>
<tr>
<td>id型特征</td>
<td>listing_id、index</td>
</tr>
</tbody>
</table>
</div>
<p>看一下训练集和测试集分别有多少</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">"Train Rows:"</span>,train_df.shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> <span class="string">"Test Rows:"</span>,test_df.shape[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>Train Rows: 49352<br>Test Rows: 74659</p>
<p>训练集有49352个样例，测试集有74659个样例。</p>
<p>接下来我们一一对这些特征进行探索。</p>
<h3 id="2-2-目标变量"><a href="#2-2-目标变量" class="headerlink" title="2.2 目标变量"></a>2.2 目标变量</h3><p>在深入探索之前，我们先看看目标变量Interest level</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">int_level = train_df[<span class="string">'interest_level'</span>].value_counts()</div><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">sns.barplot(int_level.index,int_level.values,alpha=<span class="number">0.8</span>,color=color[<span class="number">2</span>])</div><div class="line">plt.xlabel(<span class="string">"number of occurrences"</span>,fontsize = <span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">"Interest Level"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/fsvg.png" alt="fsvg"></p>
<p>兴趣度在大多数情况下都是低的，其次是中等，只有少部分的样例为高分。</p>
<h3 id="2-3-数值型特征"><a href="#2-3-数值型特征" class="headerlink" title="2.3 数值型特征"></a>2.3 数值型特征</h3><h4 id="2-3-1-浴室（bathrooms）"><a href="#2-3-1-浴室（bathrooms）" class="headerlink" title="2.3.1 浴室（bathrooms）"></a>2.3.1 浴室（bathrooms）</h4><p>先看看浴室的数量分布<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cnt_srs = train_df[<span class="string">'bathrooms'</span>].value_counts()</div><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">sns.barplot(cnt_srs.index,cnt_srs.values,alpha=<span class="number">0.8</span>,color=color[<span class="number">2</span>])</div><div class="line">plt.xlabel(<span class="string">"number of occurrences"</span>,fontsize = <span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">"bathrooms"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%885.23.17.png" alt="屏幕快照 2017-06-19 下午5.23.17"></p>
<p>可以看到绝大多数的样例的浴室数量为1，其次为2个浴室。</p>
<p>再看看不同兴趣程度的浴室数量分布，运用小提琴图来呈现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#浴室数量大于3的记为3</span></div><div class="line">train_df[<span class="string">'bathrooms'</span>].loc[train_df[<span class="string">'bathrooms'</span>]&gt;<span class="number">3</span>]=<span class="number">3</span></div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">sns.violinplot(x = <span class="string">'interest_level'</span>,y = <span class="string">'bathrooms'</span>,data= train_df,alpha=<span class="number">0.8</span>)</div><div class="line">plt.xlabel(<span class="string">"interest level"</span>,fontsize = <span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">"bathrooms"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%885.24.02.png" alt="屏幕快照 2017-06-19 下午5.24.02"></p>
<p>可以看到在不同的兴趣程度上，浴室数量的分布差不多。</p>
<h4 id="2-3-2-卧室（bedrooms）"><a href="#2-3-2-卧室（bedrooms）" class="headerlink" title="2.3.2 卧室（bedrooms）"></a>2.3.2 卧室（bedrooms）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cnt_bedrooms = train_df[<span class="string">'bedrooms'</span>].value_counts()</div><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">sns.barplot(cnt_bedrooms.index,cnt_bedrooms.values,alpha=<span class="number">0.8</span>,color=color[<span class="number">3</span>])</div><div class="line">plt.ylabel(<span class="string">"number of occurrences"</span>,fontsize = <span class="number">12</span>)</div><div class="line">plt.xlabel(<span class="string">"bedrooms"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/dfrgv.png" alt="dfrgv"></p>
<p>卧室数量基本集中在1和2，也有不少没有卧室，3个卧室的房子也不少。</p>
<p>看看不同兴趣程度的卧室数量分布，同样也用小提琴图来呈现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">sns.countplot(x=<span class="string">'bedrooms'</span>,hue=<span class="string">'interest_level'</span>,data=train_df)</div><div class="line">plt.ylabel(<span class="string">"number of occurrences"</span>,fontsize = <span class="number">12</span>)</div><div class="line">plt.xlabel(<span class="string">"bedrooms"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%885.30.15.png" alt="屏幕快照 2017-06-19 下午5.30.15"></p>
<h4 id="2-3-3-价格（price）"><a href="#2-3-3-价格（price）" class="headerlink" title="2.3.3 价格（price）"></a>2.3.3 价格（price）</h4><p>对价格排序，看一下价格的分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">plt.scatter(range(train_df.shape[<span class="number">0</span>]),np.sort(train_df.price.values))</div><div class="line">plt.xlabel(<span class="string">'index'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">'price'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%885.37.33.png" alt="屏幕快照 2017-06-19 下午5.37.33"></p>
<p>可以观察到有几个价格格外的高，视为异常值，我们把它们移除掉，然后再绘制分布直方图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#99%分位数</span></div><div class="line">ulimit = np.percentile(train_df.price.values,<span class="number">99</span>)</div><div class="line">train_df[<span class="string">'price'</span>].loc[train_df[<span class="string">'price'</span>]&gt;ulimit]=ulimit</div><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">sns.distplot(train_df.price.values,bins=<span class="number">50</span>,kde=<span class="keyword">True</span>,color=color[<span class="number">3</span>])</div><div class="line">plt.xlabel(<span class="string">'price'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%885.42.29.png" alt="屏幕快照 2017-06-19 下午5.42.29"></p>
<p>可以观察到分布略微有点右偏。</p>
<h3 id="2-4-地理位置型"><a href="#2-4-地理位置型" class="headerlink" title="2.4 地理位置型"></a>2.4 地理位置型</h3><h4 id="2-4-1-纬度（latitude）"><a href="#2-4-1-纬度（latitude）" class="headerlink" title="2.4.1 纬度（latitude）"></a>2.4.1 纬度（latitude）</h4><p>先看看纬度的分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#避免极端情况</span></div><div class="line">llimit = np.percentile(train_df.latitude.values,<span class="number">1</span>)</div><div class="line">ulimit = np.percentile(train_df.latitude.values,<span class="number">99</span>)</div><div class="line"></div><div class="line">train_df[<span class="string">'latitude'</span>].loc[train_df[<span class="string">'latitude'</span>]&lt;llimit]=llimit</div><div class="line">train_df[<span class="string">'latitude'</span>].loc[train_df[<span class="string">'latitude'</span>]&gt;ulimit]=ulimit</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line">sns.distplot(train_df.latitude.values,bins=<span class="number">50</span>,kde=<span class="keyword">True</span>,color=color[<span class="number">3</span>])</div><div class="line">plt.xlabel(<span class="string">'latitude'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.07.07.png" alt="屏幕快照 2017-06-19 下午9.07.07"></p>
<p>由图可知，纬度基本上介于40.6到40.9之间</p>
<h4 id="2-4-2-经度（longitude）"><a href="#2-4-2-经度（longitude）" class="headerlink" title="2.4.2 经度（longitude）"></a>2.4.2 经度（longitude）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#避免极端情况</span></div><div class="line">llimit = np.percentile(train_df.longitude.values,<span class="number">1</span>)</div><div class="line">ulimit = np.percentile(train_df.longitude.values,<span class="number">99</span>)</div><div class="line"></div><div class="line">train_df[<span class="string">'longitude'</span>].loc[train_df[<span class="string">'longitude'</span>]&lt;llimit]=llimit</div><div class="line">train_df[<span class="string">'longitude'</span>].loc[train_df[<span class="string">'longitude'</span>]&gt;ulimit]=ulimit</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">sns.distplot(train_df.longitude.values,bins=<span class="number">50</span>)</div><div class="line">plt.xlabel(<span class="string">'longitude'</span>,fontsize=<span class="number">14</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.10.31.png" alt="屏幕快照 2017-06-19 下午9.10.31"></p>
<p>经度介于-73.8和-74.02之间。<br>接下来，我们尝试把经纬度对应到地图上，绘制成热图，也就是房源在地理位置上的分布密度图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mpl_toolkits.basemap <span class="keyword">import</span> Basemap</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</div><div class="line">west,south,east,north =<span class="number">-74.02</span>,<span class="number">40.64</span>,<span class="number">-73.85</span>,<span class="number">40.86</span></div><div class="line">fig =plt.figure(figsize=(<span class="number">16</span>,<span class="number">12</span>))</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">m=Basemap(projection=<span class="string">'merc'</span>,</div><div class="line">          llcrnrlat=south,urcrnrlat=north,</div><div class="line">          llcrnrlon=west,urcrnrlon=east,</div><div class="line">          lat_ts=south,resolution=<span class="string">'i'</span>)</div><div class="line">x,y=m(train_df[<span class="string">'longitude'</span>].values,train_df[<span class="string">'latitude'</span>].values)</div><div class="line">m.hexbin(x,y,gridsize=<span class="number">200</span>,bins=<span class="string">'log'</span>,cmap=cm.YlOrRd_r)</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E4%B8%8B%E8%BD%BD.png" alt="下载"><br>基本和纽约市的城市热图相匹配。</p>
<h3 id="2-5-时间型"><a href="#2-5-时间型" class="headerlink" title="2.5 时间型"></a>2.5 时间型</h3><h4 id="2-5-1-发布时间（Created）"><a href="#2-5-1-发布时间（Created）" class="headerlink" title="2.5.1 发布时间（Created）"></a>2.5.1 发布时间（Created）</h4><p>先看一下不同时间的分布状况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">train_df[<span class="string">'created'</span>]=pd.to_datetime(train_df[<span class="string">'created'</span>])</div><div class="line">train_df[<span class="string">'date_created'</span>]=train_df[<span class="string">'created'</span>].dt.date</div><div class="line">cnt_srs = train_df[<span class="string">'date_created'</span>].value_counts()</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">ax = plt.subplot(<span class="number">111</span>)</div><div class="line">ax.bar(cnt_srs.index,cnt_srs.values,alpha=<span class="number">0.8</span>)</div><div class="line">ax.xaxis_date()</div><div class="line">plt.xticks(rotation=<span class="string">'vertical'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.21.21.png" alt="屏幕快照 2017-06-19 下午9.21.21"></p>
<p>从图中观察到发布时间是从2016年的4月至6月，当然这是训练集的情况，对应的，再看看测试集的发布时间状况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">test_df[<span class="string">'created'</span>]=pd.to_datetime(test_df[<span class="string">'created'</span>])</div><div class="line">test_df[<span class="string">'date_created'</span>]=test_df[<span class="string">'created'</span>].dt.date</div><div class="line">cnt_srs = test_df[<span class="string">'date_created'</span>].value_counts()</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">ax = plt.subplot(<span class="number">111</span>)</div><div class="line">ax.bar(cnt_srs.index,cnt_srs.values,alpha=<span class="number">0.8</span>)</div><div class="line">ax.xaxis_date()</div><div class="line">plt.xticks(rotation=<span class="string">'vertical'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.24.59.png" alt="屏幕快照 2017-06-19 下午9.24.59"></p>
<p>可知，测试集的时间分布和训练集类似。</p>
<p>再看看不同时刻的样本分布情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">train_df[<span class="string">'hour_created'</span>] = train_df[<span class="string">'created'</span>].dt.hour</div><div class="line">cnt_srs = train_df[<span class="string">'hour_created'</span>].value_counts()</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">sns.barplot(cnt_srs.index,cnt_srs.values,alpha=<span class="number">0.8</span>,color=color[<span class="number">4</span>])</div><div class="line">plt.xticks(rotation=<span class="string">'vertical'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.25.37.png" alt="屏幕快照 2017-06-19 下午9.25.37"></p>
<p>看起来像是一天中比较早的几个小时创建的比较多。可能是那个时候流量不拥挤，数据就更新了。</p>
<h3 id="2-6-其他类型特征"><a href="#2-6-其他类型特征" class="headerlink" title="2.6 其他类型特征"></a>2.6 其他类型特征</h3><h4 id="2-6-1-展示地址（Display-Address）"><a href="#2-6-1-展示地址（Display-Address）" class="headerlink" title="2.6.1 展示地址（Display Address）"></a>2.6.1 展示地址（Display Address）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cnt_srs = train_df.groupby(<span class="string">'display_address'</span>)[<span class="string">'display_address'</span>].count()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">2</span>,<span class="number">10</span>,<span class="number">50</span>,<span class="number">100</span>,<span class="number">500</span>]:</div><div class="line">    <span class="keyword">print</span> <span class="string">"Display_address that appear less than &#123;&#125; \</span></div><div class="line">    times:&#123;&#125;%".format(i,round((cnt_srs&lt;i).mean()*<span class="number">100</span>,<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>上述代码中（cnt_srs&lt;i）返回的是布尔值True | False。再求一个<br>得到的结果为：<br>Display_address that appear less than 2     times:63.22%<br>Display_address that appear less than 10     times:89.6%<br>Display_address that appear less than 50     times:97.73%<br>Display_address that appear less than 100     times:99.26%<br>Display_address that appear less than 500     times:100.0%</p>
<p>绘制展示地址频次分布直方图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">plt.hist(cnt_srs.values,bins=<span class="number">150</span>,log=<span class="keyword">True</span>,alpha=<span class="number">0.9</span>)</div><div class="line">plt.xlabel(<span class="string">'Number of times display_adress appeared'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">'log(Count)'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%8810.03.35.png" alt="屏幕快照 2017-06-19 下午10.03.35"></p>
<p>大部分的展览地址出现次数在给定的数据集中少于100次。没有超过500次的。</p>
<p>再看看展示地址的词云图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wordcloud for display address</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</div><div class="line">wordcloud = WordCloud(background_color=<span class="string">'white'</span>, width=<span class="number">600</span>, height=<span class="number">300</span>, max_font_size=<span class="number">50</span>, max_words=<span class="number">40</span>).generate(text_da)</div><div class="line">wordcloud.recolor(random_state=<span class="number">0</span>)</div><div class="line">plt.imshow(wordcloud)</div><div class="line">plt.title(<span class="string">"Wordcloud for Display Address"</span>, fontsize=<span class="number">30</span>)</div><div class="line">plt.axis(<span class="string">"off"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%8810.01.10.png" alt="屏幕快照 2017-06-19 下午10.01.10"></p>
<h4 id="2-6-2-照片数量（Photos）"><a href="#2-6-2-照片数量（Photos）" class="headerlink" title="2.6.2 照片数量（Photos）"></a>2.6.2 照片数量（Photos）</h4><p>这个比赛也有巨大的照片数据。让我们先看看照片的数量:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">train_df[<span class="string">"num_photos"</span>] = train_df[<span class="string">"photos"</span>].apply(len)</div><div class="line">cnt_srs = train_df[<span class="string">'num_photos'</span>].value_counts()</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=<span class="number">0.8</span>)</div><div class="line">plt.xlabel(<span class="string">"number of photos"</span>,fontsize=<span class="number">14</span>)</div><div class="line">plt.ylabel(<span class="string">'number of occurrences'</span>,fontsize=<span class="number">14</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.52.55.png" alt="屏幕快照 2017-06-19 下午9.52.55"></p>
<p>大多数样例的照片数量集中在3~8张。</p>
<p>再来看看不同兴趣程度下的照片数量分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">train_df[<span class="string">'num_photos'</span>].loc[train_df[<span class="string">'num_photos'</span>]&gt;<span class="number">12</span>]=<span class="number">12</span></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">sns.violinplot(x=<span class="string">'num_photos'</span>,y=<span class="string">'interest_level'</span>,data=train_df,order=[<span class="string">'low'</span>,<span class="string">'medium'</span>,<span class="string">'high'</span>])</div><div class="line">plt.xlabel(<span class="string">'Number of photos'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">"Interest Level"</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.55.02.png" alt="屏幕快照 2017-06-19 下午9.55.02"></p>
<h4 id="2-6-3-描述特征的数量（features）"><a href="#2-6-3-描述特征的数量（features）" class="headerlink" title="2.6.3 描述特征的数量（features）"></a>2.6.3 描述特征的数量（features）</h4><p>每一个房源都对应一个features列，它描述了该样例的特征，比如位于市中心呀、能养猫呀、可以肆意遛狗，类似于这种亲民的特点。有的时候，这种利民条件越多，或许会提高消费者的兴趣，当然也不一定，可以先来看看特征数量的分布：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">train_df[<span class="string">'num_features'</span>] = train_df[<span class="string">'features'</span>].apply(len)</div><div class="line">cnt_srs = train_df[<span class="string">'num_features'</span>].value_counts()</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=<span class="number">0.8</span>)</div><div class="line">plt.ylabel(<span class="string">'Number of Occurrences'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.xlabel(<span class="string">'Number of features'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.57.13.png" alt="屏幕快照 2017-06-19 下午9.57.13"></p>
<p>再看看不同兴趣程度下的描述特征数量分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#避免极端情况</span></div><div class="line">train_df[<span class="string">'num_features'</span>].loc[train_df[<span class="string">'num_features'</span>]&gt;<span class="number">17</span>]=<span class="number">17</span></div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">sns.violinplot(y=<span class="string">'num_features'</span>,x=<span class="string">'interest_level'</span>,\</div><div class="line">               data=train_df,order=[<span class="string">'low'</span>,<span class="string">'medium'</span>,<span class="string">'high'</span>])</div><div class="line">plt.xlabel(<span class="string">'Interest Level'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.ylabel(<span class="string">'Number of features'</span>,fontsize=<span class="number">12</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%889.59.05.png" alt="屏幕快照 2017-06-19 下午9.59.05"></p>
<p>也可以看看描述特征的词云：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</div><div class="line">text = <span class="string">''</span></div><div class="line">text_da = <span class="string">''</span></div><div class="line"><span class="keyword">for</span> index,row <span class="keyword">in</span> train_df.iterrows():</div><div class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> row[<span class="string">'features'</span>]:</div><div class="line">        text = <span class="string">' '</span>.join([text,<span class="string">"_"</span>.join(feature.strip().split(<span class="string">" "</span>))])</div><div class="line">        text_da = <span class="string">" "</span>.join([text_da,<span class="string">"_"</span>.join(row[<span class="string">'display_address'</span>].strip().split(<span class="string">" "</span>))])</div><div class="line">text = text.strip()</div><div class="line">text_da = text_da.strip()</div><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</div><div class="line">wordcloud = WordCloud(background_color=<span class="string">'white'</span>,width=<span class="number">600</span>,                     height=<span class="number">300</span>,max_font_size=<span class="number">50</span>,max_words=<span class="number">40</span>).generate(text)</div><div class="line">wordcloud.recolor(random_state=<span class="number">0</span>)</div><div class="line">plt.imshow(wordcloud)</div><div class="line">plt.title(<span class="string">"Wordcloud for features"</span>,fontsize=<span class="number">30</span>)</div><div class="line">plt.axis(<span class="string">"off"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-19%20%E4%B8%8B%E5%8D%8810.00.21.png" alt="屏幕快照 2017-06-19 下午10.00.21"></p>
<p>以上这些探索性分析只是对原始数据初步的认识与了解，完了就可以建立一个base model。随着之后的特征工程对其进行更深层次的探索挖掘，不断迭代，使得我们的模型的预测效果越来越好。下一篇就开始着手建立一些base model。</p>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EDA </tag>
            
            <tag> 特征工程 </tag>
            
            <tag> kaggle </tag>
            
            <tag> 可视化 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析系列（3）：数据倾斜]]></title>
      <url>/2017/06/11/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      <content type="html"><![CDATA[<p>数据倾斜是大数据领域绕不开的拦路虎，当你所需处理的数据量到达了上亿甚至是千亿条的时候，数据倾斜将是横在你面前一道巨大的坎。很可能有几周甚至几月都要头疼于数据倾斜导致的各类诡异的问题。</p>
<a id="more"></a>
<p>数据倾斜是指：mapreduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。Hive的执行是分阶段的，map处理数据量的差异取决于上一个stage的reduce输出，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。</p>
<p>以下是一些常见的数据倾斜情形：<br><img src="http://orsw4brg1.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-10-30%20%E4%B8%8B%E5%8D%889.07.31.png" alt=""></p>
<h2 id="一、Group-by-倾斜"><a href="#一、Group-by-倾斜" class="headerlink" title="一、Group by 倾斜"></a>一、Group by 倾斜</h2><p>group by造成的倾斜相对来说比较容易解决。hive提供两个参数可以解决：</p>
<h3 id="1-1-hive-map-aggr"><a href="#1-1-hive-map-aggr" class="headerlink" title="1.1 hive.map.aggr"></a>1.1 hive.map.aggr</h3><p>一个是hive.map.aggr，默认值已经为true，他的意思是做map aggregation，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，但是却实现了类似combiner的效果。事实上各种基于mr的框架如pig，cascading等等用的都是map aggregation（或者叫partial aggregation）而非combiner的策略，也就是在mapper里面直接做聚合操作而不是输出到buffer给combiner做聚合。对于map aggregation，hive还会做检查，如果aggregation的效果不好，那么hive会自动放弃map aggregation。判断效果的依据就是经过一小批数据的处理之后，检查聚合后的数据量是否减小到一定的比例，默认是0.5，由hive.map.aggr.hash.min.reduction这个参数控制。所以如果确认数据里面确实有个别取值倾斜，但是大部分值是比较稀疏的，这个时候可以把比例强制设为1，避免极端情况下map aggr失效。hive.map.aggr还有一些相关参数，比如map aggr的内存占用等，具体可以参考<a href="http://dev.bizo.com/2013/02/map-side-aggregations-in-apache-hive.html" target="_blank" rel="noopener">这篇文章</a>。</p>
<h3 id="1-2-hive-groupby-skewindata"><a href="#1-2-hive-groupby-skewindata" class="headerlink" title="1.2 hive.groupby.skewindata"></a>1.2 hive.groupby.skewindata</h3><p>另一个参数是hive.groupby.skewindata。这个参数的意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟hive.map.aggr做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。</p>
<h3 id="1-3-count-distinct-改写"><a href="#1-3-count-distinct-改写" class="headerlink" title="1.3 count distinct 改写"></a>1.3 count distinct 改写</h3><p>另外需要注意的是count distinct操作往往需要改写SQL，可以按照下面这么做：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">/*改写前*/</div><div class="line">select a, count(distinct b) as c from tbl group by a;</div><div class="line"></div><div class="line">/*改写后*/</div><div class="line">select a, count(*) as c from (select a, b from tbl group by a, b) group by a;</div></pre></td></tr></table></figure>
<h2 id="二、Join倾斜"><a href="#二、Join倾斜" class="headerlink" title="二、Join倾斜"></a>二、Join倾斜</h2><h3 id="2-1-skew-join"><a href="#2-1-skew-join" class="headerlink" title="2.1 skew join"></a>2.1 skew join</h3><p>join造成的倾斜，常见情况是不能做map join的两个表(能做map join的话基本上可以避免倾斜)，其中一个是行为表，另一个应该是属性表。比如我们有三个表，一个用户属性表users，一个商品属性表items，还有一个用户对商品的操作行为表日志表logs。假设现在需要将行为表关联用户表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select * from logs a join users b on a.user_id = b.user_id;</div></pre></td></tr></table></figure>
<p>其中logs表里面会有一个特殊用户user_id = 0，代表未登录用户，假如这种用户占了相当的比例，那么个别reduce会收到比其他reduce多得多的数据，因为它要接收所有user_id = 0的记录进行处理，使得其处理效果会非常差，其他reduce都跑完很久了它还在运行。</p>
<p>hive给出的解决方案叫skew join，其原理把这种user_id = 0的特殊值先不在reduce端计算掉，而是先写入hdfs，然后启动一轮map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。当然你要告诉hive这个join是个skew join，即：set </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hive.optimize.skewjoin = true;</div></pre></td></tr></table></figure>
<p>还有要告诉hive如何判断特殊值，根据hive.skewjoin.key设置的数量hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。总结起来，skew join的流程可以用下图描述：</p>
<p><img src="http://orsw4brg1.bkt.clouddn.com/skew_join.jpg" alt=""></p>
<h3 id="2-2-特殊值分开处理法"><a href="#2-2-特殊值分开处理法" class="headerlink" title="2.2 特殊值分开处理法"></a>2.2 特殊值分开处理法</h3><p>不过，上述方法还要去考虑阈值之类的情况，其实也不够通用。所以针对join倾斜的问题，一般都是通过改写sql解决。对于上面这个问题，我们已经知道user_id = 0是一个特殊key，那么可以把特殊值隔离开来单独做join，这样特殊值肯定会转化成map join，非特殊值就是没有倾斜的普通join了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">select *</div><div class="line">from (select * from logs where user_id = 0)  a </div><div class="line">join (select * from users where user_id = 0) b </div><div class="line">on a.user_id =  b.user_id</div><div class="line">union all</div><div class="line">select * </div><div class="line">from logs a join users b</div><div class="line">on a.user_id &lt;&gt; 0 and a.user_id = b.user_id;</div></pre></td></tr></table></figure>
<h3 id="2-3-随机数分配法"><a href="#2-3-随机数分配法" class="headerlink" title="2.3 随机数分配法"></a>2.3 随机数分配法</h3><p>上面这种个别key倾斜的情况只是一种倾斜情况。最常见的倾斜是因为数据分布本身就具有长尾性质，比如我们将日志表和商品表关联：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select * from logs a join items b on a.item_id = b.item_id;</div></pre></td></tr></table></figure>
<p>这个时候，分配到热门商品的reducer就会很慢，因为热门商品的行为日志肯定是最多的，而且我们也很难像上面处理特殊user那样去处理item。这个时候就会用到加随机数的方法，也就是在join的时候增加一个随机数，随机数的取值范围n相当于将item给分散到n个reducer：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">select a.*, b.*</div><div class="line">from (select *, cast(rand() * 10 as int) as r_id from logs)a</div><div class="line">join (select *, r_id from items </div><div class="line">lateral view explode(range_list(1,10)) rl as r_id)b</div><div class="line">on a.item_id = b.item_id and a.r_id = b.r_id</div></pre></td></tr></table></figure>
<p>上面的写法里，对行为表的每条记录生成一个1-10的随机整数，对于item属性表，每个item生成10条记录，随机key分别也是1-10，这样就能保证行为表关联上属性表。其中range_list(1,10)代表用udf实现的一个返回1-10整数序列的方法。这个做法是一个解决join倾斜比较根本性的通用思路，就是如何用随机数将key进行分散。当然，可以根据具体的业务场景做实现上的简化或变化。</p>
<h3 id="2-4-业务设计"><a href="#2-4-业务设计" class="headerlink" title="2.4 业务设计"></a>2.4 业务设计</h3><p>除了上面两类情况，还有一类情况是因为业务设计导致的问题，也就是说即使行为日志里面join key的数据分布本身并不明显倾斜，但是业务设计导致其倾斜。比如对于商品item_id的编码，除了本身的id序列，还人为的把item的类型也作为编码放在最后两位，这样如果类型1（电子产品）的编码是00，类型2（家居产品）的编码是01，并且类型1是主要商品类，将会造成以00为结尾的商品整体倾斜。这时，如果reduce的数量恰好是100的整数倍，会造成partitioner把00结尾的item_id都hash到同一个reducer，引爆问题。这种特殊情况可以简单的设置合适的reduce值来解决，但是这种坑对于不了解业务的情况下就会比较隐蔽。</p>
<h2 id="三、典型的业务场景"><a href="#三、典型的业务场景" class="headerlink" title="三、典型的业务场景"></a>三、典型的业务场景</h2><h3 id="3-1-空值产生的数据倾斜"><a href="#3-1-空值产生的数据倾斜" class="headerlink" title="3.1 空值产生的数据倾斜"></a>3.1 空值产生的数据倾斜</h3><p>场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。</p>
<ul>
<li>解决方法1： user_id为空的不参与关联（红色字体为修改后）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">select * from log a</div><div class="line">  join users b</div><div class="line">  on a.user_id is not null</div><div class="line">  and a.user_id = b.user_id</div><div class="line">union all</div><div class="line">select * from log a</div><div class="line">  where a.user_id is null;</div></pre></td></tr></table></figure>
<ul>
<li>解决方法2 ：赋与空值分新的key值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">select *</div><div class="line">  from log a</div><div class="line">  left outer join users b</div><div class="line">  on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;</div></pre></td></tr></table></figure>
<p>结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。</p>
<h3 id="3-2-不同数据类型关联产生数据倾斜"><a href="#3-2-不同数据类型关联产生数据倾斜" class="headerlink" title="3.2 不同数据类型关联产生数据倾斜"></a>3.2 不同数据类型关联产生数据倾斜</h3><p>场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。</p>
<p>解决方法：把数字类型转换成字符串类型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">select * from users a</div><div class="line">  left outer join logs b</div><div class="line">  on a.usr_id = cast(b.user_id as string)</div></pre></td></tr></table></figure>
<h3 id="3-3-小表不小不大，怎么用-map-join-解决倾斜问题"><a href="#3-3-小表不小不大，怎么用-map-join-解决倾斜问题" class="headerlink" title="3.3 小表不小不大，怎么用 map join 解决倾斜问题"></a>3.3 小表不小不大，怎么用 map join 解决倾斜问题</h3><p>使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">select * from log a</div><div class="line">  left outer join users b</div><div class="line">  on a.user_id = b.user_id;</div></pre></td></tr></table></figure>
<p>users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">select /*+mapjoin(x)*/* from log a</div><div class="line">  left outer join (</div><div class="line">    select  /*+mapjoin(c)*/d.*</div><div class="line">      from ( select distinct user_id from log ) c</div><div class="line">      join users d</div><div class="line">      on c.user_id = d.user_id</div><div class="line">    ) x</div><div class="line">  on a.user_id = b.user_id;</div></pre></td></tr></table></figure>
<p>假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。</p>
<h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表疏忽或业务逻辑可以规避的。在此给出较为通用的步骤：</p>
<ul>
<li>1）采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。</li>
<li>2）数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join生成tmp2,把tmp2读到distribute file cache。这是一个map过程。</li>
<li>3）map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成<user_id,value>的key,value对，假如记录来自member,生成<user_id,value>的key,value对，进入reduce阶段。</user_id,value></user_id,value></li>
<li>4）最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。</li>
</ul>
<p>如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案：</p>
<ul>
<li>1）对于join，在判断小表不大于1G的情况下，使用map join</li>
<li>2）对于group by或distinct，设定 hive.groupby.skewindata=true</li>
<li>3）尽量使用上述的SQL语句调节进行优化</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 数据分析 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据倾斜 </tag>
            
            <tag> Hive </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析系列（2）：卡方检验]]></title>
      <url>/2017/06/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/</url>
      <content type="html"><![CDATA[<p>$\chi^2$检验（chi-square test）或称卡方检验，是一种用途较广的假设检验方法，，主要是比较两个及两个以上样本率( 构成比）以及两个分类变量的关联性分析。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。</p>
<a id="more"></a>
<p>它的发明者卡尔·皮尔逊是一位历史上罕见的百科全书式的学者，研究领域涵盖了生物、历史、宗教、哲学、法律。在文本分类中可以用卡方值做特征选择（降维），也可以用卡方检验做异常用户的检测。</p>
<h2 id="一、四格表资料的卡方检验"><a href="#一、四格表资料的卡方检验" class="headerlink" title="一、四格表资料的卡方检验"></a>一、四格表资料的卡方检验</h2><p>两组大白鼠在不同致癌剂作用下的发癌率如下表，问两组发癌率有无差别？<br><img src="http://omu7tit09.bkt.clouddn.com/14993074249833.png" alt=""></p>
<p>这四格资料表就专称四格表（fourfold table），或称2行2列表（2×2 contingency table）。从该资料算出的两组发癌率分别为73.24%和92.86%，两者的差别可能是抽样误差所致，亦可能是两组发癌率（总体率）确有所不同。这里可通过卡方检验来区别其差异有无统计学意义，检验的基本公式为：</p>
<script type="math/tex; mode=display">\chi^2=\sum \frac{(A-T)^2}{T}</script><p>式中A为实际数，以上四格表的四个数据就是实际数。T为理论数，是根据检验假设推断出来的；即假设这两组的发癌率本无不同，差别仅是由抽样误差所致。这里可将两组合计发癌率作为理论上的发癌率，即91/113=80.3%，以此为依据便可推算出四格表中相应的四格的理论数。以表1资料为例检验如下。</p>
<p>检验步骤：</p>
<ul>
<li>1）建立检验假设：$H_0:\ n_1=n_2 \ H_1 : n_1 ≠n_2;$</li>
<li>2）计算理论数（TRC）,计算公式为：<script type="math/tex; mode=display">TRC = \frac{n_r\times n_c}{n}</script>式中$TRC$是表示第R行C列格子的理论数，$n_r$是与理论数同行的合计数，$n_c$是与理论数同列的合计数，$n$为总例数。</li>
</ul>
<p>第1行1列： 71×91/113=57.18<br>第1行2列： 71×22/113=13.82<br>第2行1列： 42×91/113=33.82<br>第2行2列： 42×22/113=8.18</p>
<p>以推算结果，可与原四项实际数并列成下表：<br><img src="http://omu7tit09.bkt.clouddn.com/14993078536823.png" alt=""><br>因为上表每行和每列合计数都是固定的，所以只要用TRC式求得其中一项理论数（例如T1.1=57.18），则其余三项理论数都可用同行或同列合计数相减，直接求出。</p>
<ul>
<li>3）计算卡方值按公式代入<script type="math/tex; mode=display">\chi^2 = \frac{(52-57.18)^2}{57.18}···+ \frac{(3-8.18)^2}{8.18}=6.48</script></li>
<li>4）查卡方值表求$P$值</li>
</ul>
<p>在查表之前应知本题自由度。按卡方检验的自由度v=（行数-1）（列数-1），则该题的自由度v=（2-1）（2-1）=1，查卡方界值表，找到$\chi^2_{0.05}(1)=3.85$，而本题卡方=6.48即卡方＞$\chi^2_{0.05}(1)$，P＜0.05，差异有显著统计学意义，按α=0.05水准，拒绝H0，可以认为两组发癌率有差别。</p>
<p>通过实例计算，读者对卡方的基本公式有如下理解：若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零，而卡方永远为正值。又因为每一对理论数和实际数都加入卡方值中，分组越多，即格子数越多，卡方值也会越大，因而每考虑卡方值大小的意义时同时要考虑到格子数。因此自由度大时，卡方的界值也相应增大。</p>
<h2 id="二、四格表卡方值的校正"><a href="#二、四格表卡方值的校正" class="headerlink" title="二、四格表卡方值的校正"></a>二、四格表卡方值的校正</h2><p>卡方值表是数理统计根据正态分布中$\chi^2 = \sum (\frac{x_i-\mu }{\sigma})^2$的定义计算出来的。是一种近似。在自由度大于1、理论数皆大于5时，这种近似很好；当自由度为1时，尤其当1＜T＜5，而n＞40时，应用以下校正公式：</p>
<script type="math/tex; mode=display">\chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}</script><p>例2.某医师用甲、乙两疗法治疗小儿单纯性消化不良，结果小表试比较两种疗法效果有无差异？</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14993090223178.png" alt=""><br>从表可见，T1.2和T2.2数值都＜5，且总例数大于40，故宜用校正公式检验。步骤如下：</p>
<ul>
<li><p>1）检验假设：$H_0：π1=π2；H_1：π1≠π2；α=0.05$</p>
</li>
<li><p>2）计算理论数：（已完成列入四格表括弧中）</p>
</li>
<li><p>3）计算卡方值：应用校正公式运算如下：</p>
<script type="math/tex; mode=display">\chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}=2.746</script><p>查卡方界值表$\chi^2_{0.05}(1)=3.84$，，故卡方＜$\chi^2_{0.05}(1)$，P＞0.05。按α=0.05水准，接受H0，两种疗效差异无统计学意义。 </p>
</li>
</ul>
<p>如果不采用校正公式，而用原基本公式，算得的结果卡方=4.068，则结论就不同了。</p>
<p>如果观察资料的T＜1或n＜40时，四格表资料用上述校正法也不行，可参考预防医学专业用的医学统计学教材中的Fisher精确检验法直接计算概率以作判断。</p>
<h2 id="三、行列表的卡方检验"><a href="#三、行列表的卡方检验" class="headerlink" title="三、行列表的卡方检验"></a>三、行列表的卡方检验</h2><p>适用于两个组以上的率或百分比差别的显著性检验。其检验步骤与上述相同，简单计算公式如下：</p>
<script type="math/tex; mode=display">\chi^2 = n(\sum \frac{A^2}{n_rn_c}-1 )</script><p>式中n为总例数；A为各观察值；$n_r$和$n_c$为与各A值相应的行和列合计的总数。</p>
<p>例3.北方冬季日照短而南移，居宅设计如何适应以获得最大日照量，增强居民体质，减少小儿佝偻病，实属重要。胡氏等1986年在北京进行住宅建筑日照卫生标准的研究，对214幢楼房居民的婴幼儿712人体检，检出轻度佝偻病333例，比较了居室朝向与患病的关系。现将该资料归纳如表4作行列检验。</p>
<p>该表资料由2行4列组成，称2×4表，可用行×列卡方公式检验。</p>
<ul>
<li>1）检验假设：H0：四类朝向居民婴幼儿佝偻病患病率相同；H1：四类朝向居民婴幼儿佝偻病患率不同；α=0.05</li>
<li>2）计算卡方值：<script type="math/tex; mode=display">\chi^2 = 712(\frac{180^2}{379\times 380 }+···+\frac{33^2}{333\times 98}-1)=15.079</script></li>
<li>3）确定P值和分析：本题v=（2-1)（4-3）=3，据此查卡方界值表：$\chi^2_{0.05}(3)=7.81$，本题卡方=15.08，卡方＞ $\chi^2_{0.05}(3)$，P＜0.05，拒绝$H_0$，可以认为居室朝向不同的居民，婴幼儿佝偻病患病率有差异。</li>
</ul>
<p>一般认为行列表中不宜有1/5以上格子的理论数小于5，或有小于1的理论数。当理论数太小可采取下列方法处理：①增加样本含量以增大理论数；②删去上述理论数太小的行和列；③将太小理论数所在行或列与性质相近的邻行邻列中的实际数合并，使重新计算的理论数增大。由于后两法可能会损失信息，损害样本的随机性，不同的合并方式有可能影响推断结论，故不宜作常规方法。另外，不能把不同性质的实际数合并，如研究血型时，不能把不同的血型资料合并。</p>
<p>如检验结果拒绝检验假设，只能认为各总体率或总体构成比之间总的来说有差别，但不能说明它们彼此之间都有差别，或某两者间有差别。</p>
<h2 id="四、应用场景"><a href="#四、应用场景" class="headerlink" title="四、应用场景"></a>四、应用场景</h2><p>卡方检验的一个典型应用场景是衡量特定条件下的分布是否与理论分布一致，比如：特定用户某项指标的分布与大盘的分布是否差异很大，这时通过临界概率可以合理又科学的筛选异常用户。</p>
<p>另外，x2值描述了自变量与因变量之间的相关程度：x2值越大，相关程度也越大，所以很自然的可以利用x2值来做降维，保留相关程度大的变量。再回到刚才新闻分类的场景，如果我们希望获取和娱乐类别相关性最强的100个词，以后就按照标题是否包含这100个词来确定新闻是否归属于娱乐类，怎么做？很简单，对娱乐类新闻标题所包含的每个词按上述步骤计算x2值，然后按x2值排序，取x2值最大的100个词。</p>
]]></content>
      
        <categories>
            
            <category> 数据分析 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 卡方检验 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析系列（1）：SQL查询执行顺序]]></title>
      <url>/2017/06/09/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9ASQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/</url>
      <content type="html"><![CDATA[<p>SQL语句有一个让大部分人都感到困惑的特性，就是：SQL语句的执行顺序跟其语句的语法顺序并不一致。SQL语句的执行顺序是：</p>
<a id="more"></a>
<ol>
<li>FROM</li>
<li>ON</li>
<li>JOIN</li>
<li>WHERE</li>
<li>GROUP BY</li>
<li>HAVING</li>
<li>SELECT</li>
<li>DISTINCT</li>
<li>UNION</li>
<li>ORDER BY</li>
<li>LIMIT</li>
</ol>
<h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><p>这里的测试操作都是在MySQL数据库上完成的。</p>
<h3 id="1-1-新建数据库"><a href="#1-1-新建数据库" class="headerlink" title="1.1 新建数据库"></a>1.1 新建数据库</h3><p>首先我们新建一个测试数据库TestDB</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create database TestDB;</div></pre></td></tr></table></figure>
<h3 id="1-2-创建测试集table1和table2"><a href="#1-2-创建测试集table1和table2" class="headerlink" title="1.2 创建测试集table1和table2"></a>1.2 创建测试集table1和table2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE table1</div><div class="line"> (</div><div class="line">     customer_id VARCHAR(10) NOT NULL,</div><div class="line">     city VARCHAR(10) NOT NULL,</div><div class="line">     PRIMARY KEY(customer_id)</div><div class="line"> )ENGINE=INNODB DEFAULT CHARSET=UTF8;</div><div class="line"></div><div class="line"> CREATE TABLE table2</div><div class="line"> (</div><div class="line">     order_id INT NOT NULL auto_increment,</div><div class="line">     customer_id VARCHAR(10),</div><div class="line">     PRIMARY KEY(order_id)</div><div class="line"> )ENGINE=INNODB DEFAULT CHARSET=UTF8;</div></pre></td></tr></table></figure>
<h3 id="1-3-插入测试数据"><a href="#1-3-插入测试数据" class="headerlink" title="1.3 插入测试数据"></a>1.3 插入测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">INSERT INTO table1(customer_id,city) VALUES(&apos;163&apos;,&apos;hangzhou&apos;);</div><div class="line"> INSERT INTO table1(customer_id,city) VALUES(&apos;9you&apos;,&apos;shanghai&apos;);</div><div class="line"> INSERT INTO table1(customer_id,city) VALUES(&apos;tx&apos;,&apos;hangzhou&apos;);</div><div class="line"> INSERT INTO table1(customer_id,city) VALUES(&apos;baidu&apos;,&apos;hangzhou&apos;);</div><div class="line"></div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;163&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;163&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(&apos;tx&apos;);</div><div class="line"> INSERT INTO table2(customer_id) VALUES(NULL);</div></pre></td></tr></table></figure>
<p>准备工作做完以后，table1和table2看起来应该像下面这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">mysql&gt; select * from table1;</div><div class="line">+-------------+----------+</div><div class="line">| customer_id | city     |</div><div class="line">+-------------+----------+</div><div class="line">| 163         | hangzhou |</div><div class="line">| 9you        | shanghai |</div><div class="line">| baidu       | hangzhou |</div><div class="line">| tx          | hangzhou |</div><div class="line">+-------------+----------+</div><div class="line">4 rows in set (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; select * from table2;</div><div class="line">+----------+-------------+</div><div class="line">| order_id | customer_id |</div><div class="line">+----------+-------------+</div><div class="line">|        1 | 163         |</div><div class="line">|        2 | 163         |</div><div class="line">|        3 | 9you        |</div><div class="line">|        4 | 9you        |</div><div class="line">|        5 | 9you        |</div><div class="line">|        6 | tx          |</div><div class="line">|        7 | NULL        |</div><div class="line">+----------+-------------+</div><div class="line">7 rows in set (0.00 sec)</div></pre></td></tr></table></figure>
<h3 id="1-4-准备SQL查询语句"><a href="#1-4-准备SQL查询语句" class="headerlink" title="1.4 准备SQL查询语句"></a>1.4 准备SQL查询语句</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">SELECT a.customer_id, COUNT(b.order_id) as total_orders</div><div class="line"> FROM table1 AS a</div><div class="line"> LEFT JOIN table2 AS b</div><div class="line"> ON a.customer_id = b.customer_id</div><div class="line"> WHERE a.city = &apos;hangzhou&apos;</div><div class="line"> GROUP BY a.customer_id</div><div class="line"> HAVING count(b.order_id) &lt; 2</div><div class="line"> ORDER BY total_orders DESC;</div></pre></td></tr></table></figure>
<p>这些测试表和测试数据均来自《MySQL技术内幕：SQL编程》，接下来根据这个语句来详细地讲述SQL逻辑查询语句的执行顺序。</p>
<h2 id="二、SQL查询语句执行顺序"><a href="#二、SQL查询语句执行顺序" class="headerlink" title="二、SQL查询语句执行顺序"></a>二、SQL查询语句执行顺序</h2><p>现在，我们先给出一个查询语句的执行顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">(7)     SELECT </div><div class="line">(8)     DISTINCT &lt;select_list&gt;</div><div class="line">(1)     FROM &lt;left_table&gt;</div><div class="line">(3)     &lt;join_type&gt; JOIN &lt;right_table&gt;</div><div class="line">(2)     ON &lt;join_condition&gt;</div><div class="line">(4)     WHERE &lt;where_condition&gt;</div><div class="line">(5)     GROUP BY &lt;group_by_list&gt;</div><div class="line">(6)     HAVING &lt;having_condition&gt;</div><div class="line">(9)     ORDER BY &lt;order_by_condition&gt;</div><div class="line">(10)    LIMIT &lt;limit_number&gt;</div></pre></td></tr></table></figure>
<p>在这些SQL语句的执行过程中，都会产生一个虚拟表，用来保存SQL语句的执行结果（这是重点），现在就追踪这个虚拟表的变化，得到最终的查询结果的过程，来分析整个SQL逻辑查询的执行顺序和过程。</p>
<h3 id="2-1-执行FROM语句"><a href="#2-1-执行FROM语句" class="headerlink" title="2.1 执行FROM语句"></a>2.1 执行FROM语句</h3><p>第一步，执行<code>FROM</code>语句。我们首先需要知道最开始从哪个表开始的，这就是<code>FROM</code>告诉我们的。现在有了<code>left table</code>和<code>right table</code>两个表，我们到底从哪个表开始，还是会从两个表进行某种联系以后再开始呢？它们之间如何产生联系呢？——笛卡尔积<br>，笛卡尔积是所有可能的有序对组成的集合，其中有序对的第一个对象是X的成员，第二个对象是Y的成员。</p>
<p>经过<code>FROM</code>语句对两个表执行笛卡尔积，会得到一个虚拟表，暂且叫VT1(vitual table 1)，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 9you        | shanghai |        1 | 163         |</div><div class="line">| baidu       | hangzhou |        1 | 163         |</div><div class="line">| tx          | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        2 | 163         |</div><div class="line">| baidu       | hangzhou |        2 | 163         |</div><div class="line">| tx          | hangzhou |        2 | 163         |</div><div class="line">| 163         | hangzhou |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| baidu       | hangzhou |        3 | 9you        |</div><div class="line">| tx          | hangzhou |        3 | 9you        |</div><div class="line">| 163         | hangzhou |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| baidu       | hangzhou |        4 | 9you        |</div><div class="line">| tx          | hangzhou |        4 | 9you        |</div><div class="line">| 163         | hangzhou |        5 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| baidu       | hangzhou |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        5 | 9you        |</div><div class="line">| 163         | hangzhou |        6 | tx          |</div><div class="line">| 9you        | shanghai |        6 | tx          |</div><div class="line">| baidu       | hangzhou |        6 | tx          |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| 163         | hangzhou |        7 | NULL        |</div><div class="line">| 9you        | shanghai |        7 | NULL        |</div><div class="line">| baidu       | hangzhou |        7 | NULL        |</div><div class="line">| tx          | hangzhou |        7 | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>总共有28（table1的记录数*table2的记录总数）条记录。这就是VT1的结果，接下来的操作就在VT!的基础上进行。</p>
<h3 id="2-2-执行ON过滤"><a href="#2-2-执行ON过滤" class="headerlink" title="2.2 执行ON过滤"></a>2.2 执行ON过滤</h3><p>执行完笛卡尔积以后，接着就进行<code>ON a.customer_id = b.customer_id</code>条件过滤，根据<code>ON</code>中指定的条件，去掉那些不符合条件的数据，得到VT2表，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>VT2就是经过<code>ON</code>条件筛选以后得到的有用数据，而接下来的操作将在VT2的基础上继续进行。</p>
<h3 id="2-3-添加外部行"><a href="#2-3-添加外部行" class="headerlink" title="2.3 添加外部行"></a>2.3 添加外部行</h3><p>这一步只有在连接类型为<code>OUTER JOIN</code>时才发生，如<code>LEFT OUTER JOIN</code>、<code>RIGHT OUTER JOIN</code>和<code>FULL OUTER JOIN</code>。在大多数的时候，我们都是会省略掉<code>OUTER</code>关键字的，但<code>OUTER</code>表示的就是外部行的概念。</p>
<p><code>LEFT OUTER JOIN</code>把左表记为保留表，得到的结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p><code>RIGHT OUTER JOIN</code>把右表记为保留表，得到的结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| NULL        | NULL     |        7 | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p><code>FULL OUTER JOIN</code>把左右表都作为保留表，得到的结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">| NULL        | NULL     |        7 | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>添加外部行的工作就是在VT2表的基础上添加保留表中被过滤条件过滤掉的数据，非保留表中的数据被赋予了NULL值，最后生成虚拟表VT3。</p>
<p>由于在准备的测试SQL查询逻辑语句中使用的是<code>LEFT JOIN</code>，过滤掉了以下这条数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">| baidu       | hangzhou |</div></pre></td></tr></table></figure>
<p>现在就把这条数据添加到VT2表中，得到的VT3表如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| 9you        | shanghai |        3 | 9you        |</div><div class="line">| 9you        | shanghai |        4 | 9you        |</div><div class="line">| 9you        | shanghai |        5 | 9you        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>接下里的操作都会在该VT3表上进行。</p>
<h3 id="2-4-执行WHERE过滤"><a href="#2-4-执行WHERE过滤" class="headerlink" title="2.4 执行WHERE过滤"></a>2.4 执行WHERE过滤</h3><p>对添加外部行得到的VT3进行WHERE过滤，只有符合<where_condition>的记录才会输出到虚拟表VT4中。当我们执行<code>WHERE a.city = &#39;hangzhou&#39;</code>的时候，就会得到以下内容，并存在虚拟表VT4中：</where_condition></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>但是在使用WHERE字句时，需要注意以下两点：</p>
<ul>
<li><ol>
<li>由于数据还没有分组，因此还不能在WHERE过滤器中使用<code>where_condition =MIN(col)</code>这类分组统计的过滤；</li>
</ol>
</li>
<li><ol>
<li>由于还没有进行列的选取操作，因此在WHERE中使用列的别名也是不被允许的，如：<code>SELECT city AS c FROM t WHERE c=&#39;shanghai&#39;;</code>是不允许出现的。</li>
</ol>
</li>
</ul>
<h3 id="2-5-执行GROUP-BY分组"><a href="#2-5-执行GROUP-BY分组" class="headerlink" title="2.5 执行GROUP BY分组"></a>2.5 执行GROUP BY分组</h3><p><code>GROUP BY</code>子句主要是对使用<code>WHERE</code>子句得到的虚拟表进行分组操作。即“根据(by)一定的规则进行分组(Group)”。它的作用是通过一定的规则将一个数据集划分成若干个小的区域，然后针对若干个小区域进行数据处理。我们执行测试语句中的<code>GROUP BY a.customer_id</code>，就是对VT4按照<code>a.customer_id</code>进行了分组，这里就得到了以下三个组别</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">第一组</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| 163         | hangzhou |        1 | 163         |</div><div class="line">| 163         | hangzhou |        2 | 163         |</div><div class="line">第二组</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">第三组</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>得到的内容会存入虚拟表VT5中，此时，我们就得到了一个VT5虚拟表，接下来的操作都会在该表上完成。</p>
<h3 id="2-6-执行HAVING过滤"><a href="#2-6-执行HAVING过滤" class="headerlink" title="2.6 执行HAVING过滤"></a>2.6 执行HAVING过滤</h3><p>HAVING字句主要和<code>GROUP BY</code>字句配合使用，对分组得到的VT5虚拟表进行条件过滤。当我执行测试语句中的<code>HAVING COUNT(b.order_id)&lt;2</code>时，将得到以下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| customer_id | city     | order_id | customer_id |</div><div class="line">+-------------+----------+----------+-------------+</div><div class="line">| baidu       | hangzhou |     NULL | NULL        |</div><div class="line">| tx          | hangzhou |        6 | tx          |</div><div class="line">+-------------+----------+----------+-------------+</div></pre></td></tr></table></figure>
<p>这就是虚拟表6</p>
<h3 id="2-7-SELECT列表"><a href="#2-7-SELECT列表" class="headerlink" title="2.7 SELECT列表"></a>2.7 SELECT列表</h3><p>现在才会执行到SELECT子句，不要以为<code>SELECT</code>子句被写在第一行，就是第一个被执行的。</p>
<p>执行测试语句中的<code>SELECT a.customer_id ,COUNT(b.oredr_id) as total_orders</code>，我们从虚拟表VT6中选择我们需要的内容。我们将得到以下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">+-------------+--------------+</div><div class="line">| customer_id | total_orders |</div><div class="line">+-------------+--------------+</div><div class="line">| baidu       |            0 |</div><div class="line">| tx          |            1 |</div><div class="line">+-------------+--------------+</div></pre></td></tr></table></figure>
<h3 id="2-8-执行DISTINCT子句"><a href="#2-8-执行DISTINCT子句" class="headerlink" title="2.8 执行DISTINCT子句"></a>2.8 执行DISTINCT子句</h3><p>如果在查询中指定了<code>DISTINCT</code>子句，则会创建一张内存临时表（如果内存放不下，就需要存放在硬盘了）。这张临时表的表结构和上一步产生的虚拟表VT7是一样的，不同的是对进行DISTINCT操作的列增加了一个唯一索引，以此来去除重复数据。</p>
<p>由于测试SQL语句中并没有使用DISTINCT，所以，在该查询中，这一步不会生成一个虚拟表。</p>
<h3 id="2-9-执行ORDER-BY子句"><a href="#2-9-执行ORDER-BY子句" class="headerlink" title="2.9 执行ORDER BY子句"></a>2.9 执行ORDER BY子句</h3><p>对虚拟表中的内容按照指定的列进行排序，然后返回一个新的虚拟表，我们执行测试SQL语句中的<code>ORDER BY total_orders DESC</code>，就会得到以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">+-------------+--------------+</div><div class="line">| customer_id | total_orders |</div><div class="line">+-------------+--------------+</div><div class="line">| tx          |            1 |</div><div class="line">| baidu       |            0 |</div><div class="line">+-------------+--------------+</div></pre></td></tr></table></figure></p>
<p>可以看到这是对<code>total_orders</code>列进行降序排列。上述结果会存储在VT8中。</p>
<h3 id="2-10执行LIMIT子句"><a href="#2-10执行LIMIT子句" class="headerlink" title="2.10执行LIMIT子句"></a>2.10执行LIMIT子句</h3><p><code>LIMIT</code>子句从上一步得到的VT8虚拟表中选出从指定位置开始的指定行数据。对于没有营养ORDER BY的LIMIT子句，得到的结果同样是无序的，所以，很多时候，我们都会看到LIMIT子句会和ORDER BY子句一起使用。</p>
<p>MYSQL数据库的LIMIT支持如下形式的选择：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LIMIT n, m</div></pre></td></tr></table></figure></p>
<p>表示从第n条记录开始选择m条记录。而很多开发人员喜欢使用该语句来解决分页问题。对于小数据，使用LIMIT子句没有任何问题，当数据量非常大的时候，使用LIMIT n, m是非常低效的。因为LIMIT的机制是每次都是从头开始扫描，如果需要从第60万行开始，读取3条数据，就需要先扫描定位到60万行，然后再进行读取，而扫描的过程是一个非常低效的过程。所以，对于大数据处理时，是非常有必要在应用层建立一定的缓存机制</p>
]]></content>
      
        <categories>
            
            <category> 数据分析 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle系列（1）：Kaggle 数据挖掘比赛经验分享]]></title>
      <url>/2017/06/05/Kaggle%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AKaggle%20%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/</url>
      <content type="html"><![CDATA[<p>转载自知乎：<a href="https://zhuanlan.zhihu.com/p/26820998" target="_blank" rel="noopener">Kaggle 数据挖掘比赛经验分享</a> 作者是陈成龙，目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。笔者从 2013 年开始，陆续参加了多场 Kaggle上面举办的比赛，相继获得了 CrowdFlower 搜索相关性比赛第一名（1326支队伍）和 HomeDepot 商品搜索相关性比赛第三名（2125支队伍），曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。笔者目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。此文分享笔者在参加数据挖掘比赛过程中的一点心得体会。</p>
<a id="more"></a>
<h2 id="一、Kaggle基本介绍"><a href="#一、Kaggle基本介绍" class="headerlink" title="一、Kaggle基本介绍"></a>一、Kaggle基本介绍</h2><p>Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。在 Kaggle 上，企业或者研究机构发布商业和科研难题，悬赏吸引全球的数据科学家，通过众包的方式解决建模问题。而参赛者可以接触到丰富的真实数据，解决实际问题，角逐名次，赢取奖金。诸如 Google，Facebook，Microsoft 等知名科技公司均在 Kaggle 上面举办过数据挖掘比赛。2017年3月，Kaggle 被 Google CloudNext 收购。</p>
<h3 id="1-1-参赛方式"><a href="#1-1-参赛方式" class="headerlink" title="1.1 参赛方式"></a>1.1 参赛方式</h3><p>可以以个人或者组队的形式参加比赛。组队人数一般没有限制，但需要在 Merger Deadline 前完成组队。为了能参与到比赛中，需要在 Entry Deadline 前进行至少一次有效提交。最简单地，可以直接提交官方提供的 Sample Submission。关于组队，建议先单独个人进行数据探索和模型构建，以个人身份进行比赛，在比赛后期（譬如离比赛结束还有 2~3 周）再进行组队，以充分发挥组队的效果（类似于模型集成，模型差异性越大，越有可能有助于效果的提升，超越单模型的效果）。当然也可以一开始就组好队，方便分工协作，讨论问题和碰撞火花。</p>
<p>Kaggle 对比赛的公正性相当重视。在比赛中，每个人只允许使用一个账号进行提交。在比赛结束后 1~2 周内，Kaggle 会对使用多账号提交的 Cheater 进行剔除（一般会对 Top 100 的队伍进行 Cheater Detection）。在被剔除者的 Kaggle 个人页面上，该比赛的成绩也会被删除，相当于该选手从没参加过这个比赛。此外，队伍之间也不能私自分享代码或者数据，除非在论坛上面公开发布。</p>
<p>比赛一般只提交测试集的预测结果，无需提交代码。每人（或每个队伍）每天有提交次数的限制，一般为2次或者5次，在 Submission 页面会有提示。</p>
<h3 id="1-2-比赛获奖"><a href="#1-2-比赛获奖" class="headerlink" title="1.2 比赛获奖"></a>1.2 比赛获奖</h3><p>Kaggle 比赛奖金丰厚，一般前三名均可以获得奖金。在最近落幕的第二届 National Data Science Bowl 中，总奖金池高达 100W 美刀，其中第一名可以获得 50W 美刀的奖励，即使是第十名也能收获 2.5W 美刀的奖金。获奖的队伍需要在比赛结束后 1~2 周内，准备好可执行的代码以及 README，算法说明文档等提交给 Kaggle 来进行获奖资格的审核。Kaggle 会邀请获奖队伍在 Kaggle Blog 中发表 Interview，来分享比赛故事和经验心得。对于某些比赛，Kaggle 或者主办方会邀请获奖队伍进行电话/视频会议，获奖队伍进行 Presentation，并与主办方团队进行交流。</p>
<h3 id="1-3-比赛类型"><a href="#1-3-比赛类型" class="headerlink" title="1.3 比赛类型"></a>1.3 比赛类型</h3><p>从 Kaggle 提供的官方分类来看，可以划分为以下类型（如下图1所示）：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-14%20%E4%B8%8B%E5%8D%884.28.58.png" alt="屏幕快照 2017-06-14 下午4.28.58"></p>
<ul>
<li>Featured：商业或科研难题，奖金一般较为丰厚；</li>
<li>Recruitment：比赛的奖励为面试机会；</li>
<li>Research：科研和学术性较强的比赛，也会有一定的奖金，一般需要较强的领域和专业知识；</li>
<li>Playground：提供一些公开的数据集用于尝试模型和算法；</li>
<li>Getting Started：提供一些简单的任务用于熟悉平台和比赛；</li>
<li>In Class：用于课堂项目作业或者考试。</li>
</ul>
<p>从领域归属划分：包含搜索相关性，广告点击率预估，销量预估，贷款违约判定，癌症检测等。<br> 从任务目标划分：包含回归，分类（二分类，多分类，多标签），排序，混合体（分类+回归）等。<br> 从数据载体划分：包含文本，语音，图像和时序序列等。<br> 从特征形式划分：包含原始数据，明文特征，脱敏特征（特征的含义不清楚）等。</p>
<h3 id="1-4-比赛流程"><a href="#1-4-比赛流程" class="headerlink" title="1.4 比赛流程"></a>1.4 比赛流程</h3><p>一个数据挖掘比赛的基本流程如下图2所示，具体的模块我将在下一章进行展开陈述。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-14%20%E4%B8%8B%E5%8D%884.30.58.png" alt="屏幕快照 2017-06-14 下午4.30.58"></p>
<p>这里想特别强调的一点是，Kaggle 在计算得分的时候，有Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。用于计算 Public LB 和 Private LB 的数据有不同的划分方式，具体视比赛和数据的类型而定，一般有随机划分，按时间划分或者按一定规则划分。</p>
<p>这个过程可以概括如下图3所示，其目的是避免模型过拟合，以得到泛化能力好的模型。如果不设置 Private LB（即所有的测试数据都用于计算 Public LB），选手不断地从 Public LB（即测试集）中获得反馈，进而调整或筛选模型。这种情况下，测试集实际上是作为验证集参与到模型的构建和调优中来。Public LB上面的效果并非是在真实未知数据上面的效果，不能可靠地反映模型的效果。划分 Public LB 和 Private LB 这样的设置，也在提醒参赛者，我们建模的目标是要获得一个在未知数据上表现良好的模型，而并非仅仅是在已知数据上效果好。</p>
<h2 id="二、数据挖掘比赛流程"><a href="#二、数据挖掘比赛流程" class="headerlink" title="二、数据挖掘比赛流程"></a>二、数据挖掘比赛流程</h2><p>从上面图2可以看到，做一个数据挖掘比赛，主要包含了数据分析，数据清洗，特征工程，模型训练和验证等四个大的模块，以下来一一对其进行介绍。</p>
<h3 id="2-1-数据分析"><a href="#2-1-数据分析" class="headerlink" title="2.1 数据分析"></a>2.1 数据分析</h3><p>数据分析可能涉及以下方面：</p>
<ul>
<li><p>分析特征变量的分布</p>
<ul>
<li>特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。</li>
<li>特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。</li>
</ul>
</li>
<li><p>分析目标变量的分布</p>
<ul>
<li>目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模（在这种情况下，需要对预测结果进行逆变换）。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。</li>
<li>目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样（Stratified Sampling）。</li>
</ul>
</li>
<li>分析变量之间两两的分布和相关度<ul>
<li>可以用于发现高相关和共线性的特征。</li>
</ul>
</li>
</ul>
<p>通过对数据进行探索性分析（甚至有些情况下需要肉眼观察样本），还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。</p>
<h3 id="2-2-数据清洗"><a href="#2-2-数据清洗" class="headerlink" title="2.2 数据清洗"></a>2.2 数据清洗</h3><p>数据清洗是指对提供的原始数据进行一定的加工，使得其方便后续的特征抽取。其与特征抽取的界限有时也没有那么明确。常用的数据清洗一般包括：</p>
<ul>
<li>数据的拼接<ul>
<li>提供的数据散落在多个文件，需要根据相应的键值进行数据的拼接。</li>
</ul>
</li>
<li><p>特征缺失值的处理</p>
<ul>
<li>特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受 outlier 的影响；  </li>
<li>特征值为离散值：使用众数代替</li>
</ul>
</li>
<li><p>文本数据的清洗</p>
<ul>
<li>在比赛当中，如果数据包含文本，往往需要进行大量的数据清洗工作。如去除HTML 标签，分词，拼写纠正, 同义词替换，去除停词，抽词干，数字和单位格式统一等。     </li>
</ul>
</li>
</ul>
<h3 id="2-3-特征工程"><a href="#2-3-特征工程" class="headerlink" title="2.3 特征工程"></a>2.3 特征工程</h3><p>有一种说法是，特征决定了效果的上限，而不同模型只是以不同的方式或不同的程度来逼近这个上限。这样来看，好的特征输入对于模型的效果至关重要，正所谓”Garbage in, garbage out”。要做好特征工程，往往跟领域知识和对问题的理解程度有很大的关系，也跟一个人的经验相关。特征工程的做法也是Case by Case，以下就一些点，谈谈自己的一些看法。</p>
<h4 id="2-3-1-特征变换"><a href="#2-3-1-特征变换" class="headerlink" title="2.3.1 特征变换"></a>2.3.1 特征变换</h4><p>主要针对一些长尾分布的特征，需要进行幂变换或者对数变换，使得模型（LR或者DNN）能更好的优化。需要注意的是，Random Forest 和 GBDT 等模型对单调的函数变换不敏感。其原因在于树模型在求解分裂点的时候，只考虑排序分位点。</p>
<h4 id="2-3-2-特征编码"><a href="#2-3-2-特征编码" class="headerlink" title="2.3.2 特征编码"></a>2.3.2 特征编码</h4><p>对于离散的类别特征，往往需要进行必要的特征转换/编码才能将其作为特征输入到模型中。常用的编码方式有 LabelEncoder，OneHotEncoder（sklearn里面的接口）。譬如对于”性别”这个特征（取值为男性和女性），使用这两种方式可以分别编码为$\{0,1\}$和$\{[1,0], [0,1]\}$。</p>
<p>对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理：</p>
<ul>
<li>统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优；</li>
<li>统计每个 ID 特征的一些统计量（譬如历史平均点击率，历史平均浏览率）等代替该 ID 取值作为特征，具体可以参考 Avazu 点击率预估比赛第二名的获奖方案；</li>
<li>参考 word2vec 的方式，将每个类别特征的取值映射到一个连续的向量，对这个向量进行初始化，跟模型一起训练。训练结束后，可以同时得到每个ID的Embedding。具体的使用方式，可以参考 Rossmann 销量预估竞赛第三名的获奖方案(<a href="https://github.com/entron/entity-embedding-rossmann" target="_blank" rel="noopener">entron/entity-embedding-rossmann</a>)</li>
</ul>
<p>对于 Random Forest 和 GBDT 等模型，如果类别特征存在较多的取值，可以直接使用 LabelEncoder 后的结果作为特征。</p>
<h3 id="2-4-模型训练与验证"><a href="#2-4-模型训练与验证" class="headerlink" title="2.4 模型训练与验证"></a>2.4 模型训练与验证</h3><h4 id="2-4-1-模型选择"><a href="#2-4-1-模型选择" class="headerlink" title="2.4.1 模型选择"></a>2.4.1 模型选择</h4><p>在处理好特征后，我们可以进行模型的训练和验证。</p>
<ul>
<li>对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式；</li>
<li>对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好；</li>
<li>数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模，具体可以参考2.5.2节 Stacking 部分。</li>
</ul>
<h4 id="2-4-2-调参和模型验证"><a href="#2-4-2-调参和模型验证" class="headerlink" title="2.4.2 调参和模型验证"></a>2.4.2 调参和模型验证</h4><p>对于选定的特征和模型，我们往往还需要对模型进行超参数的调优，才能获得比较理想的效果。调参一般可以概括为以下三个步骤：</p>
<ul>
<li><p>训练集和验证集的划分。根据比赛提供的训练集和测试集，模拟其划分方式对训练集进行划分为本地训练集和本地验证集。划分的方式视具体比赛和数据而定，常用的方式有：</p>
<ul>
<li>随机划分：譬如随机采样 70% 作为训练集，剩余的 30% 作为测试集。在这种情况下，本地可以采用 KFold 或者 Stratified KFold 的方法来构造训练集和验证集。</li>
<li>按时间划分：一般对应于时序序列数据，譬如取前 7 天数据作为训练集，后 1 天数据作为测试集。这种情况下，划分本地训练集和验证集也需要按时间先后划分。常见的错误方式是随机划分，这种划分方式可能会导致模型效果被高估。</li>
<li>按某些规则划分：在 HomeDepot 搜索相关性比赛中，训练集和测试集中的 Query 集合并非完全重合，两者只有部分交集。而在另外一个相似的比赛中（CrowdFlower 搜索相关性比赛），训练集和测试集具有完全一致的 Query 集合。对于 HomeDepot 这个比赛中，训练集和验证集数据的划分，需要考虑 Query 集合并非完全重合这个情况，其中的一种方法可以参考<a href="https://github.com/ChenglongChen/Kaggle_HomeDepot" target="_blank" rel="noopener">第三名的获奖方案</a>。</li>
</ul>
</li>
<li>指定参数空间。在指定参数空间的时候，需要对模型参数以及其如何影响模型的效果有一定的了解，才能指定出合理的参数空间。譬如DNN或者XGBoost中学习率这个参数，一般就选 0.01 左右就 OK 了（太大可能会导致优化算法错过最优化点，太小导致优化收敛过慢）。再如 Random Forest，一般设定树的棵数范围为 100~200 就能有不错的效果，当然也有人固定数棵数为 500，然后只调整其他的超参数。</li>
</ul>
<ul>
<li>按照一定的方法进行参数搜索。常用的参数搜索方法有，Grid Search，Random Search以及一些自动化的方法（如 Hyperopt）。其中，Hyperopt 的方法，根据历史已经评估过的参数组合的效果，来推测本次评估使用哪个参数组合更有可能获得更好的效果。有关这些方法的介绍和对比，可以参考文献 [2]。</li>
</ul>
<h4 id="2-4-3-适当利用Public-LB的反馈"><a href="#2-4-3-适当利用Public-LB的反馈" class="headerlink" title="2.4.3 适当利用Public LB的反馈"></a>2.4.3 适当利用Public LB的反馈</h4><p>在2.4.2节中我们提到本地验证（Local Validation）结果，当将预测结果提交到 Kaggle 上时，我们还会接收到 Public LB 的反馈结果。如果这两个结果的变化趋势是一致的，如 Local Validation 有提升，Public LB 也有提升，我们可以借助 Local Validation 的变化来感知模型的演进情况，而无需靠大量的 Submission。如果两者的变化趋势不一致，需要考虑2.4.2节中提及的本地训练集和验证集的划分方式，是否跟训练集和测试集的划分方式一致。</p>
<p>另外，在以下一些情况下，往往 Public LB 反馈亦会提供有用信息，适当地使用这些反馈也许会给你带来优势。如图4所示，(a)和(b)表示数据与时间没有明显的关系（如图像分类），(c)和(d)表示数据随时间变化（如销量预估中的时序序列）。(a)和(b)的区别在于，训练集样本数相对于 Public LB 的量级大小，其中(a)中训练集样本数远超于 Public LB 的样本数，这种情况下基于训练集的 Local Validation 更可靠；而(b)中，训练集数目与 Public LB 相当，这种情况下，可以结合 Public LB 的反馈来指导模型的选择。一种融合的方式是根据 Local Validation 和 Public LB 的样本数目，按比例进行加权。譬如评估标准为正确率，Local Validation 的样本数为$N_l$，正确率为$A_l$；Public LB 的样本数为 $N_p$，正确率为 $A_p$。则可以使用融合后的指标：$（N_l <em> A_l + N_p </em> A_p）/(N_l + N_p)$，来进行模型的筛选。对于(c)和(d)，由于数据分布跟时间相关，很有必要使用 Public LB 的反馈来进行模型的选择，尤其对于(c)图所示的情况。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-14%20%E4%B8%8B%E5%8D%885.17.50.png" alt="屏幕快照 2017-06-14 下午5.17.50"><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-14%20%E4%B8%8B%E5%8D%885.17.59.png" alt="屏幕快照 2017-06-14 下午5.17.59"></p>
<h3 id="2-5-模型集成"><a href="#2-5-模型集成" class="headerlink" title="2.5 模型集成"></a>2.5 模型集成</h3><p>如果想在比赛中获得名次，几乎都要进行模型集成（组队也是一种模型集成）。关于模型集成的介绍，已经有比较好的博文了，可以参考 [3]。在这里，我简单介绍下常用的方法，以及个人的一些经验。</p>
<h4 id="2-5-1-Averaging-和-Voting"><a href="#2-5-1-Averaging-和-Voting" class="headerlink" title="2.5.1 Averaging 和 Voting"></a>2.5.1 Averaging 和 Voting</h4><p>直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。</p>
<h4 id="2-5-2-Stacking"><a href="#2-5-2-Stacking" class="headerlink" title="2.5.2 Stacking"></a>2.5.2 Stacking</h4><p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-14%20%E4%B8%8B%E5%8D%885.26.44.png" alt="屏幕快照 2017-06-14 下午5.26.44"><br>图5展示了使用 5-Fold 进行一次 Stacking 的过程（当然在其上可以再叠加 Stage 2, Stage 3 等）。其主要的步骤如下：</p>
<ul>
<li><strong>数据集划分</strong>。将训练数据按照5-Fold进行划分（如果数据跟时间有关，需要按时间划分，更一般的划分方式请参考3.4.2节，这里不再赘述）；</li>
<li><strong>基础模型训练 I</strong>（如图5第一行左半部分所示）。按照交叉验证（Cross Validation）的方法，在训练集（Training Fold）上面训练模型（如图灰色部分所示），并在验证集（Validation Fold）上面做预测，得到预测结果（如图黄色部分所示）。最后综合得到整个训练集上面的预测结果（如图第一个黄色部分的CV Prediction所示）。</li>
<li><strong>基础模型训练 II</strong>（如图5第二和三行左半部分所示）。在全量的训练集上训练模型（如图第二行灰色部分所示），并在测试集上面做预测，得到预测结果（如图第三行虚线后绿色部分所示）。</li>
<li><strong>Stage 1 模型集成训练 I</strong>（如图5第一行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集，按照步骤 2 可以得到 Stage 1模型集成的 CV Prediction。</li>
<li><strong>Stage 1 模型集成训练 II</strong>（如图5第二和三行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集和步骤 3 中得到的 Prediction 当作新的测试集，按照步骤 3 可以得到 Stage 1 模型集成的测试集 Prediction。此为 Stage 1 的输出，可以提交至 Kaggle 验证其效果。</li>
</ul>
<p>在图5中，基础模型只展示了一个，而实际应用中，基础模型可以多种多样，如SVM，DNN，XGBoost 等。也可以相同的模型，不同的参数，或者不同的样本权重。重复4和5两个步骤，可以相继叠加 Stage 2, Stage 3 等模型。</p>
<h4 id="2-5-3-Blending"><a href="#2-5-3-Blending" class="headerlink" title="2.5.3 Blending"></a>2.5.3 Blending</h4><p>Blending 与 Stacking 类似，但单独留出一部分数据（如 20%）用于训练 Stage X 模型。</p>
<h4 id="2-5-4-Bagging-Ensemble-Selection"><a href="#2-5-4-Bagging-Ensemble-Selection" class="headerlink" title="2.5.4 Bagging Ensemble Selection"></a>2.5.4 Bagging Ensemble Selection</h4><p>Bagging Ensemble Selection [5] 是我在 CrowdFlower 搜索相关性比赛中使用的方法，其主要的优点在于可以以优化任意的指标来进行模型集成。这些指标可以是可导的（如 LogLoss 等）和不可导的（如正确率，AUC，Quadratic Weighted Kappa等）。它是一个前向贪婪算法，存在过拟合的可能性，作者在文献 [5] 中提出了一系列的方法（如 Bagging）来降低这种风险，稳定集成模型的性能。使用这个方法，需要有成百上千的基础模型。为此，在 CrowdFlower 的比赛中，我把在调参过程中所有的中间模型以及相应的预测结果保留下来，作为基础模型。这样做的好处是，不仅仅能够找到最优的单模型（Best Single Model），而且所有的中间模型还可以参与模型集成，进一步提升效果。</p>
<h3 id="2-6-自动化框架"><a href="#2-6-自动化框架" class="headerlink" title="2.6 自动化框架"></a>2.6 自动化框架</h3><p>从上面的介绍可以看到，做一个数据挖掘比赛涉及到的模块非常多，若有一个较自动化的框架会使得整个过程更加的高效。在 CrowdFlower 比赛较前期，我对整一个项目的代码架构进行了重构，抽象出来特征工程，模型调参和验证，以及模型集成等三大模块，极大的提高了尝试新特征，新模型的效率，也是我最终能斩获名次的一个有利因素。这份代码开源在 Github 上面，目前是 Github 有关 Kaggle 竞赛解决方案的 Most Stars，<a href="ChenglongChen/Kaggle_CrowdFlower">地址链接</a>。</p>
<p>其主要包含以下部分：</p>
<ul>
<li>模块化特征工程<ul>
<li>接口统一，只需写少量的代码就能够生成新的特征；</li>
<li>自动将单独的特征拼接成特征矩阵。</li>
</ul>
</li>
<li>自动化模型调参和验证<ul>
<li>自定义训练集和验证集的划分方法；</li>
<li>使用 Grid Search / Hyperopt 等方法，对特定的模型在指定的参数空间进行调优，并记录最佳的模型参数以及相应的性能。 </li>
</ul>
</li>
<li>自动化模型集成 <ul>
<li>对于指定的基础模型，按照一定的方法（如Averaging/Stacking/Blending 等）生成集成模型。 </li>
</ul>
</li>
</ul>
<h2 id="三、Kaggle竞赛方案盘点"><a href="#三、Kaggle竞赛方案盘点" class="headerlink" title="三、Kaggle竞赛方案盘点"></a>三、Kaggle竞赛方案盘点</h2><h3 id="3-1-图像分类"><a href="#3-1-图像分类" class="headerlink" title="3.1 图像分类"></a>3.1 图像分类</h3><p>到目前为止，Kaggle 平台上面已经举办了大大小小不同的赛事，覆盖图像分类，销量预估，搜索相关性，点击率预估等应用场景。在不少的比赛中，获胜者都会把自己的方案开源出来，并且非常乐于分享比赛经验和技巧心得。这些开源方案和经验分享对于广大的新手和老手来说，是入门和进阶非常好的参考资料。以下笔者结合自身的背景和兴趣，对不同场景的竞赛开源方案作一个简单的盘点，总结其常用的方法和工具，以期启发思路。</p>
<h4 id="3-1-1-图像分类"><a href="#3-1-1-图像分类" class="headerlink" title="3.1.1 图像分类"></a>3.1.1 图像分类</h4><p>National Data Science Bowl</p>
<h4 id="3-1-2-任务详情"><a href="#3-1-2-任务详情" class="headerlink" title="3.1.2 任务详情"></a>3.1.2 任务详情</h4><p>随着深度学习在视觉图像领域获得巨大成功，Kaggle 上面出现了越来越多跟视觉图像相关的比赛。这些比赛的发布吸引了众多参赛选手，探索基于深度学习的方法来解决垂直领域的图像问题。NDSB就是其中一个比较早期的图像分类相关的比赛。这个比赛的目标是利用提供的大量的海洋浮游生物的二值图像，通过构建模型，从而实现自动分类。</p>
<h4 id="3-1-3-获奖方案"><a href="#3-1-3-获奖方案" class="headerlink" title="3.1.3 获奖方案"></a>3.1.3 获奖方案</h4><p><strong>1st place:</strong>Cyclic Pooling + Rolling Feature Maps + Unsupervised and Semi-Supervised Approaches。值得一提的是，这个队伍的主力队员也是Galaxy Zoo行星图像分类比赛的第一名，其也是Theano中基于FFT的Fast Conv的开发者。在两次比赛中，使用的都是 Theano，而且用的非常溜。方案链接：<a href="http://benanne.github.io/2015/03/17/plankton.html" target="_blank" rel="noopener">Classifying plankton with deep neural networks</a></p>
<p><strong>2nd place：</strong>Deep CNN designing theory + VGG-like model + RReLU。这个队伍阵容也相当强大，有前MSRA 的研究员Xudong Cao，还有大神Tianqi Chen，Naiyan Wang，Bing XU等。Tianqi 等大神当时使用的是 CXXNet（MXNet 的前身），也在这个比赛中进行了推广。Tianqi 大神另外一个大名鼎鼎的作品就是 XGBoost，现在 Kaggle 上面几乎每场比赛的 Top 10 队伍都会使用。方案链接：<a href="https://www.kaggle.com/c/datasciencebowl/discussion/13166" target="_blank" rel="noopener">National Data Science Bowl</a></p>
<p><strong>17th place：</strong>Realtime data augmentation + BN + PReLU。方案链接：ChenglongChen/caffe-windows</p>
<h4 id="3-1-4-常用工具"><a href="#3-1-4-常用工具" class="headerlink" title="3.1.4 常用工具"></a>3.1.4 常用工具</h4><ul>
<li>Theano: Welcome – <a href="http://deeplearning.net/software/theano/" target="_blank" rel="noopener">Theano 0.9.0 documentation</a></li>
<li>Keras: <a href="https://keras.io/" target="_blank" rel="noopener">Keras Documentation</a></li>
<li>Cuda-convnet2: <a href="https://github.com/akrizhevsky/cuda-convnet2" target="_blank" rel="noopener">akrizhevsky/cuda-convnet2</a></li>
<li>Caffe: <a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe | Deep Learning Framework</a></li>
<li>CXXNET: <a href="https://github.com/dmlc/cxxnet" target="_blank" rel="noopener">dmlc/cxxnet</a></li>
<li>MXNet: <a href="https://github.com/dmlc/mxnet" target="_blank" rel="noopener">dmlc/mxnet</a></li>
</ul>
<h3 id="3-2-销量估计"><a href="#3-2-销量估计" class="headerlink" title="3.2 销量估计"></a>3.2 销量估计</h3><h4 id="3-2-1-任务名称"><a href="#3-2-1-任务名称" class="headerlink" title="3.2.1 任务名称"></a>3.2.1 任务名称</h4><p>Walmart Recruiting – Store Sales Forecasting</p>
<h4 id="3-2-2-任务详情"><a href="#3-2-2-任务详情" class="headerlink" title="3.2.2 任务详情"></a>3.2.2 任务详情</h4><p>Walmart 提供 2010-02-05 到 2012-11-01 期间的周销售记录作为训练数据，需要参赛选手建立模型预测 2012-11-02 到 2013-07-26 周销售量。比赛提供的特征数据包含：Store ID, Department ID, CPI，气温，汽油价格，失业率，是否节假日等。</p>
<h4 id="3-2-3-获奖方案"><a href="#3-2-3-获奖方案" class="headerlink" title="3.2.3 获奖方案"></a>3.2.3 获奖方案</h4><p><strong>1st place：</strong>Time series forecasting method: stlf + arima + ets。主要是基于时序序列的统计方法，大量使用了 Rob J Hyndman 的 forecast R 包。方案链接：<a href="https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8125" target="_blank" rel="noopener">Walmart Recruiting – Store Sales Forecasting</a><br><strong>2nd place：</strong>Time series forecasting + ML: arima + RF + LR + PCR。时序序列的统计方法+传统机器学习方法的混合，方案链接：<a href="https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8023" target="_blank" rel="noopener">Walmart Recruiting – Store Sales Forecasting</a><br><strong>16th place</strong>Feature engineering + GBM。方案链接：<a href="https://github.com/ChenglongChen/Kaggle_Walmart-Recruiting-Store-Sales-Forecasting" target="_blank" rel="noopener">ChenglongChen/Kaggle_Walmart-Recruiting-Store-Sales-Forecasting</a></p>
<h4 id="3-2-4-常用工具"><a href="#3-2-4-常用工具" class="headerlink" title="3.2.4 常用工具"></a>3.2.4 常用工具</h4><ul>
<li>R forecast package: <a href="https://cran.r-project.org/web/packages/forecast/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/forecast/index.html</a></li>
<li>R GBM package: <a href="https://cran.r-project.org/web/packages/gbm/index.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/gbm/index.html</a></li>
</ul>
<h3 id="3-3-搜索相关性"><a href="#3-3-搜索相关性" class="headerlink" title="3.3 搜索相关性"></a>3.3 搜索相关性</h3><h4 id="3-3-1-任务名称"><a href="#3-3-1-任务名称" class="headerlink" title="3.3.1 任务名称"></a>3.3.1 任务名称</h4><p>CrowdFlower Search Results Relevance</p>
<h4 id="3-3-2-任务详情"><a href="#3-3-2-任务详情" class="headerlink" title="3.3.2 任务详情"></a>3.3.2 任务详情</h4><p>比赛要求选手利用约几万个 (query, title, description) 元组的数据作为训练样本，构建模型预测其相关性打分 {1, 2, 3, 4}。比赛提供了 query, title和description的原始文本数据。比赛使用 Quadratic Weighted Kappa 作为评估标准，使得该任务有别于常见的回归和分类任务。</p>
<h4 id="3-3-3-获奖方案"><a href="#3-3-3-获奖方案" class="headerlink" title="3.3.3 获奖方案"></a>3.3.3 获奖方案</h4><p><strong>1st place：</strong>Data Cleaning + Feature Engineering + Base Model + Ensemble。对原始文本数据进行清洗后，提取了属性特征，距离特征和基于分组的统计特征等大量的特征，使用了不同的目标函数训练不同的模型（回归，分类，排序等），最后使用模型集成的方法对不同模型的预测结果进行融合。方案链接：<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower" target="_blank" rel="noopener">ChenglongChen/Kaggle_CrowdFlower</a></p>
<h4 id="3-3-4-常用工具"><a href="#3-3-4-常用工具" class="headerlink" title="3.3.4 常用工具"></a>3.3.4 常用工具</h4><ul>
<li>NLTK: <a href="http://www.nltk.org/" target="_blank" rel="noopener">Natural Language Toolkit</a></li>
<li>Gensim: <a href="https://radimrehurek.com/gensim/" target="_blank" rel="noopener">gensim: topic modelling for humans</a></li>
<li>XGBoost: <a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">dmlc/xgboost</a></li>
<li>RGF: <a href="https://github.com/baidu/fast_rgf" target="_blank" rel="noopener">baidu/fast_rgf</a></li>
</ul>
<h3 id="3-4-点击率预估I"><a href="#3-4-点击率预估I" class="headerlink" title="3.4 点击率预估I"></a>3.4 点击率预估I</h3><h4 id="3-4-1-任务名称"><a href="#3-4-1-任务名称" class="headerlink" title="3.4.1 任务名称"></a>3.4.1 任务名称</h4><p>Criteo Display Advertising Challenge</p>
<h4 id="3-4-2-任务详情"><a href="#3-4-2-任务详情" class="headerlink" title="3.4.2 任务详情"></a>3.4.2 任务详情</h4><p>经典的点击率预估比赛。该比赛中提供了7天的训练数据，1 天的测试数据。其中有13 个整数特征，26 个类别特征，均脱敏，因此无法知道具体特征含义。</p>
<h4 id="3-4-3-获奖方案"><a href="#3-4-3-获奖方案" class="headerlink" title="3.4.3 获奖方案"></a>3.4.3 获奖方案</h4><p><strong>1st place：</strong>GBDT 特征编码 + FFM。台大的队伍，借鉴了Facebook的方案 [6]，使用 GBDT 对特征进行编码，然后将编码后的特征以及其他特征输入到 Field-aware Factorization Machine（FFM） 中进行建模。方案链接：<a href="https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10555" target="_blank" rel="noopener">Display Advertising Challenge | Kaggle</a></p>
<p><strong>3rd place：</strong>Quadratic Feature Generation + FTRL。传统特征工程和 FTRL 线性模型的结合。方案链接：<a href="https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10534" target="_blank" rel="noopener">Display Advertising Challenge | Kaggle</a></p>
<p><strong>4th place：</strong>Feature Engineering + Sparse DNN</p>
<h4 id="3-4-4-常用工具"><a href="#3-4-4-常用工具" class="headerlink" title="3.4.4 常用工具"></a>3.4.4 常用工具</h4><ul>
<li>Vowpal Wabbit: <a href="https://github.com/JohnLangford/vowpal_wabbit" target="_blank" rel="noopener">JohnLangford/vowpal_wabbit</a></li>
<li>XGBoost: <a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">dmlc/xgboost</a></li>
<li>LIBFFM: <a href="http://www.csie.ntu.edu.tw/~r01922136/libffm/" target="_blank" rel="noopener">LIBFFM: A Library for Field-aware Factorization Machines</a></li>
</ul>
<h3 id="3-5-点击率预估II"><a href="#3-5-点击率预估II" class="headerlink" title="3.5 点击率预估II"></a>3.5 点击率预估II</h3><h4 id="3-5-1-任务名称"><a href="#3-5-1-任务名称" class="headerlink" title="3.5.1 任务名称"></a>3.5.1 任务名称</h4><p>Avazu Click-Through Rate Prediction</p>
<h4 id="3-5-2-任务详情"><a href="#3-5-2-任务详情" class="headerlink" title="3.5.2 任务详情"></a>3.5.2 任务详情</h4><p>点击率预估比赛。提供了 10 天的训练数据，1 天的测试数据，并且提供时间，banner 位置，site, app, device 特征等，8个脱敏类别特征。</p>
<h4 id="3-5-3-获奖方案"><a href="#3-5-3-获奖方案" class="headerlink" title="3.5.3 获奖方案"></a>3.5.3 获奖方案</h4><p><strong>1st place：</strong>Feature Engineering + FFM + Ensemble。还是台大的队伍，这次比赛，他们大量使用了 FFM，并只基于 FFM 进行集成。方案链接：<a href="https://www.kaggle.com/c/avazu-ctr-prediction/discussion/12608" target="_blank" rel="noopener">Click-Through Rate Prediction | Kaggle</a></p>
<p><strong>2nd place：</strong>Feature Engineering + GBDT 特征编码 + FFM + Blending。Owenzhang（曾经长时间雄霸 Kaggle 排行榜第一）的竞赛方案。Owenzhang 的特征工程做得非常有参考价值。方案链接：<a href="https://github.com/owenzhang/kaggle-avazu" target="_blank" rel="noopener">owenzhang/kaggle-avazu</a></p>
<h4 id="3-5-4-常用工具"><a href="#3-5-4-常用工具" class="headerlink" title="3.5.4 常用工具"></a>3.5.4 常用工具</h4><ul>
<li>LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines</li>
<li>XGBoost: dmlc/xgboost</li>
</ul>
<h2 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h2><p>[1] Owenzhang 的分享： Tips for Data Science Competitions</p>
<p>[2] Algorithms for Hyper-Parameter Optimization</p>
<p>[3] MLWave博客：Kaggle Ensembling Guide</p>
<p>[4] Jeong-Yoon Lee 的分享：Winning Data Science Competitions</p>
<p>[5] Ensemble Selection from Libraries of Models</p>
<p>[6] Practical Lessons from Predicting Clicks on Ads at Facebook</p>
<h2 id="五、结语"><a href="#五、结语" class="headerlink" title="五、结语"></a>五、结语</h2><p>作为曾经的学生党，十分感激和庆幸有 Kaggle 这样的平台，提供了不同领域极具挑战的任务以及丰富多样的数据。让我这种空有满（yi）腔（xie）理（wai）论（li）的数据挖掘小白，可以在真实的问题场景和业务数据中进行实操练手，提升自己的数据挖掘技能，一不小心，还能拿名次，赢奖金。如果你也跃跃欲试，不妨选一个合适的任务，开启数据挖掘之旅吧。</p>
<p>转载自知乎：<a href="https://zhuanlan.zhihu.com/p/26820998" target="_blank" rel="noopener">Kaggle 数据挖掘比赛经验分享</a></p>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> EDA </tag>
            
            <tag> 特征工程 </tag>
            
            <tag> Voting </tag>
            
            <tag> Stacking </tag>
            
            <tag> Blending </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（19）：机器学习性能评价指标]]></title>
      <url>/2017/05/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      <content type="html"><![CDATA[<h2 id="一、分类问题的评价指标"><a href="#一、分类问题的评价指标" class="headerlink" title="一、分类问题的评价指标"></a>一、分类问题的评价指标</h2><h3 id="1-1-混淆矩阵"><a href="#1-1-混淆矩阵" class="headerlink" title="1.1 混淆矩阵"></a>1.1 混淆矩阵</h3><p>对一个二分类问题，将实例分成正类（postive）或者负类（negative），但在实际分类中，会出现以下四种情况：</p>
<ul>
<li>True Positive（真正，TP）：将正类预测为正类数</li>
<li>True Negative（真负，TN）：将负类预测为负类数</li>
<li>False Positive（假正，FP）：将负类预测为正类数</li>
<li>False  Negative（假负，FN）：将正类预测为负类数</li>
</ul>
<a id="more"></a>
<p>从下图可以直观的看出四者的关系：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%885.10.54.png" alt="屏幕快照 2017-03-29 下午5.10.54"></p>
<p>混淆矩阵（Confusion matrix）又被称为错误矩阵，它是一种特定的矩阵来呈现算法性能的可视化呈现。其每一列代表预测值，每一行代表的是实际的类别，这个名字来源于他是否可以非常容易的表明多个类别是否有混淆（也就是一个class被预测为另一个class）混淆矩阵的$i$行$j$列是列别$i$被分为类别$j$的样本个数。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8810.06.48.png" alt="屏幕快照 2017-05-30 上午10.06.48"></p>
<h3 id="1-2-精确率、召回率与F1值"><a href="#1-2-精确率、召回率与F1值" class="headerlink" title="1.2 精确率、召回率与F1值"></a>1.2 精确率、召回率与F1值</h3><p> 精确率（precision rate）定义为：</p>
<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script><p> 这里需要注意的是精确率（precision）和准确率（accuracy）是不一样的</p>
<script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+TN+FP+FN}</script><p> 在非平衡数据的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击），ACC也达到了99%以上，这就没有意义了。</p>
<p>召回率（Recall rate）定义为：</p>
<script type="math/tex; mode=display">
R=\frac{TP}{TP+FN}</script><p>此外，还有F1值，它是精确率和召回率的调和均值，即</p>
<script type="math/tex; mode=display">
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script><script type="math/tex; mode=display">
F_1=\frac{2TP}{2TP+FP+FN}</script><p>精确率与召回率都很高时，$F_1$值也会很高。</p>
<h3 id="1-4-通俗理解"><a href="#1-4-通俗理解" class="headerlink" title="1.4 通俗理解"></a>1.4 通俗理解</h3><p>通俗来讲，精确率是针对我们的预测结果而言的，他表示的是预测为正的样本中有多少是对的，那么预测为正就有两种可能了，一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP）。</p>
<p>而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN）。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-29%20%E4%B8%8B%E5%8D%888.44.11.png" alt="屏幕快照 2017-05-29 下午8.44.11"></p>
<p>在信息搜索领域，精确率和召回率又被称为查准率和查全率</p>
<script type="math/tex; mode=display">查准率=\frac{检索出的相关信息量}{检索出的信息总量}</script><script type="math/tex; mode=display">查全率=\frac{检索出的相关信息量}{系统中的相关信息总量}</script><h3 id="1-5-ROC曲线"><a href="#1-5-ROC曲线" class="headerlink" title="1.5 ROC曲线"></a>1.5 ROC曲线</h3><p>ROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具（飞机、船舰），也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、犯罪心理学领域中，而且最近在机器学习（machine learning）和数据挖掘（data mining）领域也得到了很好的发展。</p>
<p>下图是一个ROC曲线的示例图。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8811.53.33.png" alt="屏幕快照 2017-05-30 上午11.53.33"></p>
<p>在这个ROC曲线的示例图中，横坐标为false<br> positive rate(FPR)，纵坐标为true positive rate（TPR）。由混淆矩阵可得到横纵轴的计算公式。</p>
<ul>
<li>1）$TPR=\frac{TP}{TP+FN}$ 代表分类器预测的正类中实际正实例占所有正实例的比例。直观上代表能将正例分对的概率。</li>
<li>2）$FPR=\frac{FP}{FP+TN}$ 代表分类器预测的正类中实际负实例占所有负实例的比例。直观上代表将负类错分为正例的概率。</li>
</ul>
<p>假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着更多的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点（0，0），阈值最小时，对应坐标点（1，1）。</p>
<p>接下来我们考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。</p>
<p>如何绘制ROC曲线呢？</p>
<p>假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，“class”一栏表示每个测试样本真正的标签（P表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8811.53.51.png" alt="屏幕快照 2017-05-30 上午11.53.51"></p>
<p>接下来，我们从高到低，依次将“Score”值作为阈值的threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第四个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8811.54.03.png" alt="屏幕快照 2017-05-30 上午11.54.03"></p>
<h3 id="1-6-AUC"><a href="#1-6-AUC" class="headerlink" title="1.6 AUC"></a>1.6 AUC</h3><p>AUC（Area under Curve）指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观地评价分类器的好坏，值越大越好。</p>
<blockquote>
<p>The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.</p>
</blockquote>
<p>首先AUC是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</p>
<p>以下是根据AUC判断分类器优劣的标准：</p>
<ul>
<li>1）AUC=1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数场合，不存在完美的分类器。</li>
<li>2）0.5&lt;AUC&lt;1，优于随机猜测。这个分类器妥善设定阈值的话，能有预测价值。</li>
<li>3）AUC=0.5，跟随机猜测一样（如丢硬币），模型没有预测价值。</li>
<li>4）AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8811.54.13.png" alt="屏幕快照 2017-05-30 上午11.54.13"></li>
</ul>
<p>那我们为什么使用ROC曲线呢？</p>
<p>既然已经有那么多的评价标准，为何还要使用ROC和AUC曲线呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现非平衡数据的现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-05-30%20%E4%B8%8A%E5%8D%8811.54.26.png" alt="屏幕快照 2017-05-30 上午11.54.26"></p>
<p>在上图中，a和c为ROC曲线，b和d为Precision-Recall曲线。a和b展示的是分类器在原始测试集（正负样本分布平衡）的结果，c和d是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall则变化较大。</p>
<h2 id="二、回归问题的评价指标"><a href="#二、回归问题的评价指标" class="headerlink" title="二、回归问题的评价指标"></a>二、回归问题的评价指标</h2><h3 id="2-1-平均绝对误差"><a href="#2-1-平均绝对误差" class="headerlink" title="2.1 平均绝对误差"></a>2.1 平均绝对误差</h3><p>平均绝对误差MAE（Mean Absolute Reeor）又被称为L1范数损失（L1-norm loss）：</p>
<script type="math/tex; mode=display">{\rm MAE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}|y_i-\hat{y}_i|</script><h3 id="2-2-平均平方误差"><a href="#2-2-平均平方误差" class="headerlink" title="2.2 平均平方误差"></a>2.2 平均平方误差</h3><p>平均平方误差MSE（Mean Squared Error）又被称为L2范数损失（L2-norm loss）:</p>
<script type="math/tex; mode=display">{\rm MSE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}(y_i-\hat{y}_i)^2</script>]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 评价指标 </tag>
            
            <tag> 精确率 </tag>
            
            <tag> 召回率 </tag>
            
            <tag> ROC </tag>
            
            <tag> AUC </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（18）：方差偏差权衡（Bias-Variance Tradeoff）]]></title>
      <url>/2017/04/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8818%EF%BC%89%EF%BC%9A%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E6%9D%83%E8%A1%A1%EF%BC%88Bias-Variance%20Tradeoff%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h2><h3 id="1-1-感性解释"><a href="#1-1-感性解释" class="headerlink" title="1.1 感性解释"></a>1.1 感性解释</h3><p>Bias和Variance是针对Generalization（泛化、一般化）来说的。在机器学习中，我们用训练数据集学习一个模型，我们通常会定义一个损失函数（Loss Function），然后将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（即将训练出来的模型运用于预测集），单纯地将训练数据集的Loss最小化，并不能保证解决更一般的问题时模型仍然是最优的，甚至不能保证模型是可用的。这个训练数据集的Loss与一般化的数据集（预测数据集）的Loss之间的差异就叫做Generalization error。</p>
<a id="more"></a>
<p>而Generalization error又可以细分为Random Error、Bias和Variance三个部分。</p>
<p>首先需要说的是随机误差。它是数据本身的噪声带来的，这种误差是不可避免的。其次如果我们能够获得所有可能的数据集合，并在这个数据集合上将Loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以真实模型一定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。</p>
<p>Bias和Variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距（除去随机误差）。</p>
<p>Bias描述的是对于测试数据集，“用所有可能的训练数据集训练出的所有模型的输出预测结果的期望”与“真实模型”的输出值（样本真实结果）之间的差异。简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就是复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)。</p>
<p>Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。</p>
<p>在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。</p>
<h3 id="1-2-图示解释"><a href="#1-2-图示解释" class="headerlink" title="1.2 图示解释"></a>1.2 图示解释</h3><p>下图将机器学习任务描述为一个打靶的活动：根据相同算法、不同训练数据集训练出的模型，对同一个样本进行预测；每个模型作出的预测相当于是一次打靶。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8B%E5%8D%885.47.28.png" alt="屏幕快照 2017-04-19 下午5.47.28"></p>
<p>左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。</p>
<p>右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。</p>
<p>左下角的示例表示偏差大而方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。</p>
<p>右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。</p>
<p>再看一个来自PRML的例子：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-20%20%E4%B8%8B%E5%8D%883.52.48.png" alt="屏幕快照 2017-04-20 下午3.52.48"><br>这是一个曲线拟合的问题，对同分布的不同数据集进行了多次的曲线拟合，左边表示方差（variance），右边表示偏差（bias），绿色是真实值函数。$In \lambda$表示的是模型的复杂度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低的时候，方差是很小的，但是偏差很大；但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了，所以方差就很大，但此时偏差就很小了。</p>
<h3 id="1-3-数学解释"><a href="#1-3-数学解释" class="headerlink" title="1.3 数学解释"></a>1.3 数学解释</h3><p>排除人为的失误，人们一般会遇到三种误差来源：随机误差、偏差和方差。</p>
<p>首先需要说明的是随机误差。随机误差是数据本身的噪声带来的，这种误差是不可避免的。一般认为随机误差服从高斯分布，记作$\varepsilon ~N\left(0,\sigma_{\varepsilon}\right)$。因此，若有变量$y$作为预测值，以及$X$作为自变量（协变量），那么我们将数据背后的真实规律$f$记作</p>
<script type="math/tex; mode=display">y=f(X)+\epsilon</script><p>偏差和方差则需要在统计上做对应的定义。</p>
<ul>
<li><strong>偏差（Bias）</strong>描述的是通过学习拟合出来的结果的期望，与真实结果之间的差距，记作<script type="math/tex; mode=display">Bias\left(X\right)=E\left[\hat{f}\left(X\right)\right]-f\left(X\right)</script></li>
<li><strong>方差（Variance）</strong>即为统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作<script type="math/tex; mode=display">E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2</script></li>
</ul>
<p>以均方误差为例，有如下推论：</p>
<script type="math/tex; mode=display">
Err\left(X\right)=E\left[\left(y-\hat{f}\left(X\right)\right)^2\right]\\
=E\left[\left(f\left(X\right)+\varepsilon -\hat{f}\left(X\right)\right)^2\right]</script><script type="math/tex; mode=display">
=\left(E\left[\hat{f}\left(X\right)\right]-f\left(X\right)\right)^2+E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2+\sigma_{\varepsilon}^{2}</script><script type="math/tex; mode=display">
=Bias^2+Variance+Random\ Error</script><h2 id="二、如何Tradeoff"><a href="#二、如何Tradeoff" class="headerlink" title="二、如何Tradeoff"></a>二、如何Tradeoff</h2><h3 id="2-1-最佳平衡点"><a href="#2-1-最佳平衡点" class="headerlink" title="2.1 最佳平衡点"></a>2.1 最佳平衡点</h3><p>假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习）。在训练的起始，Bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大的差异。</p>
<p>而随着训练过程的进行，Bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能的训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了了我们在很多其他的数据集上就无法获得很好地效果，也就是所谓的Overfitting（过拟合）。</p>
<p>考虑到模型误差是偏差与方差的加和，因此我们可以绘制出这样的图像。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-20%20%E4%B8%8B%E5%8D%884.18.39.png" alt="屏幕快照 2017-04-20 下午4.18.39"></p>
<p>图中的最优位置，实际上是Total Error曲线的拐点。我们知道，连续函数的拐点意味着此处一阶导数的值为0。即</p>
<script type="math/tex; mode=display">
\frac{d\left(Total\ Error\right)}{d\left(Complexity\right)}=\frac{d\left(Bias+Variance\right)}{d\left(Complexity\right)}=\frac{d\left(Bias\right)}{d\left(Complexity\right)}+\frac{d\left(Variance\right)}{d\left(Complexity\right)}=0</script><p>这个公式给出了寻找最优平衡点的数学描述。若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于欠拟合；若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合。</p>
<h3 id="3-2-过拟合与欠拟合的外在表现"><a href="#3-2-过拟合与欠拟合的外在表现" class="headerlink" title="3.2 过拟合与欠拟合的外在表现"></a>3.2 过拟合与欠拟合的外在表现</h3><p>尽管有了上述的数学表述，但是在现实环境中，有时候我们很难计算模型的偏差与方差。因此，我们需要通过外在表现，判断模型的拟合状态：是欠拟合还是过拟合。</p>
<p>同样地，在有限的训练数据集中，不断增加模型的复杂度，意味着模型会尽可能多地降低在训练集上的误差。因此在训练集上，不断地增加模型的复杂度，训练集上的误差会一直下降。</p>
<p>我们把数据分为三个部分：训练数据集、验证数据集、测试数据集。</p>
<p>因此，我们可以绘制出这样的图像。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-20%20%E4%B8%8B%E5%8D%884.42.13.png" alt="屏幕快照 2017-04-20 下午4.42.13"><br>在上图左边区域，训练集与验证集的误差都很高，这块区域的偏差比较高。在右边区域，在验证集上误差很高，但是在训练集上偏差很低，这块区域的方差比较高。我们希望在中间的区域得到一个最优平衡点。</p>
<p>所以，偏差较高（欠拟合）有以下两个特征：</p>
<ul>
<li>1）训练集误差很高</li>
<li>2）验证集误差和训练集误差差不多大</li>
</ul>
<p>方差较高（过拟合）</p>
<ul>
<li>1）训练集误差较低</li>
<li>2）非常高的验证集误差</li>
</ul>
<h3 id="3-3-如何处理欠拟合与过拟合"><a href="#3-3-如何处理欠拟合与过拟合" class="headerlink" title="3.3 如何处理欠拟合与过拟合"></a>3.3 如何处理欠拟合与过拟合</h3><p>有了以上的分析，我们就能比较容易地判断模型所处的拟合状态。接下来，我们可以参考Ng提供的处理模型欠拟合与过拟合的一般方法了。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-20%20%E4%B8%8B%E5%8D%884.51.42.png" alt="屏幕快照 2017-04-20 下午4.51.42"></p>
<p>当模型处于欠拟合状态时，根本的办法是增加模型的复杂度。我们一般有以下一些办法：</p>
<ul>
<li>1）增加模型迭代次数；</li>
<li>2）训练一个复杂度更高的模型：比如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM</li>
<li>3）获取更多的特征以供训练使用：特征少，对模型信息的刻画就不足够了</li>
<li>4）降低正则化权重：正则化正是为了限制模型的灵活度（复杂度）而设定的，降低其权值可以在模型训练中增加模型复杂度。</li>
</ul>
<p>当模型处于过拟合状态时，根本的办法是降低模型的复杂度。我们一般有以下一些办法：</p>
<ul>
<li>1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。</li>
<li>2）减少特征数量</li>
<li>3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Tradeoff </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（17）：非平衡数据处理]]></title>
      <url>/2017/04/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><p>常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到不平衡的数据集，比如广告点击预测（点击转化率一般都很小）、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测等等。</p>
<p>对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很高。</p>
<p>但对于极不均衡的分类问题，比如仅有1%的人是坏人，99%的人是好人，最简单的分类模型就是将所有人都划分为好人，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。</p>
<a id="more"></a>
<p>在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。</p>
<p>处理不平衡数据，可以从两方面考虑：一是改变数据分布，从数据层面使得类别更为平衡；</p>
<p>二是改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。</p>
<p>本部分对数据层面的一些方法做一个介绍，改变数据分布的方法主要是重采样：</p>
<ul>
<li>1）过采样：增加少数类样本的数量</li>
<li>2）欠采样：减少多数类样本的数量</li>
<li>3）综合采样：将过采样和欠采样结合</li>
</ul>
<h2 id="二、过采样"><a href="#二、过采样" class="headerlink" title="二、过采样"></a>二、过采样</h2><h3 id="2-1-随机过采样"><a href="#2-1-随机过采样" class="headerlink" title="2.1 随机过采样"></a>2.1 随机过采样</h3><p>采样算法通过某一种策略改变样本的类别分布，以达到将不平衡分布的样本转化为相对平衡分布的样本的目的，而随机采样是采样算法中最简单也最直观易懂的一种方法。</p>
<p>随机过抽样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下： </p>
<p> 1.首先在少数类$S_{min}$集合中随机选中一些少数类样本<br> 2.然后通过复制所选样本生成样本集合$E$<br> 3.将它们添加到$S_{min}$中来扩大原始数据集从而得到新的少数类集合$S_{min-new}$</p>
<p>$S_{min}$中的总样本数增加了 $|E| $个新样本，且$S_{min-new}$ 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。</p>
<p>重复样本过多，容易造成分类器的过拟合</p>
<h3 id="2-2-SMOTE算法-Synthetic-Minority-Oversampling-Technique"><a href="#2-2-SMOTE算法-Synthetic-Minority-Oversampling-Technique" class="headerlink" title="2.2 SMOTE算法(Synthetic Minority Oversampling Technique)"></a>2.2 SMOTE算法(Synthetic Minority Oversampling Technique)</h3><p>在合成抽样技术方面，Chawla NY等人提出的SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。</p>
<p>SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集$S_{min}$ $\subset$  $S$，对于每一个样本$x_i\subset S_{min}$使用K-近邻法，其中K-近邻被定义为考虑$S_{min}$中的K个元素本身与$x_i$的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。算法流程如下：</p>
<ul>
<li>1）对于少数类中的每一个样本$(x_i)$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到K近邻；</li>
<li>2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其K近邻中随机选择若干个样本，<br>假设选择的近邻为$\tilde{x}$；</li>
<li>3）对于每一个随机选出的近邻$\tilde{x}$，分别与原样本按照如下的公式构建新的样本:$x_{new}=x+rand\left(0,1\right)\times\left(\tilde{x}-x\right)$</li>
</ul>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.01.40.png" alt="屏幕快照 2017-04-19 上午10.01.40"> </p>
<h3 id="2-3-Borderline-SMOTE算法"><a href="#2-3-Borderline-SMOTE算法" class="headerlink" title="2.3 Borderline-SMOTE算法"></a>2.3 Borderline-SMOTE算法</h3><p>原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。算法流程如下：</p>
<p> 1.首先，对于每个$x_{i}\subset S_{min}$确定一系列K-近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN}\subset S$；<br> 2.然后，对每个样本$x_{i}$，判断出最近邻样本集中属于多数类样本的个数，即：|$S_{i-kNN}\cap S_{maj}$|；<br> 3.最后，选择满足下面不等式的$x_{i}$:$\frac{k}{2}$&lt;|$S_{i-kNN} \cap S_{maj}$|&lt;$k$,将其加入危险集$DANGER$，</p>
<p>对危险集中的每一个样本点（最容易被错分的样本），采用普通的$SMOTE$算法生成新的少数类样本。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.02.36.png" alt="屏幕快照 2017-04-19 上午10.02.36"></p>
<h2 id="三、欠采样"><a href="#三、欠采样" class="headerlink" title="三、欠采样"></a>三、欠采样</h2><h3 id="3-1-随机欠采样"><a href="#3-1-随机欠采样" class="headerlink" title="3.1 随机欠采样"></a>3.1 随机欠采样</h3><p>减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。</p>
<ul>
<li><p>1）首先我们从$S_{maj}$中随机选取一些多数类样本$E$</p>
</li>
<li><p>2）将这些样本从$S_{maj}$中移除，就有|$S_{maj-new}|=|S_{maj}-|E$|</p>
</li>
</ul>
<p>优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。</p>
<h3 id="3-2-Tomek-Links方法"><a href="#3-2-Tomek-Links方法" class="headerlink" title="3.2 Tomek Links方法"></a>3.2 Tomek Links方法</h3><p>定义：Tomek links被定义为相反类最近邻样本之间的一对连接。</p>
<p>符号约定：给定一个样本对$\left(x_i,x_j\right)$，其中$x_{i}$ $\in$ $S_{maj}$，$x_{j}$ $\in$ $S_{min}$，记$d\left(x_i,x_j\right)$是样本$x_i$和$x_j$之间的距离</p>
<p>公式表示：如果不存在任何样本$x_k$，使得$d\left( x_i,x_k \right)$ &lt;$d\left( x_i,x_j \right)$ ，那么样本对$\left(x_i,x_j\right)$被称为Tomek Links</p>
<p>使用这种方法，如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声要么它们都在两类的边界上。所以Tomek Links一般有两种用途：在欠采样中：将Tomek Links中属于是多数类的样本剔除；在数据清洗中，将Tomek Links中的两个样本都剔除。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.11.png" alt="屏幕快照 2017-04-19 上午10.07.11"></p>
<h3 id="3-3-NearMiss方法"><a href="#3-3-NearMiss方法" class="headerlink" title="3.3 NearMiss方法"></a>3.3 NearMiss方法</h3><p>NearMiss方法是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助KNN，总结起来有以下几类：</p>
<ul>
<li>1）NearMiss-1：在多数类样本中选择与最近的三个少数类样本的平均距离最小的样本</li>
<li>2）NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本</li>
<li>3）NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本</li>
</ul>
<p>NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。</p>
<p>NearMiss-1方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离。</p>
<p>NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。</p>
<p>实验结果表明得到NearMiss-2的不均衡分类性能最优。</p>
<h2 id="四、Informed-Understanding"><a href="#四、Informed-Understanding" class="headerlink" title="四、Informed Understanding"></a>四、Informed Understanding</h2><p>Informed欠抽样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中有一些集成（ensemble）的想法，主要有两种方法，分别是EasyEnsemble算法和BalanceCascade算法。</p>
<h3 id="4-1-EasyEnsemble算法"><a href="#4-1-EasyEnsemble算法" class="headerlink" title="4.1 EasyEnsemble算法"></a>4.1 EasyEnsemble算法</h3><p>它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本$S_{maj}$，通过$n$次有放回抽样生成$n$份子集，少数类样本$S_{min}$分别和这$n$份样本合并训练AdaBoost分类器，这样可以得到$n$个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为N，少数类样本为P，算法流程如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.45.png" alt="屏幕快照 2017-04-19 上午10.07.45"></p>
<p>EasyEnsemble的想法是多次随机欠抽样，尽可能全面地涵盖所有信息，算法特点是利用boosting减小偏差（Adaboost）、bagging减小方差（集成分类器）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.08.png" alt="屏幕快照 2017-04-19 上午10.08.08"></p>
<h3 id="4-2-BalanceCascade算法"><a href="#4-2-BalanceCascade算法" class="headerlink" title="4.2 BalanceCascade算法"></a>4.2 BalanceCascade算法</h3><p>EasyEnsemble算法训练的子过程是独立的，BalanceCascade则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法流程如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.28.png" alt="屏幕快照 2017-04-19 上午10.08.28"></p>
<p>BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。</p>
<h2 id="五、综合采样"><a href="#五、综合采样" class="headerlink" title="五、综合采样"></a>五、综合采样</h2><p>目前为止我们使用的重采样方法几乎都是只针对某一类样本：对多数类样本欠采样，对少数类样本过采样。也有人提出将欠采样和过采样综合的方法，解决样本类别分布不平衡和过拟合问题，本部分介绍其中的SMOTE+Tomek Links和SMOTE+ENN。</p>
<h3 id="5-1-SMOTE-Tomek-Links"><a href="#5-1-SMOTE-Tomek-Links" class="headerlink" title="5.1 SMOTE+Tomek Links"></a>5.1 SMOTE+Tomek Links</h3><p>SMOTE+Tomek Links方法的算法流程非常简单：<br> 1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T<br> 2.剔除T中的Tomek Links对</p>
<p>普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合。</p>
<p>Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题，下图红色加号为SMOTE产生的少数类样本，可以看到，红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.56.png" alt="屏幕快照 2017-04-19 上午10.08.56"></p>
<p>由于第一步SMOTE方法已经很好地平衡了类别分布，因此在使用Tomek Links对的时候考虑剔除所有的Tomek Links对。</p>
<h3 id="5-2-SMOTE-ENN"><a href="#5-2-SMOTE-ENN" class="headerlink" title="5.2 SMOTE+ENN"></a>5.2 SMOTE+ENN</h3><p>SMOTE+ENN方法和SMOTE+Tomek Links方法的想法和过程都是很类似的：</p>
<ul>
<li>1）利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T</li>
<li>2）对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 非平衡数据 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人大应统部落（1）：2016人大应统学长倾心经验贴]]></title>
      <url>/2017/04/04/%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E5%AD%A6%E9%95%BF%E5%80%BE%E5%BF%83%E7%BB%8F%E9%AA%8C%E8%B4%B4/</url>
      <content type="html"><![CDATA[<p>2016人大考研总算是落下了帷幕，大半年来的努力终于落定。在北京的春天里，窗外飘着杨絮，在图书馆的沙发上码下这难忘时光的的锤炼与凝结。</p>
  <a id="more"></a>
<ul>
<li><p><strong>初试成绩：</strong>总分392（政治68、英语73、数学三121、统计学130）</p>
</li>
<li><p><strong>复试成绩：</strong>总分292（英语笔试40、英语面试40、专业课笔试77、专业课面试135）</p>
</li>
</ul>
<ul>
<li><p><strong>复习起始：</strong>初试： 8月10日-12月26日   |      复试：   2月20日-3月11日</p>
</li>
<li><p><strong>日常安排</strong></p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>时刻</th>
<th>事项</th>
</tr>
</thead>
<tbody>
<tr>
<td>6:30-7:30</td>
<td>起床洗漱早餐、七点半准时到海洋楼坐定，考研四个小伙伴轮流占座。</td>
</tr>
<tr>
<td>7:30-8:30</td>
<td>扇贝单词打卡一小时（这个习惯坚持到了考研最后一天还是蛮给力的）</td>
</tr>
<tr>
<td>8:30-11:00</td>
<td>数学（雷打不动的看书做题）</td>
</tr>
<tr>
<td>11:00-11:30</td>
<td>午餐（总会看到一个法兰西女孩，像西西里的美丽传说里的莫妮卡一样风姿卓越，食欲大增）</td>
</tr>
<tr>
<td>11:30-13:00</td>
<td>数学（消化午餐的同时还能做做数学）</td>
</tr>
<tr>
<td>13:00-14:00</td>
<td>午睡（这个是每日必备良药，不午睡下午的效率等于0）</td>
</tr>
<tr>
<td>14:00-17:00</td>
<td>英语（有几天没午睡，这段时间就会是睡过去的）</td>
</tr>
<tr>
<td>17:00-17:30</td>
<td>晚餐（晚餐的时候见不到那个法兰西女孩，食欲大减）</td>
</tr>
<tr>
<td>17:30-20:00</td>
<td>政治（每天两个半小时足够了）</td>
</tr>
<tr>
<td>20:00-23:00</td>
<td>专业课（因为跨专业、这个是真的有很努力在学）</td>
</tr>
</tbody>
</table>
</div>
<p>当然每天都不会完完全全的严丝合缝的来，到后期就是固定这个模式了，考研的四个小伙伴也是在不断地磨合中才基本保持了一致的计划，抱团的作用就在于此，当你投入不进去的时候，自然会有外力来催促你，除非你的个人毅力可以抵制该死的懒癌，你求知的欲望超过你对舒适的渴求，否则，建议抱团。</p>
<h2 id="一、考研专业选择"><a href="#一、考研专业选择" class="headerlink" title="一、考研专业选择"></a>一、考研专业选择</h2><p>本科学的工科，学的不精，但对计算机有接触，算是有点编程的底子，慢慢的了解到大数据专业，自己也摸索着学习了一些有关数据挖掘的模型算法，蛮感兴趣的。最近在拜读吴军博士的《数学之美》，科普下这个领域，更深入的也正处于摸索阶段，之前在人大大数据老胡学长那里要到了几本大块头的书，英文版，头疼中，坚持啃完。当然敲代码的日子我想应该就会换来蓬头垢面的自己吧。</p>
<p>个人对这种跨领域的东西特别着迷，就像最近博弈论的发展方向，复旦韦森教授总结道：“沿着道德哲学、政治哲学把休谟、卢梭、康德到罗尔斯的思想和理论程式化，致力于回复经济学的亚当·斯密的古典传统，即从审慎推理（prudential reasoning——即目前经济学家所言的理性最大化推理）和道德推理（moral reasoning）两个维度研究人们的经济与社会行为，并从中折射出制度的伦理维度和道德基础来。在这个研究向量上的经济学家主要有已过世的诺奖得主哈森伊、宾默尔以及萨金。笔者也发现，由一些谙熟现代博弈论分析工具的一些当代思想家——如美国加州大学洛杉矶分校的人类学家Rob Boyd，美国麻省大学的经济学家Herbert Gintis，慕尼黑大学的经济学家Ernst Fehr，以及麻省理工学院的著名博弈论大师弗登博格（DrewFudenberg）等，最近从文化、互惠合作（reciprocity）、利己和利他行为的产生及其在社会选择中作用等相关领域的探索非常值得我们注意。笔者这里贸然推断，这些文化人类学、经济学和博弈论的跨学科的合作研究，也许在不久的将来会汇合哈森伊、宾默尔以及萨金在伦理与社会选择探索向量上的已有理论探索。这一研究向量的理论从任何当今一门社会科学的学科视角来看均无疑代表了人类认识社会和自身的目前最前沿思考，且与经济学的制度分析的最深层基础密切相关联（这是我把他们的工作也视作为经济学的制度分析的主要理由）。笔者目前甚至乐观地估计，如果在这一研究向量中以哈森伊、宾默尔所代表的“经济学—伦理学—政治哲学—博弈论的交叉分析”与另一方面的“文化人类学—经济学—博弈论”的跨学科研究汇融起来的话，这又将会在二十一世纪谱写并演奏出一首宏大与辉煌的“理论交响曲”，从而极大地推进人类对自身所处的社会和人本身的认识和理解。”</p>
<p>之所以摘录那么长一段，是因为个人对于像韦森教授的文本话语中所展现出来的厚重的学术穿透力，像何怀宏教授这样的对正义美好的生活崇尚、像香港中文大学周保松教授对自由民主的理性而温柔的认可与支持、像钱理群教授对生命的关切和对知识的渴求所呈现的高尚的个人品格尤为佩服，他们的精神与智慧必将永远的接续下去，徜徉于他们的文字中，我最为真切感受到的莫过于他们所严肃实践的一种生活理念，这是一种热切的关怀、是一种直抵生命深处的质问、亦是一种现实世界中的身体力行的实践思想，它们似乎与主流思维相悖，这种影响逐渐在我们这一辈的学子中发生，生根发芽，我们开始从无数年被灌输出来的满脑子“绝对真理”中走出来，开始接受世界的丰富性，这即是自由的开端。感谢他们这样的勇士。</p>
<h2 id="二、考研院校选择"><a href="#二、考研院校选择" class="headerlink" title="二、考研院校选择"></a>二、考研院校选择</h2><p>本科在北京，这个学校唯一让我眷恋的就是，他可能是全中国最幸福的院校，从学校出发去北大人大，半小时车程，清华徒步即可，那时候挚爱哲学，跑去北大人大哲学院蹭课，每个老师的课都会零零碎碎的听一点，北大戴锦华老师、人大周濂老师的课尤为喜爱。北大国家发展研究院（ccer）的课也去蹭了一学期，那时候汪丁丁的新政治经济学和行为经济学，第一次去上课的时候，斗胆坐在第一排，捧着那本融合了脑神经学、认知心理学、经济学、哲学、政治学、社会学、伦理学、宗教和神秘主义的《新政治经济学》，扑面而来的知识模块，令人头晕目眩，结果便是全程睡过去的，万圣书园刘苏里称道他或许是我们这个时代少有的文艺复兴式的知识人，考研完了再次拜读老师的书，书中呈现出来的渊博之魅让人瞠目结舌。北京最让人眷恋的便是这种藏匿在深处的知识图景，学识对撞的读书会、严肃学术的研讨会、陶冶人心的音乐会、感同身受的小剧场话剧，纵然周围是雾霾缭绕，但当你挖掘到内核，或许就再也不想离开。对北京的这种念想让我毅然决然的选择了北京的研究生院，考研之前，对高校的应用统计专业略微地进行了考查。</p>
<ul>
<li><p>人大：经济统计、精算、医学统计、国民经济这几块人大是老字号了，在全国都名列前茅。但最让我热血沸腾的是近两年刚开办的五校协同交叉培养大数据方向专业硕士，对于这个平台，袁卫老师有很赞的叙述：“5所学校参与培养，就是出于学科交叉的考虑。中国人民大学统计学院的学科、专业设置是综合的、应用的，理论和应用兼而有之，应用领域涉及卫生、健康、经济、社会、管理等，总体实力较强。而北京大学和中国科学院大学，大家都知道，他们在计算机、数学和统计理论研究方面相当强，掌握大数据分析技术的前沿。中央财经大学和首都经贸大学是财经类为主的院校，这两所学校侧重于应用人才的培养，特别是面向经济、管理、社会这样的领域。他们和很多行业企业、金融机构有着密切联系。这5所高校分别属于教育部直属高校、中国科学院的高校和地方高校3种类型，各有特色，优势互补，能够建成一个很好的、学科交叉的人才培养协同体。”所以，心里其实早就锁定这个了。</p>
</li>
<li><p>清华：招生人数太少，信息严重不对称，也清楚认识到自己没那么强悍的能力，就没怎么考虑。</p>
</li>
<li><p>北大：之前在网上看到北大444哥考应用统计的雄文，看得我是荷尔蒙暴增，一冲动就买来了所有的参考书，结果发现，专业课的难度自己很难把握。而且北大分数线蛮高的，几乎每年都在390左右。学长应该是毕业了，感谢他，因为我的很多经验都会参考他，当时是打印出来一个字一个字拜读。给出他的经验贴链接吧：<a href="http://bbs.kaoyan.com/t5174553p1" target="_blank" rel="noopener">我的444分北大考研成功经验谈</a></p>
</li>
<li><p>其他：考研帮有个帖子，比较全面的分析了全国各考研院校的应用统计专业，大家可以参考下：<a href="http://bbs.kaoyan.com/t6974015p1" target="_blank" rel="noopener">应用统计全国高校分析</a></p>
</li>
</ul>
<h2 id="三、各科复习方法"><a href="#三、各科复习方法" class="headerlink" title="三、各科复习方法"></a>三、各科复习方法</h2><h3 id="3-1-英语"><a href="#3-1-英语" class="headerlink" title="3.1 英语"></a>3.1 英语</h3><p><strong>复习用书：</strong></p>
<ul>
<li>扇贝APP</li>
<li>张剑考研真题黄皮书</li>
<li>张剑阅读理解150篇</li>
<li>王江涛高分写作</li>
</ul>
<p><strong>复习方案：</strong></p>
<ul>
<li><p><strong>单词</strong>：没有用很厚的单词书，感觉会压垮自己，所以选择了app来背，一开始使用的是新东方的乐词，但是亲测效果不佳，后来经研友推荐上了扇贝的船，从此每天早上就开始打卡背诵，保证一个小时的量。特别要注意的是背单词必须要保持连续性，不能中间隔开半个月或者一个月搁置在那里而不背单词了，每天背，这是一个积少成多的过程，我就是一个反面例子，中间有段时间特别抵触背单词，荒了快一个月，结果就是做阅读的时候单词在耳边就是想不起来意思。到考研前要达到的效果就是看到一个单词就立马反应过来，背上三四遍之后就比较省力了，有的时候一小时可以背诵七百个，当然这是建立在前期不断反复不断反复的基础上，基本每个单词app都是依照记忆曲线帮你安排任务，所以也不用担心会漏背或者背不熟。</p>
</li>
<li><p><strong>完形填空、新题型、翻译：</strong>这三部分分值占的比较小，基本上每个人都可以得到个基本分数，所以就没太花时间在这上面，只做了历年真题里面的这些部分，然后看黄皮书的分析，掌握一些技巧即可。</p>
</li>
<li><p><strong>阅读：</strong> 张剑黄皮书系列的真题基本上是人手一册，真题的研究对于阅读来说是至关重要的，至少要保证两遍以上的联系和琢磨，网上都说英语考80分以上的都是真题研究了四遍五遍的，这是有一定的道理的，阅读的正确与你对命题老师的出题思路的熟悉程度正相关，基本上每一类题都会有特定的规律性，只有当你顺应了老师们的思维模式，在琢磨选项的正误时你都可以类似于套圈子一样套进去，这正是真题的重要性所在，我们可以接受这种思维模式训练的第一手资料就是真题，任何模拟题都无法与之比拟。当然不是说模拟题不需要，在这里推荐张剑的150篇，还是比较贴近真题的，但他的功效仅仅在于提高对新篇章文本的适应性，在考场上难免会遇到和历年真题风格不一致的文章，这就是模拟题的优势所在，这本书基本涵盖了考研英语阅读出现率比较高的话题，你可以拿它当做拓宽英语话语体系的佐料。我当时是花了大约一个月的时间来做这本书，每天下午四十分钟左右做两篇阅读，其他的时间研究前一天做的那两篇，一直循环下去。这里还要说下怎么研究模拟题和真题。拿到一篇文章，按照你的方法做完，然后就是挨个查单词，分析长难句，挨个解读选项和分析，自己从文中找出依据来，最好自己搭建一个文章的框架。</p>
</li>
<li><p><strong>作文：</strong>今年的作文幸亏了王江涛，一开始心存侥幸想背个模板就完事了，后来越来越不安，觉得模板作文我难以驾驭的好，如果成篇的万能句型上去，老师必然不买账，估计就是个低分的下场。当然如果真的可以把一个模板变通到炉火纯青，那也一样OK。但对于我这样四级飘过的，六级未过的naïve青年来说怎么会有那么好的英语修养呢，所以最最后一个月算是逼着自己按照王道长的指示安安分分的背诵了8篇范文，滚瓜烂熟，倒背如流，只能这样。道长说了，英语作文看的是你的语言功底，只要和主题搭上边了，没啥语法错误，词汇量足够，还能写出漂亮的句型来，那就不会差到哪里去。所以，背诵历年真题就是最好的办法，踏踏实实的背诵、默写、仿写，王江涛在书里说的都很清楚。</p>
</li>
<li><p><strong>友情提醒：</strong>再一次华丽丽的证明了自己的心理素质是多么不堪一击，哈哈，我边上的童鞋老抖腿，以至于我总是有意无意的要观察他的抖腿频率，这让我几近奔溃，没举报老师怪我太善良，结果就是做阅读完全不在状态，基本上就靠语感在做了，我谢谢他全家，我不造其他人被影响到没，怪只怪自己心理素质就像一块薄冰，一碰就会碎。故，学弟学妹们在考场上一定要杜绝这种危害社会健康的事情发生，一经发现，向朝阳群众学习，立马举报上级。</p>
</li>
</ul>
<hr>
<h3 id="3-2-政治"><a href="#3-2-政治" class="headerlink" title="3.2 政治"></a>3.2 政治</h3><p>考了68，不敢拿出来献丑，对于这种被人戏谑的学科，其实是拒绝介绍经验的，先说几个好玩的政治段子：“靠别人，你永远是右倾投降主义，靠自己你才是工农武装割据”、“如果全世界都对你恶语相加，我愿对你说上一世纪社会主义核心价值观”、“你我之间本无缘分，全靠党的章程死撑”、“别低头！GDP会掉！别落泪！资本主义会笑”、“想和你谈一场弘扬社会主义正能量的恋爱，你却要我好好做自己的中国梦”。茶余饭后看点段子还是蛮有意思的。</p>
<p><strong>复习用书：</strong>（按出场顺序排列）</p>
<ul>
<li>《思想政治理论考研大纲解析》</li>
<li>肖秀荣《命题人1000题》</li>
<li>风中劲草《冲刺背诵核心考点》</li>
<li>肖秀荣《命题人冲刺八套卷》</li>
<li>启航《20天20题》</li>
<li>肖秀荣《命题人形势与政策》</li>
<li>肖秀荣《终极预测四套卷》、任汝芬《最后四套卷》、任燕翔《考前预测4套卷》</li>
</ul>
<p><strong>复习方案：</strong></p>
<ul>
<li><p><strong>9月5日-10月10日：</strong>九月份考研大纲解析发布，开始快马加鞭的看，一块让人厌恶的砖头，不过你还是得静下心来去啃，尝试着一字一句的过，网上流传着一张图，内容是这样的：恩格斯问大胡子马克思先生：“你在干嘛呢”。马克思心平气和的回答道：“管他呢，反正又不是我背”。中国学生对其抵触的心理可见一斑。但其实真正的马克思先生的思想很可贵，以至于20世纪法国解构主义哲学大师德里达在他的著作《马克思的幽灵》中写道：“不能没有马克思，没有马克思，没有对马克思的记忆，没有马克思的遗产，也就没有将来；无论如何得有某个马克思，得有他的才华，至少得有他的某种精神。现在该维护马克思的幽灵们了。”只是在这个极权盛行的国度，马克思变成了鬼魂，笼罩这苍茫的大地，最后沦落为官方口腔，这才是知识分子真正的悲哀，依附于权力而放弃说理。言归正传，大纲解析的脉络其实是很清晰的，看不懂也没有大碍，拿出肖秀荣先生的《1000题》，看完大纲一个章节的内容你就要把《1000题》上对应的章节的选择题给做了，不要看书，把答案写在一张A4上，注意1000题是要做三遍的，第一遍做都会错的惨不忍睹，错了没事，切忌欺骗自己看看答案把错的题给改对了，因为学长就是这样喜欢自我欺骗的前车之鉴哈哈。马克思主义和毛中特部分或许会让你略微头疼，这种理论性质的东西充斥着新闻联播的气质，但也务必沉住气，到后面解决史纲和思修就是分分钟的事，在高中阶段谁都学过维新变法、辛亥革命之类的，学起来还可能会让你增生一些兴趣。在这里安利一部良心巨制《走向共和》，看完这部电视剧会让你对中国近代史的基本脉络有一个清晰的呈现，记得本科的时候看徐中约先生的《中国近代史》，看得我是心力交瘁，后经学长推荐才去看的这部戏，看完后再回头看那本书，不适感就下降很多。下一步要做的事很重要，就是把你做错的题目，从大纲解析里面找答案，用晨光彩色标记笔标注出来。好了这一遍下来，务必请你自己做一个粗略的知识回顾和框架的搭建，每个章节在讲什么内容，拿毛中特部分举个例子，第一章提纲挈领先介绍了马克思主义中国化的两大理论：毛爷爷思想和中特理论体系，也是同上一部分的马克思主义的衔接，然后从第二章开始，分别是第二章新民主主义革命（1919-1949），第三章社会主义改造（1949-1956），第四章社会主义建设（1956-1978），第五章和第六章插播了总依据和总任务，因为接下来的是第七章改革开放了（1978-不知道啥时候结束），再然后是考研政治的重点，第八章总布局，这章内容及其丰富，包括中特经济、政治、文化、社会、生态文明。紧接着就是祖国统一、外交国际战略，最后两章是建设中特的相关问题，总结陈词就是党好党棒棒哒领导好领导棒棒哒。你可以自己建立一个思维导图，带着这个脉络你可以顺利的进入到下一关，这些基础工作是为后期服务的。</p>
</li>
<li><p><strong>10月10日-11月5日：</strong>第二遍重复上一步的内容，看大纲，做1000题，纠错回大纲标注。注意第二遍的时候你可以把答案写在1000题上了，然后看1000题后面的答案，把错的以及你觉得好的题的解析在题目边上标注下，要知道为什么错，举一反三。</p>
</li>
<li><p><strong>11月6日-考研结束：</strong>刷完了两编大纲和1000题，这时候会出现一本震撼人心的资料出现—风中劲草，这本书的编排和印刷是下了一番功夫的，他的细节之处可以让你真的佩服这本书的作者，跟进大纲，条理清晰，标注分明，重点突出，考研资料中的扛鼎之作。所以，你一定要把这本书当做是你考研政治的制胜法宝，你该怎么做呢？看，一个字一个字的看，我当时就有种心态，自己可以假装看懂普鲁斯特的《追忆似水年华》，我就必须要看透杨杰先生的《风中劲草》，两个时代的回响多璀璨。直到考研之前，你也不要放下这本书，看的遍数越积越多，你就会达到你自己都意想不到的层次，就是合上书，你大概可以知道哪个知识点在那一页的哪块位置。我保持的速度大概是1天15页左右，15天一本书，到考研结束加起来看了三遍。马原毛中特部分你可以多看几遍，四遍五遍无上限。对了，这时候我还同时做了一件事，就是第一遍的时候只是把1000题上的每一道题都在风中劲草上标注，若是单选题第一题，就标注“单1”，多选类推，同时用彩色笔标注。此外，肖秀荣的《形势与政策》也出来了，买来利用空余时间看两遍。</p>
</li>
<li><p><strong>12月15日-考研结束：</strong>各种模拟预测题纷纷登场，这里首推肖秀荣的《8套卷》和《4套卷》，可以去学校打印店购买，便宜实惠，八套卷你都要当做考试一样对待，基本上三十分钟就可以完成一套，完成一套之后看下答案分析，纠错，在风中劲草上标注，如肖秀荣第三套第1题，就标注“肖8三1”。因为之前咱们已经完成了以下任务：大纲解析两遍，1000题两遍，风中劲草两遍左右，再加上不断的标注，8套卷就是检验你复习成果的最佳试题，肖秀荣老师编的资料都棒呆，个人灰常喜欢他的讲课风格，一口流利的方言普通话，考研期间要随时关注肖老师的微博微信，都是同步的，关注一个就可以，把它发布的一些重要文件下载下来，打印研究。八套卷你可以做两遍三遍，注意要举一反三，尤其是错题。然后过几天4套卷就会隆重登场，基本是人手一套，不要迟疑，买来赶紧把客观题做了，按照之前的流程对待客观题。接下来就是万众瞩目的主观题，大家从开始到现在还没有接触过主观题，4套卷就是专门为了主观题准备的，如果你不想留太多的时间和精力在政治上（毕竟政治只有100分，数学和专业课才是重中之重），那就建议你只背诵4套卷的主观题，之前提到的启航20天20题可以翻翻，虽然好，但知识点太多，没时间应付，咱们把赌注押在肖秀荣的4套卷上，背诵的时候主要要挑关键词背诵，自己想法子变通式的背下来，切忌原封不动的机械背诵，虽然这几年老爷子押原题的能力衰弱了，但每年的知识点还是压得相当准的，当然像今年很多人说蒋中挺几乎全压中原题了，没怎么看过他的资料，不予置评。</p>
</li>
<li><p><strong>考场上：</strong>考政治的时候觉得选择题so easy啊，做到主观题，有点懵逼了，肖大大押中的题的答案全变成了考研的题干，只能硬着头皮上了，相关的知识点全答上去了，生死未卜的赶脚，有一道家庭美德的就全靠扯了。所以，我的复习方案可以给大家敲响警钟，要是想要考高分的，主观题也是要早点准备的，尽量多参考几个考研机构的预测卷，稍微整理下答题的思路，预防真题出现一些偏题。</p>
</li>
<li><p><strong>说在后面：</strong>考研政治的时效性特别明显，16年开了十八届五中全会，那么有关它的内容一定会是考研的重头戏，今年客观题和主观题都有一定的体现，而且比重还不低。所以，形势与政治也要多加注意，有时间就多看几遍，尤其是考前那几天，加深印象，有的关键词列点背诵。肖大大解答过考研命题人如何出题：先确定要考的知识点，然后去报纸和杂志上找相应的材料。所以，我们必须要对知识点分外敏感，在不同的模拟题中间总结出知识点来。</p>
</li>
<li><p>另外推荐几个<strong>不错的复习资料</strong>：</p>
<ul>
<li><p>肖秀荣的<strong>【马克思主义基本原理概论逻辑图】</strong>，哲学的主观题就全靠这个资料来沥青脉络了。</p>
</li>
<li><p>肖秀荣的<strong>【近代史时间轴】</strong>：把近代发生的事件按照时间顺序排列，一些重要的知识点也有叙述。</p>
</li>
<li><p>肖秀荣的<strong>《知识点提要》八个附录</strong>：网上也有，可以看看背背啥的。</p>
</li>
</ul>
</li>
</ul>
<p>这样下来政治需要看的东西也蛮多的，时间要自己控制好，到了后期，专业课要背，英语作文要背，政治也要背，别被他们压垮，挺过去！</p>
<h3 id="3-3-数学三"><a href="#3-3-数学三" class="headerlink" title="3.3 数学三"></a>3.3 数学三</h3><p> <strong>复习资料</strong></p>
<ul>
<li>张宇数学三的视频课</li>
<li>《李永乐复习全书》大红色</li>
<li>《李永乐660题》</li>
<li>《李永乐历年真题》</li>
<li>40套模拟题（《张宇8套卷》、《4套卷》、《永乐6套卷》、《历年合工大最后五套卷》、《400题》）</li>
</ul>
<p><strong>复习方案：</strong></p>
<ul>
<li><strong>8月10日-9月1日：</strong>花了将近一个月的时间来看张宇的视频，他的整个讲解的框架体系蛮成熟的，按照他的指示把笔记全部抄下来，然后自己尝试着背诵，这样下来就可以搭建起数学三的整体的脉络，知道要考的知识点和题型。我个人认为这个框架的搭建对于学习数学来说太重要了，他可以帮助你以一种高屋建瓴的视角来面对你所遇见的各种题型，而不至于迷失在茫茫的题海中无法自拔，它就像在你的脑海中植入了一份详尽的探险地图，遇见一个题，你可以将其归入体系中的某个知识点，这样的训练增加之后，对你的做题速度和准确率也会有很大的提升。一定要有这样的意识将知识归整而不是碎片化存在于你的大脑中，一个成熟的知识体系都会是如此，麻省理工大学的数学大咖林达华在讲解自己的数学体系时，必然脑海中有这样的一个完备的详尽的清晰的图景。</li>
</ul>
<ul>
<li><p><strong>9月1日-10月15日：</strong>进入考研攻坚期，复习全书是必备的，因为数学是上午考，我也象征性的把复习时间安排在了上午，每天看10页左右，消化不了太多，一些原则：1、必须自己拿笔写，切忌眼高手低以为看看就会了/2、切忌还没怎么思考就看答案解析，不会做没事，自己思考的过程尤为关键，在每道题的边上写下自己的思考推算过程以及这道题的关键之处、3、琢磨好久都搞不明白的，可以询问大神研友，或者自己做个标记，以后来解决（我后来忘记我曾经有不会的题了，就是这么大马哈）。数学其实有点像练书法，一开始可能你的水准只能够临摹大师们的作品，还只能学个皮毛，但重复训练达到一定的层次之后，你会形成自己的笔法（数学思维方式），在之后对于从未涉猎的新帖（新题型）也可以驾轻熟重。</p>
</li>
<li><p><strong>10月16日-11月10日：</strong>复习全书完了之后，一本虐人无数的660题登场，别以为他全是选择题和填空题，但他的每一道题都是精心锤炼过得，所体现的数学思想方法绝对会让你获益匪浅，他的题目的设置真的恰到好处，细细的琢磨每一道题的精髓，虽然真题是绝对达不到这样的难度的，关键的是思想方法。我大概刷了20多天，但有些题后来又忘记回过头去考虑了，这就落下不少病根，    所以建议复习的早一点，可以有更多的时间来调整自己的复习计划。</p>
</li>
<li><p><strong>11月11日-11月20日：</strong>从光棍节那天清晨开始，我拿起了真题，花了十天时间每天完成一套卷子，因为很多题其实在你做全书的时候已经遇到过了，所以其实真题对于真实水平的评估还是有很大偏差的，对完答案，订正，错的题的解题思路思考一遍就完事，当时的分数基本保持在120-140之间，也没有太大的失常，第一，历年真题相对于之前的训练还是简单一些的，第二，平时的训练不紧张，三个小时基本上都是轻松愉快的度过的，那时候每天早上起床就盼着可以做数学真题了，就像恋爱一样。</p>
</li>
<li><p><strong>11月20日-12月23日：</strong>接下来来到了我个人最为推崇的一种方法，就是数学套卷模拟，在这个阶段中，每做一套模拟题，就可以把所有的章节的重要内容复习一遍，尽管无法覆盖每一个知识点，但模拟题的编写还是有一定的规律的，可以让你随机的复习到一些重要的知识点。要遵守几个原则：1.三小时一套，时间到了就停止答题，然后根据答案自己批分数。2.必须严格遵守考研数学设定的考场规则，不能看书，不能交头接耳，不能询问学神研友，不能嚼口香糖。3.交叉训练法，每个老师出题的风格可能不一样，你可以先做两套张宇的，再做两套李永乐的，这样循环着做，可以增强你的适应能力，亲测有效。至于该做哪些模拟题，我推荐的都在上边写着，这个方法也是借鉴北大444哥的。为什么要建议这个办法呢，因为考研数学今年风格大变，像线性代数和概率论与数理统计的大题都是很难遇到的，那怎么办呢，就是不断地训练模拟题，不断地遇到新题，不断地提高自己的解题能力，而且这样的实战模拟也会让你开始意识到考场上的时间分配是何其重要，3个小时，挑大肉吃，有的是在太难的，抛开也无妨，考场的战略是需要在平时的训练中积累的。此外，你的书写也需要在这段时间里训练，张宇的8套卷和4套卷都提供了和考研一模一样的答题纸，不要大手大脚的乱答题，解题的条理性要注意。合工大的那几套题真的很不错，今年在考场上做高数的时候相当顺利的原因就是这些题型基本都在那15套卷子里遇到过了，所以，直到高数大题做完，我只花了1个小时20分钟，最后剩下一个半小时左右的时间来解决线代和概率大题，但是，我真是个天生的考场悲剧制造者，详情见下。</p>
</li>
</ul>
<p><strong>考场上：</strong></p>
<ul>
<li>因为考研期间基本上很少运动，打个球跑个步都是奢侈的不行，散步也成了浪费时间的活儿，后来自己也尝到恶果了，我的小心脏承受不住了，有时候会突然之间心跳加速到200多次每分钟，去校医院检查的时候医生说说是有阵发性室上性心动过速，吓得我够呛，问医生为啥，他说是我的心脏短路了，很多时候是因为焦虑不安或者过度兴奋造成的，好吧，那时候都到了考研的冲刺期，我也只能硬挺着，反正医生说没什么生命危险，然后就听到了考研的那天。在考研数学的考场上，我因为一个小时十分钟就完成了高数部分，high的不行了，一兴奋，犯病了，心跳开始砰砰砰的上去，以至于我无法正常答题，短时间治愈这个病的办法就是蹲下去深呼吸，于是我就申请去走廊自己做深蹲，然后深呼吸，深蹲深呼吸，当时真的快要奔溃了，以为这场试就这么完蛋了。深蹲深呼吸了好久，大约过了十五分钟，还是没有恢复过来，感觉真的没救了，老师也一直在边上看着我，该咋办，我的天哪，情急之下我开始捶自己的胸部，一锤倒是好了，但是背部还是有明显的不适感，总觉得有东西在怼我，就在这样的状态下，勉强答完了题，再加上线代和概率题和之前的训练风格太不一样了，我就有点崩溃了，连时间都看错了，原本是11点半结束，我却以为11点就结束了，情急之下把线代大题答得满卷子都是，感觉都看不清楚了，菩萨保佑吧，希望老师可以手下留情。所以，从我身上可以吸取的教训就是平时要注意身体，有时间就去跑跑步打打球，千万不能输在身体上。最后数学考了121分，也算是谢天谢地了。</li>
</ul>
<h3 id="3-4-432统计学"><a href="#3-4-432统计学" class="headerlink" title="3.4 432统计学"></a>3.4 432统计学</h3><p><strong>复习用书：</strong></p>
<ul>
<li>贾俊平《统计学》第四版（经管类）</li>
<li>贾俊平《统计学》第六版（21世纪统计学系列教材）</li>
<li>何晓群《多元统计分析》</li>
<li>王燕  《时间序列分析》</li>
<li>何晓群《应用回归分析》</li>
</ul>
<p><strong>复习方案：</strong></p>
<ul>
<li><p><strong>贾俊平《统计学》：</strong>一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架体系也比较清晰，基本上一个章节一天就OK，看了两遍，然后整理了自己的笔记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下，做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析（这俩个属于多元统计分析）都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的，幸好看了第四版。</p>
</li>
<li><p><strong>何晓群《多元统计分析》：</strong>说实在的这本书写得像哲学，感觉是直接从英文版翻译过来的，很多表述没有那么通俗易懂。我只看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去对应可以解决的问题，不需要去死抠推导过程，这不是432需要重视的，但是对于理解还是有帮助的，有兴趣的可以推推看。16年没考这部分内容，但不能预计17年会不会考，要复习的全面一些。一些问答题都要结合历年真题自己根据课本进行总结。</p>
</li>
<li><p><strong>王燕《时间序列分析》：</strong>这本书的编写就相对好得多，条理很清晰，思路引导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道08年学硕考过的题：有趋势有季节变动可建立的模型，写出模型形式并简要说明。可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点，只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。</p>
</li>
<li><p><strong>何晓群《应用回归分析》:</strong>一直以为何晓群老师是个女老师，后来复试的时候才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。</p>
</li>
<li><p><strong>关于真题：</strong>真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作用。我当时的做法就是把真题整理成八个专题，分别是：《专题一：图表展示与概括性度量》、《专题二：统计量与抽样分布》、《专题三：参数估计与假设检验》、《专题四：分类数据分析》、《专题五：方差分析与实验设计》、《专题六：回归分析》、《专题七：时间序列分析》、《专题八：多元统计分析》，学硕和专硕的历年真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像今年时序和回归的题都是曾经考过的，几乎一模一样。</p>
</li>
</ul>
<ul>
<li><p><strong>关于笔记：</strong>自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思，但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知识域相对来说也会完善一些。</p>
</li>
<li><p><strong>关于背诵：</strong>心理学中有一个广为认可的记忆机制，即：我们在记忆的时候将许多线索（诸如对一个原理的发散性理解、当时联想的事物的多样性）一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面，搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出入，但是关键词必须要锁定，大致意思要接近。</p>
</li>
<li><strong>其他：</strong>大家如果有专业课的问题，可以向我询问，我尽力解答。最近也在恶补专业知识，毕竟是跨专业，害怕一进人大就被各路大神碾压，大家互相学习吧，或许我的专业素养还比不上学弟学妹呢。和考研小伙伴一起建立了一个微信群，大家伙可以加进来在群里分享应用统计的资料、讨论复习过程中遇到的难题、分享考研路上的酸甜苦辣，啥啥啥都可以。因为微信群已满100人，可以加我的个人微信：<strong><em>zhanghua63170140</em></strong>，拉学弟学妹进群。</li>
</ul>
<h2 id="四、QA"><a href="#四、QA" class="headerlink" title="四、QA"></a>四、QA</h2><p><strong>师妹皱着眉头问：</strong></p>
<p>师哥好，我也想考人大统计，本科统计，但只是普通一本。旁边人都说人大太难考，因为我是师范学校，而且我们数科院好几年没有人考到人大，感觉挺迷茫的，也不知道该不该换个学校，但是总觉得不甘心，为什么别人能考上我不能啊？</p>
<p><strong>师兄皱着眉头答：</strong></p>
<p>在现实世界中，我们的决策往往会倚赖过往的历史经验，就像你所述的，你所在的师范学校的数科院没有人考入过人大统计学院，看到这么惨淡的景象，畏惧心理在所难免，既然他们已经为你趟过这条深水，且已证明这不是一条容易的路，那为何我还要继续当做下一个被湍急的河流卷走的“微弱的个体”呢？且不说投入进去的时间成本以及其他一些不可控因素给自身带来难以计数的艰难险阻，万一这一年的所有努力在成绩出来的那一刹那都付之一炬，名校梦从而化为泡影，岂不是做了一次失败的买卖吗？</p>
<p>可我想告诫你的却是，人大必须要去考，而且要义无反顾的前往，不要有所畏惧，从你的描述中得知并没有太多的现实因素的阻挠，你仅仅在惧怕强大而无耻的经验施加在你身上的不能承受的阻抗。先说说你会在这条幽深曲折的路上看到哪些曼妙的风景，你或许可以涉猎到从未踏入的知识盲区，当你被无数的知识模块所充盈，你会感受到这样的缓慢累积会给你带来前所未有的愉悦与渴求欲的满足，我们时代的知识分隔已经异常凸显出来了，每一个领域会将拥有一套成熟的体系，而当你掌控着庞大细密的知识网时，你便拥有了铠甲，他将带你在众人面前展示话语的力量，那种力量就是需要这些知识来支撑的。其次，你可能可以收获几个志同道合的朋友，网络的延伸将你的诉求与宣告呈现在他们面前，就像现在我正在尝试着与你促膝长谈一样，也必定会有无数的这样的人尝试着与你建立精神上的关联与挂钩，你们摸索着同一片黑夜，也凝望着同一片蓝天，为这笃定的信念挥汗。你要坚信，总有一天，你会与你精神气质相合的那些可爱的人相聚，就像家人一样聚在一起，所以，不要抗拒孤独，那仅仅是你还没有那样强烈的遇见。最后，也是最重要的，你将迎来新的人生，你说你想考人大统计，我相信你的内心必然会有一股洪荒之力在不断地催促着你，会有一种声音在耳边呼唤你，那便是我们最大的动力，这样的声音会在你颓然之时支起你的躯体，无论什么样的生活的贫乏无趣都驱散不了这种称之为信仰的东西，只要你持续地温存这样的声音，去战胜所有的困惑与不安，正是这种不甘让我们变成一个撑起自己所有维度的勇士。是的，你会变成一个勇士，即使被现实迫害的遍体鳞伤，你也依然可以坚毅地挺立原地，然后，舔舐自己的伤口，继续热烈地往前走。那种热烈，只能自己亲手栽培，别人无法给予，你也不能凭空取得，那是一个个白天黑夜的伏案所换来的最盛大的生命花园，你要在漫长的年华里种上玫瑰、植入梧桐、嵌进宝石，让它灿烂的更彻底点吧，即使荒败了，也要在极度的繁盛中逝去。</p>
<hr>
<p><strong>酷酷的师弟问：</strong></p>
<p>师兄，我是跨专业，感觉对专业课比较迷惘，不知如何下手，可不可以建议一个比较摸得着套路的专业课复习方案呀？</p>
<p><strong>严肃的师兄答：</strong></p>
<p>432统计学复习流程建议</p>
<ol>
<li><strong>贾俊平《统计学》第六版+圣才《贾俊平统计学 笔记与课后习题详解》：</strong>花费20天左右的时间进行全篇阅读，不遗漏任何一个点，包括概念、公式、解释、注释、表格、图片、例题，每一个字都要盯上至少一秒钟。每看完一章节的所有内容之后，在A4纸上写下课后思考题与练习题的答案，可以翻阅课本，但必须要自己动手整理归纳或者解答一遍，切记眼高手低，能写入教科书的例题就必然会有其存在的必要性，它可以帮助你梳理课本知识，也可以帮你抓住章节重点，整理这些问答题和计算题的过程也是再一次深入理解知识点的过程，绝不可废弃之。课后的思考题和练习题的答案可以在圣才出版的《贾俊平统计学 笔记与课后习题详解》找到，我只找到了第五版对应的（一共331页），已经上传到网盘里。大家也可以参考人大配套的学习指导书，网上可以买到。这样一遍下来对这本书的框架有一定的了解，强烈建议看完每一章节之后画一张框架结构图，理清知识脉络。重点章节是”第3章：数据的图表展示“、”第4章：数据的概括性度量“（这两个章节联合起来会在真题中考察一道大题）；第6章，统计量概念和中心极限定理是重点；第7-13章，所有的都是重点。第1、2、5、14章可以粗略看一下，非重点。</li>
<li><p><strong>贾俊平《统计学》第二遍：</strong>第二遍依旧要有如第一遍的细致程度，且在第一遍的基础上加深理解，争取可以简单的使用自己的话简述一遍，同时要开始做笔记，按照书本的结构组织笔记，且必须要把这本笔记当做是一样艺术品，用心编排、用心写字、用心画图，重点分明、内容完整、结构清晰，切不可潦草糊弄过关，这本笔记是你在日后的复习中常常会碰面的，翻阅起来可以大大地提高效率，而且看起来愉悦舒心一目了然，何乐而不为？切勿盲目求快，做笔记是一个梳理知识点、更深入理解知识点的过程，欲速则不达，抄一遍了事对理解没有丝毫的助益，下笔之前想明白这句话所指涉的是什么、是否还存留我尚未领会的含义、我能否清晰的在脑海里梳理分析流程等等等，这些都可以增益你对细节的深入探索，慢工出细活，相信我，循序渐进的来，一定会有很大的成效。同时这一遍笔记要把管理学第四版中出现的新知识补充到笔记中，其中的新知识点包括：正态性的评估、实验设计、哑变量回归、非参数统计（注：时间序列分析和多元统计分析的部分内容会在其他两本书中会详细展开，不添加进去也无妨）。</p>
</li>
<li><p><strong>《应用回归分析》：</strong>这本书囿于时间只看了前八章，也就是到主成分回归与偏最小二乘估计这一章为止，因为在《统计学》书中已经对这部分的内容有了基本的了解，加上《应用回归分析》书中的推导也不是特别艰深，基本上每一步思路都很清晰，大家可以尝试着推导一遍，加深理解，但这并不是重点，重点在于诸如违背回归方程基本假设条件的三种情况（异方差、自相关、多重共线性）的原因、影响、诊断、处理这类偏向论述、步骤与原理的知识点，所以，如果推导有困难，也不必强求。但其实后两章的非线性回归与定性变量回归模型也比较容易理解，虽然初试考察的概率不大，但为兼顾知识结构的完整性以及复试的时候有可能被老师问及，看一下肯定是有好处的。这本《应用回归分析》也是要看一遍，再做一遍笔记，做笔记的方法与上述一样，不再赘述。</p>
</li>
<li><p><strong>《应用时间序列分析》：</strong>这本书的条理狠清晰，大致就是平稳时间序列分析、非平稳时间序列的确定性分析和非平稳时间序列的随机性分析三块内容，考试重点在于若干个时间序列分析模型的结构与性质，譬如AR模型、MA模型、ARMA模型、ARIMA模型、指数平滑法、分解模型、含哑变量的多元回归预测模型等等，要搞清楚每一种模型所适用的时间序列类型、模型中每一个参数代表的含义以及分析的思路与步骤，其他的重点包括差分运算、一些基础的概念等。最后一章考的概率不高，但时间序列最后一章内容在时序分析所占地位是很高的（虚假回归、单位根检验、单整与协整）要是想扩充知识点，，也建议看一遍。同样，整个过程也是看一遍书，整理一遍笔记。</p>
</li>
<li><p><strong>《多元统计分析》：</strong>聚类分析、判别分析、主成分分析、因子分析、对应分析、典型相关分析必看，后几章个人认为考的几率不大，看个人时间分配。重点考点是这些多元统计分析方法的基本思想、过程细节、重点性质之类的，历年考过因子分析、判别分析、典型相关分析的相关内容，考的概率蛮大的，绝不能弃看。过程还是一样，第一遍，看书、理解、适当推导，第二遍，做笔记，再次理解，加深印象。</p>
</li>
<li><p><strong>八个专题整理：</strong>这是学长根据历年真理的考题分布情况总结出来的八个专题，已经在上述的复习中有所呈现，依次是：“专题一：数据的图表展示与概括性度量”；“专题二：统计量与抽样分布”；“专题三：参数估计与假设检验”；“专题四：分类数据分析”；“专题五：方差分析与实验设计”；“专题六：相关分析与回归分析”；“专题七：时间序列分析”；“专题八：多元统计分析”。几乎每一年都是在这八个专题中抽取七个专题的知识点，例如2015年432真题的排布分别为：第一题属于专题一（数据的图表展示与概括性度量）、第二题属于专题四（分类数据分析）、第三题属于专题五（方差分析与实验设计）、第四题属于专题八（多元统计分析）、第五题属于专题三（参数估计与假设检验）、第六题属于专题六（相关分析与回归分析）、第七题属于专题七（时间序列分析）。还是比较有代表性的，其他年份的分布学弟学妹们也都可以总结一下规律，有助于自主预测考题。大家可以以这个思路去归纳整理自己最终的一份笔记，八个专题，每一个专题都要有结构框架，可以借助思维导图这个工具，每一个专题包括四个部分（第一部分：这一专题的课本内容有序的整理；第二部分：重要问答题整理；第三部分：属于这一专题的真题整理（每一道题的答案一定要完整有序地整理）；第四部分：这一专题的思维导图）。整理完这份专题笔记之后时间也就剩下一个月左右，接下来的时间就是背背背，当然要理解地去背，把笔记与思维导图结合起来，背到滚瓜烂熟，背到天昏地暗，到最后阶段会特别难熬，英语作文要背，政治大题要背，专业课要背，抗住压力就是了。</p>
</li>
</ol>
<hr>
<p><strong>可爱的小师妹问：</strong><br>你的专业课思维导图咋搞咯？还有专业课的复习时间怎么分配？难点不懂怎么办？师兄你有笔记吗？：</p>
<p><strong>依旧严肃的师兄答：</strong></p>
<p>这八个专题的思维导图我已经整理完并放置在百度云群里了，这里给出百度云链接，<a href="https://pan.baidu.com/s/1nva2ydN" target="_blank" rel="noopener">人大432专业课思维导图</a> 密码: pwu9。当然思维导图要随时自己更新，如果自己觉得需要补充的，可以在思维导图上添加。要想使这个思维导图的效用最大化，就得把框架熟记在心中，在答题的时候一定会有帮助的，会让你的答案有结构有条理，列点回答更加轻松自如，不知道怎么把题目答得全面闪亮？只需把思维导图的一个个点用书本的内容或者你整理的笔记来填充就好啦。不过，话说在前头，理解才是关键。它只是一个框架工具，核心在于知识点。</p>
<p><strong>时间怎么分配呢</strong>？：基本上每天我都会花三个小时复习专业课，有的时候白天数学的任务没完成，也会适当压缩专业课的时间，大致的时间安排是《统计学》阅读及笔记25天、《应用回归分析》阅读及笔记20天、《应用时间序列分析》阅读及笔记20天、《多元统计分析》阅读及笔记20天、《八个专题整理》25天、背诵30天。</p>
<p><strong>难点不懂咋办办？</strong>：在多元统计分析或者时序分析中会出现一些自己无法理解的地方，诸如推导过程和计算证明，这些确实不是432统计学的考察重点，但是如果不搞清楚这些，对知识点就会感觉隔着一层迷雾，无法透彻地解析整个过程总会叫人不爽快，不求甚解是深层次理解知识点的大敌。但是，时间所迫，实在搞不懂这些玩意儿咋办呢，那就只能退而求其次，可以大概的知道这个推导是在干什么以及它在整个过程中的作用。也足够应对432统计学了。</p>
<p><strong>学长你有笔记吗？</strong>：哈哈哈，到最后了学长要黄婆卖瓜自卖自夸了，简要说下学长的专业课笔记，笔记分为七个部分其中前六份都是亲手整理的，就是依据上述的复习过程一步步整理下来，并且经过了精心的排版，保证大家的用户体验一级棒。如果想深入了解资料的细节，可以私聊学长，随时等候你的到来。欢迎添加个人微信：<strong>zhanghua63170140</strong></p>
<h2 id="五、复试攻略"><a href="#五、复试攻略" class="headerlink" title="五、复试攻略"></a>五、复试攻略</h2><h3 id="5-1-经验之谈"><a href="#5-1-经验之谈" class="headerlink" title="5.1 经验之谈"></a>5.1 经验之谈</h3><p>先凭借自己的记忆简要说说复试的整个流程</p>
<blockquote>
<p>第一天</p>
</blockquote>
<p>英语笔试（50分）：</p>
<p>一张卷子上有两部分考题，其一为听力，其二为翻译。</p>
<p>听力部分：依照往年师兄师姐的经验，把2005年至2016年所有的六级真题的听力部分拿出来，每一年的听力大概都听上两三遍，仅仅包含听力选择题，不包含听力，直接记答案，直至不听就可以直接把答案写出来。我记得去年有一个哥们并不知道听力的这个套路，最后因为复试英语挂了，而被刷下，好遗憾的，大家引以为鉴。还有一个就是，复试听力应该会和历年真题的某一年的某一张卷子的听力部分一模一样，而不会有任何顺序的变化或者很多年的题拼凑在一起，想想也是，要是学校变换题的顺序，听力录音就得自己搞了，工程量虽然不大，但多一事不如少一事嘛。</p>
<p>翻译部分：两道题，英译中，中译英。出卷子的老师给了我们一个难题，就是英译中的题是中译英的答案，这可如何是好，到底是按照标准答案一模一样写上去呢，还是有创造性的自己翻译呢。我选择了前者，英语蛮差的，就没有自讨没趣了。最后得分也没有特别离谱。今年应该不会出现这样的情况了吧，要是再出现，我觉得这老师肯定是喜欢考验一个人的意志力和创造性。</p>
<p>专业课笔试（100分）：</p>
<p>七八道简答题，出题风格和初试几乎一样，最后那道题，如果非统计科班出生的很难答出来，考的是数据挖掘里头分类器组合方法之一bootstrap（完全不知道是个啥），具体什么题我忘记掉了。不知道当时有没有人考完了然后回忆下来的，这几天我找找看。至于怎么复习，依今年初试的专业课风格来看，会相当注重很细的知识点，大家基本按照初试的复习感觉来吧，加一些自己觉得重要的细节，补充到思维导图上去，分成几个专题来背。那时候复试复习的时候相当迷惘，也是费尽了脑筋去找题，后来是从我的冤家学长小杰克先生那里获得了一些往年真题，才不至于瞎搞。还有，初试成绩出来了，大家可以在群里面找同样进复试的小伙伴，结伴复习，可能会更明朗一些。</p>
<hr>
<p>第二天</p>
<p>英语面试：</p>
<p>当时是把专硕学硕安排在了一个教室里候试，进去之后，老师会安排面试的顺序，英语面试和专业课面试分开进行。有的人可能两场都安排在早上，也有的都安排在下午，也有可能早上下午各安排异常，这样就相当煎熬了。等待的过程很漫长，大家都静坐，没有人交头接耳，气氛很紧张。</p>
<p>英语面试的内容大概就是，进去之后，老师会让你抽一道篇文章，然后让你完整的念一遍，完了之后提问另三个关于这篇文章的问题，问题都比较简单，比如说这篇短文的主题是什么？都是显而易见的问题，所以你要在读的过程中思考些许，不要一股脑儿读了一遍就完事。这一步结束之后，老师会让你自我介绍下，这个自我介绍提前准备好，两三分钟。说下自己的简介，兴趣所在，本科学了啥，做过什么统计相关的，会什么软件，为什么考人大统计等等，简单说说。自我介绍结束还会问几个简单的问题了解下你，比如你对哪一门专业课最感兴趣？你学过哪几门专业课？注意用英语表达，我好像突然之间还真想不起来这些词，所以提前准备下好了。</p>
<p>专业课面试：</p>
<p>进去之后，同样是抽一道题，题目涉及统计各方面的内容，也基本和初试的内容差不多，但是有小伙伴抽到了国民经济和抽样的题，复试前攒下人品应该就不会抽到了（希望是如此）。回答之前要在脑海里清晰的列好点，要回答几个关键的点。老师们会根据你的回答扩展提问，也都是课本范围内的。能答上来就答，不会的话就实话实说好了，老师也不会为难你。抽到国民和抽样的题，不要慌，老师一定要你说点啥的时候，咱们虽然没有学过（时间充裕的觉初试分数处在边缘的同学可以提前看一下这俩本书，我也没看过，据说知识点很多，短时间内很难攻克），可以根据自己的理解答一些也蛮好的。</p>
<p>接下来是自我介绍，和英语面试一样，介绍下自己，说说自己的兴趣，做过什么小项目，为什么考人大统计，会什么统计软件，将来想研究的大致方向。自我介绍完了，老师会根据你的介绍提一些问题，所以你在自我介绍的时候尽量往自己掌握的比较好的方向引。五个老师面试，我的感觉应该是属于同一个研究方向的老师会在一起，有的运气好的，可能你个人的感兴趣的是数据挖掘，而这五个老师恰好都是来自精算教研室，这样就不至于问的太深。当然，老师们都是涉猎广泛的，我们那点三脚猫功夫很容易就露馅了哈哈。</p>
<h3 id="5-2-复试资料集合"><a href="#5-2-复试资料集合" class="headerlink" title="5.2 复试资料集合"></a>5.2 复试资料集合</h3><ul>
<li><p>六级听力：搜一下APP就好了，这个是我当时用的，也可以用其他APP替代。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2018-02-28 下午4.35.59.png" alt="屏幕快照 2018-02-28 下午4.35.59"></p>
</li>
<li><p>翻译：不知道什么资料好，用初试真题的翻译部分训练也行。然后再背背单词啥的，不至于到时候遗忘了。</p>
</li>
<li>专业课笔试面试：链接: <a href="https://pan.baidu.com/s/1dF0VQA" target="_blank" rel="noopener">https://pan.baidu.com/s/1dF0VQA</a> 密码: sxub</li>
</ul>
<h3 id="5-3-其他学习建议"><a href="#5-3-其他学习建议" class="headerlink" title="5.3 其他学习建议"></a>5.3 其他学习建议</h3><p>所以，在复试之前还有很长一段时间，这段时间可以给自己安排一些除了复试内容之外的学习任务，比如学点R语言、学点算法啥的，我在下面列出大家可以自学的东西，基本是大数据班会学的，大家在毕业设计之余或者实习之余可以一步步的学，我也只是个初学者，只能推荐一些我觉得相对比较好的资料和教程，当然也结合了其他的学长学姐的建议和网络的推荐。<br>复试前不用把所有的都学了，但建议把R语言看下，那样面试的时候就可以突出你学过以及会R了。人大是R语言国内的开山鼻祖，每年的R语言会议都会吸引数据分析界很多大牛来讲演。我觉得这些可以放到复试之后，那么长一段时间，找个实习啥的，有空的时候学学。</p>
<p>R语言</p>
<ul>
<li><a href="https://m.douban.com/book/subject/20382244/" target="_blank" rel="noopener">《R语言实战》</a>：从基础开始讲，基本涵盖了R语言基础的内容，因为研究生很多作业都会用R来实现，提前学会了，到时候就可以直接上手了，不至于像我们这一届蛮多人都是重头开始学R。</li>
<li>薛薇老师<a href="http://blog.sina.com.cn/s/blog_6248f74f0102w720.html" target="_blank" rel="noopener">《R语言数据挖掘算法及应用》</a>：这本书是薛薇老师上课的教材，我个人觉得写的很好，而且老师的讲课水平很高，会把一些艰深的算法用相对易懂的方式讲授出来。里面用R语言实现算法，课堂作业也会让你用R实现，所以，提前学起来很有好处。需要的数据集在下面的链接找</li>
</ul>
<p>PYTHON</p>
<ul>
<li><p><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000" target="_blank" rel="noopener">廖雪峰的python2.7教程</a>：python基础，比外国人写的书易懂多了</p>
</li>
<li><p><a href="https://m.douban.com/book/subject/3288908/" target="_blank" rel="noopener">《集体智慧编程》</a>：算法的python实现</p>
</li>
</ul>
<p>SQL</p>
<ul>
<li><a href="http://www.w3school.com.cn/sql/" target="_blank" rel="noopener">w3school的SQL教程</a>：SQL入门很快，基本上几天就能学会，基本的语法在下面教程里有</li>
</ul>
<p>Linux </p>
<ul>
<li><a href="http://m.imooc.com/learn/175" target="_blank" rel="noopener">慕课网的linux达人养成计划</a>   </li>
</ul>
<p>机器学习算法</p>
<ul>
<li><a href="https://m.douban.com/book/subject/10590856/" target="_blank" rel="noopener">李航的《统计学习方法》</a>：全是干货</li>
</ul>
<h2 id="六、写在最后"><a href="#六、写在最后" class="headerlink" title="六、写在最后"></a>六、写在最后</h2><p>这一路下来，感谢的人很多，人大陈思聪学长（见过学长本人，沉稳贴心哈哈，考研的时候不会的题去问他都会耐心解答），小杰克学长（这个群真的建设的太赞了，复试的资料是向学长要的，帮助很大），老胡学长（最近给我发了好几本砖头书，全英文版，据说是装逼神器），以及在中海洋读研的王淼淼童鞋（跨专业考统计真心不容易，磕磕碰碰了很多次，王淼淼鼎力相助；我后期整理了一个专业课思维导图，也是王淼淼给我的灵感），在幼儿园种花种草的冯涵小朋友，三个考研小伙伴大象、赫姐还有小姨妈（每次拿小姨妈开玩笑真的屡试不爽），还有强悍的北大444哥（他的经验贴真的是棒呆）。最后想感谢的是我的女朋友，她脸上的笑容总能够化解我的忧愁与不安，每次复习到夜深，想起在这座城市的另一边有那么一个人与我一同牵手向前，就会充满力量。 </p>
<p>在本科期间，我们所接受的更多的是老师所教授的知识，而到了研究生期间，我们要准备开始制造新的知识，更高层次的目标便是对人类普遍的知识有所贡献，而达到如此境界的来源正是你对知识的渴求与不断地追索，你不再满足于单一的知识面，而渴望搭建更坚固的知识架构。我们不能功利的对待这场磨练你意志的战役，不能仅仅把它看成是获取更多外在利益的工具，虽然确实可以达到这样的效果，但这样的动力绝对不会持久的催促你往更高的知识领域探索。</p>
<p>很多人在中途放弃或是马马虎虎的应付，就是自己内心缺乏行动的信念支撑，倘若只是觉得考上研就一劳永逸了，就没有必要花费那么厚重的时间成本了，因为，考上研只是一个起点，更艰难的路，在前方。</p>
<p>祝愿学弟学妹可以在考研路上发现更多的风景，顺利考上人大。</p>
]]></content>
      
        <categories>
            
            <category> 人大应统部落 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 人大应统 </tag>
            
            <tag> 考研 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（16）：统计学习概论]]></title>
      <url>/2017/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/</url>
      <content type="html"><![CDATA[<h2 id="一、统计学习"><a href="#一、统计学习" class="headerlink" title="一、统计学习"></a>一、统计学习</h2><h3 id="1-1-特点"><a href="#1-1-特点" class="headerlink" title="1.1 特点"></a>1.1 特点</h3><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。赫尔伯特·西蒙曾对学习定义为：“如果一个系统能够执行某个过程改进它的性能，这就是学习。”按照这一观点，统计学习就是计算机系统通过数据及统计方法提高系统性能的机器学习。</p>
<a id="more"></a>
<h3 id="1-2-对象"><a href="#1-2-对象" class="headerlink" title="1.2 对象"></a>1.2 对象</h3><p>统计学习的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。数据包括存在于计算机及网络上的各种数字、文字、图像、视频、音频及它们的组合。统计学习对于数据的基本假设是同类数据具有一定###的统计规律性。</p>
<h3 id="1-3-目的"><a href="#1-3-目的" class="headerlink" title="1.3 目的"></a>1.3 目的</h3><p>考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要尽可能地提高学习效率。</p>
<h3 id="1-4-方法"><a href="#1-4-方法" class="headerlink" title="1.4 方法"></a>1.4 方法</h3><p>统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）、强化学习（reinforcement learning）等组成</p>
<ul>
<li><p>监督学习：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据独立同分布，并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space），应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。</p>
</li>
<li><p>三要素：模型的假设空间（模型）、模型选择的准则（策略）、模型学习的算法（算法）</p>
</li>
<li>步骤：<ul>
<li>1）得到一个有限的训练数据集合</li>
<li>2）确定包含所有可能的模型的假设空间，即学习模型的集合</li>
<li>3）确定模型选择的准则，即学习的策略</li>
<li>4）实现求解最优模型的算法，即学习的算法</li>
<li>5）通过学习方法选择最优模型</li>
<li>6）利用学习的最优模型对新数据进行预测或分析</li>
</ul>
</li>
</ul>
<h2 id="二、监督学习"><a href="#二、监督学习" class="headerlink" title="二、监督学习"></a>二、监督学习</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p>监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做一个好的预测。它从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。</p>
<h3 id="2-2-基本概念"><a href="#2-2-基本概念" class="headerlink" title="2.2 基本概念"></a>2.2 基本概念</h3><h3 id="2-2-1-输入空间、特征空间与输出空间"><a href="#2-2-1-输入空间、特征空间与输出空间" class="headerlink" title="2.2.1 输入空间、特征空间与输出空间"></a>2.2.1 输入空间、特征空间与输出空间</h3><ul>
<li>输入空间与输出空间：输出与输出所有可能值的集合。通常输出空间远远小于输入空间</li>
<li>特征空间：所有特征向量存在的空间。特征空间的每一维对应于一个特征。模型都定义在特征空间上。</li>
<li>输入实例$x$的特征向量：<script type="math/tex; mode=display">
x=\left(x^{\left(1\right)},x^{\left(2\right)},···,x^{\left(i\right)},···,x^{\left(n\right)}\right)^T</script>其中$x^{(i)}$表示$x$的第$i$个特征，$x_i$表示多个输入向量的第$i$个，即<script type="math/tex; mode=display">
x_i=\left(x_i^{\left(1\right)},x_i^{\left(2\right)},···,x_i^{\left(n\right)}\right)^T</script></li>
<li>训练数据和测试数据由输入输出对（即样本）组成，通常表示为：<script type="math/tex; mode=display">
T=\left\{\left(x_1,y_1\right),\left(x_2,y_2\right),···,\left(x_N,y_N\right)\right\}</script></li>
<li>回归问题：输入变量与输出变量均为连续变量的预测问题。</li>
<li>分类问题：输出变量为有限个离散变量的预测问题。</li>
<li>标注问题：输入变量与输出变量均为变量序列的预测问题。</li>
</ul>
<h3 id="2-2-2-联合概率分布"><a href="#2-2-2-联合概率分布" class="headerlink" title="2.2.2 联合概率分布"></a>2.2.2 联合概率分布</h3><p>统计学习假设数据存在一定的统计规律，监督学习的基本假设为$X$和$Y$具有联合概率分布的假设，我们把训练数据与测试数据看作是依联合概率分布$P(X,Y)$独立同分布产生的。</p>
<h3 id="2-2-3-假设空间"><a href="#2-2-3-假设空间" class="headerlink" title="2.2.3 假设空间"></a>2.2.3 假设空间</h3><p>监督学习的目的在于找到由输入到输出的映射模型集合中最好的一个。这个集合即假设空间。模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数$Y=f(X)$表示。</p>
<h2 id="三、统计学习三要素"><a href="#三、统计学习三要素" class="headerlink" title="三、统计学习三要素"></a>三、统计学习三要素</h2><h3 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1 模型"></a>3.1 模型</h3><p>模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布（概率模型）或决策函数（非概率模型）。假设空间用$\mathscr{F}$表示，它是由一个参数向量决定的决策函数族：</p>
<script type="math/tex; mode=display">
\mathscr{F}=\left\{f\ |\ Y=f_{\theta}\left(X\right),\theta\in R^n\right\}</script><p>参数向量$\theta$取值于$n$维欧式空间$R^n$，称为参数空间。<br>也可以是一个参数向量决定的条件概率分布族：</p>
<script type="math/tex; mode=display">
\mathscr{F}=\left\{P\ |\ P_{\theta}\left(Y|X\right),\theta\in R^n\right\}</script><h3 id="3-2-策略"><a href="#3-2-策略" class="headerlink" title="3.2 策略"></a>3.2 策略</h3><p>有了模型的假设空间，接下来需要考虑按照什么样的准则学习或选择最优的模型。</p>
<h3 id="3-2-1-损失函数和风险函数"><a href="#3-2-1-损失函数和风险函数" class="headerlink" title="3.2.1 损失函数和风险函数"></a>3.2.1 损失函数和风险函数</h3><p><strong>损失函数（loss function）</strong>度量一次预测的好坏。损失函数越小，模型就越好常用的损失函数有：</p>
<ul>
<li>0-1损失函数（0-1 loss function）：<script type="math/tex; mode=display">
L=\left\{\begin{matrix}{}
  1&        Y\ne f\left(X\right)\\
  0&        Y=f\left(X\right)\\
\end{matrix}\right.</script></li>
<li><p>平方损失函数（quadratic loss function）:</p>
<script type="math/tex; mode=display">
L=\left(Y-f\left(X\right)\right)^2</script></li>
<li><p>绝对损失函数（absolute loss function）：</p>
<script type="math/tex; mode=display">
L=|Y-f\left(X\right)|</script></li>
<li>对数损失函数（logarithmic loss function）：<script type="math/tex; mode=display">
L=-\log P\left(Y|X\right)</script></li>
</ul>
<p>风险函数（risk function）或期望损失（expected loss）是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失。</p>
<script type="math/tex; mode=display">
R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}</script><p>我们学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$未知，风险函数不能直接计算。这样，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就沦为病态问题。<br>但我们可以计算训练数据集的平均损失，即经验风险（empirical risk）或经验损失（empirical loss）：</p>
<script type="math/tex; mode=display">
\textrm{R}_{emp}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}</script><p>根据大数定理，当样本容量$N$趋于无穷时，经验风险趋于期望风险。自然而然想到可以使用经验风险来估计期望风险。但现实中训练样本数目很小，这种估计往往不理想，需要矫正，以此引出经验风险最小化和结构风险最小化。</p>
<h3 id="3-2-2-经验风险最小化和结构风险最小化"><a href="#3-2-2-经验风险最小化和结构风险最小化" class="headerlink" title="3.2.2 经验风险最小化和结构风险最小化"></a>3.2.2 经验风险最小化和结构风险最小化</h3><p>经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题：</p>
<script type="math/tex; mode=display">
\underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}</script><p>当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p>
<p>但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为：</p>
<script type="math/tex; mode=display">
R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。<br>结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题：</p>
<script type="math/tex; mode=display">
\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。</p>
<h3 id="3-3-算法"><a href="#3-3-算法" class="headerlink" title="3.3 算法"></a>3.3 算法</h3><p>学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优化。如何用数值计算求解，如何保证找到全局最优化，并使求解过程高效，是一个重要的问题。</p>
<h2 id="四、模型评估与模型选择"><a href="#四、模型评估与模型选择" class="headerlink" title="四、模型评估与模型选择"></a>四、模型评估与模型选择</h2><h3 id="4-1-训练误差与测试误差"><a href="#4-1-训练误差与测试误差" class="headerlink" title="4.1 训练误差与测试误差"></a>4.1 训练误差与测试误差</h3><p>训练误差（training error）是模型关于训练数据集的平均损失：</p>
<script type="math/tex; mode=display">
R_{emp}\left(\hat{f}\right)=\frac{1}{N_1}\sum_{i=1}^{N_1}{L\left(y_i,\hat{f}\left(x_i\right)\right)}</script><p>测试误差(test error)是模型关于测试数据集的平均损失：</p>
<script type="math/tex; mode=display">
R_{emp}\left(\hat{f}\right)=\frac{1}{N_2}\sum_{i=1}^{N_2}{L\left(y_i,\hat{f}\left(x_i\right)\right)}</script><p>测试误差反映了学习方法对未知的测试数据集的预测能力，即泛化能力。</p>
<h3 id="4-2-过拟合与模型选择"><a href="#4-2-过拟合与模型选择" class="headerlink" title="4.2 过拟合与模型选择"></a>4.2 过拟合与模型选择</h3><p>我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。</p>
<p>过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。</p>
<p>模型选择旨在避免过拟合并提高模型的预测能力，模型选择时，不仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力。下图描述了训练误差和测试误差与模型的复杂度之间的关系：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%883.21.31.png" alt="屏幕快照 2017-03-29 下午3.21.31"></p>
<p>当模型复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最小值后又增大。当选择的模型复杂度过大时，过拟合现象就会发生。所以要选择复杂度适当的模型，已达到测试误差最小的目的。以此引出正则化与交叉验证。</p>
<h2 id="五、正则化与交叉验证"><a href="#五、正则化与交叉验证" class="headerlink" title="五、正则化与交叉验证"></a>五、正则化与交叉验证</h2><h3 id="5-1-正则化"><a href="#5-1-正则化" class="headerlink" title="5.1 正则化"></a>5.1 正则化</h3><h3 id="5-1-1-定义"><a href="#5-1-1-定义" class="headerlink" title="5.1.1 定义"></a>5.1.1 定义</h3><p>模型选择的典型方法是正则化（regularzation）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下：</p>
<script type="math/tex; mode=display">
\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)</script><p>第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。</p>
<h3 id="5-1-2-不同形式"><a href="#5-1-2-不同形式" class="headerlink" title="5.1.2 不同形式"></a>5.1.2 不同形式</h3><p>正则化项可以取不同的形式。例如，回归问题中，损失函数是平方误差，正则化项可以是参数向量的$L_2$范数:</p>
<script type="math/tex; mode=display">
L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\frac{\lambda}{2}||w||^2</script><p>也可以是参数向量的$L_1$范数：</p>
<script type="math/tex; mode=display">
L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\lambda ||w||_1</script><p>第一项的经验风险较小的模型可能较复杂（有多个非零参数），这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。</p>
<h3 id="5-1-3-奥卡姆剃刀"><a href="#5-1-3-奥卡姆剃刀" class="headerlink" title="5.1.3 奥卡姆剃刀"></a>5.1.3 奥卡姆剃刀</h3><p>正则化符合奥卡姆剃刀原理，应用于模型选择时变为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有很小的先验概率，简单的模型有较大的先验概率。</p>
<h3 id="5-2-交叉验证"><a href="#5-2-交叉验证" class="headerlink" title="5.2 交叉验证"></a>5.2 交叉验证</h3><h3 id="5-2-1-定义"><a href="#5-2-1-定义" class="headerlink" title="5.2.1 定义"></a>5.2.1 定义</h3><p>如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别是训练集（training set）用来训练模型、验证集（validation set）用于模型的选择、测试集（test set）用于最终对学习方法的评估，最终选择对验证集有最小预测误差的模型。</p>
<p>但是实际应用中数据不充足，所以我们采用交叉验证，它的基本思想是重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试和模型选择。</p>
<h3 id="5-2-2-方法"><a href="#5-2-2-方法" class="headerlink" title="5.2.2 方法"></a>5.2.2 方法</h3><ul>
<li>简单交叉验证：首先随机地将已给数据分为两个部分，一部分作为训练集（70%），另一部分作为测试集（30%）；然后用训练集在各种条件下（如不同的参数个数）训练模型，从而得到不同的模型；在测试机上评价各个模型的测试误差，选出测试误差最小的模型。</li>
<li>S折交叉验证（S-fold cross validation）：应用最广泛。首先随即将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。</li>
<li>留一交叉验证leave-one-out cross validation）：S折交叉验证的特殊情形是S=N（N为给定数据集的容量），往往在数据缺乏的情况下使用</li>
</ul>
<h2 id="六、泛化能力"><a href="#六、泛化能力" class="headerlink" title="六、泛化能力"></a>六、泛化能力</h2><h3 id="6-1-泛化误差"><a href="#6-1-泛化误差" class="headerlink" title="6.1 泛化误差"></a>6.1 泛化误差</h3><p>泛化能力是指由该方法学习到的模型对未知数据的预测能力。现实中常常通过测试误差来评价学习方法的泛化能力，但因为测试数据及有限，评价结果不一定可靠。</p>
<p>理论上，通过泛化误差来反映学习方法的泛化能力，泛化误差即用学习到的模型对未知数据预测的误差：</p>
<script type="math/tex; mode=display">
R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}</script><p>泛化误差越小，模型效果就好。泛化误差就是所学习到的模型的期望风险。</p>
<h2 id="七、生成模型与判别模型"><a href="#七、生成模型与判别模型" class="headerlink" title="七、生成模型与判别模型"></a>七、生成模型与判别模型</h2><h3 id="7-1-判别模型"><a href="#7-1-判别模型" class="headerlink" title="7.1 判别模型"></a>7.1 判别模型</h3><p>判别模型由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。它关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：K近邻法、感知机、决策树、逻辑斯谛回归、最大熵模型、支持向量机、提升方法、条件随机场。</p>
<p>判别方法的特点：</p>
<ul>
<li>直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率很高；</li>
<li>由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li>
</ul>
<h3 id="7-2-生成模型"><a href="#7-2-生成模型" class="headerlink" title="7.2 生成模型"></a>7.2 生成模型</h3><p>生成模型由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型：</p>
<script type="math/tex; mode=display">
P\left(Y|X\right)=\frac{P\left(X,Y\right)}{P\left(X\right)}</script><p>因为模型表示了给定输入$X$产生输出$Y$的生成关系，所以被称为生成模型。典型的生成模型有：朴素贝叶斯、隐马尔科夫模型</p>
<p>生成方法的特点：</p>
<ul>
<li>生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能；</li>
<li>生成方法的学习收敛速度快，即当样本容量增加时，学到的模型可以很快收敛于真实模型；</li>
<li>当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用。</li>
</ul>
<h2 id="八、分类问题"><a href="#八、分类问题" class="headerlink" title="八、分类问题"></a>八、分类问题</h2><h3 id="8-1-定义"><a href="#8-1-定义" class="headerlink" title="8.1 定义"></a>8.1 定义</h3><p>在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。它从数据中学习一个分类模型或分类决策函数，即学习一个分类器，然后对新的输入进行输出的预测，即进行分类。分为多类分类和二类分类问题。</p>
<h3 id="8-2-学习过程"><a href="#8-2-学习过程" class="headerlink" title="8.2 学习过程"></a>8.2 学习过程</h3><p>如图所示，分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%884.50.21.png" alt="屏幕快照 2017-03-29 下午4.50.21"></p>
<h3 id="8-3-分类准确率"><a href="#8-3-分类准确率" class="headerlink" title="8.3 分类准确率"></a>8.3 分类准确率</h3><h3 id="8-3-1-定义"><a href="#8-3-1-定义" class="headerlink" title="8.3.1 定义"></a>8.3.1 定义</h3><p>评价分类器性能的指标一般是分类准确率（accuracy）,即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也即损失函数是0-1损失时测试数据集上的准确率。</p>
<h3 id="8-3-2-常用指标"><a href="#8-3-2-常用指标" class="headerlink" title="8.3.2 常用指标"></a>8.3.2 常用指标</h3><p>通常将关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP（正类预测为正类数）、FN（正类预测为负类书）、FP（负类预测为正类数）、TN（负类预测为负类数）<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%885.10.54.png" alt="屏幕快照 2017-03-29 下午5.10.54"></p>
<ul>
<li>精确率：<script type="math/tex; mode=display">
P=\frac{TP}{TP+FP}</script><ul>
<li>召回率：<script type="math/tex; mode=display">
R=\frac{TP}{TP+FN}</script></li>
<li>$F_1$值，是精确率和召回率的调和均值，即<script type="math/tex; mode=display">
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}</script><script type="math/tex; mode=display">
F_1=\frac{2TP}{2TP+FP+FN}</script>精确率与召回率都很高时，$F_1$值也会很高。</li>
</ul>
</li>
</ul>
<h3 id="8-3-3-分类方法与应用"><a href="#8-3-3-分类方法与应用" class="headerlink" title="8.3.3 分类方法与应用"></a>8.3.3 分类方法与应用</h3><ul>
<li>常见分类统计方法：K近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、支持向量机、提升、贝叶斯网络、神经网络Winnow</li>
<li>应用：银行业务构建客户分类模型，对客户按照贷款风险大小分类；网络非法入侵检测；人脸是否出现的检测；网页分类；文本分类等。</li>
</ul>
<h2 id="九、标注问题"><a href="#九、标注问题" class="headerlink" title="九、标注问题"></a>九、标注问题</h2><h3 id="9-1-定义"><a href="#9-1-定义" class="headerlink" title="9.1 定义"></a>9.1 定义</h3><p>标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。它的目的在于学习一个模型，使它能够对观测序列给出标记序列作为预测。标注问题分为学习和标注两个过程：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%885.35.56.png" alt="屏幕快照 2017-03-29 下午5.35.56"></p>
<h3 id="9-2-应用"><a href="#9-2-应用" class="headerlink" title="9.2 应用"></a>9.2 应用</h3><p>标注常用的统计学习方法有：隐马尔科夫模型、条件随机场</p>
<p>它在信息抽取、自然语言处理领域被广泛应用。</p>
<ul>
<li>自然语言处理的词性标注：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。</li>
</ul>
<h2 id="十、回归问题"><a href="#十、回归问题" class="headerlink" title="十、回归问题"></a>十、回归问题</h2><h3 id="10-1-定义"><a href="#10-1-定义" class="headerlink" title="10.1 定义"></a>10.1 定义</h3><p>回归模型表示输入变量和输出变量之间映射的函数，等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。可分为一元回归和多元回归，线性回归和非线性回归。它最常用的损失函数为平方损失函数，可以用最小二乘法求解。回归问题分为学习和标注两个过程：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-29%20%E4%B8%8B%E5%8D%885.25.58.png" alt="屏幕快照 2017-03-29 下午5.25.58"></p>
<h3 id="10-2-应用"><a href="#10-2-应用" class="headerlink" title="10.2 应用"></a>10.2 应用</h3><p>股价预测：将影响股价的信息视作自变量，将股价视为因变量，将过去的数据作为训练数据，学习一个回归模型，并对未来的股价进行预测。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习概论 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（15）：EM算法]]></title>
      <url>/2017/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8815%EF%BC%89%EF%BC%9AEM%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>期望最大值（Expectation Maximization，简称EM算法）是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。其主要思想就是通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数。本文将尽可能详尽地描述EM算法的原理。并结合高斯混合模型介绍EM算法是如何求解的。</p>
<a id="more"></a>
<h2 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h2><p>EM算法是一种迭代算法，用于含有隐变量（hidden variable）的改了吧模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：<code>E步</code>-求期望（expectation）；<code>M步</code>-求极大（maximization）。故称为期望极大算法（expectation maximization），简称EM算法。</p>
<h2 id="二、Jensen不等式"><a href="#二、Jensen不等式" class="headerlink" title="二、Jensen不等式"></a>二、Jensen不等式</h2><p>设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f^{‘’}(x)≥0$，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的即$H≥0$，那么$f$是凸函数。如果$f^{‘’}(x)&gt;0$或$H&gt;0$，那么称$f$是严格凸函数。</p>
<p>$Jensen$不等式表述如下：</p>
<ul>
<li>如果$f$是凸函数，$x$是随机变量，那么：$E[f(x)]≥f(E[x])$。特别地，如果$f$是严格凸函数，$E[f(x)]≥f(E[x])$，那么当且仅当$P(x=E[x])=1$(也就是说$x$是常量)，$E[f(x)]=f(E[x])$;</li>
<li>如果$f$是凹函数，$x$是随机变量，则$E[f(x)]≥f(E[x])$。当$f$是（严格）凹函数当且仅当$-f$是（严格）凸函数。</li>
</ul>
<p>通过下面这张图，我们可以加深理解：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-24%20%E4%B8%8A%E5%8D%8812.11.32.png" alt="屏幕快照 2017-03-24 上午12.11.32"></p>
<p>上图中，函数$f$是凸函数，$X$是随机变量，有0.5的概率为$a$，有0.5的概率是b（就像抛硬币一样）。$X$的期望值就是a和b的中值了，图中可以看到$E[f(x)]≥f(E[x])$成立。</p>
<h2 id="三、EM思想"><a href="#三、EM思想" class="headerlink" title="三、EM思想"></a>三、EM思想</h2><h3 id="3-1-极大似然估计"><a href="#3-1-极大似然估计" class="headerlink" title="3.1 极大似然估计"></a>3.1 极大似然估计</h3><p>EM算法推导过程中，会使用到极大似然估计参数。</p>
<p>极大似然估计是一种概率论在统计学的应用。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察结果，利用结果推出参数的大概值。极大似然估计建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<p>这里再给出求极大似然估计值的一般步骤：</p>
<ul>
<li>1）写出似然函数；</li>
<li>2）对似然函数取对数，并整理；</li>
<li>3）求导数，令导数为0，得到似然方程；</li>
<li>4）解似然方程，得到的参数即为所求；</li>
</ul>
<p>关于极大似然估计的实例，可以参考<a href="https://zh.wikipedia.org/wiki/最大似然估计" target="_blank" rel="noopener"><code>wikipedia最大似然估计条目</code></a></p>
<h3 id="3-2-EM算法思想"><a href="#3-2-EM算法思想" class="headerlink" title="3.2 EM算法思想"></a>3.2 EM算法思想</h3><p>下面介绍EM算法的思想：</p>
<p>给定的训练样本是$x^{(1)}，x^{(2)}，···，x^{(m)}$，样例间相互独立，但每个样本对应的类别$z^{(i)}$是未知的，也即隐含变量。我们想找到每个样例隐含的类别$z$，能使得$P(x,z)$最大。$P(x,z)$的最大似然估计如下：</p>
<script type="math/tex; mode=display">
l\left(\theta\right)=\sum_{i=1}^m{\log p\left(x;\theta\right)}=\sum_{i=1}^m{\log\sum_z{}p\left(x,z;\theta\right)}</script><p>第一步是对极大似然函数取对数，第二步是对每个样本实例的每个可能的类别$z$求联合分布概率之和。但是直接求$\theta$一般比较困难，因为有隐藏变量$z$存在，如果$z$是一个已知的数，那么使用极大似然估计来估算会很容易。在这种$z$不确定的情形下，EM算法就派上用场了。</p>
<p>EM算法是一种解决存在隐变量优化问题的有效方法。对于上述情况，由于存在隐变量，不能直接最大化$l(\theta)$，我们可以不断地建立$l$的下界（E步），然后优化下界（M步），依次迭代，直至算法收敛到局部最优。这就是EM算法的核心思想，简单的归纳一下：</p>
<p>EM算法通过引入隐变量，使用MLE进行迭代求解参数。通常引入隐含变量后会有两个参数，EM算法首先会固定其中的第一个参数，然后使用MLE计算第二个变量值；接着通过固定第二个变量，再使用MLE估计第一个变量值，依次迭代，直至收敛到局部最优解。</p>
<h2 id="四、EM推导"><a href="#四、EM推导" class="headerlink" title="四、EM推导"></a>四、EM推导</h2><p>下面来推导EM算法：</p>
<p>对于每一个样例$i$，让$Q_i$表示该样例隐含变量$z$的某种分布，$Q_i$满足的条件是</p>
<script type="math/tex; mode=display">
\sum_z{Q_i\left(z\right)=1\ \\ Q_i\left(z\right)\geqslant 0}</script><p>（如果$z$是连续的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果是按照隐藏变量是男女，那么就是伯努利分布。</p>
<p>可以由前面阐述的内容得到下面的公式：</p>
<script type="math/tex; mode=display">
\sum_i{\log p\left(x^{\left(i\right)};\theta\right)=\sum_i{\log\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}}}
········（1）</script><script type="math/tex; mode=display">
=\sum_i{\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}}
········（2）</script><script type="math/tex; mode=display">
\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}\right)}}······（3）</script><p>上面三个式子中，式（1）是根据联合概率密度下某个变量的边缘密度求解的（这里把$z$当做是随机变量）。对每一个样本$i$的所有可能类别求等式右边的联合概率密度函数和，也就是得到等式左边为随机变量$x$的边缘概率密度。由于对式（1）直接求导非常困难，我们可以做一个简单的变化，将其分子分母都乘以一个相等的函数$Q_i(Z^{(i)})$，得到式（2）。那么如何从式（2）推导出式（3）呢，这就需要用到之前提到的Jensen不等式。</p>
<p>以下为具体的分析过程：</p>
<p>首先，把（1）式中的$log$函数看成是一个整体，即令$f(x)=log(x)$，因为$(log(x))^”=-1/x^{2}&lt;0$，根据定理可知其为凹函数。</p>
<p>再根据凹函数的Jensen不等式：$f(E[X])&gt;=E[f(x)]$。</p>
<p>到这里，我们可以观察到，在式（2）中，当把$log(x)$看成$f(x)$时，后边的</p>
<script type="math/tex; mode=display">\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}</script><p>其实就是可以类比为离散型随机变量的期望公式。具体的求解可以参照下图中离散型随机变量的期望公式。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-24%20%E4%B8%8A%E5%8D%8811.20.14.png" alt="屏幕快照 2017-03-24 上午11.20.14"></p>
<p>我们可以把$Q_i^{(z^{(i)})}$看成是相应的概率$p_i$，把</p>
<script type="math/tex; mode=display">
\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}</script><p>看作是$z^{(i)}$的函数$g(z)$，根据期望公式$E\left[g\left(x\right)\right]=\sum_{i=1}^{\infty}{g\left(x_i\right)·p_i}$可以得到：</p>
<script type="math/tex; mode=display">
E\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}\right)=\sum_{z^{\left(i\right)}}{Q_i\left(z\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}}</script><p>把上述根据Jensen不等式整合到一起得到：</p>
<script type="math/tex; mode=display">
f\left[E\left(g\left(X\right)\right)\right]=\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}</script><script type="math/tex; mode=display">
\geqslant E\left[f\left(g\left(X\right)\right)\right]=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}</script><p>这样我们就得到了式（3）。</p>
<p>现在我们把式（2）和式（3）的不等式次而成：似然函数$L(\theta)≥J(z,Q)$的形式，其中$z$为隐变量，那么我们可以通过不断地最大化$J$的下界，来使得$L(\theta)$不断提高，最终达到它的最大值。借助下图来解释下这个过程：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-24%20%E4%B8%8A%E5%8D%8811.55.29.png" alt="屏幕快照 2017-03-24 上午11.55.29"></p>
<p>首先我们固定$\theta$，调整$Q(z)$使下界（绿色曲线）$J(z,Q)$沿着绿色虚线上升至与$L(\theta)$在此点$\theta$处相等（绿色曲线至蓝色曲线），然后固定$Q(z)$，调整$\theta$使下界$J(z,Q)$达到最大值($\theta {_t}$至$\theta _{t+1}$)，然后再固定$\theta$，调整$Q(z)$…….直到收敛到似然函数$L(\theta)$的最大值处的$\theta^*$</p>
<p>这里有两个问题：</p>
<ul>
<li>什么时候下界$J(z,Q)$与$L(\theta)$在此点$\theta$处相等？</li>
<li>为什么一定会收敛？</li>
</ul>
<p>首先来解释下第一个问题。在Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立。而在这里，为：</p>
<script type="math/tex; mode=display">
\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}=c</script><p>对该式做个变换，将分母移到等号右边，并对所有的$z$求和，得到第一个等号；又因为前面提到的$\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)=1}$得到第二个等号。</p>
<script type="math/tex; mode=display">
\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)c}=c</script><p>根据上面两个式子可以得到</p>
<script type="math/tex; mode=display">
Q_i\left(z^{\left(i\right)}\right)=\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{\sum_z{p\left(x^{\left(i\right)},z;\theta\right)}}</script><script type="math/tex; mode=display">
\\\\\ =\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{p\left(x^{\left(i\right)};\theta\right)}</script><script type="math/tex; mode=display">
\\\\ =p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)</script><p>到这里，我们推出了在固定参数$\theta$后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），解决了$Q(z)$如何选择的问题。此步就是EM算法的E步，目的是建立$L(\theta)$的下界。接下来的M步，目的是在给定$Q(z)$后，调整$\theta$，从而极大化$L(\theta)$的下界$J$（在固定$Q(z)$后，下界还可以调整的更大）。那么一般的EM算法的步骤如下：</p>
<ul>
<li>第一步：初始化分布参数$\theta$；</li>
<li>第二步：重复E步和M步直到收敛：<ul>
<li>E步：根据参数的初始值或上一次迭代的模型参数来计算出的因变量的后验概率（条件概率），其实就是隐变量的期望值，来作为隐变量的当前估计值：<script type="math/tex; mode=display">
\\\\ Q_i\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)</script></li>
<li>M步：最大化似然函数从而获得新的参数值：<script type="math/tex; mode=display">
\theta :=arg\underset{\theta}{\max}\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}}</script></li>
</ul>
</li>
</ul>
<p>通过不断地迭代，然后就可以得到使似然函数$L(\theta)$最大化的参数$\theta$了。</p>
<p>接下来我们看第二个问题。上面多次说到直到收敛，那为什么一定会收敛呢？证明如下：</p>
<p>假定$\theta^{(t)}$和$\theta^{(t+1)}$是EM第t次和t+1次迭代后的结果。如果我们证明了$l(\theta^{(t)})≤l(\theta^{(t+1)})$，也就是说极大似然估计单调增加，那么最终我们就会得到极大似然估计的最大值。</p>
<p>下面来证明，选定$\theta^{(t)}$后，我们得到E步：</p>
<script type="math/tex; mode=display">
\\\\ Q_i^{(t)}\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)</script><p>这一步保证了在给定$\theta^(t)$时，Jensen不等式中的等式成立，也就是</p>
<script type="math/tex; mode=display">
l\left(\theta^{\left(t\right)}\right)=\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}</script><p>然后进行M步，固定$Q_i^{(t)}(z^{(i)})$，并将$\theta^{(t)}$视作变量，对上面的$l(\theta^{(t)})$求导后，得到$\theta^{(t+1)}$,这样经过一些推导会有以下式子成立：</p>
<script type="math/tex; mode=display">
l\left(\theta^{\left(t+1\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t+1\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（4）}</script><script type="math/tex; mode=display">
\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（5）}</script><script type="math/tex; mode=display">
=l\left(\theta^{\left(t\right)}\right)······\textrm{（6）}</script><p>解释第（4）步，得到$\theta^{(t+1)}$时，只是最大化$l(\theta^{(t)})$，也就是$l(\theta^{(t+1)})$的下界，而没有使等式成立，要想使等式成立只有在固定$\theta$，并按E步得到$Q_i$时才能成立。<br>况且根据我们前面得到的下式，对于所有的$Q_i$和$\theta$都成立</p>
<script type="math/tex; mode=display">
l\left(\theta^{\left(t\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}</script><p>第（5）式利用的M步的定义，M步就是将$\theta^{(t)}$调整到$\theta^{(t+1)}$，使得下界最大化。这样（5）、（6）就都证明成立了。</p>
<p>再结合之前那个图解释一下这几步推导：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-24%20%E4%B8%8A%E5%8D%8811.55.29.png" alt="屏幕快照 2017-03-24 上午11.55.29"></p>
<p>首先（4）对所有的参数都满足，而其等式成立条件只是在固定$\theta$，并调整好$Q$时成立，而第（4）步只是固定$Q$，调整$\theta$，不能保证等式一定成立。对应到图上就是蓝色曲线的峰值与$l(\theta^{(t+1)})$的关系，要使它们相等还必须要固定$\theta$，调整好$Q$；（4）到（5）就是M步的定义，也就是固定$Q$，调整$\theta^{(t)}$至$\theta^{(t+1)}$，对应到图上即为蓝色曲线与红色曲线交点处至蓝色曲线峰值。（5）到（6）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$l(\theta)$一个特定值（这里为$\theta^{(t)}$）一样的高度，而此时发现下界仍然可以上升，因此经过$M$步后，下界又被拉升，但达不到与$l(\theta)$另外一个特定值（$\theta^{(t+1)}$）一样的高度，之后E步又将下界拉到了与这个特定值一样的高度，循环往复，直到达到最大值。</p>
<p>这样就证明了$l(\theta)$会单调增加。如果要判断收敛情况，可以这样来做：一种收敛方法是$l(\theta)$不再变化，还有一种就是变化幅度很小，即根据$l(\theta^{(t+1)})=l(\theta^{(t)})$的值来决定。</p>
<p>从前面的推导中我们知道$l(\theta)≥J(Q,\theta)$，EM也可以看做是$J$的坐标上升法，如下图所示：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-24%20%E4%B8%8B%E5%8D%882.53.51.png" alt="屏幕快照 2017-03-24 下午2.53.51"></p>
<p>图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。<br>这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步：固定θ，优化Q；M步：固定Q，优化θ；交替将极值推向最大。</p>
<h2 id="五、EM的应用：混合高斯模型"><a href="#五、EM的应用：混合高斯模型" class="headerlink" title="五、EM的应用：混合高斯模型"></a>五、EM的应用：混合高斯模型</h2><p>待补充</p>
<h2 id="六、EM的应用：EM聚类"><a href="#六、EM的应用：EM聚类" class="headerlink" title="六、EM的应用：EM聚类"></a>六、EM的应用：EM聚类</h2><p>以下的聚类图来自维基百科，可以生动的看出<br><img src="http://omu7tit09.bkt.clouddn.com/EM_Clustering_of_Old_Faithful_data.gif" alt="EM_Clustering_of_Old_Faithful_data"></p>
<p>待补充</p>
<h2 id="七、参考资料"><a href="#七、参考资料" class="headerlink" title="七、参考资料"></a>七、参考资料</h2><p><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html" target="_blank" rel="noopener"><code>The EM Algorithm</code></a><br><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006924.html" target="_blank" rel="noopener"><code>混合高斯模型和EM算法</code></a><br><a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf" target="_blank" rel="noopener"><code>cs229-notes8</code></a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> EM算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（14）：关联分析]]></title>
      <url>/2017/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h2 id="一、关联分析"><a href="#一、关联分析" class="headerlink" title="一、关联分析"></a>一、关联分析</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><p>在数据挖掘与机器学习中，关联规则（Association Rules）是一种较为常用的无监督学习算法，与分类、聚类等算法不同的是，这一类算法的主要目的在于发掘数据内在结构特征之间的关联性。</p>
<p>简单一点来说，就是在大规模的数据集中寻找一些有意义有价值的关系。有了这些关系，一方面，可以帮助我们拓宽对数据及其特征的理解；另一方面，则可以实现推荐系统的构建与应用（例如购物篮分析等）。</p>
<p>在对关联规则有了基本的认识后，我们对其进行进一步的细分，以日常生活中的关联性举例，在逛超市的顾客中，购买面包的人很大程度上会购买牛奶，这一类的关联性被称为简单关联规则；再例如，购买汽车遮阳板的很多顾客会在近期内购买零度玻璃水，这样的事例不仅反映了事物间的关联关系，而且还具有时间上的先后顺序，因此这一类的关联性被称为序列关联规则。</p>
<p>广义上的关联规则包含了简单关联和序列关联，接下来我们分别对这两块知识进行深入学习。</p>
<a id="more"></a>
<h3 id="1-2-简单关联规则初探"><a href="#1-2-简单关联规则初探" class="headerlink" title="1.2 简单关联规则初探"></a>1.2 简单关联规则初探</h3><p>首先我们需要明确关联分析中的一些基本概念：</p>
<ul>
<li>事务：指关联分析中的分析对象，我们可以把它理解成为一种宽泛行为（例如顾客的一次超市购买行为，电脑的使用者的一次网页浏览行为等都可以称之为事务），由事务标识（TID）与项目集合组成。</li>
<li><p>项集：即事务中的一组项目的集合，单个的项目可以是一种商品、一个网页链接等。假设$X$为项集，$I$为项目全体且$I=\{i_1,i_2,···,i_n\}$，那么项集$X\subseteq I$。进一步的，如果$X$中包含$p$个项目，则称该项集为$p-$项集。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8B%E5%8D%886.27.11.png" alt="屏幕快照 2017-04-02 下午6.27.11">以上图为例，这里包含了4个事务，$I$包含了5个项目。对于第一个事务而言，由于$X$包含了三个项目，所以该$X$是一个$3-$项集。<br>明确了基本概念后，接下来学习关联规则的一般表现形式</p>
<script type="math/tex; mode=display">
X\rightarrow Y\left(S=s\%,C=c\%\right)</script><p>其中：</p>
</li>
<li><p>$X$和$Y$分别为规则的前项和后项，前项为项目或项集，后项表示某种结论或事实。</p>
</li>
<li>$S=s\%$表示规则支持度为$s\%$，$C=c\%$表示规则置信度为$c\%$</li>
</ul>
<p>到这里大家可能会疑惑，直接得到关联规则不就可以了吗？为什么要在结论中加入支持度和置信度呢？这就涉及到关联分析中非常重要的一块内容——<strong>有效性的判别</strong></p>
<h3 id="1-3-简单关联规则的有效性"><a href="#1-3-简单关联规则的有效性" class="headerlink" title="1.3 简单关联规则的有效性"></a>1.3 简单关联规则的有效性</h3><p>实际上，在数据中使用关联分析进行探索时，我们可以找出很多关联规则，但并非所有的关联规则都是有效的，有的可能令人信服的程度并不高，也有的可能适用范围很有限，带有这些特征的所谓“关联规则”，我们则称之为不具有“有效性”。<br>判断一条关联规则是否有效，需要用到以下两大测度指标，即规则置信度与规则支持度。</p>
<p><strong>1.规则置信度（Confidence）</strong><br>置信度是对简单关联规则准确度的测量，定义为包含项目$A$的事务中同时也包含项目$B$的概率，数学表述为：</p>
<script type="math/tex; mode=display">
Confidence\left(A\rightarrow B\right)=P\left(B|A\right)=\frac{P\left(AB\right)}{P\left(A\right)}</script><p>置信度的本质就是我们所学过的条件概率，置信度越高，则说明$A$出现则$B$出现的可能性也就越高。假设在电脑$\rightarrow$杀毒软件的关联规则中，置信度$C=60\%$，表示购买电脑的顾客中有$60\%$的顾客也购买了杀毒软件。</p>
<p><strong>2.规则支持度（Support）</strong></p>
<p>支持度测量了简单关联规则应用的普适性，定义为项目$A$与项目$B$同时出现的概率，数学表述为：</p>
<script type="math/tex; mode=display">
Support\left(A\rightarrow B\right)=P\left(B\cap A\right)=P\left(AB\right)</script><p>假设某天共有100个顾客到商场购买物品，其中有10个顾客同时购买了电脑和杀毒软件，那么上述关联规则的支持度就为10%，同样，支持度越高，表明某一关联规则的适用性就越大。</p>
<p>一个有效的简单关联规则，势必同时具有较高的置信度与支持度。因为，如果支持度较高而置信度较低，则证明规则的可信度差；而相反，如果支持度较低而置信度较高，则说明规则的应用范围较小。</p>
<p>举例来说，假设在1000个顾客购买行为的事务中，只有一个顾客购买了烧烤炉，同时也只有他购买了碳，虽然规则“烧烤炉$\rightarrow$碳”的置信度很高，为100%，但支持度仅有0.1%，说明这条规则缺乏普遍性，应用价值不高。</p>
<p>所以一个有效的关联规则，必须具有较高的置信度与支持度，那么在实际应用中，我们就需要给定最小的置信度$C_{min}$与支持度$S_{min}$，只要同时大于$C_{min}$和$S_{min}$的规则，我们才可以将其定义为是“有效”的。</p>
<h3 id="1-4-简单关联规则的实用性"><a href="#1-4-简单关联规则的实用性" class="headerlink" title="1.4 简单关联规则的实用性"></a>1.4 简单关联规则的实用性</h3><p>在对关联规则的有效性有一个基本的掌握后，我们在此基础上进行进一步的探讨——关联规则的实用性。</p>
<p>关联规则的实用性主要体现在以下两个方面：</p>
<ul>
<li>1）是否具有实际意义。例如“怀孕$\rightarrow $女性”的关联规则就没有实用价值。</li>
<li>2）是否具有指导意义，即帮助我们在现有的基础上做出有价值的优化。</li>
</ul>
<p>对第二点进一步展开说明，假设“牛奶$\rightarrow $男性顾客（$S=40\%，C=40\%$）”在$C_{min}$和$S_{min}$均为20%时是一条有效规则时，如果进一步计算发现顾客中男性的比例也为40%，也就是说购买牛奶的男性顾客等于所有顾客中的男性比例，那么这条规则就是一条前后项无关的随机性关联，因此它就没有有意义的指导信息，不具有实用性。</p>
<p>如何衡量关联规则具有实用性呢？这里我们就需要借助规则的提升度了。</p>
<p>规则提升度（Lift）：置信度与后项支持度之比，数学表述为：</p>
<script type="math/tex; mode=display">
Lift\left(A\rightarrow B\right)=\frac{Confidence\left(A\rightarrow B\right)}{P\left(B\right)}=\frac{P\left(AB\right)}{P\left(A\right)P\left(B\right)}</script><p>提升度反映了项目$A$的出现对项目$B$出现的影响程度。从统计角度来看，如果$A$的出现对项$B$的出现没有影响，即$A$与$B$相互独立的化，$P(AB)=P(A)P(B)$，此时规则提升度为1。所以，具有实用性的关联规则应该是提升度大于1的规则，即$A$的出现对$B$的出现有促进作用。同样，提升度越大，证明规则实用性越强。</p>
<p>这样我们就阐述清楚了关联规则的一些基本假定与判别标准，当数据集较小时，关联规则的使用较为简单，但是如果数据集很大的话，如何在这海量的数据中快速找出关联规则呢？这就引出了进一步要叙述的内容——简单关联规则下的$Apriori$算法。</p>
<h2 id="二、Apriori算法"><a href="#二、Apriori算法" class="headerlink" title="二、Apriori算法"></a>二、Apriori算法</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>在数据量庞大的前提下，由于简单搜索可能产生大量无效的关联规则，并导致计算效率底下。出于克服这些弊端的目的，Apriori算法应运而生，该算法自1996年提出后，经过不断地完善和发展，已成为简单关联分析中的核心算法。</p>
<h3 id="2-2-频繁项集的相关定义"><a href="#2-2-频繁项集的相关定义" class="headerlink" title="2.2 频繁项集的相关定义"></a>2.2 频繁项集的相关定义</h3><p>频繁项集很好理解，他是指大于等于最小支持度$S_{min}$的项集。其中，若频繁项集中包含一个项目，则成为频繁$1-$项集，记为$L_1$；若包含$k$个项目，则成为频繁$k-$项集，记为$L_k$。<br>频繁项集具有以下两个性质，这俩条性质将应用于我们后面频繁项集及其关联规则的寻找中：</p>
<ul>
<li>1）频繁项集的子集必为频繁项集（假设项集$\{A,C\}$是频繁项集，那么$\{A\}$和$\{C\}$也为频繁项集）</li>
<li>2）非频繁集的超集一定也是非频繁的（假设项集$\{D\}$不是频繁项集，那么$\{A,D\}$和$\{C,D\}$也不是频繁项集）</li>
</ul>
<p>进一步，当某一个$L_k$的所有超集都是频繁项集时，我们就可以称此$L_k$为最大频繁$k-$项集，确定它的目的就在于使之后的到的关联规则具有较高的普适性。</p>
<h3 id="2-3-寻找频繁项集"><a href="#2-3-寻找频繁项集" class="headerlink" title="2.3 寻找频繁项集"></a>2.3 寻找频繁项集</h3><p>对频繁项集的寻找，是Apriori算法提高寻找规则效率的关键。它采用迭代的方式逐层寻找下层的超集，并在超集中发现频繁项集。经过层层迭代，直到最顶层得到最大频繁项集为止。在每一轮的迭代中都包含以下两个步骤：</p>
<ul>
<li>1）产生候选集$C_k$，它是有可能成为频繁项集的项目集合；</li>
<li>2）修剪候选集$C_k$，即基于$C_k$计算相应的支持度，并依据最小支持度$S_{min}$对候选集$C_k$进行删减，得到新的候选集$C_{k+1}$，如此循环迭代，直到无法产生候选项集为止，这样最后一轮所得到的频繁项集就是Apriori所要求的最大频繁项集。</li>
</ul>
<p>接下来我们以一个下例子帮助理解：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8B%E5%8D%8810.31.15.png" alt="屏幕快照 2017-04-02 下午10.31.15"><br>假设我们指定的最小支持阀度为0.5（计数≥2）</p>
<ul>
<li>在第一轮迭代过程中，由于$D$的支持度小于0.5（只有0.25），所以没有进入频繁项集，其余均进入频繁项集，定义为$L_1$。</li>
<li>在第二轮迭代中，候选集$C_2$是$L_1$中所有项目的组合，计算各项目支持度，淘汰$\{A,B\}$和$\{A,E\}$，其余进入频繁项集，定义为$L_2$。</li>
<li>在第三轮迭代中，只有$\{B,C,E\}$进入候选集$C_3$，而其余都没有进入，之所以会这样，是因为这里使用到了前面所提到的频繁项集的第二个性质：<strong>非频繁项集的超集一定也是非频繁的</strong>。所以，包含$\{A,B\}$与$\{A,E\}$的超集是不可能成为频繁项集的。</li>
</ul>
<p>由于$L_3$不能继续构成候选集$C_4$，所以迭代结束，得到的最大频繁项集为$L_3\{B,C,E\}$。</p>
<h3 id="2-4-在最大频繁项集的基础上产生简单关联规则"><a href="#2-4-在最大频繁项集的基础上产生简单关联规则" class="headerlink" title="2.4 在最大频繁项集的基础上产生简单关联规则"></a>2.4 在最大频繁项集的基础上产生简单关联规则</h3><p>得到最大频繁项集并不是最终的目的。之前在判断关联规则的有效性时，我们学习了置信度与支持度两个指标。其中，支持度已经在寻找最大频繁项集的过程中发挥了作用，那么，在接下来关联规则的产生上，就轮到置信度大显身手了。</p>
<p>首先，每个频繁项集都需要计算所有非空子集$L^*$的置信度，公式为</p>
<script type="math/tex; mode=display">
C_{L'\rightarrow\left\{L-L'\right\}}=\frac{P\left(L\right)}{P\left(L'\right)}</script><p>如果所求得的$C_{L’\rightarrow\left\{L-L’\right\}}$大于我们自行指定的$C_{min}$，则生成相应的关联规则${L’\rightarrow\left\{L-L’\right\}}$</p>
<p>在上面的例子中，$L_3{\{B,C,E\}}$的非空子集就包括$\{B\}$，$\{C\}$，$\{E\}$，$\{B,C\}$，$\{B,E\}$，$\{C,E\}$，举例来说，根据公式可计算得到</p>
<script type="math/tex; mode=display">
C_{C\rightarrow\left\{B,E\right\}}=\frac{P\left(B,C,E\right)}{P\left(C\right)}=\frac{2}{3}=66.7\%</script><p>其余置信度依次为：$C_{B\rightarrow\left\{C,E\right\}}=66.7\%$，$C_{E\rightarrow\left\{B,C\right\}}=66.7\%$，$C_{\left\{B,C\right\}\rightarrow E}=100\%$，$C_{\left\{B,E\right\}\rightarrow C}=66.7\%$，$C_{\left\{C,E\right\}\rightarrow B}=100\%$</p>
<p>如果我么设定$C_{min}=80\%$的话，只有$C_{\left\{C,E\right\}\rightarrow B}$和$C_{\left\{B,C\right\}\rightarrow E}$可以入围，如果设定为$50\%$，那么六条规则就都是有效规则了。置信度的选取和支持度一样，只有结合具体应用情况，算法才能给到我们切合实际的结论。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 关联分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（13）：推荐系统（3）—矩阵分解技术]]></title>
      <url>/2017/03/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8A%80%E6%9C%AF/</url>
      <content type="html"><![CDATA[<p>随着Netflix Prize推荐比赛的成功举办，近年来隐语义模型（Latent Factor MOdel，LFM）受到越来越多的关注。隐语义模型最早在文本挖掘领域被提出，用于寻找文本的隐含语义，相关的模型常见的有潜在语义分析（Latent Semantic Analysis,LSA）、LDA（Latent Dirichlet Allocation）的主题模型（Topic Model）、矩阵分解（Matrix Factorization）等等。</p>
<a id="more"></a>
<p>其中<a href="https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf" target="_blank" rel="noopener">矩阵分解技术</a>是实现隐语义模型使用最广泛的一种方法，其思想也正来源于此，注明的推荐领域大神Yehuda Koren更是凭借矩阵分解模型勇夺Netflix Prize推荐比赛冠军，以矩阵分解为基础，Yehuda Koren在数据挖掘和机器学习相关的国际顶级会议（SIGIR,SIGKDD,RecSys等）发表了很多文章，将矩阵分解模型的优势发挥得淋漓尽致。实验结果表明，在个性化推荐中使用矩阵分解模型要明显优于传统的基于邻域的协同过滤(又称基于领域的协同过滤)方法，如UserCF、ItemCF等，这也使得矩阵分解成为了目前个性化推荐研究领域中的主流模型。</p>
<p>需要说明的是，协同过滤方法分为两大类，一类为上述基于领域的方法，第二类为基于模型的方法，即隐语义模型，矩阵分解模型是隐语义模型最为成功的一种实现，不作特别说明的情况下，本文将隐语义模型和矩阵分解看做同一概念，User-Item矩阵和User-Item评分矩阵为同一概念。</p>
<h2 id="一、传统的SVD算法"><a href="#一、传统的SVD算法" class="headerlink" title="一、传统的SVD算法"></a>一、传统的SVD算法</h2><p>说到矩阵分解，我们首先想到的就是奇异值分解SVD。我们可以把User-Item评分矩阵M进行SVD分解，并通过选择部分较大的一些奇异值来同时进行降维，也就是说矩阵M此时分解为：</p>
<script type="math/tex; mode=display">M_{m\times n}=U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}</script><p>其中k是矩阵MM中较大的部分奇异值的个数，一般会远远的小于用户数和物品数。</p>
<p>如果我们要预测第i个用户对第j个物品的评分$m_{ij}$,则只需要计算$u^T_iΣv_j$即可。通过这种方法，我们可以将评分表里面所有没有评分的位置得到一个预测评分，通过找到最高的若干个评分对应的物品推荐给用户。</p>
<p>可以看出这种方法简单直接，似乎很有吸引力。但是有一个很大的问题我们忽略了，就是SVD分解要求矩阵是稠密的，也就是说矩阵的所有位置不能有空白。有空白时我们的MM是没法直接去SVD分解的。大家会说，如果这个矩阵是稠密的，那不就是说我们都已经找到所有用户物品的评分了嘛，那还要SVD干嘛! 的确，这是一个问题，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用SVD分解并降维。但填充本身会造成很多问题，其一，填充大大增加了数据量，增加了算法复杂度。其二，简单粗暴的数据填充很容易造成数据失真。</p>
<p>虽然有了上面的补全策略，我们的传统SVD在推荐算法上还是较难使用。因为我们的用户数和物品一般都是超级大，随便就成千上万了。这么大一个矩阵做SVD分解是非常耗时的。那么有没有简化版的矩阵分解可以用呢？我们下面来看看实际可以用于推荐系统的矩阵分解。</p>
<h2 id="二、Funk-SVD算法"><a href="#二、Funk-SVD算法" class="headerlink" title="二、Funk-SVD算法"></a>二、<a href="http://sifter.org/~simon/journal/20061211.html" target="_blank" rel="noopener">Funk-SVD算法</a></h2><h3 id="2-1-基本思想"><a href="#2-1-基本思想" class="headerlink" title="2.1 基本思想"></a>2.1 基本思想</h3><p>Funk-SVD的核心思想认为用户的兴趣只受少数几个因素的影响，因此将稀疏且高维的User-Item评分矩阵分解为两个低维矩阵，即通过User、Item评分信息来学习到的用户特征矩阵P和物品特征矩阵Q，通过重构的低维矩阵预测用户对产品的评分。由于用户和物品的特征向量维度比较低，因而可以通过梯度下降(Gradient Descend)的方法高效地求解，分解示意图如下所示。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14999362159467.jpg" alt=""></p>
<h3 id="2-2-Funk-SVD"><a href="#2-2-Funk-SVD" class="headerlink" title="2.2 Funk-SVD"></a>2.2 Funk-SVD</h3><p>Simon Funk在博客上公开发表了一个只考虑已有评分记录的矩阵分解方法，称为Funk-SCD，也就是被Yehuda Koren称为隐语义模型的矩阵分解方法。</p>
<p>它的出发点为，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵M这样进行分解：</p>
<script type="math/tex; mode=display">M_{m\times n}=P^T_{m\times k}Q_{k\times n}</script><p>我们知道SVD分解已经很成熟了，但是Funk-SVD如何将矩阵M分解成为P和Q呢？这里采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损失函数，来寻找最终的P和Q。</p>
<p>对于某一个用户评分$m_{ij}$如果用Funk-SVD进行矩阵分解，则对应的表示为$q_j^Tp_i$，采用均方差作为损失函数，则我们期望$(m_{ij}-q_j^Tp_i)^2$尽可能的小，如果考虑所有的物品和样本的组合，则我们期望最小化下式：</p>
<script type="math/tex; mode=display">\sum_{i，j}(m_{ij}-q^T_jp_i)^2</script><p>只要我们能够最小化上面的式子，并求出极值所对应的$p_i,q_j$，则我们最终可以得到矩阵P和Q，那么对于任意矩阵M任意一个空白评分的位置，我们可以通过$q_j^Tp_i$计算预测评分，很漂亮的方法！</p>
<p>当然，在实际应用中，为了防止过拟合，会加入一个$L_2$的正则化项，因此正是的Funk-SVD的优化目标函数$J(p,q)$是这样的：</p>
<script type="math/tex; mode=display">arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2)</script><p>其中$K$为已有评分记录的$(i,j)$对集合，$m_{ij}$为用户$i$对物品$$j$的真实评分，$\lambda$是正则化系数，需要调参。假设输入评分矩阵为M，它是$M \times N$维矩阵，通过直接优化以上损失函数得到用户特征矩阵$P(M\times N)$和物品特征矩阵$Q(K\times N)$,其中$K&lt;&lt;M，N$。对于这个优化问题，一般通过梯度下降法来进行优化得到结果。</p>
<p>将上式分别对$p_i,q_j$求导我们得到：</p>
<script type="math/tex; mode=display">\frac{∂J}{∂p_i}=-2(m_{ij}-q_j^Tp_i)q_j+2\lambda p_i$$$$\frac{∂J}{∂q_j}=-2(m_{ij}-q_j^Tp_i)p_i+2\lambda q_j</script><p>在梯度下降法迭代时，$p_i,q_j$的迭代公式为：</p>
<script type="math/tex; mode=display">p_i=p_i=\alpha [(m_{ij}-q_j^Tp_i)q_j-\lambda p_i]</script><script type="math/tex; mode=display">q_j=q_j+\alpha [(m_{ij}-q_j^Tp_i)p_i-\lambda q_j ]</script><p>通过迭代我们最终可以得到P和Q，进而用于推荐。Funk-SVD算法虽然思想很简单，但在实际应用中效果非常好，这真是验证了大道至简。</p>
<h2 id="三、Bias-SVD"><a href="#三、Bias-SVD" class="headerlink" title="三、Bias-SVD"></a>三、Bias-SVD</h2><p>在Funk-SVD算法火爆之后，出现了很多Funk-SVD的改进版算法。其中Bias算是改进的比较成功的一种算法。</p>
<p>Funk-SVD方法通过学习用户和物品的特征向量进行预测，即用户和物品的交互信息。用户的特征向量代表了用户的兴趣，物品的特征向量代表了物品的特点，且每一个维度相互对应，两个向量的内积表示用户对该物品的喜好程度。但是我们观测到的评分数据大部分都是都是和用户或物品无关的因素产生的效果，即有很大一部分因素是和用户对物品的喜好无关而只取决于用户或物品本身特性的。例如，对于乐观的用户来说，它的评分行为普遍偏高，而对批判性用户来说，他的评分记录普遍偏低，即使他们对同一物品的评分相同，但是他们对该物品的喜好程度却并不一样。同理，对物品来说，以电影为例，受大众欢迎的电影得到的评分普遍偏高，而一些烂片的评分普遍偏低，这些因素都是独立于用户或产品的因素，而和用户对产品的的喜好无关。</p>
<p>我们把这些独立于用户或独立于物品的因素称为偏置(Bias)部分，将用户和物品的交互即用户对物品的喜好部分称为个性化部分。事实上，在矩阵分解模型中偏好部分对提高评分预测准确率起的作用要大大高于个性化部分所起的作用，以Netflix Prize推荐比赛数据集为例为例，Yehuda Koren仅使用偏置部分可以将评分误差降低32%，而加入个性化部分能降低42%，也就是说只有10%是个性化部分起到的作用，这也充分说明了偏置部分所起的重要性，剩下的58%的误差Yehuda Koren将称之为模型不可解释部分，包括数据噪音等因素。</p>
<p>偏置部分主要由三个子部分组成，分别是</p>
<ul>
<li>训练集中所有评分记录的全局平均数$\mu$，表示了训练数据的总体评分情况，对于固定的数据集，它是一个常数。</li>
<li>用户偏置$b_i$，独立于物品特征的因素，表示某一特定用户的打分习惯。例如，对于批判性用户对于自己的评分比较苛刻，倾向于打低分；而乐观型用户则打分比较保守，总体打分要偏高。</li>
<li>物品偏置$b_j$，特立于用户兴趣的因素，表示某一特定物品得到的打分情况。以电影为例，好片获得的总体评分偏高，而烂片获得的评分普遍偏低，物品偏置捕获的就是这样的特征。</li>
</ul>
<p>则偏置部分表示为</p>
<script type="math/tex; mode=display">b_{ij}=\mu+b_i+b_j</script><p>则加入了偏置项以后的优化目标函数$J(p,q)$是这样</p>
<script type="math/tex; mode=display">arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2)</script><p>这个优化目标也可以采用梯度下降法求解。和Funk-SVD不同的是，此时我们多了两个偏置项$b_i,b_j$,$p_i,p_j$的迭代公式和$Funk-SVD$类似，只是每一步的梯度导数稍有不同而已。而$b_i,b_j$一般可以初始设置为0向量，然后参与迭代。</p>
<script type="math/tex; mode=display">b_i=b_i+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_i)</script><script type="math/tex; mode=display">b_j=b_j+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_j)</script><p>通过迭代我们最终可以得到$P$和$Q$，进而用于推荐。Bias-SVD增加了一些额外因素的考虑，因此在某些场景会比FunkSVD表现好。</p>
<h2 id="四、SVD"><a href="#四、SVD" class="headerlink" title="四、SVD++"></a>四、SVD++</h2><p>SVD++算法在Bias-SVD算法上进一步做了增强，这里它增加考虑用户的隐式反馈。</p>
<p>对于某一个用户$i$，它提供了隐式反馈的物品集合定义为$N(i)$，这个用户对某个物品$j$对应的隐式反馈修正的评分值为$c_{ij}$，那么该用户所有的评分修正值为$\sum_{s\in N(i)} c_{sj}$，一般我们将它们表示为用$q_j^Ty_s$形式，则加入了隐式反馈以后的优化目标函数$J(p,q)$是这样的：</p>
<script type="math/tex; mode=display">arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i-q_j^T|N(i)|^{-1/2}\Sigma_{s\in N(i)} y_s)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2+\Sigma_{s\in N(i)}||y_s||^2)</script><p>其中，引入$|N(i)|^{-1/2}$是为了消除不同$|N(i)|$个数引起的差异。如果需要考虑用户的隐式反馈时，使用SVD++是个不错的选择。</p>
<h2 id="五、矩阵分解的优缺点"><a href="#五、矩阵分解的优缺点" class="headerlink" title="五、矩阵分解的优缺点"></a>五、矩阵分解的优缺点</h2><p>矩阵分解方法将高维User-Item评分矩阵映射为两个低维用户和物品矩阵，解决了数据稀疏性问题。</p>
<p>使用矩阵分解具有以下优点：</p>
<ul>
<li>比较容易编程实现，随机梯度下降方法依次迭代即可训练出模型。比较低的时间和空间复杂度，高维矩阵映射为两个低维矩阵节省了存储空间，训练过程比较费时，但是可以离线完成；评分预测一般在线计算，直接使用离线训练得到的参数，可以实时推荐。</li>
<li>预测的精度比较高，预测准确率要高于基于领域的协同过滤以及内容过滤等方法。</li>
<li>非常好的扩展性，很方便在用户特征向量和物品特征向量中添加其它因素，例如添加隐性反馈因素的SVD++，此方法的详细实现参见文献《Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model》；添加时间动态time SVD++，此方法将偏置部分和用户兴趣都表示成一个关于时间的函数，可以很好的捕捉到用户的兴趣漂移，欲知详细实现请阅读文献《Koren Y. Collaborative filtering with temporal dynamics》。</li>
</ul>
<p>矩阵分解的不足主要有：</p>
<ul>
<li>模型训练比较费时。</li>
<li>推荐结果不具有很好的可解释性，分解出来的用户和物品矩阵的每个维度* 无法和现实生活中的概念来解释，无法用现实概念给每个维度命名，只能理解为潜在语义空间。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（13）：推荐系统（2）—基于领域的协同过滤]]></title>
      <url>/2017/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
      <content type="html"><![CDATA[<p>基于邻域的算法是推荐系统中最基本的算法，在学术界和业界都有广泛研究与应用。它分为两大类，一类是基于用户的协同过滤算法，另一类是基于物品的协同过滤算法。</p>
<a id="more"></a>
<h2 id="一、基于用户的协同过滤算法（user-based-collaborative-filtering）"><a href="#一、基于用户的协同过滤算法（user-based-collaborative-filtering）" class="headerlink" title="一、基于用户的协同过滤算法（user-based collaborative filtering）"></a>一、基于用户的协同过滤算法（user-based collaborative filtering）</h2><p>UserCF是推荐系统的元老级算法，也是最为著名的算法，它标志了推荐系统的诞生。这里首先介绍最基础的算法，然后在此基础上提出不同的改进方法。</p>
<h3 id="1-1-基础算法"><a href="#1-1-基础算法" class="headerlink" title="1.1 基础算法"></a>1.1 基础算法</h3><p>在一个在线个性推荐系统中，当一个用户A需要个性化推荐时，可以先找到和他有相似兴趣的其他用户，然后将那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法就是<strong>基于用户的协同过滤算法</strong><br><img src="http://omu7tit09.bkt.clouddn.com/14999350479600.png" alt=""></p>
<p>如上图，我们收集到用户-电影评价矩阵，假设用户A对于物品D的评价为NULL，这时我们对比用户A、用户B、用户C的特征向量（以物品评价为特征），可以发现用户A和用户C的相似度较大，这时我们可以认为，对于用户C喜欢的物品D，用户A也应该喜欢它，这时就把物品D推荐给用户A。<br><img src="http://omu7tit09.bkt.clouddn.com/14999350511996.png" alt=""></p>
<p><strong>UserCF</strong>主要包括两个步骤：</p>
<ul>
<li><p>1）寻找和目标用户兴趣相似的用户集合<br>这里的关键就是计算两个用户的兴趣相似度，这里，协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令$N(u)$表示用户$u$曾经有过正反馈（用户的行为倾向于指用户喜欢该物品，反之，负反馈指用户的行为倾向于指用户不喜欢该物品）的物品集合，令$N(v)$为用户$v$曾经有过正反馈的物品集合。我们可以使用Jaccard或者余弦相似度来计算它们之间的兴趣相似度。</p>
<ul>
<li>Jaccard公式：<script type="math/tex; mode=display">
w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{|N\left(u\right)\cup N\left(v\right)|}</script></li>
<li>余弦相似度：<script type="math/tex; mode=display">
w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{\sqrt{|N\left(u\right)||N\left(v\right)|}}</script></li>
</ul>
</li>
<li><p>2）找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。</p>
</li>
</ul>
<h3 id="1-2-离线算法评测"><a href="#1-2-离线算法评测" class="headerlink" title="1.2 离线算法评测"></a>1.2 离线算法评测</h3><p>书中通过MovieLens数据集上的离线试验来测评算法的性能。UserCF只有一个重要参数K，即为每个用户选出K个和他兴趣最相似的用户，然后推荐那K个用户感兴趣的物品。下图为选择不同的K值时算法的性能。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-28%20%E4%B8%8A%E5%8D%8810.44.16.png" alt="屏幕快照 2017-03-28 上午10.44.16"><br>逐一分析各个指标：</p>
<ul>
<li>准确率和召回率：准确率、召回率与K不成线性关系，在此数据集中，K=80左右会获得比较高的准确率和召回率。选择合适的K对获得高的推荐系统精要比较重要，但对K值不是很敏感，保持在一定区域内即可。</li>
<li>流行度：K越大则UserCF推荐结果就越热门，流行度就越高。因为K决定了UserCFA给你做推荐时参考多少和你兴趣相似的其他用户的兴趣，如果K越大，参考的人越多，结果就越来越趋近于全局热门的物品。</li>
<li>覆盖率：覆盖率随着K的增大而减小。因为随着K的增大，UserCF越来越倾向于推荐热门的物品，从而对长尾物品的推荐越来越少，覆盖率就越来越小了</li>
</ul>
<p>此外对于Random算法（每次随机挑选10个用户没有产生过行为的物品推荐给当前用户）和MostPopular算法（按照物品的流行度给用户推荐他没有产生过行为的物品中最热门的10个物品）这两种基础算法（选取K=80），MostPopular算法准确率和召回率很高，但覆盖率非常低。与这两个极端相比，UserCF的准确率和召回率高得多，覆盖率也很高。</p>
<h3 id="1-3-用户相似度计算的改进"><a href="#1-3-用户相似度计算的改进" class="headerlink" title="1.3 用户相似度计算的改进"></a>1.3 用户相似度计算的改进</h3><p>用余弦相似度来度量用户之间兴趣相似度过于粗糙，因为两个用户对热门物品采取过相同的行为并不能说明他们兴趣相似，但对于冷门物品才去过相同的行为更能说明他们兴趣的相似度。John S.Breese提出了以下公式：</p>
<script type="math/tex; mode=display">
w_{uv}=\frac{\sum_{i\in N\left(u\right)\cap N\left(v\right)}{\frac{1}{\log\left(1+|N\left(i\right)|\right)}}}{\sqrt{|N\left(u\right)||N\left(v\right)|}}</script><p>该公式通过$1/\log\left(1+|N\left(i\right)|\right)<br>$惩罚了用户$u$和用户$v$共同兴趣列表中热门物品对他们相似度的影响。将基于上述用户相似度公式的UserCF算法记为User-IIF算法。</p>
<h3 id="1-4-实际应用"><a href="#1-4-实际应用" class="headerlink" title="1.4 实际应用"></a>1.4 实际应用</h3><p>相比基于物品的协同过滤算法ItemCF，UserCFA在目前的实际应用中使用并不多。其中最著名的使用者是<a href="http://digg.com/" target="_blank" rel="noopener">$Digg$</a>，它的推荐思路为：<br>用户在$Digg$中主要通过”顶”和”踩”两种行为表达自己对文章的看法。当用户顶了一篇文章，$Digg$就认为该用户对这篇文章有兴趣，而且愿意把这篇文章推荐给其他用户。然后$Digg$找到所有在该用户顶文章之前也顶了这一篇文章的其他用户，然后给他推荐那些人最近顶的其他文章。</p>
<h2 id="二、基于物品的协同过滤算法（item-based-collaborative-filtering）"><a href="#二、基于物品的协同过滤算法（item-based-collaborative-filtering）" class="headerlink" title="二、基于物品的协同过滤算法（item-based collaborative filtering）"></a>二、基于物品的协同过滤算法（item-based collaborative filtering）</h2><p>基于物品的协同过滤算法是目前业界应用最多的算法。亚马逊、Netflix、Hulu、Youtube的推荐算法的基础都是ItemCF。</p>
<h3 id="2-1-基础算法"><a href="#2-1-基础算法" class="headerlink" title="2.1 基础算法"></a>2.1 基础算法</h3><p>UserCF存在一些缺点：</p>
<ul>
<li>1）随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系</li>
<li>2）很难对推荐结果作出解释</li>
</ul>
<p>所以，亚马逊提出了基于物品的协同过滤算法。它给用户推荐那些和他们之前喜欢的物品相似的物品，主要通过分析用户的行为记录计算物品之间的相似度，它认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B，它也可以利用用户的历史行为给推荐结果提供解释。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/14999350842179.png" alt=""><br>同样，我们对比物品A、物品B、物品C的特征向量（以用户对该物品的喜好程度为特征），发现物品A和物品C很像，就把用品C推荐给喜欢物品A的用户C。</p>
<p>ItemCF主要包括两个步骤：</p>
<ul>
<li>1）计算物品之间的相似度</li>
</ul>
<p>可以使用下面的公式定义物品的相似度：</p>
<script type="math/tex; mode=display">
w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{|N\left(i\right)|}</script><p>分母$|N(i)|$是喜欢物品$i$的用户数，而分子$|N\left(i\right)\cap N\left(j\right)|$是同时喜欢物品$i$和物品$j$的用户数。可以理解为喜欢物品$i$的用户中有多少比例的用户也喜欢物品$j$。但是如果$j$很热门，很多人喜欢，那么$w_{ij}$就会很大，接近1，也就是会造成任何物品都会和热门的物品有很大的相似度。为了避免推荐出热门的物品，可以使用下面的公式：</p>
<script type="math/tex; mode=display">
w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{\sqrt{|N\left(i\right)||N\left(j\right)|}}</script><p>它惩罚了物品$j$的权重，因此减轻了热门物品会和很多物品相似的可能性。从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是每个用户都可以通过它们的历史兴趣列表给物品“贡献”相似度。</p>
<ul>
<li>2）根据物品的相似度和用户的历史行为给用户生成推荐列表</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（13）：推荐系统（1）—简介]]></title>
      <url>/2017/03/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B/</url>
      <content type="html"><![CDATA[<h2 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h2><h3 id="1-1-从搜索引擎说起"><a href="#1-1-从搜索引擎说起" class="headerlink" title="1.1 从搜索引擎说起"></a>1.1 从搜索引擎说起</h3><p>人们在寻找信息时，常常需要借助搜索引擎，主动地提供准确的关键词来进行相应的搜索，但是当用户无法找到准确描述自己需求的关键字时，搜索引擎就无能为力了。</p>
<a id="more"></a>
<p>和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。但是和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣与需求的信息。</p>
<h3 id="1-2-推荐系统的具体应用"><a href="#1-2-推荐系统的具体应用" class="headerlink" title="1.2 推荐系统的具体应用"></a>1.2 推荐系统的具体应用</h3><p>近十年来，推荐引擎对互联网用户来说随处可见。以亚马逊为例，简单说说实际的应用。<br>Amazon有个性化商品推荐列表和相关商品的推荐列表，以下为一种基于物品的的推荐算法（item-based method），它会给我推荐那些和之前我喜欢的物品相似的物品，因为我之前查找过关于算法数据结构相关的书籍，所以就给我推荐了以下这些计算机领域的经典图书<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-27%20%E4%B8%8B%E5%8D%883.03.04.png" alt="屏幕快照 2017-03-27 下午3.03.04"><br>除了个性化推荐列表，亚马逊另一个重要的推荐应用就是相关推荐列表。一种是包含购买此商品的顾客也同时购买，另一种是包含浏览过这个商品的顾客购买的其他商品.<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-27%20%E4%B8%8B%E5%8D%883.08.39.png" alt="屏幕快照 2017-03-27 下午3.08.39"><br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-27%20%E4%B8%8B%E5%8D%883.08.51.png" alt="屏幕快照 2017-03-27 下午3.08.51"><br>此外，还有一个应用就是打包销售，同时购买这些商品往往会给你一定的折扣。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-27%20%E4%B8%8B%E5%8D%883.10.28.png" alt="屏幕快照 2017-03-27 下午3.10.28"><br>除了亚马逊等电子商务领域的推荐系统应用，此外还有像Netflix会向用户推荐电影，豆瓣电台给用户推荐喜欢的歌曲，Facebook给用户推荐个性化物品和好友，Flipboard给特定的用户推荐特定领域的阅读资讯，雅虎的个性化广告投放等等。</p>
<h2 id="二、推荐系统评测"><a href="#二、推荐系统评测" class="headerlink" title="二、推荐系统评测"></a>二、推荐系统评测</h2><p>一个好的推荐系统不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。同时还要能帮助商家将那些被埋没在长尾的好商品介绍给可能对他们感兴趣的用户。</p>
<h3 id="2-1-推荐系统实验方法"><a href="#2-1-推荐系统实验方法" class="headerlink" title="2.1 推荐系统实验方法"></a>2.1 推荐系统实验方法</h3><p>首先，人们需要通过实验的办法来计算和获取一些指标，从而全面地测评推荐系统的好坏。主要的实验方法有离线实验、用户调查、在线AB测试。</p>
<h4 id="2-1-1-离线实验"><a href="#2-1-1-离线实验" class="headerlink" title="2.1.1 离线实验"></a>2.1.1 离线实验</h4><p>一般离线试验的步骤如下：</p>
<ul>
<li>1）通过日志系统获取用户行为数据，并按照一定格式生成一个标准的数据集；</li>
<li>2）将数据集按照一定的规则分成训练集和测试集；</li>
<li>3）在训练集上训练用户兴趣模型，在测试集上进行预测；</li>
<li>4）通过事先定义的离线指标评测算法在测试集上的预测结果。<br>离线试验不需要有对实际系统的控制权、不需要用户参与实验、速度快、可以测试大量算法。但是它无法计算商业上关心的指标（点击率、转化率）、离线试验的指标和商业指标存在差距。</li>
</ul>
<h4 id="2-1-2-用户调查"><a href="#2-1-2-用户调查" class="headerlink" title="2.1.2 用户调查"></a>2.1.2 用户调查</h4><p>因为离线实验的指标与实际的商业指标存在差距，所以需要将算法直接上线测试，但在这之前必须进行用户调查，否则直接进行在线实验会有较高的风险，因为对算法会不会降低用户满意度谁都没有把握。<br>用户调查可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。但是招募测试用户代价较大，很难组织大规模的测试一款能过户，因此会使得测试结果的统计意义不足。</p>
<h4 id="2-1-3-在线实验"><a href="#2-1-3-在线实验" class="headerlink" title="2.1.3 在线实验"></a>2.1.3 在线实验</h4><p>AB测试时最常用的在线测评算法的实验方法。它通过一定的规则将用户随机分成机组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。<br>AB测试可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。但是周期较长，必须进行长期的实验才能得到可靠的结果，因此常常只会用它测试那些在离线实验和用户调查中表现很好的算法。<br>以上就是一个新的推荐算法最终上线所需要做的事。总结一下：</p>
<ul>
<li>1）首先，需要通过离线试验证明它在很多离线指标上优于现有算法；</li>
<li>2）然后，需要通过用户调查确定它的用户满意度不低于现有算法。</li>
<li>3）最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。</li>
</ul>
<h3 id="2-2-离线评测指标"><a href="#2-2-离线评测指标" class="headerlink" title="2.2 离线评测指标"></a>2.2 离线评测指标</h3><p>令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。</p>
<ul>
<li>准确率（Precison）:<script type="math/tex; mode=display">
Precision=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|R\left(u\right)|}}</script></li>
<li>召回率（Recall）：<script type="math/tex; mode=display">
\textrm{Re}call=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|T\left(u\right)|}}</script></li>
<li><p>覆盖率(Coverage)，描述了一个推荐系统对物品长尾的发掘能力，</p>
<ul>
<li><p>最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例</p>
</li>
<li><p>信息熵 :</p>
<script type="math/tex; mode=display">
H=-\sum_{i=1}^n{p\left(i\right)\log p\left(i\right)}</script></li>
<li>基尼系数：<script type="math/tex; mode=display">
Gini=\sum_{i=1}^n{p_i\left(1-p_i\right)}</script></li>
<li>马太效应：即强者更强，弱者更弱。若一个系统会增大热门物品和非热门物品的流行度差异，让热门的物品更加热门，不认的物品更加不热门，那么这个系统就有马太效应。推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给它们感兴趣的某一类人群。但是现在主流的推荐算法都具有马太效应。可以使用基尼系数来评测推荐系统是否具有马太效应。如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2&gt;G1，就说明推荐系统具有马太效应。</li>
</ul>
</li>
<li><p>多样性：如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即多样性。</p>
<ul>
<li>多样性描述了推荐列表物品两两之间的不相似性。假设$s(i,j)\in[0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下：</li>
<li><script type="math/tex; mode=display">
Diversity\left(R\left(u\right)\right)=1-\frac{\sum_{i,j\in R\left(u\right),i\ne j}{s\left(i,j\right)}}{\frac{1}{2}|R\left(u\right)|\left(|R\left(u\right)-1|\right)}</script></li>
<li>而推荐系统整体的多样性可以定义为所有用户推荐列表多样性的平均值：<script type="math/tex; mode=display">
Diversity=\frac{1}{|U|}\sum_{u\in U}{Diversity\left(R\left(u\right)\right)}</script></li>
</ul>
</li>
<li><p>新颖性：给用户推荐那些它们以前没有听说过的物品。最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。但是这个指标比较粗略，因为不同用户不知道的东西是不同的。需要用户调查来准确统计新颖性。</p>
</li>
<li>惊喜度：惊喜度和新颖性两者之间是有区别的，如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（6）—SVM与LR的异同]]></title>
      <url>/2017/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94SVM%E4%B8%8ELR%E7%9A%84%E5%BC%82%E5%90%8C/</url>
      <content type="html"><![CDATA[<p>本文讲述LR与SVM的异同点</p>
<a id="more"></a>
<h2 id="一、LR与SVM的相同点"><a href="#一、LR与SVM的相同点" class="headerlink" title="一、LR与SVM的相同点"></a>一、LR与SVM的相同点</h2><ol>
<li><p>LR和SVM都是分类算法，都是监督学习算法。</p>
</li>
<li><p>如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？</p>
</li>
<li><p>LR和SVM都是判别模型。判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。</p>
</li>
<li><p>LR和SVM在学术界和工业界都广为人知并且应用广泛。</p>
</li>
</ol>
<h2 id="二、LR与SVM的不同点"><a href="#二、LR与SVM的不同点" class="headerlink" title="二、LR与SVM的不同点"></a>二、LR与SVM的不同点</h2><p><strong>1.损失函数</strong></p>
<p>我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：<img src="http://omu7tit09.bkt.clouddn.com/14995092441592.png" alt="">其中 $g(z)=(1+exp(−z))^{−1}$<br>可以将两者统一起来:<img src="http://omu7tit09.bkt.clouddn.com/14995093179927.png" alt=""><img src="http://omu7tit09.bkt.clouddn.com/14995093274307.png" alt=""><br>这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。即支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。</p>
<p>影响SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。用下图进行说明：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15015516972426.jpg" alt=""><br>支持向量机改变非支持向量样本并不会引起决策面的变化<br><img src="http://omu7tit09.bkt.clouddn.com/15015517109584.jpg" alt=""><br>逻辑回归中改变任何样本都会引起决策面的变化</p>
<p>因此线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做平衡处理。​</p>
<p><strong>2.核技巧</strong></p>
<p>在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。</p>
<p>​这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM转化为对偶问题后，只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​</p>
<p><strong>3.正则项</strong><br>​<br>​根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。</p>
<p><strong>4.异常值</strong></p>
<p>两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。</p>
<p><strong>5.normalization</strong></p>
<p>两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？</p>
<p>因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。</p>
<p><a href="https://www.zhihu.com/question/26768865" target="_blank" rel="noopener">Linear SVM 和 LR 有什么异同？</a></p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> LR </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（5）—对偶]]></title>
      <url>/2017/03/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%E5%AF%B9%E5%81%B6/</url>
      <content type="html"><![CDATA[<p>原文地址：<a href="http://blog.pluskid.org/?p=702" target="_blank" rel="noopener">支持向量机：Duality</a><br>在之前关于 support vector 的推导中，我们提到了 dual ，这里再来补充一点相关的知识。这套理论不仅适用于 SVM 的优化问题，而是对于所有带约束的优化问题都适用的，是优化理论中的一个重要部分。简单来说，对于任意一个带约束的优化都可以写成这样的形式：</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.33.28.png" alt="屏幕快照 2017-08-01 上午10.33.28"><br>形式统一能够简化推导过程中不必要的复杂性。其他的形式都可以归约到这样的标准形式，例如一个 $maxf(x)$ 可以转化为 $min−f(x)$等。假如 $f_0,f_1,…,f_m $全都是凸函数，并且 $h_1,…,h_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化（Convex Optimization）问题。凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题。</p>
<p>虽然约束条件能够帮助我们减小搜索空间，但是如果约束条件本身就是比较复杂的形式的话，其实是一件很让人头痛的问题，为此我们希望把带约束的优化问题转化为无约束的优化问题。为此，我们定义 Lagrangian 如下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.35.31.png" alt="屏幕快照 2017-08-01 上午10.35.31"><br>它通过一些系数把约束条件和目标函数结合在了一起。当然 Lagrangian 本身并不好玩，现在让我们来让他针对 $λ$ 和 $ν$ 最大化，令：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.38.17.png" alt="屏幕快照 2017-08-01 上午10.38.17"><br>这里 $λ⪰0$ 理解为向量 $λ$ 的每一个元素都非负即可。这个函数 $z(x)$ 对于满足原始问题约束条件的那些 $x$ 来说，其值等于 $f_0(x)$ ，这很容易验证，因为满足约束条件的$x$ 会使得 $h_i(x)=0$ ，因此最后一项消掉了，而 $f_i(x)≤0$ ，并且我们要求了 $λ⪰0$ ，因此 $λ_if_i(x)≤0$ ，所以最大值只能在它们都取零的时候得到，这个时候就只剩下 $f_0(x)$ 了。因此，对于满足约束条件的那些 $x$ 来说，$f_0(x)=z(x)$ 。这样一来，原始的带约束的优化问题其实等价于如下的无约束优化问题：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.40.24.png" alt="屏幕快照 2017-08-01 上午10.40.24"><br>因为如果原始问题有最优值，那么肯定是在满足约束条件的某个 $x^∗$ 取得，而对于所有满足约束条件的 $x ，z(x) 和 f_0(x)$ 都是相等的。至于那些不满足约束条件的 $x$ ，原始问题是无法取到的，否则极值问题无解。很容易验证对于这些不满足约束条件的$x$ 有 $z(x)=∞$，这也和原始问题是一致的，因为求最小值得到无穷大可以和“无解”看作是相容的。</p>
<p>到这里，我们成功把带约束问题转化为了无约束问题，不过这其实只是一个形式上的重写，并没有什么本质上的改变。我们只是把原来的问题通过 Lagrangian 写作了如下形式：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.43.10.png" alt="屏幕快照 2017-08-01 上午10.43.10"><br>这个问题（或者说原始的带约束的形式）称作 primal problem 。如果你看过之前关于 SVM 的推导，那么肯定就知道了，相对应的还有一个 dual problem ，其形式非常类似，只是把 min 和 max 交换了一下：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.43.34.png" alt="屏幕快照 2017-08-01 上午10.43.34"><br>交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样来理解：胖子中最瘦的那个都比瘦骨精中最胖的那个要胖。当然这是很不严格的说法，而且扣字眼的话可以纠缠不休，所以我们还是来看严格数学描述。和刚才的 $z(x)$ 类似，我们也用一个记号来表示内层的这个函数，记：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.44.12.png" alt="屏幕快照 2017-08-01 上午10.44.12"><br>并称 $g(λ,ν)$ 为 Lagrange dual function （不要和 L 的 Lagrangian 混淆了）。g 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 $p^∗$ ，那么对于所有的 $λ⪰0$ 和 $ν$ ，我们有：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.45.17.png" alt="屏幕快照 2017-08-01 上午10.45.17"><br>因为对于极值点（实际上包括所有满足约束条件的点）$x^∗$，注意到 $λ⪰0$ ，我们总是有<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.46.37.png" alt="屏幕快照 2017-08-01 上午10.46.37"><br>因此<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.47.00.png" alt="屏幕快照 2017-08-01 上午10.47.00"><br>于是<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.47.43.png" alt="屏幕快照 2017-08-01 上午10.47.43"><br>这样一来就确定了 g 的下界性质，于是<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.48.22.png" alt="屏幕快照 2017-08-01 上午10.48.22"></p>
<p>实际上就是最大的下界。这是很自然的，因为得到下界之后，我们自然地就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 $d^∗$ 的话，根据上面的推导，我们就得到了如下性质：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.48.53.png" alt="屏幕快照 2017-08-01 上午10.48.53"><br>这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^∗−d^∗$ 被称作 duality gap 。需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题——它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。</p>
<p>这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $λ⪰0$ 和 $ν$ ，带到 $g(λ,ν)$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 λ 和 ν 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $λ$ 和 $ν$ 带入 $g$ 会得到 $−∞ $，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息。</p>
<p>故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality 。所谓 strong duality ，就是<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.50.45.png" alt="屏幕快照 2017-08-01 上午10.50.45"></p>
<p>这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。</p>
<p>不过，这个问题如果要讲清楚，估计写一本书都不够，应该也有不少专门做优化方面的人在研究这相关的问题吧，我没有兴趣（当然也没有精力和能力）来做一个完整的介绍，相信大家也没有兴趣来看这样的东西——否则你肯定是专门研究优化方面的问题的了，此时你肯定比我懂得更多，也就不用看我写的介绍啦。</p>
<p>所以，这里我们就简要地介绍一下 Slater 条件和 KKT 条件。Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $f_i(x)≤0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 x 满足<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午10.52.09.png" alt="屏幕快照 2017-08-01 上午10.52.09"></p>
<p>我们有：如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立。需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立。这里我们不妨回顾一下 SVM 的 primal problem ，那是一个 convex optimization 问题（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”。当数据不可分是，strong duality 不能成立，不过，这个时候我们寻找分隔平面这个问题本身也就是没有意义的了，至于我们如何通过把数据映射到特征空间中来解决不可分的问题，这个当时已经介绍过了，这里就不多说了。</p>
<p>让我们回到 duality 的话题。来看看 strong duality 成立的时候的一些性质。</p>
<p>假设 $x^∗$ 和 $(λ^∗,ν^∗)$ 分别是 primal problem 和 dual problem 的极值点，相应的极值为 $p^∗$ 和 $d^∗$ ，首先 $p^∗=d^∗$ ，此时我们可以得到<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午11.13.37.png" alt="屏幕快照 2017-08-01 上午11.13.37"><br>由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^∗$ 是 $L(x,λ^∗,ν^∗)$ 的一个极值点，由此可以知道 $L(x,λ^∗,ν^∗)$ 在 $x^∗$ 处的梯度应该等于 0 ，亦即：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午11.14.37.png" alt="屏幕快照 2017-08-01 上午11.14.37"><br>此外，由第二个不等式，又显然$ λ^∗_if_i(x^∗)$ 都是非正的，因此我们可以得到<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午11.15.09.png" alt="屏幕快照 2017-08-01 上午11.15.09"><br>这个条件叫做 complementary slackness 。显然，如果 $λ^∗_i&gt;0$，那么必定有 $f_i(x^∗)=0$ ；反过来，如果 $f_i(x^∗)&lt;0$ 那么可以得到 $λ^∗_i=0$ 。这个条件正是我们在介绍支持向量的文章末尾时用来证明那些非支持向量（对应于 $f_i(x^∗)&lt;0$）所对应的系数 $α_i$ （在本文里对应 $λ_i$ ）是为零的.)再将其他一些显而易见的条件写到一起，就是传说中的 KKT (Karush-Kuhn-Tucker) 条件：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午11.16.44.png" alt="屏幕快照 2017-08-01 上午11.16.44"><br>任何满足 strong duality （不一定要求是通过 Slater 条件得到，也不一定要求是凸优化问题）的问题都满足 KKT 条件，换句话说，这是 strong duality 的一个必要条件。</p>
<p>不过，当原始问题是凸优化问题的时候（当然还要求原函数是可微的，否则 KKT 条件的最后一个式子就没有意义了），KKT 就可以升级为充要条件。换句话说，如果 primal problem 是一个凸优化问题，且存在 $x^˜$ 和 $(λ^˜,ν^˜)$ 满足 KKT 条件，那么它们分别是 primal problem 和 dual problem 的极值点并且 strong duality 成立。其证明也比较简单，首先 primal problem 是凸优化问题的话，$g(λ,ν)=min_xL(x,λ,ν)$ 的求解对每一组固定的 $(λ,ν)$ 来说也是一个凸优化问题，由 KKT 条件的最后一个式子，知道 $x^˜$ 是 $min_xL(x,λ^˜,ν^˜)$ 的极值点（如果不是凸优化问题，则不一定能推出来），亦即：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-08-01 上午11.21.15.png" alt="屏幕快照 2017-08-01 上午11.21.15"></p>
<p>最后一个式子是根据 KKT 条件的第二和第四个条件得到。由于 g 是 f0 的下界，这样一来，就证明了 duality gap 为零，也就是说，strong duality 成立。 到此为止，做一下总结。我们简要地介绍了 duality 的概念，基本上没有给什么具体的例子。不过由于内容比较多，为了避免文章超长，就挑了一些重点讲了一下。总的来说，一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> 对偶 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（4）—SMO]]></title>
      <url>/2017/03/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94SMO/</url>
      <content type="html"><![CDATA[<p>SMO算法是一种启发式算法，其基本思想是：如果所有变量的解都满足最优化问题的KKT条件，那么这个优化问题的解就得到了，因为KKT条件是该优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该是更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提升整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。如果SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。</p>
<a id="more"></a>
<h2 id="四、序列最小最优化算法（SMO）"><a href="#四、序列最小最优化算法（SMO）" class="headerlink" title="四、序列最小最优化算法（SMO）"></a>四、序列最小最优化算法（SMO）</h2><p>通常对于优化问题，我们没有办法的时候就会想到最笨的办法，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以$Gradient  Descent$和$Gradient Ascend$并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的$Gradient Descent$并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于二次规划问题，极值只有一个，所以是没有局部极值的问题。</p>
<p>另外还有一种叫做$Coordinate Descend$的变种，它每次只选择一个维度，例如$a=(a_1,···,a_n)$，它每次选取$a_i$为变量，而将其他都看成是常数，从而原始的问题在这一步编程一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。$Coordinate Descend$的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当$a$只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。</p>
<p>然后这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的二次规划问题，也是很好求解的。但是有了约束条件，结果让$Coordinate Descend$变得很尴尬了，直接根据第二个约束条件$<br>\sum_{i=1}^N{a_iy_i=0}<br>$,$a_1$的值立即就可以定下来，事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切停留在初始值。</p>
<p>所以SMO一次选取了两个坐标来进行优化。例如，我们假设现在选取$a_1$和$a_2$为变量，其余为常量，则根据约束条件我们有：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N{a_iy_i=0}\Longrightarrow a_2=\frac{1}{y_2}\left( -\sum_{i=3}^N{a_iy_i-a_1y_1} \right) \Longleftrightarrow y_2\left( K-a_1y_1 \right)</script><p>其中那个从3到n的作和都是常量，我们统一记作K。将这个式子代入原来的目标函数中，可以消去$a_2$，从而变成一个一元二次函数。总之现在变成了一个带区间约束的一元二次函数极值问题。唯一要注意的就是这里的约束条件，一个就是$a_1$本身需要满足$0≤a_i≤C$,然后由于$a_2$也要满足同样的约束，即：$0≤y_2(K-a_1y_1)≤C$，可以得带$a_1$的一个可行区间，同$[0,C]$交集即可得到最终的可行区间。投影到$a_1$轴上所对应的区间即是$a_1$的取值范围，在这个区间内求二次函数的最大值即可完成SMO的一步迭代。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15015197568787.png" alt=""></p>
<p>同$Coordinate Descent$一样，SMO也会选取不同的两个$coordinate$维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO也并不是一次或随机地选取两个坐标函数极值问题，而是有一些启发式的策略来选取最优的两个坐标维度。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> SMO </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（3）—非线性支持向量机]]></title>
      <url>/2017/03/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      <content type="html"><![CDATA[<p>当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。</p>
<a id="more"></a>
<h2 id="三、非线性支持向量机与核函数"><a href="#三、非线性支持向量机与核函数" class="headerlink" title="三、非线性支持向量机与核函数"></a>三、非线性支持向量机与核函数</h2><h4 id="3-1-核技巧"><a href="#3-1-核技巧" class="headerlink" title="3.1 核技巧"></a>3.1 核技巧</h4><p>前面我们介绍了线性情况下的支持向量机，他通过寻找一个现行的超平面来达到对数据线性分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。</p>
<p>例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要他是线性的，就没有办法处理，SVM也不行。因为这样的数据本身就是线性不可分的。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8811.03.17.png" alt="屏幕快照 2017-03-20 上午11.03.17"></p>
<p>此数据集为两个半径不同的圆圈加上了少量的噪音得到，所以一个理想的分界应该是一个圆圈而不是一条直线。如果用$X_1和X_2$来表示这个二维平面的两个坐标的话，则此方程可以写作</p>
<script type="math/tex; mode=display">
a_1X_1+a_2X_{1}^{2}+a_3X_2+a_4X_{2}^{2}+a_5X_1X_2+a_6=0</script><p>注意上面的形式，如果我们构造另外一个无谓的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1·X_2$</p>
<p>那么显然，上面的方程在新的坐标系下可以写作：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^5{a_iZ_i+}a_6=0</script><p>如果我们做一个映射$<br>\phi :R^2\rightarrow R^5<br>$，将$X$按照上面的规则映射为$Z$那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推倒的线性分类算法就可以进行处理了。这正是核方法处理非线性问题的基本思想。</p>
<p>总结一下，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。</p>
<p>现在回到SVM的情形，假设原始的数据是非线性的，我们通过一个映射$\phi \left( · \right) $将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量$w$，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的高斯核函数），要表示一个无穷维的向量描述起来就比较麻烦。</p>
<p>我们似乎可以这样做，拿到非线性数据，就找一个映射$\phi（·） $，然后一股脑把原来的数据映射到新空间，再做线性SVM即可。但是在之前对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；但如果原始空间是三维，我们就会得到19维的新空间，这个数目是呈爆炸性增长的，这给映射的计算带来了很大困难，而且如果遇到无穷维的情况，就根本无从计算了，所以就需要核函数出马了。</p>
<p>核技巧的想法是，再学习与预测中只定义核函数$K\left( x,z \right) $，而不显式地定义映射函数$\phi（·） $。不像之前是映射到高维空间中，然后再根据内积公式进行计算，现在我们直接在原来的低维空间中进行计算，而不需要显式的写出映射后的结果。通常，直接计算$K(x,z)$比较容易，而通过$\phi \left( x \right) \mathrm{和}\phi \left( z \right) $计算$K(x,z)$并不容易。</p>
<p>最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的$\phi（·） $，然后通过这个$\phi（·） $得到对应的$K(·，·)$进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到的，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常是“胡乱”选择一个核函数即可——我们直到她对应了某个映射，虽然我们不知道这个映射具体是什么，由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。</p>
<p>我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i·x_j)=\phi \left( x_i \right) ·\phi \left( x_j \right) $来代替，此时对偶问题的目标函数成为：</p>
<script type="math/tex; mode=display">
\underset{a}{\max}\ \ \ \ \sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK⟨ x_i,x_j ⟩}}}</script><p>同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为</p>
<script type="math/tex; mode=display">
sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK⟨ x_i,x⟩}+b \right)</script><p>在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐性地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。</p>
<h4 id="3-2-常用核函数"><a href="#3-2-常用核函数" class="headerlink" title="3.2 常用核函数"></a>3.2 常用核函数</h4><p>通常人们会从一些常用的核函数中选择，根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数。</p>
<ol>
<li>多项式核函数<code>polynomial kernel function</code><script type="math/tex; mode=display">
K\left( x,z \right) =\left( x·z+1 \right) ^p</script>对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为<script type="math/tex; mode=display">f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\left( x_i·x+1 \right) ^p+b} \right)</script></li>
</ol>
<ol>
<li>高斯核函数<code>gaussian kernel function</code><script type="math/tex; mode=display">
K\left( x,z \right) =\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right)</script>这个核就是会将原始空间映射为无穷维空间的那个家伙。不过如果$\sigma$选得很大的话，高次特征上的权重实际上衰减的非常快，所以实际上相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分，当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最为广泛的核函数之一。它对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数称为<br><script type="math/tex">f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right) +b} \right)</script><img src="http://omu7tit09.bkt.clouddn.com/15015514050397.jpg" alt=""></li>
</ol>
<ol>
<li>字符串核函数：核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上，比如，字符串核实定义在字符串集合上的核函数，字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。</li>
<li>线性核 $κ(x_1,x_2)=⟨x_1,x_2⟩$ ，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。</li>
</ol>
<p>最后，总结一下：对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。由于核函数的优良品质，这样的非线性扩展在计算量上并没有比原来复杂多少，这一点是非常难得的。当然，这要归功于核方法——除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<p>非线性支持向量机学习算法步骤如下：</p>
<ol>
<li><p>选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题</p>
<script type="math/tex; mode=display">
\underset{a}{\max}\,\,\,\,\,\,\,\,\sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}}</script><script type="math/tex; mode=display">
s.t.\ \ \ \ \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
0\le a_i\le C\ \ ,\ \ i=1,2,···,N</script><p>求得最优解</p>
<script type="math/tex; mode=display">
a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T</script></li>
<li><p>选择$a^<em>$的一个正分量$a_i^</em>$适合约束条件$0&lt;a_i&lt;C$,计算</p>
<script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^N{y_ia_{i}^{*}K\left( x_i·x_j \right)}</script></li>
<li><p>构造决策函数：</p>
<script type="math/tex; mode=display">
f(X)=sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)</script><p>当$K(x,z)$是正定核函数时，该问题为凸二次规划问题，解是存在的。</p>
</li>
</ol>
<h3 id="3-3-感性认识"><a href="#3-3-感性认识" class="headerlink" title="3.3 感性认识"></a>3.3 感性认识</h3><p>比如我们有一个一维的数据分布是如下图的样子，你想把它用一个直线来分开，你发现是不可能的，因为他们是间隔的。所以不论你画在哪，比如绿色竖线，都不可能把两个类分开。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15015532285157.jpg" alt=""><br>但是我们使用一个简单的升维的方法，把原来一维的空间投射到二维中，$x-&gt;(x, x^2)<br>$。比如:</p>
<p>0-&gt;(0,0)<br>1-&gt;(1,1)<br>2-&gt;(2,4)</p>
<p>这时候就线性可分了</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15015532573289.jpg" alt=""></p>
<p>再举个例子，在一个二维平面里面，这样的情况是不可能只用一个平面来分类的，但是只要把它投射到三维的球体上，就可能很轻易地分类。<br><img src="http://omu7tit09.bkt.clouddn.com/15015532782016.jpg" alt=""></p>
<p>理论上，由于train set是有限的，当你把data投射到无限维度的空间上是一定可以在train set上完美分类的，至于在test set上就不一定了。</p>
<p>在实用中，很多使用者都是盲目地试验各种核函数，并扫描其中的参数，选择效果最好的，来“避免过拟合”。至于什么样的核函数适用于什么样的问题，大多数人都不懂。核函数要满足的条件称为Mercer’s condition。使用SVM的很多人甚至都不知道这个条件，也不关心它；有些不满足该条件的函数也被拿来当核函数用。</p>
<p><a href="http://www.svms.org/mercer/" target="_blank" rel="noopener">Mercer’s Condition</a><br><a href="https://www.zhihu.com/question/24627666/answer/35507744" target="_blank" rel="noopener">机器学习有很多关于核函数的说法，核函数的定义和作用是什么？</a></p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> 非线性支持向量机 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（2）—线性支持向量机]]></title>
      <url>/2017/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      <content type="html"><![CDATA[<p>当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。</p>
<a id="more"></a>
<h2 id="二、线性支持向量机与软间隔最大化"><a href="#二、线性支持向量机与软间隔最大化" class="headerlink" title="二、线性支持向量机与软间隔最大化"></a>二、线性支持向量机与软间隔最大化</h2><h3 id="2-1-线性支持向量机"><a href="#2-1-线性支持向量机" class="headerlink" title="2.1 线性支持向量机"></a>2.1 线性支持向量机</h3><p>通常情况是，训练数据中有一些特异点<code>outlier</code>，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。</p>
<p>线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\xi \geqslant 0$，使函数间隔加上松弛变量大于等于1.这样，约束条件变成</p>
<script type="math/tex; mode=display">
y_i\left( w·x_i+b \right) \geqslant 1-\xi _i</script><p>同时，对每个松弛变量$\xi \geqslant 0$，支付一个代价$\xi \geqslant 0$。当然，如果我们允许$\xi \geqslant 0$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0$的总和也要最小：目标函数由原来的$\frac{1}{2}||w||^2$变成</p>
<script type="math/tex; mode=display">\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}</script><p>这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，控制目标函数中两项（“寻找 $margin$ 最大的超平面”和“保证数据点偏差量最小”）之间的权重，$C$越大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。<br>则有以下优化问题：</p>
<script type="math/tex; mode=display">
\underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}</script><script type="math/tex; mode=display">
s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N</script><script type="math/tex; mode=display">
\xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N</script><p>可证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间。</p>
<p>用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数：</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi ,a,u \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i-\sum_{i=1}^N{a_i\left( y_i\left( w·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^N{u_i\xi _i}}}</script><p>首先求拉格朗日函数针对$w,b,\xi $的极小。</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^N{a_iy_ix_i}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial \xi _i}=0\Rightarrow C-a_i-u_i=0，i=1,2,3···,N</script><p>将它们代入拉格朗日函数，得到和原来一样的目标函数。</p>
<script type="math/tex; mode=display">
\underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}</script><script type="math/tex; mode=display">
s.t.\ \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
C-a_i-u_i=0</script><script type="math/tex; mode=display">
a_i\geqslant 0</script><script type="math/tex; mode=display">
u_i\geqslant 0</script><p>不过，由于我们得到$C-a_i-u_i=0$，而又有$u_i&gt;0$（作为拉格朗日乘子的条件）,因此有$a_i≤C$,所以整个<code>dual</code>问题现在写作：</p>
<script type="math/tex; mode=display">
\underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j⟨x_i·x_j ⟩+\sum_{i=1}^N{a_i}}}</script><script type="math/tex; mode=display">
s.t.\ \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
0\le a_i\le C\ ,\ \ i=1,2,···,N</script><p>和之前的结果对比一下，可以看到唯一的区别就是现在拉格朗日乘子$a$多了一个上限$C$。而 Kernel 化的非线性形式也是一样的，只要把$⟨x_i,x_j⟩$ 换成 $κ(x_i,x_j)$ 即可。</p>
<p>构造并求解上述二次规划问题后求得最优解</p>
<script type="math/tex; mode=display">
a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T</script><p>然后计算</p>
<script type="math/tex; mode=display">
w^*=\sum_{i=1}^N{a_{i}^{*}y_ix_i}</script><p>选择$a^<em>$的一个分量$a_i^</em>$适合约束条件$0&lt;a_i&lt;C$,计算</p>
<script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i⟨x_i·x_j ⟩}</script><p>对任一适合条件都可求得一个$b^*$，但是由于原始问题对$b$的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。</p>
<h3 id="2-2-支持向量"><a href="#2-2-支持向量" class="headerlink" title="2.2 支持向量"></a>2.2 支持向量</h3><p>再现性不可分的情况下，将对偶问题的解中对应于$a_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。如图所示，这时的支持向量要比线性可分时的情况复杂一些。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-20%20%E4%B8%8A%E5%8D%8810.22.45.png" alt="屏幕快照 2017-03-20 上午10.22.45"></p>
<p>图中，分离超平面由实线表示，间隔边界由虚线表示。正例点由$。$表示，负例点由$×$表示。图中还标出了实例$x_i$到间隔边界的距离$<br>\frac{\xi _i}{||w||}<br>$。</p>
<p>软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。</p>
<p>若$a_i^*&lt;C$，则$<br>\xi _i=0<br>$，支持向量恰好落在间隔边界上；</p>
<p>若$a_i^*=C,0&lt;<br>\xi _i&lt;1<br>$，则分类正确，$x_i$在间隔边界与分离超平面之间；</p>
<p>若$a_i^*=C，\xi _i=1$则$x_i$在分隔超平面上；</p>
<p>若$a_i^*=C,\xi _i&gt;1$，则$x_i$位于分离超平面误分一侧。</p>
<h3 id="2-3-Hinge损失函数"><a href="#2-3-Hinge损失函数" class="headerlink" title="2.3 Hinge损失函数"></a>2.3 Hinge损失函数</h3><p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数：</p>
<script type="math/tex; mode=display">\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2</script><p>目标函数的第一项是经验损失或经验风险，函数</p>
<script type="math/tex; mode=display">L(y·(w·x+b))=[1-y(w·x+b)]_+</script><p>称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数：</p>
<script type="math/tex; mode=display">
\left[z\right]_+=\left\{\begin{array}{l}
    z\ ,\ z>0\\
    0\ ,\ z\le 0\\
\end{array}\right.</script><p>这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。</p>
<p>接下来证明线性支持向量机原始最优化问题：</p>
<script type="math/tex; mode=display">
\underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}</script><script type="math/tex; mode=display">
s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N</script><script type="math/tex; mode=display">
\xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N</script><p>等价于最优化问题</p>
<script type="math/tex; mode=display">\underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2</script><p>先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作</p>
<script type="math/tex; mode=display">\underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2</script><p>若取$\lambda =\frac{1}{2C}$则<script type="math/tex">\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)</script>与原始最优化问题等价。</p>
<p>合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。</p>
<p>图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-08 下午6.11.53.png" alt="屏幕快照 2017-07-08 下午6.11.53"><br>图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
            <tag> 线性支持向量机 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（12）：SVM（1）—线性可分支持向量机]]></title>
      <url>/2017/02/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      <content type="html"><![CDATA[<p>当训练数据线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机。</p>
<p>一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求分离超平面，解是唯一的。也就是它不仅将正负实例点分开，而且对最难分的实例点（离分离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<a id="more"></a>
<h2 id="一、线性可分支持向量机与硬间隔最大化"><a href="#一、线性可分支持向量机与硬间隔最大化" class="headerlink" title="一、线性可分支持向量机与硬间隔最大化"></a>一、线性可分支持向量机与硬间隔最大化</h2><h4 id="1-1-线性可分支持向量机"><a href="#1-1-线性可分支持向量机" class="headerlink" title="1.1 线性可分支持向量机"></a>1.1 线性可分支持向量机</h4><p>假设给定一个特征空间上的训练数据集</p>
<script type="math/tex; mode=display">
T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}</script><p>其中$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N<br>$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。</p>
<p>给定线性可分训练数据集，通过间隔最大化得到的分离超平面为</p>
<script type="math/tex; mode=display">
w^T·x+b=0</script><p>以及相应的分类决策函数</p>
<script type="math/tex; mode=display">
f\left( x \right) =sign\left( w^T·x+b \right)</script><p>该决策函数称为线性可分支持向量机</p>
<h4 id="1-2-函数间隔与几何间隔"><a href="#1-2-函数间隔与几何间隔" class="headerlink" title="1.2 函数间隔与几何间隔"></a>1.2 函数间隔与几何间隔</h4><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面确定的情况下$|w^T·x+b|$能够相对地表示点$x$距离超平面的远近。而$w^T·x+b$的符号与类标记的符号是否一致能够表示分类是否正确，所以可用$y(w^T·x+b)$来表示分类的正确性与确信度，这就是函数间隔<code>functional margin</code>的概念</p>
<p>但是，函数间隔有一个不足之处，就是在选择分离超平面时，只要成比例地改变$w$和$b$，超平面并没有变化，而函数间隔却以同样比例变化了。因此，我们可以对分离超平面的法向量$w$加上某些约束，使得间隔确定，此时函数间隔成为几何间隔<code>geometric margin</code>。</p>
<p>对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为</p>
<script type="math/tex; mode=display">
\gamma _i=y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right)</script><p>定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即</p>
<script type="math/tex; mode=display">
\gamma =\underset{i=1,···,N}{\min}\gamma _i</script><p>超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。</p>
<p>函数间隔与几何间隔的关系为</p>
<script type="math/tex; mode=display">
\gamma =\frac{\hat{\gamma}}{||w||}</script><p>若$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。</p>
<h4 id="1-3-间隔最大化"><a href="#1-3-间隔最大化" class="headerlink" title="1.3 间隔最大化"></a>1.3 间隔最大化</h4><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大的分离超平面是唯一的。</p>
<p>间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据记性分类。即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<p>最大间隔分离超平面</p>
<p>接下来求一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，可以表示为下面的约束最优化问题：</p>
<script type="math/tex; mode=display">
\underset{w,b}{\max}\,\,\gamma</script><script type="math/tex; mode=display">
s.t\,\,\,\,y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right) \geqslant \gamma \,\,,\,\,i=1,2,···,N</script><p>即最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma $，约束条件表示的是超平面$(w,b)$关于每个训练样本点的几何间隔至少是$\gamma $</p>
<p>根据几何间隔和函数间隔的关系，可以将此问题改写为</p>
<script type="math/tex; mode=display">
\underset{w,b}{\max}\ \frac{\hat{\gamma}}{||w||}</script><script type="math/tex; mode=display">
s.t\ \ y_i\left( w·x_i+b \right) \geqslant \hat{\gamma}\ ,\ i=1,2,···,N</script><p>函数间隔$\hat{\gamma}$的取值不影响最优化的解。函数间隔因为$w$，$b$按比例改变为$\lambda w\mathrm{，}\lambda b$而成为$\lambda \hat{\gamma}$，但是对最优化问题中的不等式约束没有影响，对目标函数的优化也没有影响，即两者等价。这样，我们可以取$ \hat{\gamma}=1$，代入后注意到最大化$<br>\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，因为我们关心的并不是最优情况下目标函数的具体数值。于是就得到下面的线性可分支持向量机学习的最优化问题。</p>
<ol>
<li>构造并求解约束最优化问题：<script type="math/tex; mode=display">
\underset{w,b}{\min}\frac{1}{2}||w||^2</script><script type="math/tex; mode=display">
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ y_i\left( w·x_i+b \right) -1\geqslant 0,\ i=1,2,···,N</script>求得最优解$w^<em>$,$b^</em>$</li>
<li>由此得到分割超平面：<script type="math/tex; mode=display">
w^*·x+b^*=0</script>分类决策函数<script type="math/tex; mode=display">
f\left( x \right) =sign\left( w^*·x+b \right)</script></li>
</ol>
<p>这其实是一个凸二次规划<code>convex quadratic programming</code> 问题，凸优化问题是指约束最优化问题</p>
<script type="math/tex; mode=display">
\underset{w}{\min}\ \ \ f\left( w \right)</script><script type="math/tex; mode=display">
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ \ g_i\left( w \right) \le 0\ ,\ i=1,2,···,k</script><script type="math/tex; mode=display">
\ \ \ \ \ \ \ \ \ \ \ \ \ h_i\left( w \right) =0\ ,\ i=1,2,···,l\</script><p>其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，上述凸优化问题成为凸二次规划问题。</p>
<p>支持向量和间隔边界</p>
<p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例成为支持向量<code>support vector</code>。支持向量是使约束条件式等号成立的点，即</p>
<script type="math/tex; mode=display">
y_i\left( w·x_i+b \right) -1=0</script><p>对$y_i=+1$的实例点，支持向量在超平面</p>
<script type="math/tex; mode=display">
H_1\mathrm{：}w·x_i+b=1</script><p>对$y_i=-1$的负例点，支持向量在超平面</p>
<script type="math/tex; mode=display">
H_2\mathrm{：}w·x_i+b=-1</script><p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-19%20%E4%B8%8B%E5%8D%882.15.14.png" alt="屏幕快照 2017-03-19 下午2.15.14"></p>
<p>可以看到两个支撑着中间的长带的超平面，它们到中间的分离超平面的距离相等，为什么一定是相等的呢？，即我们所能得到的最大的几何间隔 $\tilde{\gamma}$ 。而“支撑”这两个超平面的必定会有一些点，试想，如果某超平面没有碰到任意一个点的话，那么我就可以进一步地扩充中间的 gap ，于是这个就不是最大的 margin 了。由于在 $n$ 维向量空间里一个点实际上是和以原点为起点，该点为终点的一个向量是等价的，所以这些“支撑”的点便叫做支持向量。</p>
<p>注意到$H_1$和$H_2$平行，并且没有实例点落在他们中间。在$H_1$和$H_2$之间形成一条长带，分离超平面与他们平行且位于他们中间。长带的宽度，即$H_1$与$H_2$之间的距离成为间隔<code>margin</code>，间隔依赖于分离超平面的法向量$w$，等于$<br>\frac{2}{||w||}<br>$。$H_1$和$H_2$称为间隔边界。</p>
<p>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在将俄边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量机的个数一般都很少，所以支持向量机由很少的“重要的”训练样本确定。</p>
<p>很显然，由于这些 supporting vector 刚好在边界上，所以它们是满足 $y(w^Tx+b)=1$ ，而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有$y(w^Tx+b)&gt;1$ 。事实上，当最优的超平面确定下来之后，这些后方的点就完全成了路人甲了，它们可以在自己的边界后方随便飘来飘去都不会对超平面产生任何影响。这样的特性在实际中有一个最直接的好处就在于存储和计算上的优越性，例如，如果使用 100 万个点求出一个最优的超平面，其中是支持向量的有 100 个，那么我只需要记住这 100 个点的信息即可，对于后续分类也只需要利用这 100 个点而不是全部 100 万个点来做计算。</p>
<h4 id="1-4-学习的对偶算法"><a href="#1-4-学习的对偶算法" class="headerlink" title="1.4 学习的对偶算法"></a>1.4 学习的对偶算法</h4><p>为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法<code>dual algorithm</code>。</p>
<p>这样做的优点是，一是对偶问题往往更容易求解；二是引入核函数，进而推广到非线性分类问题。</p>
<p>首先构建拉格朗日函数，为此，对每一个不等式约束引进拉格朗日乘子$a_i≥0，i=1,2,···,N$定义拉格朗日函数：</p>
<script type="math/tex; mode=display">
L\left( w,b,a \right) =\frac{1}{2}||w||^2-\sum_{i=1}^N{a_i(y_i\left( w·x_i+b )-1\right)}</script><p>其中，$a=\left( a_1,a_2,···,a_N \right) ^T$<br>然后我们令</p>
<script type="math/tex; mode=display">θ(w)=\underset{a_i≥0}{max}L(w,b,α)</script><p>容易验证，当某个约束条件不满足时，例如 $y_i(w^Tx_i+b)&lt;1$，那么我们显然有 $θ(w)=∞$ （只要令$α_i=∞$即可）。而当所有约束条件都满足时，则有 $θ(w)=\frac{1}{2}||w||_2$ ，亦即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}||w||_2$ 实际上等价于直接最小化 $θ(w)$（当然，这里也有约束条件，就是 $α_i≥0,i=1,…,n$），因为如果约束条件没有得到满足，$θ(w)$ 会等于无穷大，自然不会是我们所要求的最小值。 具体写出来，我们现在的目标函数变成了：</p>
<script type="math/tex; mode=display">
\underset{w,b}{min}\theta(w)=\underset{w,b}{\min}\underset{a_i≥0}{\max}L\left( w,b,a \right) =p^*</script><p>这里用 $p^∗$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。不过，现在我们来把最小和最大的位置交换一下：</p>
<script type="math/tex; mode=display">\underset{a_i≥0}{\max}\underset{w,b}{\min}L\left( w,b,a \right) =d^∗</script><p>当然，交换以后的问题不再等价于原问题，这个新问题的最优值用 $d^∗$ 来表示。并，我们有 $d^∗≤p^∗$，这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！总之，第二个问题的最优值$d^∗$ 在这里提供了一个第一个问题的最优值$p^∗$ 的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。具体来说，就是要满足 KKT 条件，这里暂且先略过不说，直接给结论：我们这里的问题是满足 KKT 条件的，因此现在我们便转化为求解第二个问题。</p>
<p>为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$a$的极大。</p>
<ol>
<li><p>求$<br>\underset{w,b}{\min}L\left( w,b,a \right)<br>$<br>将拉格朗日函数$L(w,b,a)$分别对$w,b$求偏导数并令其为$0$。</p>
<script type="math/tex; mode=display">
\nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i}=0</script><script type="math/tex; mode=display">
\nabla _bL\left( w,b,a \right) =\sum_{i=1}^N{a_iy_i}=0</script><p>得到</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^N{a_iy_ix_i}</script><script type="math/tex; mode=display">
\sum_{i=1}^N{a_iy_i=0}</script><p>将其代入拉格朗日函数，得到</p>
<script type="math/tex; mode=display">
L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_iy_i\left( \left( \sum_{j=1}^N{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^N{a_i}}}}</script><script type="math/tex; mode=display">
=-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}</script></li>
<li><p>求$<br>\underset{w,b}{\min}L\left( w,b,a \right)<br>$对$a$的极大，即是对偶问题</p>
<script type="math/tex; mode=display">
\underset{a}{\max} \sum_{i=1}^N{a_i}-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}</script><script type="math/tex; mode=display">
s.t.\ \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
a_i\geqslant 0,\ \ i=1,2,···,N</script></li>
</ol>
<p>将目标函数由求极大转换为极小，就得到下面与之等价的对偶最优化问题。</p>
<script type="math/tex; mode=display">
\underset{a}{\min}\ \ \ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}-\sum_{i=1}^N{a_i}</script><script type="math/tex; mode=display">
s.t.\ \sum_{i=1}^N{a_iy_i=0}</script><script type="math/tex; mode=display">
a_i\geqslant 0,\ \ i=1,2,···,N</script><p>让我们先来看看推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 x 进行分类，实际上是通过把 x 带入到 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到$f(w)=∑^n_{i=1}α_iy_ix_i$ ，因此</p>
<script type="math/tex; mode=display">f(x)=(∑^n_{i=1}α_iy_ix_i)^Tx+b=∑^n_{i=1}α_iy_i ⟨x_i,x⟩+b</script><p>这里的形式的有趣之处在于，对于新点 x 的预测，只需要计算它与训练数据点的内积即可（这里 ⟨⋅,⋅⟩ 表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数 α 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。</p>
<p>为什么非支持向量对应的 α 等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。</p>
<p>这个结论也可由刚才的推导中得出，回忆一下我们刚才通过 Lagrange multiplier 得到的目标函数：</p>
<script type="math/tex; mode=display">\underset{a_i≥0}{max}L(w,b,a)=\underset{a_i≥0}{max}\frac{1}{2}||w||^2-\sum_{i=1}^na_i(y_i(w^Tx_i+b)-1)</script><p>注意到如果 $x_i$ 是支持向量的话，上式中$(y_i(w^Tx_i+b)-1)$部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，函数间隔会大于 1 ，因此这个部分是大于零的，而 $a_i$ 又是非负的，为了满足最大化，$α_i$ 必须等于 0 。</p>
<p><code>线性可分支持向量机学习算法</code></p>
<ul>
<li>输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $,其中,$<br>x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$</li>
<li>输出：最大间隔分离超平面和分类决策函数</li>
</ul>
<p>步骤如下</p>
<ol>
<li><p>构造并求解约束最优化问题</p>
<script type="math/tex; mode=display">
\underset{a}{\min}\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_i}}}</script><script type="math/tex; mode=display">
s.t.\ \ \ \ \sum_{i=1}^N{a_iy_j=0}</script><script type="math/tex; mode=display">
a_i\geqslant 0,\ i=1,2,···,N</script><p>求得最优解$a^<em>=(a_1^</em>,a_2^<em>,···,a_N^</em>)$</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
w^*=\sum_i^{}{a_{i}^{*}y_ix_i}</script><p>并选择$a^<em>$的一个正分量$a_j^</em>&gt;0$，计算</p>
<script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)}</script></li>
<li><p>求得分离超平面</p>
<script type="math/tex; mode=display">w^*·x+b^*=0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">f(x)=sign(w^*·x+b^*)</script></li>
</ol>
<p>在线性可分支持向量机中，$w^<em>和b^</em>$只依赖于训练数据中对应于$a_i^<em>&gt;0$的样本点$x_i,y_i$,而其他样本点对$w^</em>和b^<em>$没有影响。我们将训练数据中对应于$a_i^</em>&gt;0$的实例点$<br>x_i\in R^n<br>$称为支持向量。</p>
<p>对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。</p>
<h4 id="1-5-KKT条件"><a href="#1-5-KKT条件" class="headerlink" title="1.5 KKT条件"></a>1.5 KKT条件</h4><p>对于包含等式和不等式约束的一般优化问题<br><img src="http://omu7tit09.bkt.clouddn.com/15015158199160.jpg" alt=""><br>KKT条件（$x^*$是最优解的必要条件）为</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/15015158544941.jpg" alt=""><br>上式便称为不等式约束优化问题的KKT（Karush-Kuhn-Tucker）条件.$\mu _{j}$称为KKT乘子，当约束起作用时$\mu_1&gt;0,g_1(x)=0$，当约束不起作用时$\mu_1=0,g_1(x)&lt;0$</p>
<p>更细致的推导可以看这篇文章：<a href="https://zhuanlan.zhihu.com/p/26514613" target="_blank" rel="noopener">浅谈最优化问题的KKT条件</a></p>
<p>证明可以将线性可分支持向量机的原始问题和对偶问题等同起来的充分必要条件KKT条件，即得</p>
<script type="math/tex; mode=display">
\nabla _wL\left( w^*,b^*,a^* \right) =w^*-\sum_{i=1}^N{a_iy_ix_i=0}</script><script type="math/tex; mode=display">
\nabla _bL\left( w^*,b^*,a^* \right) =-\sum_{i=1}^N{a_{i}^{*}y_i=0}</script><script type="math/tex; mode=display">
a_{i}^{*}\left( y_i\left( w^*·x_i+b^* \right) -1 \right) =0\ ,\ i=1,2,···,N</script><script type="math/tex; mode=display">
y_i\left( w^*·x_i+b^* \right) -1\geqslant 0\ ,\ 1,2,···,N</script><script type="math/tex; mode=display">
a_{i}^{*}\geqslant 0\ ,\ i=1,2,···,N</script><p>由此得 </p>
<script type="math/tex; mode=display">
w^*=\sum_i^{}{a_{i}^{*}y_ix_i}</script><p>其中至少有一个$a_j^<em>&gt;0$(反证法，假设$a^</em>=0$，由上可知$w^<em>=0$，而$w^</em>=0$不是原始最优化问题的解，产生矛盾)，对此$j$有</p>
<script type="math/tex; mode=display">
y_j\left( w^*·x_j+b^* \right) -1=0</script><script type="math/tex; mode=display">
a_{j}^{*}y_jx_j·x_i+b^*=1/y_j=y_j</script><script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)}</script><p>综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题的解$a^<em>$;再利用求得原始问题的解$w^</em>,b^*$,从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（11）：聚类（4）—密度最大值聚类]]></title>
      <url>/2017/02/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%884%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="五、密度最大值聚类"><a href="#五、密度最大值聚类" class="headerlink" title="五、密度最大值聚类"></a>五、密度最大值聚类</h2><h3 id="5-1-引言"><a href="#5-1-引言" class="headerlink" title="5.1 引言"></a>5.1 引言</h3><p>2014年6月，Alex Rodriguez和Alessandro Laio在$Science$上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。<br>在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。</p>
<a id="more"></a>
<h3 id="5-2-若干概念"><a href="#5-2-若干概念" class="headerlink" title="5.2 若干概念"></a>5.2 若干概念</h3><ul>
<li>局部密度$\rho_i$的定义为：<script type="math/tex; mode=display">\rho_i=\sum_j{\chi\left(d_{ij}-d_c\right)}</script>其中，<script type="math/tex; mode=display">
\chi\left(x\right)=\left\{\begin{array}{l}
  1\ if\ x<0\\
  0\ if\ otherwise\\
\end{array}\right.</script>其中$d_c$是一个截断距离，$\rho_i$即到对象$i$的距离小于$d_c$的对象的个数。由于该算法只对$\rho_i$的相对值敏感，所以对$d_c$的选择是比较稳健的。</li>
<li>高局部密度点距离$\delta_i$，其定义为：<script type="math/tex; mode=display">
\delta_i=\underset{j:\rho_j>\rho_i}{\min}\left(d_{ij}\right)</script>即在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离。<br>而极端地，对于密度最大的那个对象，我们设置$\delta=max(d_{ij})$；<br>只有那些密度是局部或者全局最大的点才会有远大于正常值的高局部密度点距离。</li>
</ul>
<h3 id="5-3-聚类过程"><a href="#5-3-聚类过程" class="headerlink" title="5.3 聚类过程"></a>5.3 聚类过程</h3><p>这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下：</p>
<ul>
<li><p>1、首先计算每一个点的局部密度$\rho_i$，如图中，$\rho_1=7,\rho_8=5,\rho_{10}=4$<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.16.png" alt="屏幕快照 2017-03-25 下午5.46.16"></p>
</li>
<li><p>2、然后对于每一个点$i$计算在局部密度高于对象$i$的所有对象中，到对象$i$<br>最近的距离$\delta$<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.32.png" alt="屏幕快照 2017-03-25 下午5.46.32"></p>
</li>
<li>3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.46.46.png" alt="屏幕快照 2017-03-25 下午5.46.46"></li>
<li>4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.58.36.png" alt="屏幕快照 2017-03-25 下午5.58.36"></li>
<li>5、将其他的点分配给距离其最近的有着更高的局部密度的簇。（Assign each point to the same cluster of its nearest neighbor of higher density）<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%886.00.14.png" alt="屏幕快照 2017-03-25 下午6.00.14"><br>左图是所有点在二维空间的分布，右图是以$\rho$为横坐标，以$\delta$为纵坐标绘制的决策图。容易发现，1和10两个点的$\rho_i$和$\delta_i$都比较大，作为簇的中心点。26、27、28三个点的$\delta$也比较大，但是$\rho比较小$，所以是异常点。</li>
</ul>
<h3 id="5-4-一些关键点"><a href="#5-4-一些关键点" class="headerlink" title="5.4 一些关键点"></a>5.4 一些关键点</h3><ul>
<li><p>簇中心的识别</p>
<ul>
<li>那些有着比较大的局部密度$\rho_i$和很大的高局部密度$\delta_i$的点被认为是簇的中心；<br>而高局部密度距离$\delta_i$较大但局部密度$\rho_i$较小的点是异常点；<br>确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。<br>但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。在上图中可以分明地用肉眼判断聚类中心，但是有些情况下无法用肉眼来判断。不过，对于那些在决策图中无法用肉眼判断出聚类中心的情形，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将$\rho$值和$\delta$值综合考虑的量<script type="math/tex; mode=display">
\gamma_i=\rho_i\delta_i</script>显然$\gamma$值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。<br>我们把排序后的$\gamma$在坐标平面（下标为横轴，$\gamma$值为纵轴）画出来，由图可见，非聚类中心的$gamma$值比较平滑，而从非聚类中心过渡到聚类中心时$\gamma$有一个明显的跳跃，这个跳跃用肉眼或数值检测应该可以判断出来。作者在文末还提到，对于人工随机生成的数据集，$\gamma$的分布还满足幂次定律，即$log\gamma$，且斜率依赖于数据维度。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%885.04.17.png" alt="屏幕快照 2017-03-25 下午5.04.17"></li>
</ul>
</li>
<li><p>截断距离$d_c$的选择</p>
<ul>
<li>一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。参数$d_c$的选取，从某种意义上决定这聚类算法的成败，取得太大或者太小都不行：如果取得太大，将使得每个数据点的$\rho$值都很大以致区分度不高，极端情况是取$d_c&gt;d_{max}$，则所有的数据点都归属于一个Cluster了；如果$d_c$取得太小，同一个Cluster中就可能被拆分成多个，极端情况是$d_c&lt;d_{min}$，则每个数据点都单独称为一个Cluster。作者将比例锁定在数据量的1%~2%，也是基于肉感数据集的经验值。</li>
</ul>
</li>
<li><p>选定簇中心之后</p>
<ul>
<li>在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$d_c$的点(这个区域由这样的数据点构成：它们本身属于该Cluster，但在与其距离不超过$d_c$的范围内，存在属于其他Cluster的数据点). 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$\rho_h$. 该类簇中所有局部密度大于$\rho_h$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%884.11.42.png" alt="屏幕快照 2017-03-25 下午4.11.42"><br>A图为生成数据的概率分布，B、C二图为分别从该分布中生成了4000，1000个点。D,E分别是B,C两组数据的决策图（decision tree），可以看到两组数据都只有五个点有比较大的$\rho_i$和很大的$\delta_i$，这些点作为类簇的中心，在确定了类簇的中心之后，每个点被划分到各个类簇（彩色点），或者划分到类簇光晕（黑色点），F图展示的是随着抽样点数量的增多，聚类的错误率在逐渐下降，说明该算法是鲁棒的。</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（11）：聚类（3）—DBSCAN]]></title>
      <url>/2017/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%883%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="四、DBSCAN算法"><a href="#四、DBSCAN算法" class="headerlink" title="四、DBSCAN算法"></a>四、DBSCAN算法</h2><h3 id="4-1-密度聚类方法"><a href="#4-1-密度聚类方法" class="headerlink" title="4.1 密度聚类方法"></a>4.1 密度聚类方法</h3><p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。<br>其代表算法为DBSCAN算法和密度最大值算法。</p>
<a id="more"></a>
<h3 id="4-2-DBSCAN算法原理"><a href="#4-2-DBSCAN算法原理" class="headerlink" title="4.2 DBSCAN算法原理"></a>4.2 DBSCAN算法原理</h3><p>DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。</p>
<h3 id="4-3-若干概念"><a href="#4-3-若干概念" class="headerlink" title="4.3 若干概念"></a>4.3 若干概念</h3><ul>
<li>对象的$\varepsilon -$领域：给定对象在半径$\varepsilon$内的区域</li>
<li>核心对象：对于给定的数目$m$，如果一个对象的$\varepsilon -$领域至少包含$m$个对象，则称该对象为核心对象。</li>
<li><p>直接密度可达：给定一个对象集合$D$，如果p是在q的$\varepsilon -$领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。<br>如图$\varepsilon =1,m=5$，q是一个核心对象，从对象q出发到对象p是直接密度可达的。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.20.20.png" alt="屏幕快照 2017-03-25 上午11.20.20"></p>
</li>
<li><p>密度可达：如果存在一个对象链$p_1p_2···p_n$，$p_1=q,p_n=p$，对$p_i\in D,(1≤i≤n)$,$p_{i+1}$是从$p_i$关于$\varepsilon$和$m$直接密度可达的，则对象$p$是从对象$q$和$m$密度可达的。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.30.png" alt="屏幕快照 2017-03-25 上午11.30.30"></p>
</li>
<li><p>密度相连：如果对象集合$D$中存在一个对象$O$，使得对$p$和$q$是从$O$关于$\varepsilon $和$m$密度可达的，那么对象$p$和$q$是关于$\varepsilon $和$m$密度相连的。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.38.png" alt="屏幕快照 2017-03-25 上午11.30.38"></p>
</li>
<li><p>簇：一个基于密度的簇是最大的密度相连对象的集合。</p>
</li>
<li>噪声：不包含在任何簇中的对象称为噪声。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.30.44.png" alt="屏幕快照 2017-03-25 上午11.30.44"></li>
</ul>
<h3 id="4-4-算法步骤"><a href="#4-4-算法步骤" class="headerlink" title="4.4 算法步骤"></a>4.4 算法步骤</h3><p>下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8811.39.21.png" alt="屏幕快照 2017-03-25 上午11.39.21"></p>
<ul>
<li>1、首先随机选择A点为算法实施的切入点，我们将$\varepsilon $设置为图中圆的半径，对象个数$m（minPts）$设定为4。这里我们看到，A点的$\varepsilon - $领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。</li>
<li>2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点</li>
<li>3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。</li>
</ul>
<p>从上述算法可知：</p>
<ul>
<li>每个簇至少包含一个核心对象；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h3 id="4-5-总结"><a href="#4-5-总结" class="headerlink" title="4.5 总结"></a>4.5 总结</h3><ul>
<li><p>优点</p>
<ul>
<li>无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means. </li>
<li>可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.</li>
<li>对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers.</li>
<li>只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)</li>
<li>加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.</li>
<li><p>参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood.</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8B%E5%8D%8812.19.49.png" alt="屏幕快照 2017-03-25 下午12.19.49"></p>
</li>
</ul>
</li>
<li>缺点<ul>
<li>边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.</li>
<li>维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance.</li>
<li>不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters.</li>
<li>参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult. </li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（11）：聚类（2）—Kmeans]]></title>
      <url>/2017/02/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%882%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="三、K-Means算法"><a href="#三、K-Means算法" class="headerlink" title="三、K-Means算法"></a>三、K-Means算法</h2><h3 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h3><p>K-Means算法属于基于划分的聚类算法，对N 维欧氏空间中的点进行聚类，是一种最简单的无监督学习方法。它通过迭代来实现，其基本思想是：每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。</p>
<a id="more"></a>
<h3 id="3-2-基本假设"><a href="#3-2-基本假设" class="headerlink" title="3.2 基本假设"></a>3.2 基本假设</h3><p>K-Means聚类需要对数据进行一个基本假设：对于每一个 cluster ，我们可以选出一个中心点 (center) ，使得该 cluster 中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%889.30.11.png" alt="屏幕快照 2017-03-25 上午9.30.11"><br>由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点 2.5 来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是 2 ，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。</p>
<h3 id="3-3-算法步骤"><a href="#3-3-算法步骤" class="headerlink" title="3.3 算法步骤"></a>3.3 算法步骤</h3><p>假定输入样本为$S=x_1,x_2,···,x_n$，则算法步骤为：</p>
<ul>
<li>1、选择初始的K个类别中心$\mu_1,\mu_2,···,\mu_k$。这个过程通常是针对具体地问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑一个K-Means，并取其中最好的一次结果。</li>
<li>2、对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即：<script type="math/tex; mode=display">label_i=arg\underset{1\le j\le k}{\min}||x_i-\mu_j||</script></li>
<li>3、将每个类别中心更新为隶属于该类别的所有样本的均值<script type="math/tex; mode=display">
\mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}{x_i}</script></li>
<li>4、重复前两步，直到类别中心的变化小于某阈值或者达到最大迭代次数</li>
</ul>
<h3 id="3-4-理论分析"><a href="#3-4-理论分析" class="headerlink" title="3.4 理论分析"></a>3.4 理论分析</h3><p>基于上述的假设，我们导出K-Means所要优化的目标函数：设我们一共有N个数据点需要分为K个Cluster，K-Means需要最小化的损失函数为：</p>
<script type="math/tex; mode=display">
J=\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^K{r_{ij}||x_i-\mu_j||^2}}</script><p>这个函数，其中$r_{ij}$在数据点$n$被归类到$Cluster(j) $的时候为1，否则为0.直接寻找$r_{ij}$和$\mu_j$来最小化$J$并不容易，不过我们可以通过反复迭代以下两步的方法来进行：</p>
<ul>
<li>1、先固定$\mu_j$，选择最优的$r_{ij}$，很容易看出，只要将数据点归类到离它最近的那个中心就能保证$J$最小，通俗来讲，因为每个样本点都有一个$r_{ij}$，不是0就是1，那么我们要想让$J$最小，就要保证当一个样本的$r_{ij=1}$时，与类别中心距离的平方和达到最小。这一步即</li>
<li>2、然后固定$r_{ij}$，再求最优的$\mu_j$。将$J$对$\mu_k$求导并令导数等于零，即令<script type="math/tex; mode=display">
\frac{\partial J}{\partial\mu_j}=\sum_{i=1}^{N_j}{r_{ij}\left(x_i-\mu_j\right)}=0</script>很容易得到$J$最小的时候$\mu_j$应该满足<script type="math/tex; mode=display">
\mu_j=\frac{\sum_i{r_{ij}x_i}}{\sum_i{r_{ij}}}</script></li>
</ul>
<p>$\mu_j$的值是所有$Cluster(j)$中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$智慧不断地减小或者保持不变，而不会增加，这保证了K-Means最终或到达一个极小值。虽然K-Means并不能保证总是得到全局最优解，但是对于这样的问题，像K-Means这样复杂度的算法，这样的结果已经是很不错了。</p>
<h3 id="3-5-算法演练"><a href="#3-5-算法演练" class="headerlink" title="3.5 算法演练"></a>3.5 算法演练</h3><p>下面看一个来自WIKI的实例<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-25%20%E4%B8%8A%E5%8D%8810.29.32.png" alt="屏幕快照 2017-03-25 上午10.29.32"></p>
<ul>
<li>1、随机生成三个初始的中心点（这个中心点不一定是样本点），即图中红、绿、蓝三个小圈；</li>
<li>2、计算每个样本点与这三个中心店的距离，并将它们归属到离得最近的中心点对应的Cluster。此时图中分成了三个簇，分别是红色、绿色、蓝色部分；</li>
<li>3、重新分别计算三个簇中所有样本点的类别中心，指定为新的类别中心。此时红色、绿色、蓝色类的中点都发生了迁移。</li>
<li>4、反复迭代第2步和第3步，直至收敛。</li>
</ul>
<h3 id="3-6-总结"><a href="#3-6-总结" class="headerlink" title="3.6 总结"></a>3.6 总结</h3><ul>
<li>优点：<ul>
<li>是解决聚类问题的一种经典算法，简单、快速</li>
<li>对处理大数据集，该算法保持可伸缩性和高效率 </li>
<li>当簇近似为高斯分布时，它的效果较好</li>
</ul>
</li>
<li>缺点<ul>
<li>在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用</li>
<li>必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同</li>
<li>只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇</li>
<li>对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。比如数组1，2，3，4，100的均值为22，显然距离“大多数”数据1、2、3、4比较远，如果改成数组的中位数3，在该实例中更为稳妥，这种聚类也叫作K-mediods聚类</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（11）：聚类（1）—简介]]></title>
      <url>/2017/02/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%881%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。<br>给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都较前一次好。</p>
<a id="more"></a>
<p>聚类算法主要包括以下五类：</p>
<ul>
<li>基于分层的聚类（hierarchical methods）</li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</p>
<ul>
<li>基于划分的聚类（partitioning methods）</li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<em>K-means算法</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></p>
<ul>
<li>基于密度的聚类（density-based methods）</li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有：<em>DBSCAN（Density-Based Spatial Clustering of Applic with Noise）算法（1996）</em>、<em>OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></p>
<ul>
<li>基于网格的聚类（grid-based methods）</li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<em>STING（Statistical Information Grid）</em>、<em>CLIQUE（Clustering In Quest）算法（1998）</em>、<em>WaveCluster算法</em>。其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li>基于模型的聚类（model-based methods）</li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h2 id="二、相似度、距离计算方法"><a href="#二、相似度、距离计算方法" class="headerlink" title="二、相似度、距离计算方法"></a>二、相似度、距离计算方法</h2><ul>
<li>给定$n$维空间$R^n$中的两个向量$X=(x_1,x_2,···,x_n)^T$和$y=(y_1,y_2,···,y_n)^T$，$x,y$之间的距离可以反映两者的相似程度，一般采用$L_p$距离</li>
<li><script type="math/tex; mode=display">
dist\left( X,Y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^p} \right) ^{\frac{1}{p}}</script><p>其中$p≥1$，也称为闵可夫斯基距离（Minkowski）距离。常用的$p$为$1,2,+\infty$，此时相应的距离公式分别为</p>
<ul>
<li>1.当$p=1$时，称为曼哈顿距离（Manhattan distance），改名字的由来起源于在纽约市去测量街道之间的距离就是由人不行的步数来确定的。<script type="math/tex; mode=display">
d\left(x,y\right)=\sum_{i=1}^n{|x_i-y_i|}</script></li>
<li>当$p=2$时，称为欧几里得距离（Euclidean distance）<script type="math/tex; mode=display">
d\left(x,y\right)=\left(\sum_{i=1}^n{\left(x_i-y_i\right)^2}\right)^{\frac{1}{2}}</script></li>
<li>当$p=+\infty$时，称为最大值距离（Maximum distance）<script type="math/tex; mode=display">
d\left(x,y\right)=\underset{1\le i\le n}{\max}|x_i-y_i|</script></li>
</ul>
</li>
<li><p>杰卡德相似系数（Jaccard）</p>
<script type="math/tex; mode=display">
J\left( A,B \right) =\frac{|A\cap B|}{|A\cup B|}</script></li>
<li><p>余弦相似度（Cosine Similarity）</p>
<script type="math/tex; mode=display">
\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}</script></li>
<li>pearson相似系数<script type="math/tex; mode=display">
\rho _{XY}=\frac{cov\left( X,Y \right)}{\sigma _x\sigma _y}=\frac{E\left[ \left( x-u_x \right) \left( y-u_y \right) \right]}{\sigma _x\sigma _y}</script></li>
<li>相对熵（K-L）距离<script type="math/tex; mode=display">
D\left(p||q\right)=\sum_x{p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}}=E_{p\left(x\right)}\log\frac{p\left(x\right)}{q\left(x\right)}</script></li>
</ul>
<ul>
<li><p>Hellinger距离</p>
<script type="math/tex; mode=display">
D_a\left(p||q\right)=\frac{2}{1-a^2}\left(1-\int{p\left(x\right)^{\frac{1+a}{2}}q\left(x\right)^{\frac{1-a}{2}}}dx\right)</script></li>
<li><p>余弦相似度与pearson相似系数的比较</p>
</li>
</ul>
<p>$n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为：</p>
<script type="math/tex; mode=display">
\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}=\frac{\sum_{i=1}^n{x_iy_i}}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}}·\sqrt{\sum_{i=1}^n{y_{i}^{2}}}}</script><p>这两个向量的相关系数是：</p>
<script type="math/tex; mode=display">
\rho_{XY}=\frac{cov\left(X,Y\right)}{\sigma_x\sigma_y}=\frac{E\left[\left(x-u_x\right)\left(y-u_y\right)\right]}{\sigma_x\sigma_y}</script><script type="math/tex; mode=display">
=\frac{\sum_{i=1}^n{\left(x_i-\mu_x\right)\left(y_i-\mu_y\right)}}{\sqrt{\sum_{i=1}^n{\left(x_i-\mu_x\right)^2}}\sqrt{\sum_{i=1}^n{\left(y_i-\mu_y\right)^2}}}</script><p>相关系数即将$x,y$坐标向量各自平移到原点后的夹角余弦。这即揭示了为何文档间求距离使用夹角余弦，因为这个物理量表征了文档去均值化后的随机向量间的相关系数。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 聚类 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（10）：朴素贝叶斯]]></title>
      <url>/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
      <content type="html"><![CDATA[<p>朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件假设的分类方法。</p>
<p>对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。</p>
<p>朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用的方法。</p>
<a id="more"></a>
<h2 id="一、朴素贝叶斯的学习与分类"><a href="#一、朴素贝叶斯的学习与分类" class="headerlink" title="一、朴素贝叶斯的学习与分类"></a>一、朴素贝叶斯的学习与分类</h2><h3 id="1-1贝叶斯定理"><a href="#1-1贝叶斯定理" class="headerlink" title="1.1贝叶斯定理"></a>1.1贝叶斯定理</h3><p>先看什么是条件概率</p>
<p>$P(A|B$表示事件$B已经发生的前提下，事件$A发生的概率，叫做事件$B$<br>发生下事件$A$的条件概率。其基本求解公式为</p>
<script type="math/tex; mode=display">
P\left(A|B\right)=\frac{P\left(AB\right)}{P\left(B\right)}</script><p>贝叶斯定理便是基于条件概率，通过$P(A|B)$来求$P(B|A)$：</p>
<script type="math/tex; mode=display">
P\left(B|A\right)=\frac{P\left(A|B\right)·P\left(B\right)}{P\left(A\right)}</script><p>顺便提一下，上式中的分母，可以根据全概率公式分解为：</p>
<script type="math/tex; mode=display">
P\left(A\right)=\sum_{i=1}^n{P\left(B_i\right)P\left(A|B_i\right)}</script><h3 id="1-2-特征条件独立假设"><a href="#1-2-特征条件独立假设" class="headerlink" title="1.2 特征条件独立假设"></a>1.2 特征条件独立假设</h3><p>这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。</p>
<p>给定训练数据集$(X,Y)$，其中每个样本$X$都包括$n$维特征，即$x=(x_1,x_2,···,x_n)$，类标记集合含有$K$种类别，即$y=(y_1,y_2,···,y_k)$</p>
<p>如果现在来了一个新样本$x$我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解$P(y_1|x),P(y_2|x),P(y_k|x)$中最大的那个，即求后验概率最大的输出：$<br>arg\underset{y_k}{\max}P\left(y_k|x\right)<br>$</p>
<p>那$P(y_k|x)$怎么求解？答案就是贝叶斯定理：</p>
<script type="math/tex; mode=display">
P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{P\left(x\right)}</script><p>根据全概率公式，可以进一步分解上式中的分母：</p>
<script type="math/tex; mode=display">
P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{\sum_{i=1}^n{P\left(x|y_k\right)P\left(y_k\right)}}
（公式1）</script><p>先不管分母，分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来，而条件概率$P(x|y_k)=P(x_1,x_2,···,x_n|y_k)$，它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为$k$个，那么参数个数为$<br>k\prod_{j=1}^n{S_j}<br>$</p>
<p>这显然是不可行的。针对这个问题，朴素贝叶斯算法对条件概率分布做了独立性的假设，通俗地讲就是说假设各个维度的特征$x_1,x_2,···,x_n$互相独立，由于这是一个较强的假设，朴素贝叶斯算法也因此得名。在这个假设的前提上，条件概率可以转化为：</p>
<script type="math/tex; mode=display">
P\left(x|y_i\right)=P\left(x_1,x_2,···,x_n|y_i\right)=\prod_{i=1}^n{P\left(x_i|y_i\right)}
    （公式2）</script><p>这样参数规模就降到了$<br>\sum_{i=1}^n{S_ik}<br>$</p>
<p>以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率$P(y_k)$和条件概率$P(x|y_k)$的求解问题就都解决了，那么我们是不是可以求解我们所需要的后验概率$P(y_k|x)$了</p>
<p>答案是肯定的。我们继续上面关于$P(y_k|x)$的推导，将公式2代入公式1中得到：</p>
<script type="math/tex; mode=display">
P\left(y_k|x\right)=\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}</script><p>于是朴素贝叶斯分类器可表示为：</p>
<script type="math/tex; mode=display">
f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k|x\right)=arg\underset{y_k}{\max}\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}</script><p>因为对于所有的$y_k$，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分裂期最终表示为：</p>
<script type="math/tex; mode=display">
f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}</script><h2 id="二、朴素贝叶斯法的参数估计"><a href="#二、朴素贝叶斯法的参数估计" class="headerlink" title="二、朴素贝叶斯法的参数估计"></a>二、朴素贝叶斯法的参数估计</h2><h3 id="2-1-极大似然估计"><a href="#2-1-极大似然估计" class="headerlink" title="2.1 极大似然估计"></a>2.1 极大似然估计</h3><p>根据上述，可知朴素贝叶斯要学习的东西就是$P(Y=c_k)$和$P(X^{j}=a_{jl}|Y=c_k)$，可以应用极大似然估计法估计相应的概率（简单讲，就是用样本来推断模型的参数，或者说是使得似然函数最大的参数）。</p>
<p>先验概率$P(Y=c_k)$的极大似然估计是</p>
<script type="math/tex; mode=display">
P\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}}{N},\,\,k=1,2,···,K</script><p>也就是用样本中$c_k$的出现次数除以样本容量。</p>
<p>推导如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-22%20%E4%B8%8A%E5%8D%8811.58.54.png" alt="屏幕快照 2017-03-22 上午11.58.54"></p>
<p>设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},···,a_{jl}}$，条件概率$P(X^{j}=a_{jl}|Y=c_k)$的极大似然估计是：</p>
<script type="math/tex; mode=display">
P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}}{\sum_{i=1}^N{I\left(y_i=c_k\right)}}</script><p>式中，$x_i^{j}$是第$i$个样本的第$j$个特征。</p>
<p>例题如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-22%20%E4%B8%8A%E5%8D%8811.20.01.png" alt="屏幕快照 2017-03-22 上午11.20.01"><br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-22%20%E4%B8%8A%E5%8D%8811.20.16.png" alt="屏幕快照 2017-03-22 上午11.20.16"></p>
<h3 id="2-2-贝叶斯估计"><a href="#2-2-贝叶斯估计" class="headerlink" title="2.2 贝叶斯估计"></a>2.2 贝叶斯估计</h3><p>极大似然估计有一个隐患，假设训练数据中没有出现某种参数与类别的组合怎么办？比如上例中当$Y=1$对应的$X^{(1)}$的取值只有$1$和$2$。这样可能会出现所要估计的概率值为0的情况，但是这不代表真实数据中就没有这样的组合。这时会影响到后验概率的计算结果，使分类产生偏差。解决办法是贝叶斯估计。</p>
<p>条件概率的贝叶斯估计：</p>
<script type="math/tex; mode=display">
P_{\lambda}\left(X^{\left(j\right)}=a_{jl}\parallel Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}+\lambda}{\sum_{i=1}^N{I\left(y_i=c_k\right)}+S_j\lambda}</script><p>其中$\lambda≥0$，$S_j$表示$x_j$可能取值的中数。分子和分母分别比极大似然估计多了一点东西，其意义为在随机变量各个取值的频数上赋予一个正数$\lambda≥0$。当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。</p>
<p>先验概率的贝叶斯估计：</p>
<script type="math/tex; mode=display">
P_{\lambda}\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}+\lambda}{N+K\lambda}</script><p>例题如下：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-22%20%E4%B8%8A%E5%8D%8811.37.46.png" alt="屏幕快照 2017-03-22 上午11.37.46"></p>
<h2 id="三、python代码实现"><a href="#三、python代码实现" class="headerlink" title="三、python代码实现"></a>三、python代码实现</h2><h3 id="3-1-朴素贝叶斯文档分类"><a href="#3-1-朴素贝叶斯文档分类" class="headerlink" title="3.1 朴素贝叶斯文档分类"></a>3.1 朴素贝叶斯文档分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="string">"""</span></div><div class="line">Created on 下午5:28 22 03 2017</div><div class="line">bayes algorithm: classify a words as good or bad   [text classify]</div><div class="line">@author: plushunter</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._creteria = <span class="string">"NB"</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">#创建不重复词集</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_creatVocabList</span><span class="params">(self,dataSet)</span>:</span></div><div class="line">        vocabSet = set([])  <span class="comment"># 创建一个空的SET</span></div><div class="line">        <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</div><div class="line">            vocabSet = vocabSet | set(document)  <span class="comment"># 并集</span></div><div class="line">        <span class="keyword">return</span> list(vocabSet)  <span class="comment"># 返回不重复词表（SET的特性）</span></div><div class="line"></div><div class="line">    <span class="comment">#文档词集向量模型</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_setOfWordToVec</span><span class="params">(self,vocabList, inputSet)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        功能:给定一行词向量inputSet，将其映射至词库向量vocabList，出现则标记为1，否则标记为0.</div><div class="line">        """</div><div class="line">        returnVec = [<span class="number">0</span>] * len(vocabList)</div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</div><div class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">                returnVec[vocabList.index(word)] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> returnVec</div><div class="line"></div><div class="line">    <span class="comment">#文档词袋模型</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_bagOfsetOfWordToVec</span><span class="params">(self,vocabList, inputSet)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        功能：对每行词使用第二种统计策略，统计单个词的个数，然后映射到此库中</div><div class="line">        输出：一个n维向量，n为词库的长度，每个取值为单词出现的次数</div><div class="line">        """</div><div class="line">        returnVec = [<span class="number">0</span>] * len(vocabList)</div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</div><div class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">                returnVec[vocabList.index(word)] += <span class="number">1</span> <span class="comment">#更新此处代码</span></div><div class="line">        <span class="keyword">return</span> returnVec</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_trainNB0</span><span class="params">(self,trainMatrix, trainCategory)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        输入：训练词矩阵trainMatrix与类别标签trainCategory,格式为Numpy矩阵格式</div><div class="line">        功能：计算条件概率p0Vect、p1Vect和类标签概率pAbusive</div><div class="line">        """</div><div class="line">        numTrainDocs = len(trainMatrix)<span class="comment">#样本个数</span></div><div class="line">        numWords = len(trainMatrix[<span class="number">0</span>])<span class="comment">#特征个数，此处为词库长度</span></div><div class="line">        pAbusive = sum(trainCategory) / float(numTrainDocs)<span class="comment">#计算负样本出现概率（先验概率）</span></div><div class="line">        p0Num = ones(numWords)<span class="comment">#初始词的出现次数为1，以防条件概率为0，影响结果</span></div><div class="line">        p1Num = ones(numWords)<span class="comment">#同上</span></div><div class="line">        p0Denom = <span class="number">2.0</span><span class="comment">#类标记为2，使用拉普拉斯平滑法,</span></div><div class="line">        p1Denom = <span class="number">2.0</span></div><div class="line">        <span class="comment">#按类标记进行聚合各个词向量</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</div><div class="line">            <span class="keyword">if</span> trainCategory[i] == <span class="number">0</span>:</div><div class="line">                p0Num += trainMatrix[i]</div><div class="line">                p0Denom += sum(trainMatrix[i])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                p1Num += trainMatrix[i]</div><div class="line">                p1Denom += sum(trainMatrix[i])</div><div class="line">        p1Vect = log(p1Num / p1Denom)<span class="comment">#计算给定类标记下，词库中出现某个单词的概率</span></div><div class="line">        p0Vect = log(p0Num / p0Denom)<span class="comment">#取log对数，防止条件概率乘积过小而发生下溢</span></div><div class="line">        <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_classifyNB</span><span class="params">(self,vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        该算法包含四个输入:</div><div class="line">        vec2Classify表示待分类的样本在词库中的映射集合，</div><div class="line">        p0Vec表示条件概率P(wi|c=0)P(wi|c=0)，</div><div class="line">        p1Vec表示条件概率P(wi|c=1)P(wi|c=1)，</div><div class="line">        pClass1表示类标签为1时的概率P(c=1)P(c=1)。</div><div class="line"></div><div class="line">        p1=ln[p(w1|c=1)p(w2|c=1)…p(wn|c=1)p(c=1)]</div><div class="line">        p0=ln[p(w1|c=0)p(w2|c=0)…p(wn|c=0)p(c=0)]</div><div class="line">        log取对数为防止向下溢出</div><div class="line"></div><div class="line">        功能:使用朴素贝叶斯进行分类,返回结果为0/1</div><div class="line">        """</div><div class="line">        p1 = sum(vec2Classify * p1Vec) + log(pClass1)</div><div class="line">        p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1</span> - pClass1)</div><div class="line">        <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="comment">#test</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">(self,testSample)</span>:</span></div><div class="line">        <span class="string">"step1：加载数据集与类标号"</span></div><div class="line">        listOPosts, listClasses = loadDataSet()</div><div class="line">        <span class="string">"step2：创建词库"</span></div><div class="line">        vocabList = self._creatVocabList(listOPosts)</div><div class="line">        <span class="string">"step3：计算每个样本在词库中出现的情况"</span></div><div class="line">        trainMat = []</div><div class="line">        <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</div><div class="line">            trainMat.append(self._bagOfsetOfWordToVec(vocabList, postinDoc))</div><div class="line">        p0V, p1V, pAb = self._trainNB0(trainMat, listClasses)</div><div class="line">        <span class="string">"step4：测试"</span></div><div class="line">        thisDoc = array(self._bagOfsetOfWordToVec(vocabList, testSample))</div><div class="line">        result=self._classifyNB(thisDoc, p0V, p1V, pAb)</div><div class="line">        <span class="keyword">print</span> testSample, <span class="string">'classified as:'</span>, result</div><div class="line">        <span class="comment"># return result</span></div><div class="line"></div><div class="line"><span class="comment">###</span></div><div class="line"><span class="comment"># 加载数据集</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></div><div class="line">    postingList = [[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</div><div class="line">                   [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</div><div class="line">                   [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</div><div class="line">                   [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</div><div class="line">                   [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</div><div class="line">                   [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</div><div class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></div><div class="line">    <span class="keyword">return</span> postingList, classVec</div><div class="line"></div><div class="line"><span class="comment">#测试</span></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    clf = Naive_Bayes()</div><div class="line">    testEntry = [[<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'girl'</span>, <span class="string">'friend'</span>],</div><div class="line">                 [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>],</div><div class="line">                 [<span class="string">'Haha'</span>, <span class="string">'I'</span>, <span class="string">'really'</span>, <span class="string">"Love"</span>, <span class="string">"You"</span>],</div><div class="line">                 [<span class="string">'This'</span>, <span class="string">'is'</span>, <span class="string">"my"</span>, <span class="string">"dog"</span>],</div><div class="line">                 [<span class="string">'maybe'</span>,<span class="string">'stupid'</span>,<span class="string">'worthless'</span>]]</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> testEntry:</div><div class="line">        clf.testingNB(item)</div></pre></td></tr></table></figure>
<h3 id="3-2-使用朴素贝叶斯过滤垃圾邮件"><a href="#3-2-使用朴素贝叶斯过滤垃圾邮件" class="headerlink" title="3.2 使用朴素贝叶斯过滤垃圾邮件"></a>3.2 使用朴素贝叶斯过滤垃圾邮件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="string">"""</span></div><div class="line">Created on 下午8:47 22 03 2017</div><div class="line">Email_Classify </div><div class="line">@author: plushunter </div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> Bayes</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"><span class="comment"># mysent='This book is the best book on Python or M.L I have ever laid eyes upon.'</span></div><div class="line"><span class="comment"># regEx = re.compile('\\W*')</span></div><div class="line"><span class="comment"># listOfTokens=regEx.split(mysent)</span></div><div class="line"><span class="comment"># tok=[tok.upper() for tok in listOfTokens if len(tok)&gt;0]</span></div><div class="line"><span class="comment"># print tok</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># emailText=open('email/ham/6.txt').read()</span></div><div class="line"><span class="comment"># listOfTokens=regEx.split(emailText)</span></div><div class="line"><span class="comment"># print listOfTokens</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></div><div class="line">    <span class="keyword">import</span> re</div><div class="line">    listOfTokens=re.split(<span class="string">r'\w*'</span>,bigString)</div><div class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok)&gt;<span class="number">2</span>]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span><span class="params">()</span>:</span></div><div class="line">    clf = Bayes.Naive_Bayes()</div><div class="line">    docList=[]</div><div class="line">    classList=[]</div><div class="line">    fullText=[]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>):</div><div class="line">        wordList=textParse(open(<span class="string">'email/spam/%d.txt'</span>%i).read())</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">1</span>)</div><div class="line">        wordList=textParse(open(<span class="string">'email/ham/%i.txt'</span>%i).read())</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">0</span>)</div><div class="line">    vocabList=clf._creatVocabList(docList)</div><div class="line">    trainingSet=range(<span class="number">50</span>);testSet=[]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        randIndex=int(random.uniform(<span class="number">0</span>,len(trainingSet)))</div><div class="line">        testSet.append(trainingSet[randIndex])</div><div class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</div><div class="line">    trainMatix=[];trainClasses=[]</div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</div><div class="line">        trainMatix.append(clf._bagOfsetOfWordToVec(vocabList,docList[docIndex]))</div><div class="line">        trainClasses.append(classList[docIndex])</div><div class="line">    p0V,p1V,pSpam=clf._trainNB0(array(trainMatix),array(trainClasses))</div><div class="line">    errorCount = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</div><div class="line">        wordVector = clf._bagOfsetOfWordToVec(vocabList,docList[docIndex])</div><div class="line">        <span class="keyword">if</span> clf._classifyNB(array(wordVector), p0V, p1V, pSpam)!=classList[docIndex]:</div><div class="line">            errorCount+=<span class="number">1</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'the error rate is :'</span>,float(errorCount)/len(testSet)</div></pre></td></tr></table></figure>
<h2 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h2><p><a href="http://omu7tit09.bkt.clouddn.com/%282%29%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E3%80%81%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener"><code>判别模型·生成模型·朴素贝叶斯方法</code></a><br><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Sex_classification]" target="_blank" rel="noopener"><code>维基百科：Naive Bayes classifier</code></a><br><a href="http://blog.csdn.net/pongba/article/details/2958094" target="_blank" rel="noopener"><code>数学之美番外篇：平凡而又神奇的贝叶斯方法</code></a><br><a href="http://blog.csdn.net/u012162613/article/details/48323777" target="_blank" rel="noopener"><code>朴素贝叶斯理论推导与三种常见模型</code></a><br><a href="https://book.douban.com/subject/24703171/" target="_blank" rel="noopener"><code>机器学习实战</code></a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（9）：感知机]]></title>
      <url>/2017/02/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>感知机（perceptron）是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值。</p>
<p>感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，导入基于误分类的损失函数，利用梯度下降对损失函数进行极小化，求得感知机模型，属于判别模型</p>
<p>感知机学习算法简单易于实现，分为原始形式和对偶形式。1957年由Rosenblatt提出，是神经网络和支持向量机的基础</p>
<a id="more"></a>
<p>本章框架如下：</p>
<ul>
<li>感知机模型</li>
<li>感知机的学习策略（损失函数）</li>
<li>感知机学习算法（原始形式与对偶形式），并证明算法的收敛性</li>
</ul>
<h2 id="一、-感知机模型"><a href="#一、-感知机模型" class="headerlink" title="一、 感知机模型"></a>一、 感知机模型</h2><h3 id="1-1-感知机模型"><a href="#1-1-感知机模型" class="headerlink" title="1.1 感知机模型"></a>1.1 感知机模型</h3><p>感知机是一种线性分类器，属于判别模型。</p>
<p>假设我们的输入空间（特征空间）是$\chi \subseteq R^{\boldsymbol{n}}$，输出空间是$\boldsymbol{y}=\left\{ +1,-1 \right\}$。输入$\boldsymbol{x}\in \boldsymbol{\chi }$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \boldsymbol{y}$表示实例的类别。由输入空间到输出空间的函数</p>
<script type="math/tex; mode=display">
f\left( x \right) =\mathrm{sign}\left( \boldsymbol{w}·x+\boldsymbol{b}\right)</script><p>其中，$\boldsymbol{w}\in \boldsymbol{R}^{\boldsymbol{n}}$为权值或权值向量，$\boldsymbol{b}\in \boldsymbol{R}^{\boldsymbol{n}}$叫做偏置，$\mathrm{sign}$是符号函数，即</p>
<script type="math/tex; mode=display">
\mathrm{sign}\left( \mathrm{x} \right) =\left\{ \begin{array}{l}
    +1\mathrm{，\ x}\geqslant 0\\
    -1\mathrm{，\ x}<0\\
\end{array} \right.</script><p>它的假设空间为：函数集合</p>
<script type="math/tex; mode=display">
\left\{ f|f\left( x \right) =w·x+b \right\}</script><p>感知机学习就是由训练数据集（实例的特征向量及类别）求得感知机模型，即求得模型参数$w,b$。<br> 感知机预测即为通过学习得到的感知机模型，对新的输入实例给出其对应的输出类别。</p>
<h3 id="1-2-感知机的几何解释"><a href="#1-2-感知机的几何解释" class="headerlink" title="1.2 感知机的几何解释"></a>1.2 感知机的几何解释</h3><p>线性方程$w·x+b=0$对应于特征空间$R^{\boldsymbol{n}}$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距，超平面将特征空间划分为两个部分。两部分的特征向量被分为正、负两类，超平面$S$也称为分离超平面。</p>
<p><img src="http://img.blog.csdn.net/20161128125137360" alt="感知机模型"></p>
<h2 id="二、感知机学习策略"><a href="#二、感知机学习策略" class="headerlink" title="二、感知机学习策略"></a>二、感知机学习策略</h2><h3 id="2-1-数据集的线性可分性"><a href="#2-1-数据集的线性可分性" class="headerlink" title="2.1 数据集的线性可分性"></a>2.1 数据集的线性可分性</h3><p>给定一个数据集</p>
<script type="math/tex; mode=display">
T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}</script><p>若存在某个超平面$S$</p>
<script type="math/tex; mode=display">
w·x+b=0</script><p>能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对所有的$y_i=+1$的实例，有$w·x+b&gt;0$；对所有的$y_i=-1$的实例，有$w·x+b&lt;0$。则数据集$T$为线性可分数据集；否则称数据集线性不可分。</p>
<h3 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h3><p>我们选择将误分类点到超平面$S$的总距离作为感知机模型的损失函数</p>
<p>由几何解释可以清楚地看到，任一点到超平面$S$的距离为:</p>
<script type="math/tex; mode=display">
\frac{1}{||w||}|w·x_0+b|</script><p>而我们对于误分类点的定义为：</p>
<script type="math/tex; mode=display">
-y_i\left( w·x_0+b \right) >0</script><p>误分类点到超平面的距离：</p>
<script type="math/tex; mode=display">
-\frac{1}{||w||}y_i\left( w·x+b \right)</script><p>则误分类点到超平面的总距离：</p>
<script type="math/tex; mode=display">
-\frac{1}{||w||}\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}</script><p>据上述我们定义损失函数为:</p>
<script type="math/tex; mode=display">
L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}</script><p>其中$M$为误分类点的集合，此即为感知机学习的经验风险函数。一个特定样本点的损失函数，在误分类时是参数$w,b$的线性函数，在正确分类时是0.因此，给定训练数据集$T$，损失函数$L(w,b)$是$w,b$的连续可导函数。感知机学习的策略就是在假设空间中选取使损失函数最小的模型参数，即感知机模型。</p>
<h2 id="三、感知机学习算法"><a href="#三、感知机学习算法" class="headerlink" title="三、感知机学习算法"></a>三、感知机学习算法</h2><p>这样我们就把感知机的学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。</p>
<h3 id="3-1-感知机学习算法"><a href="#3-1-感知机学习算法" class="headerlink" title="3.1 感知机学习算法"></a>3.1 感知机学习算法</h3><p>首先我们确定要求解的最优化问题是：</p>
<script type="math/tex; mode=display">\min_{w,b}L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}</script><p>通过随机梯度下降法来求解最优化问题。首先，任意选择一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降，而不是一次使$M$中所有误分类点的梯度下降。</p>
<p>计算得到梯度为：</p>
<script type="math/tex; mode=display">
\nabla _wL\left( w,b \right) =-\sum_{x_i\in M}{y_ix_i}</script><script type="math/tex; mode=display">
\nabla _bL\left( w,b \right) =-\sum_{x_i\in M}{y_i}</script><p>对权值进行更新：</p>
<script type="math/tex; mode=display">
w\gets w+\eta y_ix_i</script><script type="math/tex; mode=display">
b\gets b+\eta y_i</script><p>其中$\eta$称为学习率，通过迭代可以期待损失函数不断减小，直到为0.</p>
<p>对于上述算法过程，我们可以有一个直观的解释：当一个实例点被误分类，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以较少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。当然感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。</p>
<h3 id="3-2-对偶形式"><a href="#3-2-对偶形式" class="headerlink" title="3.2 对偶形式"></a>3.2 对偶形式</h3><p>对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$，我们假设初始值$w_0$和$b_0$均为0。对误分类点($x_i$,$y_i$)通过</p>
<script type="math/tex; mode=display">
w\gets w+\eta y_ix_i</script><script type="math/tex; mode=display">
b\gets b+\eta y_i</script><p>逐步修改$w,b$,设修改$n$次，则最后学习到的$w,b$可以分别表示为</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^N{n_i\eta y_ix_i}=\sum_{i=1}^N{a_iy_ix_i}</script><script type="math/tex; mode=display">
b=\sum_{i=1}^N{a_iy_i}</script><p>当$\eta =1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类、换句话说，这样的实例对学习结果影响最大。</p>
<p>因为对偶形式的训练实例仅以内积的形式出现。为了方便，可预先将训练实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。</p>
<script type="math/tex; mode=display">G=\left[ x_i·x_j \right] _{N\times N}</script><p>与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。</p>
<p>总结感知机学习算法的对偶形式如下：</p>
<p>输入：线性可分的数据集训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $，其中$x_i\in \chi =\boldsymbol{R}^{\boldsymbol{n}}$<br>$y_i\in \boldsymbol{y}=\left\{ -1,+1 \right\} ,i=1,2,···,N $，学习率$<br>\eta \left( 0&lt;\eta \le 1 \right) $；</p>
<p>输出：$a,b$；感知机模型</p>
<script type="math/tex; mode=display">
f\left( x \right) =sign\left( \sum_{j=1}^N{a_jy_jx_j}·x+b \right)</script><p>其中$a=\left( a_1,a_2,···,a_N \right) ^T$</p>
<ul>
<li>1）$<br>a\gets 0,b\gets 0<br>$</li>
<li>2）在训练集中选取数据$<br>\left( x_i,y_i \right)<br>$</li>
<li>3）如果$<br>y_i\left( \sum_{j=1}^N{a_jy_jx_j·x_i+b} \right) \le 0<br>$<script type="math/tex; mode=display">
a_i\gets a_i+\eta</script><script type="math/tex; mode=display">
b\gets b+\eta y_i</script></li>
<li>4）转至(2)直到没有误分类数据</li>
</ul>
<h2 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h2><p>李航《统计学习方法》</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 感知机 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（7）：观影清单]]></title>
      <url>/2017/01/30/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%887%EF%BC%89%EF%BC%9A%E8%A7%82%E5%BD%B1%E6%B8%85%E5%8D%95/</url>
      <content type="html"><![CDATA[<p>个人观影清单</p>
<a id="more"></a>
<h2 id="广播影评人协会最佳影片"><a href="#广播影评人协会最佳影片" class="headerlink" title="广播影评人协会最佳影片"></a>广播影评人协会最佳影片</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>(2017)<a href="https://movie.douban.com/subject/26752852/" target="_blank" rel="noopener">水形物语</a></td>
<td>吉尔莫·德尔·托罗</td>
<td>剧情 / 奇幻 / 冒险</td>
<td>美国</td>
<td>7.6 (29654人评价)</td>
</tr>
<tr>
<td>✔️(2016)<a href="https://movie.douban.com/subject/25934014/" target="_blank" rel="noopener">爱乐之城</a></td>
<td>达米恩·查泽雷</td>
<td>剧情 / 爱情 / 歌舞</td>
<td>美国</td>
<td>8.3 (356505人评价)</td>
</tr>
<tr>
<td>✔️(2015)<a href="https://movie.douban.com/subject/25954475/" target="_blank" rel="noopener">聚焦</a></td>
<td>汤姆·麦卡锡</td>
<td>剧情 / 传记</td>
<td>美国/加拿大</td>
<td>8.8 (123984人评价)</td>
</tr>
<tr>
<td>✔️ (2014)<a href="https://movie.douban.com/subject/2209575/" target="_blank" rel="noopener">少年时代</a></td>
<td>理查德·林克莱特</td>
<td>剧情 / 家庭</td>
<td>美国</td>
<td>8.5 (138281人评价)</td>
</tr>
<tr>
<td>(2013)<a href="https://movie.douban.com/subject/6879185/" target="_blank" rel="noopener">为奴十二年</a></td>
<td>史蒂夫·麦奎因</td>
<td>剧情 / 传记 / 历史</td>
<td>美国/英国</td>
<td>7.9 (105351人评价)</td>
</tr>
<tr>
<td>✔️(2012)<a href="https://movie.douban.com/subject/6549903/" target="_blank" rel="noopener">逃离德黑兰</a></td>
<td>本·阿弗莱克</td>
<td>剧情 / 惊悚 / 历史</td>
<td>美国</td>
<td>8.2 (172185人评价)</td>
</tr>
<tr>
<td>(2011)<a href="https://movie.douban.com/subject/6097775/" target="_blank" rel="noopener">艺术家</a></td>
<td>阿扎纳维西于斯</td>
<td>剧情 / 喜剧 / 爱情</td>
<td>法国/比利时/美国</td>
<td>8.4 (75976人评价)</td>
</tr>
<tr>
<td>✔️(2010)<a href="https://movie.douban.com/subject/3205624/" target="_blank" rel="noopener">社交网络</a></td>
<td>大卫·芬奇</td>
<td>剧情 / 传记</td>
<td>美国</td>
<td>8.1 (269858人评价)</td>
</tr>
<tr>
<td>(2009)<a href="https://movie.douban.com/subject/2028645/" target="_blank" rel="noopener">拆弹部队</a></td>
<td>凯瑟琳·毕格罗</td>
<td>剧情 / 惊悚 / 战争</td>
<td>美国</td>
<td>7.7 (106224人评价)</td>
</tr>
<tr>
<td>✔️(2008)<a href="https://movie.douban.com/subject/2209573/" target="_blank" rel="noopener">贫民窟的百万富翁</a></td>
<td>丹尼·博伊尔 / 洛芙琳·坦丹</td>
<td>剧情 / 爱情</td>
<td>英国/美国</td>
<td>8.5 (403554人评价)</td>
</tr>
<tr>
<td>✔️(2007)<a href="https://movie.douban.com/subject/1857099/" target="_blank" rel="noopener">老无所依</a></td>
<td>伊桑·科恩 / 乔尔·科恩</td>
<td>犯罪 / 剧情 / 惊悚</td>
<td>美国</td>
<td>8.0 (130631人评价)</td>
</tr>
<tr>
<td>(2006)<a href="https://movie.douban.com/subject/1315316/" target="_blank" rel="noopener">无间道风云</a></td>
<td>马丁·斯科塞斯</td>
<td>犯罪 / 剧情 / 惊悚</td>
<td>美国/香港</td>
<td>7.2 (75973人评价)</td>
</tr>
<tr>
<td>✔️(2005)<a href="https://movie.douban.com/subject/1418834/" target="_blank" rel="noopener">断背山</a></td>
<td>李安</td>
<td>剧情 / 爱情 / 同性 / 家庭</td>
<td>美国/加拿大</td>
<td>8.6 (331163人评价)</td>
</tr>
<tr>
<td>(2004)<a href="https://movie.douban.com/subject/1291833/" target="_blank" rel="noopener">杯酒人生</a></td>
<td>亚历山大·佩恩</td>
<td>剧情 / 喜剧 / 爱情</td>
<td>美国/匈牙利</td>
<td>7.9 (34580人评价)</td>
</tr>
<tr>
<td>✔️(2003)<a href="https://movie.douban.com/subject/1291552/" target="_blank" rel="noopener">指环王3：王者无敌</a></td>
<td>彼得·杰克逊</td>
<td>剧情 / 动作 / 奇幻 / 冒险</td>
<td>美国/新西兰</td>
<td>9.1 (302742人评价)</td>
</tr>
<tr>
<td>(2002)<a href="https://movie.douban.com/subject/1307697/" target="_blank" rel="noopener">芝加哥</a></td>
<td>罗伯·马歇尔</td>
<td>喜剧 / 歌舞 / 犯罪</td>
<td>美国/德国</td>
<td>8.6 (62999人评价)</td>
</tr>
<tr>
<td>✔️(2001)<a href="https://movie.douban.com/subject/1306029/" target="_blank" rel="noopener">美丽心灵</a></td>
<td>朗·霍华德</td>
<td>传记 / 剧情</td>
<td>美国</td>
<td>8.9 (321117人评价)</td>
</tr>
<tr>
<td>(2000)<a href="https://movie.douban.com/subject/1293530/" target="_blank" rel="noopener">角斗士</a></td>
<td>雷德利·斯科特</td>
<td>剧情 / 动作 / 历史 / 冒险</td>
<td>英国/美国</td>
<td>8.4 (125775人评价)</td>
</tr>
<tr>
<td>(2000)<a href="https://movie.douban.com/subject/1292062/" target="_blank" rel="noopener">美国丽人</a></td>
<td>萨姆·门德斯</td>
<td>剧情 / 爱情 / 家庭</td>
<td>美国</td>
<td>8.4 (186250人评价)</td>
</tr>
<tr>
<td>✔️(1999)<a href="https://movie.douban.com/subject/1292849/" target="_blank" rel="noopener">拯救大兵瑞恩</a></td>
<td>史蒂文·斯皮尔伯格</td>
<td>剧情 / 历史 / 战争</td>
<td>美国</td>
<td>8.9 (250317人评价)</td>
</tr>
<tr>
<td>✔️(1998)<a href="https://movie.douban.com/subject/1292348/" target="_blank" rel="noopener">洛城机密</a></td>
<td>柯蒂斯·汉森</td>
<td>犯罪 / 剧情 / 悬疑 / 惊悚</td>
<td>美国</td>
<td>8.6 (77813人评价)</td>
</tr>
<tr>
<td>(1997)<a href="https://movie.douban.com/subject/1292067/" target="_blank" rel="noopener">冰血暴</a></td>
<td>乔尔·科恩</td>
<td>犯罪 / 剧情 / 惊悚</td>
<td>美国/英国</td>
<td>7.9 (43584人评价)</td>
</tr>
<tr>
<td>✔️(1996)<a href="https://movie.douban.com/subject/1299193/" target="_blank" rel="noopener">理智与情感</a></td>
<td>李安</td>
<td>剧情 / 爱情</td>
<td>美国/英国</td>
<td>8.3 (59885人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="戛纳电影节金棕榈奖"><a href="#戛纳电影节金棕榈奖" class="headerlink" title="戛纳电影节金棕榈奖"></a>戛纳电影节金棕榈奖</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>（2017）方形</td>
<td>鲁本·奥斯特伦德</td>
<td>剧情 / 喜剧</td>
<td>瑞典 / 德国 / 法国 / 丹麦</td>
<td>7.9 (3642人评价)</td>
</tr>
<tr>
<td>（2016）我是布莱克</td>
<td>肯·洛奇</td>
<td>剧情</td>
<td>英国 / 法国 / 比利时</td>
<td>8.2 (13369人评价)</td>
</tr>
<tr>
<td>（2015）流浪的迪潘</td>
<td>雅克·欧迪亚</td>
<td>剧情 / 犯罪</td>
<td>法国</td>
<td>7.1 (3580人评价)</td>
</tr>
<tr>
<td>（2014）冬眠</td>
<td>努里·比格·锡兰</td>
<td>剧情</td>
<td>土耳其 / 法国 / 德国</td>
<td>8.1 (13478人评价)</td>
</tr>
<tr>
<td>（2013）阿黛尔的生活</td>
<td>阿布戴·柯西胥</td>
<td>剧情 / 爱情 / 同性</td>
<td>法国/比利时/西班牙/突尼斯</td>
<td>8.3 (104583人评价)</td>
</tr>
<tr>
<td>（2012）爱</td>
<td>迈克尔·哈内克</td>
<td>剧情 / 爱情</td>
<td>美国</td>
<td>8.5 (49522人评价)</td>
</tr>
<tr>
<td>（2011）生命之树</td>
<td>泰伦斯·马力克</td>
<td>剧情 / 奇幻 / 家庭</td>
<td>美国</td>
<td>6.9 (30703人评价)</td>
</tr>
<tr>
<td>（2010）能召回前世的布米叔叔</td>
<td>韦拉斯哈古</td>
<td>剧情/奇幻</td>
<td>泰国/英国/法国/德国/西班牙</td>
<td>6.8 (5439人评价)</td>
</tr>
<tr>
<td>（2009）白丝带</td>
<td>迈克尔·哈内克</td>
<td>剧情 / 悬疑</td>
<td>德国/奥地利/法国/意大利</td>
<td>8.1 (19552人评价)</td>
</tr>
<tr>
<td>（2008）课室风云</td>
<td>劳伦·冈泰</td>
<td>剧情</td>
<td>法国</td>
<td>7.7 (4800人评价)</td>
</tr>
<tr>
<td>（2007）四月三周两天</td>
<td>克里斯蒂安·蒙吉</td>
<td>剧情</td>
<td>罗马尼亚</td>
<td>8.1 (19042人评价)</td>
</tr>
<tr>
<td>（2006）风吹麦浪</td>
<td>肯·洛奇</td>
<td>剧情 / 历史 / 战争</td>
<td>爱尔兰/英国/德国</td>
<td>8.0 (8477人评价)</td>
</tr>
<tr>
<td>（2005）孩子</td>
<td>让-皮埃尔·达内 / 吕克·达内</td>
<td>犯罪 / 剧情 / 爱情</td>
<td>比利时/法国</td>
<td>7.9 (4768人评价)</td>
</tr>
<tr>
<td>（2004）华氏911</td>
<td>迈克尔·摩尔</td>
<td>历史 / 纪录片</td>
<td>美国</td>
<td>7.9 (16924人评价)</td>
</tr>
<tr>
<td>（2003）大象</td>
<td>格斯·范·桑特</td>
<td>犯罪 / 剧情</td>
<td>美国</td>
<td>7.8 (53690人评价)</td>
</tr>
<tr>
<td>（2002）钢琴家</td>
<td>罗曼·波兰斯基</td>
<td>剧情 / 传记 / 历史 / 战争</td>
<td>法国/德国/英国/波兰</td>
<td>9.0 (212542人评价)</td>
</tr>
<tr>
<td>（2001）儿子的房间</td>
<td>南尼·莫莱蒂</td>
<td>剧情 / 悬疑 / 家庭</td>
<td>法国/意大利</td>
<td>7.9 (6632人评价)</td>
</tr>
<tr>
<td>（2000）黑暗中的舞者</td>
<td>拉斯·冯·提尔</td>
<td>剧情 / 歌舞</td>
<td>丹麦/西班牙/阿根廷</td>
<td>8.3 (64065人评价)</td>
</tr>
<tr>
<td>（1999）罗塞塔</td>
<td>让-皮埃尔·达内 / 吕克·达内</td>
<td>剧情</td>
<td>法国/比利时</td>
<td>8.1 (6241人评价)</td>
</tr>
<tr>
<td>（1998）永恒和一日</td>
<td>西奥·安哲罗普洛斯</td>
<td>剧情</td>
<td>法国/意大利/希腊/德国</td>
<td>8.9 (13001人评价)</td>
</tr>
<tr>
<td>（1997）鳗鱼</td>
<td>今村昌平</td>
<td>剧情 / 犯罪</td>
<td>日本</td>
<td>7.9 (6402人评价)</td>
</tr>
<tr>
<td>（1997）樱桃的滋味</td>
<td>阿巴斯·基亚罗斯塔米</td>
<td>剧情</td>
<td>法国/伊朗</td>
<td>7.9 (14689人评价)</td>
</tr>
<tr>
<td>（1996）秘密与谎言</td>
<td>迈克·李</td>
<td>剧情 / 喜剧 / 家庭</td>
<td>英国/法国</td>
<td>8.2 (2673人评价)</td>
</tr>
<tr>
<td>（1995）地下</td>
<td>埃米尔·库斯图里卡</td>
<td>喜剧 / 剧情 / 战争</td>
<td>法国/南斯拉夫/德国</td>
<td>9.1 (25316人评价)</td>
</tr>
<tr>
<td>（1994）低俗小说</td>
<td>昆汀·塔伦蒂诺</td>
<td>剧情 / 喜剧 / 犯罪</td>
<td>美国</td>
<td>8.8 (350669人评价)</td>
</tr>
<tr>
<td>（1993）钢琴课</td>
<td>简·坎皮恩</td>
<td>剧情 / 爱情 / 音乐</td>
<td>新西兰/澳大利亚/法国</td>
<td>8.0 (65334人评价)</td>
</tr>
<tr>
<td>（1993）霸王别姬</td>
<td>陈凯歌</td>
<td>剧情 / 爱情 / 同性</td>
<td>中国大陆/香港</td>
<td>9.5 (696448人评价)</td>
</tr>
<tr>
<td>（1992）善意的背叛</td>
<td>比利·奥古斯特</td>
<td>传记 / 剧情 / 爱情</td>
<td>瑞典/德国/英国</td>
<td>8.5 (596人评价)</td>
</tr>
<tr>
<td>（1991）巴顿·芬克</td>
<td>乔尔·科恩 / 伊桑·科恩</td>
<td>剧情 / 悬疑 / 惊悚</td>
<td>美国/英国</td>
<td>8.1 (18000人评价)</td>
</tr>
<tr>
<td>（1990）我心狂野</td>
<td>大卫·林奇</td>
<td>犯罪 / 爱情 / 惊悚</td>
<td>美国</td>
<td>7.4 (17336人评价)</td>
</tr>
<tr>
<td>（1989）性、谎言和录像带</td>
<td>史蒂文·索德伯格</td>
<td>剧情 / 情色</td>
<td>美国</td>
<td>7.6 (26294人评价)</td>
</tr>
<tr>
<td>（1988）征服者佩尔</td>
<td>比利·奥古斯特</td>
<td>剧情</td>
<td>丹麦/瑞典</td>
<td>8.6 (2413人评价)</td>
</tr>
<tr>
<td>（1987）在撒旦的阳光下</td>
<td>莫里斯·皮亚拉</td>
<td>剧情</td>
<td>法国</td>
<td>7.3 (876人评价)</td>
</tr>
<tr>
<td>（1986）教会</td>
<td>罗兰·约菲</td>
<td>冒险 / 剧情 / 历史</td>
<td>英国</td>
<td>7.8 (2003人评价)</td>
</tr>
<tr>
<td>（1985）爸爸去出差</td>
<td>埃米尔·库斯图里卡</td>
<td>剧情</td>
<td>南斯拉夫</td>
<td>8.6 (2829人评价)</td>
</tr>
<tr>
<td>（1984）德州巴黎</td>
<td>维姆·文德斯</td>
<td>剧情</td>
<td>英国/法国/美国/西德</td>
<td>8.6 (25934人评价)</td>
</tr>
<tr>
<td>（1983）楢山节考</td>
<td>今村昌平</td>
<td>剧情</td>
<td>日本</td>
<td>8.9 (16664人评价)</td>
</tr>
<tr>
<td>（1982）自由之路</td>
<td>塞里夫·格仁 / 尤马兹·古尼</td>
<td>剧情 / 爱情</td>
<td>法国/瑞士/土耳其</td>
<td>8.0 (341人评价)</td>
</tr>
<tr>
<td>（1981）大失踪</td>
<td>科斯塔-加夫拉斯</td>
<td>剧情 / 历史 / 悬疑 / 惊悚</td>
<td>美国</td>
<td>7.7 (871人评价)</td>
</tr>
<tr>
<td>（1980）铁人</td>
<td>安杰伊·瓦伊达</td>
<td>剧情 / 历史</td>
<td>波兰</td>
<td>7.7 (486人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="威尼斯电影节金狮奖"><a href="#威尼斯电影节金狮奖" class="headerlink" title="威尼斯电影节金狮奖"></a>威尼斯电影节金狮奖</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>（2017）水形物语</td>
<td>吉尔莫·德尔·托罗</td>
<td>剧情 / 奇幻 / 冒险</td>
<td>美国</td>
<td>7.6 (29462人评价)</td>
</tr>
<tr>
<td>（2016）离开的女人</td>
<td>拉夫·迪亚兹</td>
<td>剧情</td>
<td>菲律宾</td>
<td>6.9 (200人评价)</td>
</tr>
<tr>
<td>（2015）来自远方</td>
<td>洛伦佐·维加斯</td>
<td>剧情 / 同性</td>
<td>委内瑞拉/墨西哥</td>
<td>7.1 (1868人评价)</td>
</tr>
<tr>
<td>（2014）寒枝雀静</td>
<td>罗伊·安德森</td>
<td>剧情 / 喜剧</td>
<td>瑞典/德国/挪威/法国</td>
<td>7.7 (11981人评价)</td>
</tr>
<tr>
<td>（2013）罗马环城高速</td>
<td>吉安弗兰科·罗西</td>
<td>纪录片</td>
<td>意大利</td>
<td>6.2 (352人评价)</td>
</tr>
<tr>
<td>（2012）圣殇</td>
<td>金基德</td>
<td>剧情</td>
<td>韩国</td>
<td>7.7 (33407人评价)</td>
</tr>
<tr>
<td>（2011）浮士德</td>
<td>亚历山大·索科洛夫</td>
<td>剧情</td>
<td>俄罗斯</td>
<td>6.7 (2493人评价)</td>
</tr>
<tr>
<td>（2010）在某处</td>
<td>索菲亚·科波拉</td>
<td>剧情</td>
<td>美国/英国/意大利/日本</td>
<td>6.5 (9365人评价)</td>
</tr>
<tr>
<td>（2009）黎巴嫩</td>
<td>塞缪尔·毛茨</td>
<td>剧情 / 战争</td>
<td>以色列/德国/法国/黎巴嫩</td>
<td>7.2 (4506人评价)</td>
</tr>
<tr>
<td>（2008）摔角王</td>
<td>达伦·阿伦诺夫斯基</td>
<td>剧情 / 运动</td>
<td>美国/法国</td>
<td>8.3 (32441人评价)</td>
</tr>
<tr>
<td>（2007）色，戒</td>
<td>李安</td>
<td>剧情 / 爱情 / 情色</td>
<td>美国/中国大陆/台湾/香港</td>
<td>8.2 (254213人评价)</td>
</tr>
<tr>
<td>（2006）三峡好人</td>
<td>贾樟柯</td>
<td>剧情 / 爱情</td>
<td>中国大陆</td>
<td>8.0 (51655人评价)</td>
</tr>
<tr>
<td>（2005）断背山</td>
<td>李安</td>
<td>剧情 / 爱情 / 同性 / 家庭</td>
<td>美国/加拿大</td>
<td>8.6 (331126人评价)</td>
</tr>
<tr>
<td>（2004）维拉·德雷克</td>
<td>迈克·李</td>
<td>犯罪 / 剧情</td>
<td>法国/英国</td>
<td>8.0 (1433人评价)</td>
</tr>
<tr>
<td>（2003）回归</td>
<td>安德烈·萨金塞夫</td>
<td>剧情 / 家庭</td>
<td>俄罗斯</td>
<td>8.7 (12845人评价)</td>
</tr>
<tr>
<td>（2002）玛德莲堕落少女</td>
<td>彼得·穆兰</td>
<td>剧情</td>
<td>爱尔兰/英国</td>
<td>7.9 (1802人评价)</td>
</tr>
<tr>
<td>（2001）季风婚宴</td>
<td>米拉·奈尔</td>
<td>喜剧 / 剧情 / 爱情</td>
<td>印度/美国/法国/意大利/德国</td>
<td>7.6 (1484人评价)</td>
</tr>
<tr>
<td>（2000）生命的圆圈</td>
<td>贾法·帕纳西</td>
<td>剧情</td>
<td>伊朗/意大利/瑞士</td>
<td>7.6 (909人评价)</td>
</tr>
<tr>
<td>（1999）一个都不能少</td>
<td>张艺谋</td>
<td>喜剧 / 剧情</td>
<td>中国大陆</td>
<td>7.5 (84395人评价)</td>
</tr>
<tr>
<td>（1998）他们微笑的样子</td>
<td>吉安尼·阿梅利奥</td>
<td>剧情</td>
<td>意大利</td>
<td>8.0 (339人评价)</td>
</tr>
<tr>
<td>（1997）花火</td>
<td>北野武</td>
<td>犯罪 / 剧情 / 爱情 / 惊悚</td>
<td>日本</td>
<td>8.5 (37717人评价)</td>
</tr>
<tr>
<td>（1996）傲气盖天</td>
<td>尼尔·乔丹</td>
<td>传记 / 剧情 / 惊悚 / 战争</td>
<td>英国/爱尔兰/美国</td>
<td>7.7 (1626人评价)</td>
</tr>
<tr>
<td>（1995）三轮车夫</td>
<td>陈英雄</td>
<td>犯罪 / 剧情</td>
<td>越南/法国/香港</td>
<td>7.6 (9394人评价)</td>
</tr>
<tr>
<td>（1994）暴雨将至</td>
<td>米尔科·曼彻夫斯基</td>
<td>剧情 / 战争</td>
<td>马其顿/法国/英国</td>
<td>8.7 (17880人评价)</td>
</tr>
<tr>
<td>（1993）爱情万岁</td>
<td>蔡明亮</td>
<td>剧情 / 同性</td>
<td>台湾</td>
<td>7.7 (14282人评价)</td>
</tr>
<tr>
<td>（1993）蓝白红三部曲之蓝</td>
<td>基耶斯洛夫斯基</td>
<td>剧情 / 爱情 / 音乐</td>
<td>法国/波兰/瑞士</td>
<td>8.5 (68155人评价)</td>
</tr>
<tr>
<td>（1993）银色·性·男女</td>
<td>罗伯特·奥特曼</td>
<td>喜剧 / 剧情</td>
<td>美国</td>
<td>7.8 (2485人评价)</td>
</tr>
<tr>
<td>（1992）秋菊打官司</td>
<td>张艺谋</td>
<td>剧情</td>
<td>中国大陆/香港</td>
<td>7.8 (46678人评价)</td>
</tr>
<tr>
<td>（1991）蒙古精神</td>
<td>尼基塔·米哈尔科夫</td>
<td>剧情</td>
<td>法国/苏联</td>
<td>8.5 (2386人评价)</td>
</tr>
<tr>
<td>（1990）君臣人子小命呜呼</td>
<td>汤姆·斯托帕德</td>
<td>喜剧 / 剧情</td>
<td>英国/美国</td>
<td>8.5 (2522人评价)</td>
</tr>
<tr>
<td>（1989）悲情城市</td>
<td>侯孝贤</td>
<td>剧情</td>
<td>台湾/香港</td>
<td>8.8 (38473人评价)</td>
</tr>
<tr>
<td>（1988）圣洁酒徒的传奇</td>
<td>埃曼诺·奥尔米</td>
<td>剧情</td>
<td>意大利/法国</td>
<td>7.6 (176人评价)</td>
</tr>
<tr>
<td>（1987）再见，孩子们</td>
<td>路易·马勒</td>
<td>剧情 / 战争</td>
<td>法国/意大利/西德</td>
<td>8.6 (7466人评价)</td>
</tr>
<tr>
<td>（1986）绿光</td>
<td>埃里克·侯麦</td>
<td>剧情 / 爱情</td>
<td>法国</td>
<td>8.1 (6031人评价)</td>
</tr>
<tr>
<td>（1985）天涯沦落女</td>
<td>阿涅斯·瓦尔达</td>
<td>剧情</td>
<td>法国</td>
<td>8.2 (2152人评价)</td>
</tr>
<tr>
<td>（1984）寂静太阳年</td>
<td>克日什托夫·扎努西</td>
<td>剧情 / 爱情</td>
<td>德国/波兰/美国</td>
<td>7.8 (513人评价)</td>
</tr>
<tr>
<td>（1983）芳名卡门</td>
<td>让-吕克·戈达尔</td>
<td>喜剧 / 犯罪 / 剧情 / 音乐 / 爱情</td>
<td>法国</td>
<td>7.6 (2314人评价)</td>
</tr>
<tr>
<td>（1982）事物的状态</td>
<td>维姆·文德斯</td>
<td>剧情</td>
<td>西德/葡萄牙/美国</td>
<td>7.5 (649人评价)</td>
</tr>
<tr>
<td>（1981）德国姊妹</td>
<td>玛加蕾特·冯·特罗塔</td>
<td>剧情 / 历史</td>
<td>西德</td>
<td>7.7 (171人评价)</td>
</tr>
<tr>
<td>（1980）女煞葛洛莉</td>
<td>约翰·卡萨维茨</td>
<td>犯罪 / 剧情 / 惊悚</td>
<td>美国</td>
<td>7.6 (1103人评价)</td>
</tr>
<tr>
<td>（1979）大西洋城</td>
<td>路易·马勒</td>
<td>犯罪 / 剧情 / 爱情</td>
<td>加拿大/法国</td>
<td>7.6 (568人评价)</td>
</tr>
<tr>
<td>（1964）红色沙漠</td>
<td>米开朗基罗·安东尼奥尼</td>
<td>剧情</td>
<td>法国/意大利</td>
<td>8.1 (6893人评价)</td>
</tr>
<tr>
<td>（1963）城市上空的手</td>
<td>弗朗西斯科·罗西</td>
<td>剧情</td>
<td>意大利/法国</td>
<td>7.7 (286人评价)</td>
</tr>
<tr>
<td>（1962）伊万的童年</td>
<td>安德烈·塔可夫斯基</td>
<td>剧情 / 战争</td>
<td>苏联</td>
<td>8.6 (11751人评价)</td>
</tr>
<tr>
<td>（1962）家庭日记</td>
<td>瓦莱瑞奥·苏里尼</td>
<td>剧情</td>
<td>意大利</td>
<td>7.9 (75人评价)</td>
</tr>
<tr>
<td>（1961）去年在马里昂巴德</td>
<td>阿伦·雷乃</td>
<td>剧情 / 爱情 / 悬疑</td>
<td>法国/意大利</td>
<td>8.2 (8281人评价)</td>
</tr>
<tr>
<td>（1960）横渡莱茵河</td>
<td>安德烈·卡耶特</td>
<td>剧情</td>
<td>意大利/法国/西德</td>
<td>暂无评分</td>
</tr>
<tr>
<td>（1959）大战争</td>
<td>马里奥·莫尼切利</td>
<td>喜剧 / 剧情 / 战争</td>
<td>意大利/法国</td>
<td>8.0 (142人评价)</td>
</tr>
<tr>
<td>（1959）罗维雷将军</td>
<td>罗伯托·罗西里尼</td>
<td>剧情 / 战争</td>
<td>意大利/法国</td>
<td>8.3 (427人评价)</td>
</tr>
<tr>
<td>（1958）无法松的一生</td>
<td>稻垣浩</td>
<td>剧情</td>
<td>日本</td>
<td>8.4 (1399人评价)</td>
</tr>
<tr>
<td>（1957）大河之歌</td>
<td>萨蒂亚吉特·雷伊</td>
<td>剧情</td>
<td>印度</td>
<td>8.7 (1397人评价)</td>
</tr>
<tr>
<td>（1955）词语</td>
<td>卡尔·西奥多·德莱叶</td>
<td>剧情</td>
<td>丹麦</td>
<td>8.3 (1056人评价)</td>
</tr>
<tr>
<td>（1954）罗密欧与朱丽叶</td>
<td>雷纳托·卡斯特拉尼</td>
<td>剧情 / 爱情</td>
<td>意大利/英国</td>
<td>7.1 (450人评价)</td>
</tr>
<tr>
<td>（1952）禁忌的游戏</td>
<td>雷内·克莱芒</td>
<td>剧情 / 战争</td>
<td>法国</td>
<td>8.5 (3369人评价)</td>
</tr>
<tr>
<td>（1951）罗生门</td>
<td>黑泽明</td>
<td>犯罪 / 剧情 / 悬疑</td>
<td>日本</td>
<td>8.7 (126878人评价)</td>
</tr>
<tr>
<td>（1950）刑事法庭</td>
<td>安德烈·卡耶特</td>
<td>法国</td>
<td></td>
<td>7.4 (93人评价)</td>
</tr>
<tr>
<td>（1949）情妇玛侬</td>
<td>亨利-乔治·克鲁佐</td>
<td>剧情 / 犯罪</td>
<td>法国</td>
<td>7.3 (242人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="柏林电影节金熊奖"><a href="#柏林电影节金熊奖" class="headerlink" title="柏林电影节金熊奖"></a>柏林电影节金熊奖</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>（2017）肉与灵</td>
<td>伊尔蒂科·茵叶蒂</td>
<td>剧情 / 爱情</td>
<td>匈牙利</td>
<td>7.5 (5166人评价)</td>
</tr>
<tr>
<td>（2016）海上火焰</td>
<td>吉安弗兰科·罗西</td>
<td>纪录片</td>
<td>意大利/法国</td>
<td>6.6 (1223人评价)</td>
</tr>
<tr>
<td>（2015）出租车</td>
<td>贾法·帕纳西</td>
<td>剧情 / 喜剧</td>
<td>伊朗</td>
<td>8.0 (11116人评价)</td>
</tr>
<tr>
<td>（2014）白日焰火</td>
<td>刁亦男</td>
<td>剧情 / 犯罪 / 悬疑</td>
<td>中国大陆</td>
<td>7.2 (171426人评价)</td>
</tr>
<tr>
<td>（2013）孩童姿势</td>
<td>卡林·皮特·内策尔</td>
<td>剧情 / 家庭</td>
<td>罗马尼亚</td>
<td>7.3 (1400人评价)</td>
</tr>
<tr>
<td>（2012）凯撒必须死</td>
<td>保罗·塔维亚尼</td>
<td>剧情</td>
<td>意大利</td>
<td>7.8 (4326人评价)</td>
</tr>
<tr>
<td>（2011）一次别离</td>
<td>阿斯哈·法哈蒂</td>
<td>剧情 / 家庭</td>
<td>伊朗/法国</td>
<td>8.7 (134072人评价)</td>
</tr>
<tr>
<td>（2010）蜂蜜</td>
<td>赛米·卡普拉诺格鲁</td>
<td>剧情</td>
<td>土耳其/德国/法国</td>
<td>7.8 (3399人评价)</td>
</tr>
<tr>
<td>（2009）伤心的奶水</td>
<td>克劳迪亚·略萨</td>
<td>剧情 / 音乐</td>
<td>秘鲁/西班牙</td>
<td>7.2 (3860人评价)</td>
</tr>
<tr>
<td>（2008）精英部队</td>
<td>若泽·帕迪里亚</td>
<td>剧情 / 动作 / 犯罪 / 惊悚</td>
<td>巴西/美国/阿根廷</td>
<td>7.9 (27653人评价)</td>
</tr>
<tr>
<td>（2007）图雅的婚事</td>
<td>王全安</td>
<td>剧情 / 爱情</td>
<td>中国大陆</td>
<td>8.0 (19483人评价)</td>
</tr>
<tr>
<td>（2006）格巴维察</td>
<td>亚斯米拉·日巴尼奇</td>
<td>剧情</td>
<td>波黑/克罗地亚/德国/奥地利</td>
<td>7.7 (718人评价)</td>
</tr>
<tr>
<td>（2005）卡雅利沙的卡门</td>
<td>Mark Dornford-May</td>
<td>歌舞 / 剧情 / 爱情</td>
<td>南非</td>
<td>6.2 (81人评价)</td>
</tr>
<tr>
<td>（2004）勇往直前</td>
<td>法提赫·阿金</td>
<td>剧情 / 爱情</td>
<td>德国/土耳其</td>
<td>8.2 (3654人评价)</td>
</tr>
<tr>
<td>（2003）尘世之间</td>
<td>迈克尔·温特伯顿</td>
<td>剧情</td>
<td>英国</td>
<td>8.0 (1166人评价)</td>
</tr>
<tr>
<td>（2002）血腥星期天</td>
<td>保罗·格林格拉斯</td>
<td>剧情 / 历史 / 战争</td>
<td>爱尔兰/英国</td>
<td>8.0 (1581人评价)</td>
</tr>
<tr>
<td>（2002）千与千寻</td>
<td>宫崎骏</td>
<td>剧情 / 动画 / 奇幻</td>
<td>日本</td>
<td>9.2 (722352人评价)</td>
</tr>
<tr>
<td>（2001）亲密</td>
<td>帕特里斯·夏侯</td>
<td>剧情 / 情色</td>
<td>法国/英国/德国/西班牙</td>
<td>6.4 (7251人评价)</td>
</tr>
<tr>
<td>（2000）木兰花</td>
<td>保罗·托马斯·安德森</td>
<td>剧情</td>
<td>美国</td>
<td>8.2 (19274人评价)</td>
</tr>
<tr>
<td>（1999）细细的红线</td>
<td>泰伦斯·马力克</td>
<td>动作 / 剧情 / 战争</td>
<td>美国</td>
<td>7.8 (16590人评价)</td>
</tr>
<tr>
<td>（1998）中央车站</td>
<td>沃尔特·塞勒斯</td>
<td>剧情</td>
<td>巴西/法国</td>
<td>8.7 (74630人评价)</td>
</tr>
<tr>
<td>（1997）性书大亨</td>
<td>米洛斯·福尔曼</td>
<td>剧情 / 传记 / 情色</td>
<td>美国</td>
<td>7.9 (8996人评价)</td>
</tr>
<tr>
<td>（1996）理智与情感</td>
<td>李安</td>
<td>剧情 / 爱情</td>
<td>美国/英国</td>
<td>8.3 (59881人评价)</td>
</tr>
<tr>
<td>（1995）手到擒来</td>
<td>贝特朗·塔维涅</td>
<td>犯罪 / 剧情</td>
<td>法国</td>
<td>7.3 (220人评价)</td>
</tr>
<tr>
<td>（1994）因父之名</td>
<td>吉姆·谢里丹</td>
<td>传记 / 剧情</td>
<td>爱尔兰/英国/美国</td>
<td>8.8 (21533人评价)</td>
</tr>
<tr>
<td>（1993）喜宴</td>
<td>李安</td>
<td>剧情 / 喜剧 / 爱情 / 同性 / 家庭</td>
<td>台湾/美国</td>
<td>8.8 (143174人评价)</td>
</tr>
<tr>
<td>（1992）香魂女</td>
<td>谢飞</td>
<td>剧情</td>
<td>中国大陆</td>
<td>7.9 (4149人评价)</td>
</tr>
<tr>
<td>（1991）大峡谷</td>
<td>劳伦斯·卡斯丹</td>
<td>犯罪 / 剧情</td>
<td>美国</td>
<td>7.2 (86人评价)</td>
</tr>
<tr>
<td>（1990）失翼灵雀</td>
<td>伊利·曼佐</td>
<td>喜剧 / 剧情 / 爱情</td>
<td>捷克斯洛伐克</td>
<td>8.4 (1116人评价)</td>
</tr>
<tr>
<td>（1989）雨人</td>
<td>巴瑞·莱文森</td>
<td>剧情</td>
<td>美国</td>
<td>8.6 (214389人评价)</td>
</tr>
<tr>
<td>（1988）红高粱</td>
<td>张艺谋</td>
<td>剧情 / 历史 / 爱情 / 战争</td>
<td>中国大陆</td>
<td>8.2 (96309人评价)</td>
</tr>
<tr>
<td>（1961）夜</td>
<td>米开朗基罗·安东尼奥尼</td>
<td>剧情 / 爱情</td>
<td>意大利/法国</td>
<td>8.8 (5544人评价)</td>
</tr>
<tr>
<td>（1958）野草莓</td>
<td>英格玛·伯格曼</td>
<td>剧情 / 爱情 / 家庭</td>
<td>瑞典</td>
<td>8.7 (23479人评价)</td>
</tr>
<tr>
<td>（1957）十二怒汉</td>
<td>西德尼·吕美特</td>
<td>剧情</td>
<td>美国</td>
<td>9.4 (183099人评价)</td>
</tr>
<tr>
<td>（1953）恐惧的代价</td>
<td>亨利-乔治·克鲁佐</td>
<td>剧情 / 惊悚 / 冒险</td>
<td>法国/意大利</td>
<td>8.7 (5163人评价)</td>
</tr>
<tr>
<td>（1951）仙履奇缘</td>
<td>克莱德·杰洛尼米</td>
<td>爱情 / 动画 / 奇幻 / 歌舞</td>
<td>美国</td>
<td>8.1 (27899人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Metacritic历年最佳电影"><a href="#Metacritic历年最佳电影" class="headerlink" title="Metacritic历年最佳电影"></a>Metacritic历年最佳电影</h2><div class="table-container">
<table>
<thead>
<tr>
<th>片名/年份</th>
<th>导演</th>
<th>类型</th>
<th>豆瓣评分</th>
<th>其他</th>
</tr>
</thead>
<tbody>
<tr>
<td>（2017）脸庞，村庄</td>
<td>阿涅斯·瓦尔达 / 让·热内</td>
<td>纪录片</td>
<td>9.2 (12453人评价)</td>
<td>95分。2—10：敦刻尔克94，伯德小姐94，普通女人93，请以你的名字呼唤我93，狐步舞92，佛罗里达乐园92，书缘：纽约公共图书馆91，我叫他摩根90，无爱可诉90，魅影缝匠90</td>
</tr>
<tr>
<td>（2016）月光男孩</td>
<td>巴里·詹金斯</td>
<td>剧情 / 同性</td>
<td>7.2 (70138人评价)</td>
<td>99分。2—10：我不是你的黑鬼97，海边的曼彻斯特96，托尼厄德曼95，爱乐之城93，再一次体悟92，红海龟92，校塔枪击案92，抽搐症候群91，第十三修正案90，冈仁波齐90，帕特森90</td>
</tr>
<tr>
<td>（2015）卡罗尔</td>
<td>托德·海因斯</td>
<td>剧情 / 爱情 / 同性</td>
<td>8.2 (131245人评价)</td>
<td>95分。2—10：45周年94，头脑特工队94，聚焦93，夏尔巴人93，失常92，廷巴克图92，沉默之像92，出租车91，诉讼90</td>
</tr>
<tr>
<td>（2014）少年时代</td>
<td>理查德·林克莱特</td>
<td>剧情 / 家庭</td>
<td>8.5 (138262人评价)</td>
<td>100分。2—10：维龙加95，透纳先生94，利维坦91，Big Men 90，修女伊达90，两天一夜90，夜宿人89，国家美术馆89，辉夜姬物语89</td>
</tr>
<tr>
<td>（2013）鲜为人知的秘密</td>
<td>萨曼莎·布克</td>
<td>纪录片</td>
<td>暂无评分</td>
<td>100分。2—10：为奴十二载97，地心引力96，爱在午夜降临前94，隔代表亲94，基甸的部队93，醉乡民谣93，Out of the Clear Blue Sky 92，我们讲述的故事91，守门人91</td>
</tr>
<tr>
<td>（2012）猎杀本·拉登</td>
<td>凯瑟琳·毕格罗</td>
<td>剧情 / 惊悚 / 历史</td>
<td>7.6 (36287人评价)</td>
<td>95分。2—10：爱94，奥迈耶的痴梦92，这不是一部电影90，伊莲娜87，单车少年87，Gregory Crewdson: Brief Encounters 87，林肯86，瘟疫求生指南86，芭芭拉86。</td>
</tr>
<tr>
<td>（2011）一次别离</td>
<td>阿斯哈·法哈蒂</td>
<td>剧情 / 家庭</td>
<td>8.7 (134069人评价)</td>
<td>95分。2—10：曾几何时94，我的改革90，诗89，艺术家89，树荫88，失恋男人旅行日记87，能召回前世的布米叔叔87，哈利波特与死亡圣器二87，尼古拉齐奥塞斯库的自传87</td>
</tr>
<tr>
<td>（2010）社交网络</td>
<td>大卫·芬奇</td>
<td>剧情 / 传记</td>
<td>8.1 (269844人评价)</td>
<td>95分。2—10：卡洛斯94，玩具总动员三92，预言者90，冬天的骨头90，未完成的电影88，监守自盗88，国王的演讲88，45365 88，橄榄球星之死88</td>
</tr>
<tr>
<td>（2009）拆弹部队</td>
<td>凯瑟琳·毕格罗</td>
<td>剧情 / 惊悚 / 战争</td>
<td>7.7 (106215人评价)</td>
<td>94分。2—10：蓝调之歌94，35杯朗姆酒92，步履不停89，再见索罗89，图班嫁给我89，飞屋环游记88，格莫拉87，阿涅斯的海滩86，悬崖上的金鱼姬86</td>
</tr>
<tr>
<td>（2008）四月三周两天</td>
<td>克里斯蒂安·蒙吉</td>
<td>剧情</td>
<td>8.1 (19042人评价)</td>
<td>97分。2—10：机器人总动员94，课室风云92，和巴什尔跳华尔兹91，走钢丝的人89，贫民窟的百万富翁86，红气球之旅86，在人生的另一边85，亚力山娜85，属于我们的圣诞节84</td>
</tr>
<tr>
<td>（2007）美食总动员</td>
<td>布拉德·伯德 / 简·皮克瓦</td>
<td>喜剧 / 动画 / 奇幻</td>
<td>8.2 (174622人评价)</td>
<td>96分。2—10：潜水钟与蝴蝶92，血色将至92，老无所依91，我在伊朗长大90，一望无际89，曾经88，柳暗花明88，一夜大肚85，赎罪85</td>
</tr>
<tr>
<td>（2006）潘神的迷宫</td>
<td>吉尔莫·德尔·托罗</td>
<td>剧情 / 奇幻 / 悬疑 / 战争</td>
<td>7.8 (126292人评价)</td>
<td>98分。2—10：女王91，93航班91，窃听风暴89，波拉特89，硫磺岛的来信89，孩子87，命运无常87，大急救86，我们每日的面包86</td>
</tr>
<tr>
<td>（2005）卡波特</td>
<td>贝尼特·米勒</td>
<td>剧情 / 传记 / 犯罪 / 同性</td>
<td>7.9 (14485人评价)</td>
<td>88分。2—10：无人知晓88，超级无敌掌门狗：人兔的诅咒87，断背山87，轮椅上的竞技87，灰熊人87，乌龟也会飞85，达尔文的恶梦84，国王与王后84，僵尸新娘84</td>
</tr>
<tr>
<td>（2004）杯酒人生</td>
<td>亚历山大·佩恩</td>
<td>剧情 / 喜剧 / 爱情</td>
<td>7.9 (34579人评价)</td>
<td>94分。2—10：割礼龙凤斗91，爱在日落黄昏时90，超人总动员90，十面埋伏89，美丽心灵的永恒阳光89，万福玛利亚87，诅咒87，百万美元宝贝86，洛杉矶影话86</td>
</tr>
<tr>
<td>（2003）指环王3：王者无敌</td>
<td>彼得·杰克逊</td>
<td>剧情 / 动作 / 奇幻 / 冒险</td>
<td>9.1 (302713人评价)</td>
<td>94分。2—10：疯狂约会美丽都91，海底总动员90，美国荣耀90，追捕弗雷德曼家族90，迷失东京89，山村犹有读书声87，战争迷雾87，栗色伊拉克86，十段生命的律动86</td>
</tr>
<tr>
<td>（2002）千与千寻</td>
<td>宫崎骏</td>
<td>剧情 / 动画 / 奇幻</td>
<td>9.2 (722321人评价)</td>
<td>94分。2—10：冰原快跑人91，血腥星期天90，你妈妈也一样88，失序年代88，指环王二88，俄罗斯方舟86，我要回家86，对她说86，钢琴家86</td>
</tr>
<tr>
<td>（2001）指环王1：魔戒再现</td>
<td>彼得·杰克逊</td>
<td>剧情 / 动作 / 奇幻 / 冒险</td>
<td>8.9 (318973人评价)</td>
<td>92分。2—10：鲸鱼马戏团92，现代启示录91，我的意大利之旅90，高斯福庄园90，幽灵世界88，意外边缘86，沙之下86，地下孩童85，南极坚韧号85</td>
</tr>
<tr>
<td>（2000）卧虎藏龙</td>
<td>李安</td>
<td>剧情 / 动作 / 爱情 / 武侠 / 古装</td>
<td>8.0 (179373人评价)</td>
<td>93分。2—10：一一93，军中禁恋91，几近成名90，小鸡快跑88，随风而逝86，毒品网络86，当黑夜降临85，你可以信赖我85，无声的呐喊85</td>
</tr>
<tr>
<td>（1999）酣歌畅戏</td>
<td>迈克·李</td>
<td>剧情 / 喜剧 / 音乐 / 歌舞 / 传记</td>
<td>7.2 (458人评价)</td>
<td>90分。2—10：成为约翰马尔科维奇90，玩具总动员二88，关于我母亲的一切87，史崔特先生的故事86，人生七年六86，男孩不哭86，美国丽人86，钢铁巨人85，导演狂想曲84</td>
</tr>
<tr>
<td>（1998）拯救大兵瑞恩</td>
<td>史蒂文·斯皮尔伯格</td>
<td>剧情 / 历史 / 战争</td>
<td>8.9 (250282人评价)</td>
<td>90分。2—10：楚门的世界90，恋爱中的莎士比亚87，青春年少86，战略高手85，绝地计划82，爱就让我快乐81，长岛爱与死80，中央车站80，苦难79</td>
</tr>
<tr>
<td>（1997）洛城机密</td>
<td>柯蒂斯·汉森</td>
<td>犯罪 / 剧情 / 悬疑 / 惊悚</td>
<td>8.6 (77810人评价)</td>
<td>90分。2—10：意外的春天90，四个小女孩89，不羁夜85，来自天上的声音83，变脸82，一诺千金82，与男人同行81，赌城纵横78，仲夏夜玫瑰78</td>
</tr>
<tr>
<td>（1996）秘密与谎言</td>
<td>迈克·李</td>
<td>剧情 / 喜剧 / 家庭</td>
<td>8.2 (2673人评价)</td>
<td>91分。2—10：闪亮的风采87，英国病人87，冰血暴87，弹簧刀84，欢迎光临娃娃屋83，当我们是拳王的日子83，猜火车83，寒冷舒适的农庄81，与灾难调情81</td>
</tr>
<tr>
<td>（1995）克鲁伯</td>
<td>泰利·茨威戈夫</td>
<td>纪录片 / 传记</td>
<td>8.4 (641人评价)</td>
<td>93分。2—10：玩具总动员92，不惜一切86，理智与情感84，小公主83，小猪宝贝83，离开拉斯维加斯82，矮子当道82，乔治亚81，邮差81</td>
</tr>
</tbody>
</table>
</div>
<h2 id="日本《电影旬报》最佳日本电影"><a href="#日本《电影旬报》最佳日本电影" class="headerlink" title="日本《电影旬报》最佳日本电影"></a>日本《电影旬报》最佳日本电影</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>（2017）<a href="https://movie.douban.com/subject/26847445/" target="_blank" rel="noopener">夜空总有最大密度的蓝色</a></td>
<td>石井裕也</td>
<td>剧情 / 爱情</td>
<td>日本</td>
<td>7.4 (4183人评价)</td>
</tr>
<tr>
<td>（2016）<a href="https://movie.douban.com/subject/11611021/" target="_blank" rel="noopener">在这世界的角落</a></td>
<td>片渊须直</td>
<td>剧情 / 战争 / 动画</td>
<td>日本</td>
<td>7.6 (19079人评价)</td>
</tr>
<tr>
<td>（2015）<a href="https://movie.douban.com/subject/26259635/" target="_blank" rel="noopener">恋人们</a></td>
<td>桥口亮辅</td>
<td>剧情</td>
<td>日本</td>
<td>7.4 (4948人评价)</td>
</tr>
<tr>
<td>（2014）<a href="https://movie.douban.com/subject/24839101/" target="_blank" rel="noopener">只在那里发光</a></td>
<td>吴美保</td>
<td>剧情</td>
<td>日本</td>
<td>7.2 (4848人评价)</td>
</tr>
<tr>
<td>（2013）<a href="https://movie.douban.com/subject/11523496/" target="_blank" rel="noopener">去见小洋葱的母亲</a></td>
<td>森崎东</td>
<td>剧情 / 喜剧 / 家庭</td>
<td>日本</td>
<td>7.6 (2341人评价)</td>
</tr>
<tr>
<td>（2012）<a href="https://movie.douban.com/subject/6849035/" target="_blank" rel="noopener">家族的国度</a></td>
<td>梁英姬</td>
<td>剧情</td>
<td>日本</td>
<td>7.7 (1769人评价)</td>
</tr>
<tr>
<td>（2011）<a href="https://movie.douban.com/subject/4853331/" target="_blank" rel="noopener">一封明信片</a></td>
<td>新藤兼人</td>
<td>剧情</td>
<td>日本</td>
<td>7.3 (489人评价)</td>
</tr>
<tr>
<td>（2010）<a href="https://movie.douban.com/subject/4135443/" target="_blank" rel="noopener">恶人</a></td>
<td>李相日</td>
<td>剧情</td>
<td>日本</td>
<td>7.7 (17172人评价)</td>
</tr>
<tr>
<td>（2009）<a href="https://movie.douban.com/subject/3138605/" target="_blank" rel="noopener">亲爱的医生</a></td>
<td>西川美和</td>
<td>剧情</td>
<td>日本</td>
<td>8.0 (3861人评价)</td>
</tr>
<tr>
<td>（2008）<a href="https://movie.douban.com/subject/2149806/" target="_blank" rel="noopener">入殓师</a></td>
<td>泷田洋二郎</td>
<td>剧情</td>
<td>日本</td>
<td>8.8 (329224人评价)</td>
</tr>
<tr>
<td>（2006）<a href="https://movie.douban.com/subject/1919906/" target="_blank" rel="noopener">即使这样也不是我做的</a></td>
<td>周防正行</td>
<td>剧情</td>
<td>日本</td>
<td>8.3 (7048人评价)</td>
</tr>
<tr>
<td>（2006）<a href="https://movie.douban.com/subject/1788940/" target="_blank" rel="noopener">扶桑花女孩 </a></td>
<td>李相日</td>
<td>喜剧 / 剧情</td>
<td>日本</td>
<td>8.0 (21876人评价)</td>
</tr>
<tr>
<td>（2004）<a href="https://movie.douban.com/subject/1779098/" target="_blank" rel="noopener">无敌青春</a></td>
<td>井筒和幸</td>
<td>动作 / 喜剧 / 剧情 / 爱情</td>
<td>日本</td>
<td>7.7 (1040人评价)</td>
</tr>
<tr>
<td>（2004）<a href="https://movie.douban.com/subject/1292337/" target="_blank" rel="noopener">无人知晓</a></td>
<td>是枝裕和</td>
<td>剧情</td>
<td>日本</td>
<td>9.0 (53392人评价)</td>
</tr>
<tr>
<td>（2002）<a href="https://movie.douban.com/subject/1949766/" target="_blank" rel="noopener">雾岛美丽的夏天</a></td>
<td>黑木和雄</td>
<td>剧情</td>
<td>日本</td>
<td>7.2 (168人评价)</td>
</tr>
<tr>
<td>（2002）<a href="https://movie.douban.com/subject/1307306/" target="_blank" rel="noopener">黄昏的清兵卫</a></td>
<td>山田洋次</td>
<td>剧情 / 爱情</td>
<td>日本</td>
<td>8.7 (16621人评价)</td>
</tr>
<tr>
<td>（2001）<a href="https://movie.douban.com/subject/1307176/" target="_blank" rel="noopener">GO!大暴走 GO</a></td>
<td>行定勋</td>
<td>剧情</td>
<td>日本</td>
<td>8.2 (13954人评价)</td>
</tr>
<tr>
<td>（2000）<a href="https://movie.douban.com/subject/1306044/" target="_blank" rel="noopener">颜</a></td>
<td>阪本顺治</td>
<td>喜剧 / 剧情</td>
<td>日本</td>
<td>8.1 (390人评价)</td>
</tr>
<tr>
<td>（1998）<a href="https://movie.douban.com/subject/1477917/" target="_blank" rel="noopener">啊，春天</a></td>
<td>相米慎二</td>
<td>剧情</td>
<td>日本</td>
<td>8.0 (420人评价)</td>
</tr>
<tr>
<td>（1997）<a href="https://movie.douban.com/subject/1302642/" target="_blank" rel="noopener">花火</a></td>
<td>北野武</td>
<td>犯罪 / 剧情 / 爱情 / 惊悚</td>
<td>日本</td>
<td>8.5 (37717人评价)</td>
</tr>
<tr>
<td>（1997）<a href="https://movie.douban.com/subject/1303453/" target="_blank" rel="noopener">鳗鱼</a></td>
<td>今村昌平</td>
<td>剧情 / 犯罪</td>
<td>日本</td>
<td>7.9 (6402人评价)</td>
</tr>
<tr>
<td>（1996）<a href="https://movie.douban.com/subject/1308010/" target="_blank" rel="noopener">谈谈情跳跳</a></td>
<td>周防正行</td>
<td>剧情 / 喜剧 / 爱情 / 歌舞</td>
<td>日本</td>
<td>8.2 (9111人评价)</td>
</tr>
<tr>
<td>（1995）<a href="https://movie.douban.com/subject/1303598/" target="_blank" rel="noopener">午后的遗言</a></td>
<td>新藤兼人</td>
<td>剧情</td>
<td>日本</td>
<td>8.1 (805人评价)</td>
</tr>
<tr>
<td>（1994）<a href="https://movie.douban.com/subject/1302102/" target="_blank" rel="noopener">全身小说家</a></td>
<td>原一男</td>
<td>纪录片</td>
<td>日本</td>
<td>8.2 (251人评价)</td>
</tr>
<tr>
<td>（1993）<a href="https://movie.douban.com/subject/1291560/" target="_blank" rel="noopener">龙猫</a></td>
<td>宫崎骏</td>
<td>儿童 / 动画 / 奇幻 / 家庭</td>
<td>日本</td>
<td>9.1 (451130人评价)</td>
</tr>
<tr>
<td>（1985）<a href="https://movie.douban.com/subject/1294580/" target="_blank" rel="noopener">其后 </a></td>
<td>森田芳光</td>
<td>剧情</td>
<td>日本</td>
<td>8.6 (3808人评价)</td>
</tr>
<tr>
<td>（1984）<a href="https://movie.douban.com/subject/1293745/" target="_blank" rel="noopener">葬礼</a></td>
<td>伊丹十三</td>
<td>喜剧</td>
<td>日本</td>
<td>8.3 (1287人评价)</td>
</tr>
<tr>
<td>（1982）<a href="https://movie.douban.com/subject/1294287/" target="_blank" rel="noopener">蒲田进行曲</a></td>
<td>深作欣二</td>
<td>喜剧 / 爱情</td>
<td>日本</td>
<td>7.9 (2073人评价)</td>
</tr>
<tr>
<td>（1981）<a href="https://movie.douban.com/subject/1303065/" target="_blank" rel="noopener">泥之河</a></td>
<td>小栗康平</td>
<td>剧情</td>
<td>日本</td>
<td>8.7 (2334人评价)</td>
</tr>
<tr>
<td>（1979）<a href="https://movie.douban.com/subject/1294940/" target="_blank" rel="noopener">复仇在我</a></td>
<td>今村昌平</td>
<td>犯罪 / 剧情</td>
<td>日本</td>
<td>8.3 (3448人评价)</td>
</tr>
<tr>
<td>（1977）<a href="https://movie.douban.com/subject/1305294/" target="_blank" rel="noopener">幸福的黄手帕</a></td>
<td>山田洋次</td>
<td>喜剧 / 剧情</td>
<td>日本</td>
<td>8.1 (7225人评价)</td>
</tr>
<tr>
<td>（1974）<a href="https://movie.douban.com/subject/1303073/" target="_blank" rel="noopener">望乡</a></td>
<td>熊井启</td>
<td>剧情 / 历史 / 战争</td>
<td>日本</td>
<td>8.6 (5863人评价)</td>
</tr>
<tr>
<td>（1968）<a href="https://movie.douban.com/subject/1299125/" target="_blank" rel="noopener">诸神的欲望</a></td>
<td>今村昌平</td>
<td>剧情</td>
<td>日本</td>
<td>8.2 (1290人评价)</td>
</tr>
<tr>
<td>（1967）<a href="https://movie.douban.com/subject/1300738/" target="_blank" rel="noopener">夺命剑</a></td>
<td>小林正树</td>
<td>剧情</td>
<td>日本</td>
<td>8.9 (3719人评价)</td>
</tr>
<tr>
<td>（1965）<a href="https://movie.douban.com/subject/1296679/" target="_blank" rel="noopener">红胡子</a></td>
<td>黑泽明</td>
<td>剧情</td>
<td>日本</td>
<td>8.5 (3129人评价)</td>
</tr>
<tr>
<td>（1964）<a href="https://movie.douban.com/subject/1295812/" target="_blank" rel="noopener">砂之女</a></td>
<td>敕使河原宏</td>
<td>剧情 / 惊悚</td>
<td>日本</td>
<td>8.4 (4595人评价)</td>
</tr>
<tr>
<td>（1963）<a href="https://movie.douban.com/subject/1302917/" target="_blank" rel="noopener">日本昆虫记</a></td>
<td>今村昌平</td>
<td>剧情</td>
<td>日本</td>
<td>8.2 (1656人评价)</td>
</tr>
<tr>
<td>（1958）<a href="https://movie.douban.com/subject/1294737/" target="_blank" rel="noopener">楢山节考</a></td>
<td>木下惠介</td>
<td>剧情</td>
<td>日本</td>
<td>8.7 (3808人评价)</td>
</tr>
<tr>
<td>（1955）<a href="https://movie.douban.com/subject/1298870/" target="_blank" rel="noopener">浮云</a></td>
<td>成濑巳喜男</td>
<td>剧情 / 爱情</td>
<td>日本</td>
<td>8.6 (5024人评价)</td>
</tr>
<tr>
<td>（1954）<a href="https://movie.douban.com/subject/1306807/" target="_blank" rel="noopener">二十四只眼睛</a></td>
<td>木下惠介</td>
<td>剧情</td>
<td>日本</td>
<td>8.6 (2630人评价)</td>
</tr>
<tr>
<td>（1952）<a href="https://movie.douban.com/subject/1293847/" target="_blank" rel="noopener">生之欲</a></td>
<td>黑泽明</td>
<td>剧情</td>
<td>日本</td>
<td>9.0 (11690人评价)</td>
</tr>
<tr>
<td>（1951）<a href="https://movie.douban.com/subject/1401842/" target="_blank" rel="noopener">麦秋</a></td>
<td>小津安二郎</td>
<td>剧情 / 家庭</td>
<td>日本</td>
<td>8.8 (5221人评价)</td>
</tr>
<tr>
<td>（1949）<a href="https://movie.douban.com/subject/1307265/" target="_blank" rel="noopener">晚春</a></td>
<td>小津安二郎</td>
<td>剧情 / 家庭</td>
<td>日本</td>
<td>8.7 (9695人评价)</td>
</tr>
<tr>
<td>（1948）<a href="https://movie.douban.com/subject/1294859/" target="_blank" rel="noopener">泥醉天使</a></td>
<td>黑泽明</td>
<td>犯罪 / 剧情</td>
<td>日本</td>
<td>8.1 (2336人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="东京电影节"><a href="#东京电影节" class="headerlink" title="东京电影节"></a>东京电影节</h2><div class="table-container">
<table>
<thead>
<tr>
<th>年份/片名</th>
<th>导演</th>
<th>类型</th>
<th>国家</th>
<th>豆瓣评分</th>
</tr>
</thead>
<tbody>
<tr>
<td>(2016)<a href="https://movie.douban.com/subject/26608943/" target="_blank" rel="noopener">昨日之花</a></td>
<td>克里斯·克劳斯</td>
<td>剧情 / 喜剧 / 爱情</td>
<td>奥地利/德国/法国</td>
<td>6.5 (358人评价)</td>
</tr>
<tr>
<td>(2015)<a href="https://movie.douban.com/subject/26631663/" target="_blank" rel="noopener">尼斯·疯狂的心</a></td>
<td>罗伯托·柏林厄</td>
<td>剧情 / 传记 / 历史</td>
<td>巴西</td>
<td>8.2 (3697人评价)</td>
</tr>
<tr>
<td>(2014)<a href="https://movie.douban.com/subject/25934367/" target="_blank" rel="noopener">天知道</a></td>
<td>本·萨弗迪 / 约书亚·萨弗迪</td>
<td>剧情</td>
<td>美国</td>
<td>6.9 (649人评价)</td>
</tr>
<tr>
<td>(2013)<a href="https://movie.douban.com/subject/10754795/" target="_blank" rel="noopener">我们是最棒的！</a></td>
<td>鲁卡斯·穆迪森</td>
<td>剧情 / 音乐</td>
<td>瑞典</td>
<td>7.3 (889人评价)</td>
</tr>
<tr>
<td>(2012)<a href="https://movie.douban.com/subject/10438908/" target="_blank" rel="noopener">他人之子</a></td>
<td>罗兰娜·利维</td>
<td>剧情</td>
<td>法国</td>
<td>7.4 (1106人评价)</td>
</tr>
<tr>
<td>(2011)<a href="https://movie.douban.com/subject/6786002/" target="_blank" rel="noopener">触不可及</a></td>
<td>奥利维埃·纳卡什</td>
<td>剧情 / 喜剧</td>
<td>法国</td>
<td>9.1 (394864人评价)</td>
</tr>
<tr>
<td>(2010)<a href="https://movie.douban.com/subject/5349206/" target="_blank" rel="noopener">亲密文法 </a></td>
<td>尼尔·伯格曼</td>
<td>剧情</td>
<td>以色列</td>
<td>7.8 (257人评价)</td>
</tr>
<tr>
<td>(2007)<a href="https://movie.douban.com/subject/2079017/" target="_blank" rel="noopener">乐队来访</a></td>
<td>艾伦·科勒林</td>
<td>喜剧 / 剧情 / 音乐</td>
<td>以色列/法国/美国</td>
<td>8.1 (4593人评价)</td>
</tr>
<tr>
<td>(2004)<a href="https://movie.douban.com/subject/1308498/" target="_blank" rel="noopener">暖</a></td>
<td>霍建起</td>
<td>爱情 / 剧情</td>
<td>中国大陆</td>
<td>7.8 (11047人评价)</td>
</tr>
<tr>
<td>(2000)<a href="https://movie.douban.com/subject/1292216/" target="_blank" rel="noopener">爱情是狗娘</a></td>
<td>冈萨雷斯·伊纳里图</td>
<td>剧情 / 惊悚</td>
<td>墨西哥</td>
<td>8.2 (33898人评价)</td>
</tr>
<tr>
<td>(1999)<a href="https://movie.douban.com/subject/1301098/" target="_blank" rel="noopener">黑暗之光</a></td>
<td>张作骥</td>
<td>剧情</td>
<td>台湾</td>
<td>8.1 (1959人评价)</td>
</tr>
<tr>
<td>(1998)<a href="https://movie.douban.com/subject/1301899/" target="_blank" rel="noopener">睁开你的双眼</a></td>
<td>亚历杭德罗·阿梅纳瓦尔</td>
<td>剧情 / 悬疑 / 爱情 / 科幻 / 惊悚</td>
<td>西班牙/法国/意大利</td>
<td>8.1 (8446人评价)</td>
</tr>
<tr>
<td>(1997)<a href="https://movie.douban.com/subject/1303023/" target="_blank" rel="noopener">走出寂静</a></td>
<td>卡罗莉内·林克</td>
<td>剧情 / 音乐</td>
<td>德国</td>
<td>8.3 (999人评价)</td>
</tr>
<tr>
<td>(1996)<a href="https://movie.douban.com/subject/1303026/" target="_blank" rel="noopener">给我一个爸</a></td>
<td>扬·斯维拉克</td>
<td>剧情 / 喜剧 / 音乐</td>
<td>捷克/英国/法国</td>
<td>8.4 (3906人评价)</td>
</tr>
<tr>
<td>(1993)<a href="https://movie.douban.com/subject/1303967/" target="_blank" rel="noopener">蓝风筝</a></td>
<td>田壮壮</td>
<td>剧情 / 历史</td>
<td>中国大陆</td>
<td>8.6 (27532人评价)</td>
</tr>
<tr>
<td>(1987)<a href="https://movie.douban.com/subject/1308070/" target="_blank" rel="noopener">老井</a></td>
<td>吴天明</td>
<td>剧情 / 爱情</td>
<td>中国</td>
<td>7.9 (7345人评价)</td>
</tr>
<tr>
<td>(1985)<a href="https://movie.douban.com/subject/1296798/" target="_blank" rel="noopener">台风俱乐部</a></td>
<td>相米慎二</td>
<td>剧情 / 爱情</td>
<td>日本</td>
<td>8.0 (1247人评价)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="圣丹斯电影节"><a href="#圣丹斯电影节" class="headerlink" title="圣丹斯电影节"></a>圣丹斯电影节</h2><h2 id="山形国际纪录片电影节"><a href="#山形国际纪录片电影节" class="headerlink" title="山形国际纪录片电影节"></a>山形国际纪录片电影节</h2><h2 id="布宜诺斯艾利斯电影节"><a href="#布宜诺斯艾利斯电影节" class="headerlink" title="布宜诺斯艾利斯电影节"></a>布宜诺斯艾利斯电影节</h2><h2 id="奥斯卡金像奖最佳影片"><a href="#奥斯卡金像奖最佳影片" class="headerlink" title="奥斯卡金像奖最佳影片"></a>奥斯卡金像奖最佳影片</h2>]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 广播影评人 </tag>
            
            <tag> 金棕榈奖 </tag>
            
            <tag> 金狮奖 </tag>
            
            <tag> 金熊奖 </tag>
            
            <tag> Metacritic </tag>
            
            <tag> 电影旬报 </tag>
            
            <tag> 圣丹斯 </tag>
            
            <tag> 金像奖 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（8）：XgBoost]]></title>
      <url>/2017/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9AXgBoost/</url>
      <content type="html"><![CDATA[<h2 id="一、XGBoost简介"><a href="#一、XGBoost简介" class="headerlink" title="一、XGBoost简介"></a>一、XGBoost简介</h2><p>在数据建模中，经常采用Boosting方法通过将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。</p>
<a id="more"></a>
<p>针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。</p>
<p>在Kaggle的希格斯子信号识别竞赛，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。</p>
<h2 id="二、监督学习的三要素"><a href="#二、监督学习的三要素" class="headerlink" title="二、监督学习的三要素"></a>二、监督学习的三要素</h2><p>因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。</p>
<h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>模型指的是给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测</p>
<script type="math/tex; mode=display">
\hat{y}_i=\sum_j{w_jx_{ij}}</script><p>这里的预测值$y$可以由不同的解释，比如我们可以把它作为回归目标的输出，或者进行$sigmoid$变换得到概率（即用$\frac{1}{1+e^{-\hat{y}_i}}$来预测正例的概率），或者作为排序的指标等。而一个线性模型根据$y$的解释不通（以及设计对应的目标函数）用到回归、分类或者排序等场景。</p>
<h3 id="2-2-参数"><a href="#2-2-参数" class="headerlink" title="2.2 参数"></a>2.2 参数</h3><p>参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数：</p>
<script type="math/tex; mode=display">
\varTheta =\left\{w_j|j=1,2,···,d\right\}</script><h3 id="2-3-目标函数：误差函数-正则化项"><a href="#2-3-目标函数：误差函数-正则化项" class="headerlink" title="2.3 目标函数：误差函数+正则化项"></a>2.3 目标函数：误差函数+正则化项</h3><p>模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%886.12.45.png" alt="屏幕快照 2017-04-01 下午6.12.45"></p>
<ul>
<li>1）$L(\varTheta)$：损失函数$L=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}$，常见的损失函数有：<ul>
<li>平方损失：$<br>l\left(y_i,\hat{y}_i\right)=\left(y_i-\hat{y}_i\right)^2<br>$</li>
<li>Logistic损失：$<br>l\left(y_i,\hat{y}_i\right)=y_i\ln\left(1+e^{-y_i}\right)+\left(1-y_i\right)\ln\left(1+e^{y_i}\right)<br>$</li>
</ul>
</li>
<li>2）$\varOmega\left(\varTheta\right)$：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有$L1$正则与$L2$正则<ul>
<li>$L1$正则（lasso）：$\varOmega\left(w\right)=\lambda ||w||_1$ </li>
<li>$L2$正则：$\varOmega\left(w\right)=\lambda ||w||^2$</li>
</ul>
</li>
</ul>
<p>这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，$Bias$可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而$Variance$是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的$Bias$。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。</p>
<h3 id="2-4-优化算法"><a href="#2-4-优化算法" class="headerlink" title="2.4 优化算法"></a>2.4 优化算法</h3><p>上面三个部分包含了机器学习的主要成分，也是机器学习工具划分模型比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计问题，比如常见的例子如决策树的学习算法的每一步去优化基尼系数，然后剪枝，但是没有考虑到后面的目标是什么。而这些启发式优化方法背后往往隐含了一个目标函数，理解了目标函数本身也有利于我们设计相应的学习算法。</p>
<h2 id="三、回归树与树集成"><a href="#三、回归树与树集成" class="headerlink" title="三、回归树与树集成"></a>三、回归树与树集成</h2><h3 id="3-1-回归树"><a href="#3-1-回归树" class="headerlink" title="3.1 回归树"></a>3.1 回归树</h3><p>在介绍$XGBoost$之前，首先得了解一下回归树和树集成的概念，其实在$AdaBoost$算法中已经详细讲述过这一部分了。Boosting Tree最基本的组成部分叫做回归树（regression tree），下面就是一个回归树的例子。它把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会有一个实数分数。具体地，下图给出了一个判断用户是否会喜欢电脑游戏的回归树模型，每个树叶的得分对应了该用户有多可能喜欢电脑游戏（分值越大可能性越大）。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%888.50.53.png" alt="屏幕快照 2017-04-01 下午8.50.53"></p>
<h3 id="3-2-树集成"><a href="#3-2-树集成" class="headerlink" title="3.2 树集成"></a>3.2 树集成</h3><p>上图中的回归树只用到了用户年龄和性别两个信息，过于简单，预测的准确性自然有限。一个回归树往往过于简单无法有效地预测，因此一个更加强有力的模型叫做tree ensemble。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%888.58.46.png" alt="屏幕快照 2017-04-01 下午8.58.46"><br>在上图中使用两个回归树对用户是否喜欢电脑游戏进行了预测，并将两个回归树的预测结果加和得到单个用户的预测结果。在实际的预测模型建立过程中，我们通过不断地增加新的回归树，并给每个回归树赋予合适的权重，在此基础上综合不同的回归树得分获得更为准确的预测结果，这也就是树集成的基本思路。在预测算法中，随机森林和提升树都采用了树集成的方法，但是在具体地模型构造和参数调整的方法有所差别。</p>
<p>在这个树集成模型中，我们可以认为参数对应了树的结构，以及每个叶子节点上面的预测分数。</p>
<p>那么我们如何来学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。决策树学习往往充满了启发式算法，如先优化基尼系数，然后再剪枝，限制最大深度等等。其实这些启发式算法背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法。</p>
<h2 id="四、XGBoost的推导过程"><a href="#四、XGBoost的推导过程" class="headerlink" title="四、XGBoost的推导过程"></a>四、XGBoost的推导过程</h2><h3 id="4-1-XGBoost的目标函数与泰勒展开"><a href="#4-1-XGBoost的目标函数与泰勒展开" class="headerlink" title="4.1 XGBoost的目标函数与泰勒展开"></a>4.1 XGBoost的目标函数与泰勒展开</h3><p>对于tree ensemble，我们可以把某一个迭代后集成的模型写成为：<script type="math/tex">\hat{y}_i=\sum_{k=1}^K{f_k\left(x_i\right)},\ f_k\in\mathscr{F}</script><br>其中每个$f$是一个在函数空间($\mathscr{F}$)里面的函数，而$\mathscr{F}$对应了所有regression tree的集合。<br>我们设计的目标函数也需要遵循前面的主要原则，包含两部分</p>
<script type="math/tex; mode=display">
Obj\left(\varTheta\right)=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}+\sum_{k=1}^K{\varOmega\left(f_k\right)}</script><p>其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数$f$到模型里面：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%889.34.49.png" alt="屏幕快照 2017-04-01 下午9.34.49"><br>其中$\hat{y}_i\left(t-1\right)$就是前$t-1$轮的模型预测，$f_t{(x_i)}$为新$t$轮加入的预测函数。<br>这里自然就涉及一个问题：如何选择在每一轮中加入的$f(x_i)$呢？答案很直接，选取的$f(x_i)$必须使得我们的目标函数尽量最大地降低（这里应用到了Boosting的基本思想，即当前的基学习器重点关注以前所有学习器犯错误的那些数据样本，以此来达到提升的效果）。先对目标函数进行改写，表示如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%889.38.47.png" alt="屏幕快照 2017-04-01 下午9.38.47"><br>如果我们考虑平方误差作为损失函数，公式可改写为：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%889.52.21.png" alt="屏幕快照 2017-04-01 下午9.52.21"><br>更加一般的，对于不是平方误差的情况，我们可以采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行下一步的计算。</p>
<p>泰勒展开一般表达式为：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%8810.55.53.png" alt="屏幕快照 2017-04-01 下午10.55.53"><br>用泰勒展开来近似我们原来的目标：首先定义<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-01%20%E4%B8%8B%E5%8D%8811.01.22.png" alt="屏幕快照 2017-04-01 下午11.01.22"><br>得到<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.17.42.png" alt="屏幕快照 2017-04-02 上午12.17.42"><br>如果移除掉常数项，我们会发现这个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。可能有人会问，这个方式似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？</p>
<p>这是因为，这样做首先有理论上的好处，它会使我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。然后这一个抽象的形式对于工程商实现机器学习工具也是非常有帮助的。因为它包含所有可以求到的目标函数，也就是说有了这个形式，我们写出来的代码可以用来求解包括回归、分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般化。</p>
<h3 id="4-2-决策树的复杂度"><a href="#4-2-决策树的复杂度" class="headerlink" title="4.2 决策树的复杂度"></a>4.2 决策树的复杂度</h3><p>到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。<br>我们先对于$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。其中结构部分$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。<br><img src="http://omu7tit09.bkt.clouddn.com/231.png" alt=""><br>当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$范数平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图给出了复杂度计算的一个例子。<br><img src="http://omu7tit09.bkt.clouddn.com/12345.png" alt="屏幕快照 2017-04-02 上午12.39.10"></p>
<h3 id="4-3-目标函数的最小化"><a href="#4-3-目标函数的最小化" class="headerlink" title="4.3 目标函数的最小化"></a>4.3 目标函数的最小化</h3><p>接下来是最关键的一步，在这种新的顶一下，我们可以把目标函数进行如下改写，其中$I$被定义为每个叶子上面样本集合$I_j=\{i| q(x_i)=j\}$<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.42.51.png" alt="屏幕快照 2017-04-02 上午12.42.51">这一目标包含了$T$个互相独立的单变量二次函数<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.46.44.png" alt="屏幕快照 2017-04-02 上午12.46.44"><br>我们可以定义<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.48.38.png" alt="屏幕快照 2017-04-02 上午12.48.38"><br>那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.51.56.png" alt="屏幕快照 2017-04-02 上午12.51.56">可以观察到上式是由$T$个相互独立的单变量二次函数再加上$L1$范数构成。这样的特性意味着单个树叶的权重计算与其他树叶的权重无关，所以我们可以非常方便计算第$j$个树叶的权重，以及目标函数。由此，我们将目标函数转换为一个一元二次方程求最小值的问题（在此式中，变量为$w_j$，函数本质上是关于$w_j$的二次函数），略去求解步骤，最终结果如下所示：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8812.55.23.png" alt="屏幕快照 2017-04-02 上午12.55.23"><br>乍一看目标函数的计算与回归树的结构$q$函数没有什么关系，但是如果我们仔细回看目标函数的构成，就会发现其中$G_j$和$H_j$的取值是由第$j$个树叶上数据样本所决定的。而第$j$个树上所具有的数据样本则是由树结构$q$函数决定的。也就是说，一旦回归树的结构$q$确定，那么相应的目标函数就能够根据上式计算出来。那么回归树的生成问题也就转换为找到一个最优的树结构$q$，使得它具有最小的目标函数。</p>
<p>计算求得的$Obj$代表了当指定一个树的结构的时候，目标函数上面最多减少多少。我们可以把它叫做结构分数（structure score）。可以把它认为是类似于基尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子，它根据决策树的预测结果得到各样本的梯度数据，然后计算出实际的结构分数。这个分数越小，代表这个树的结构越好：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%881.18.47.png" alt="屏幕快照 2017-04-02 上午1.18.47"></p>
<h3 id="4-4-枚举树的结果——贪心法"><a href="#4-4-枚举树的结果——贪心法" class="headerlink" title="4.4 枚举树的结果——贪心法"></a>4.4 枚举树的结果——贪心法</h3><p>在前面分析的基础上，当寻找到最优的树结构时，我们可以不断地枚举不同树的结构，利用这个打分函数来寻找一个最优结构的树，加入到我们的模型中，然后再重复这样的操作。不过枚举所有树结构这个操作不太可行，在这里XGBoost采用了常用的贪心法，即每一次尝试区队已有的叶子加入一个分割。对于一个剧透的分割方案，我们可以获得的增益可以由如下公式计算得到：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%881.25.44.png" alt="屏幕快照 2017-04-02 上午1.25.44"></p>
<p>这个公式形式上跟ID3算法（采用信息熵计算增益）或者CART算法（采用基尼指数计算增益） 是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的$\gamma$即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数$\lambda$，是正则项里leaf score的$L2$模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。</p>
<p>对于每次扩展，我们还是要枚举所有可能的分割方案，那么如何高效地枚举所有的分割呢？假设需要枚举所有$x&lt;a$这样的条件，那么对于某个特定的分割$a$我们要计算$a$左边和右边的导数和，在实际应用中如下图所示：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%881.31.20.png" alt="屏幕快照 2017-04-02 上午1.31.20"></p>
<p>我们可以发现对于所有的$a$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度与$G_L$和$G_R$。然后用上面的公式计算每个分割方案的分数就可以了。</p>
<p>但需要注意是：引入的分割不一定会使得情况变好，因为在引入分割的同时也引入新叶子的惩罚项。所以通常需要设定一个阈值，如果引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。此外在XGBoost的具体实践中，通常会设置树的深度来控制树的复杂度，避免单个树过于复杂带来的过拟合问题。</p>
<p>以上介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习的整个过程。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。</p>
<h2 id="五、QA"><a href="#五、QA" class="headerlink" title="五、QA"></a>五、QA</h2><h3 id="5-1-机器学习算法中GBDT和XGBOOST的区别有哪些？"><a href="#5-1-机器学习算法中GBDT和XGBOOST的区别有哪些？" class="headerlink" title="5.1 机器学习算法中GBDT和XGBOOST的区别有哪些？"></a>5.1 机器学习算法中GBDT和XGBOOST的区别有哪些？</h3><ul>
<li><p><strong>基分类器的选择：</strong>传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p>
</li>
<li><p><strong>二阶泰勒展开：</strong>传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。</p>
</li>
<li><p><strong>方差-方差权衡：</strong>XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数$T$、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。</p>
</li>
<li><p><strong>Shrinkage（缩减）：</strong>相当于学习速率（xgboost中的$\epsilon$）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-02%20%E4%B8%8A%E5%8D%8810.11.21.png" alt="屏幕快照 2017-04-02 上午10.11.21"></p>
</li>
</ul>
<ul>
<li><strong>列抽样（column subsampling）：</strong>XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。</li>
<li><strong>缺失值处理：</strong>XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。</li>
<li><strong>XGBoost工具支持并行：</strong>Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第$t$次迭代的损失函数里包含了前面$t-1$次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
<li><strong>线程缓冲区存储：</strong>按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。</li>
<li><strong>可并行的近似直方图算法：</strong>树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。</li>
</ul>
<h3 id="5-2-为什么在实际的-kaggle-比赛中-gbdt-和-random-forest-效果非常好？"><a href="#5-2-为什么在实际的-kaggle-比赛中-gbdt-和-random-forest-效果非常好？" class="headerlink" title="5.2 为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？"></a>5.2 为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？</h3><p>转载自<a href="https://www.zhihu.com/question/51818176/answer/127706751?wechatShare=1" target="_blank" rel="noopener">知乎</a><br>这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？</p>
<p>通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。</p>
<p>我主要从三个方面来回答楼主这个问题。</p>
<ul>
<li><ol>
<li>理论模型 （站在 vc-dimension 的角度）</li>
</ol>
</li>
<li><ol>
<li>实际数据</li>
</ol>
</li>
<li><ol>
<li>系统的实现 （主要基于 xgboost）</li>
</ol>
</li>
</ul>
<p>通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。</p>
<h4 id="5-2-1-站在理论模型的角度"><a href="#5-2-1-站在理论模型的角度" class="headerlink" title="5.2.1 站在理论模型的角度"></a>5.2.1 站在理论模型的角度</h4><p>统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件：</p>
<ol>
<li>模型在我们的训练数据上的表现要不错，也就是trainning error 要足够小。</li>
<li>模型的vc-dimension要低。换句话说，就是模型的自由度不能太大，以防overfit.</li>
</ol>
<p>当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off.</p>
<p>好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis.</p>
<p>在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest).  现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？</p>
<p>区别就在于 “模型的可控性”。先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR  这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。</p>
<p>我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree.  所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfitting.</p>
<h4 id="5-2-2-站在数据的角度"><a href="#5-2-2-站在数据的角度" class="headerlink" title="5.2.2 站在数据的角度"></a>5.2.2 站在数据的角度</h4><p>除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。</p>
<p>除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。</p>
<h4 id="5-2-3-站在系统实现的角度"><a href="#5-2-3-站在系统实现的角度" class="headerlink" title="5.2.3 站在系统实现的角度"></a>5.2.3 站在系统实现的角度</h4><p>除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征：</p>
<ol>
<li>正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。</li>
<li>系统具有灵活、深度的定制功能。</li>
<li>系统简单易用。</li>
<li>系统具有可扩展性, 可以从容处理更大的数据。</li>
</ol>
<p>到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。<br>在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等</p>
<p>在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。</p>
<p>最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。</p>
<p>有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。</p>
<p>综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> XgBoost </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（7）：GBDT]]></title>
      <url>/2017/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT/</url>
      <content type="html"><![CDATA[<h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。</p>
<a id="more"></a>
<p>GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。自算法的诞生之初，它就和SVM一起被认为是泛化能力（generalization）较强的算法。近些年来更因为被用于构建搜索排序的机器学习模型而引起广泛的关注。它最早见于yahoo，后被广泛应用在搜索排序、点击率预估上。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。</p>
<p>除此之外，GBDT还是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，而且相比较于其他算法还有着出众的准确率，如此优异的性能也让GBDT收获了机器学习领域的“屠龙刀”这一赞誉。</p>
<p>本文首先介绍GBDT中的DT，即回归树，这是它的基础算法；然后叙述提升树，它是以决策树为基函数的提升方法；接着介绍GBDT中的GB，即梯度提升；最后导出GBDT算法的整个流程。</p>
<h2 id="二、Regression-Desicion-Tree：回归树"><a href="#二、Regression-Desicion-Tree：回归树" class="headerlink" title="二、Regression Desicion Tree：回归树"></a>二、Regression Desicion Tree：回归树</h2><h3 id="2-1-回归树简介"><a href="#2-1-回归树简介" class="headerlink" title="2.1 回归树简介"></a>2.1 回归树简介</h3><p>树模型也分为决策树和回归树，决策树常用来分类问题，回归树常用来预测问题。决策树常用于分类标签值，比如用户性别、网页是否是垃圾页面、用户是不是作弊；而回归树常用于预测真实数值，比如用户的年龄、用户点击的概率、网页相关程度等等。</p>
<p>回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得到一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值寻找最优切分变量和最优切分点，但衡量的准则不再是分类树中的基尼系数，而是平方误差最小化。也就是被预测错误的人数越多，平方误差就越大，通过最小化平方误差找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p>
<p>由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。</p>
<h3 id="2-2-回归树的生成"><a href="#2-2-回归树的生成" class="headerlink" title="2.2 回归树的生成"></a>2.2 回归树的生成</h3><p>首先看一个简单的回归树生成实例：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%882.32.52.png" alt="屏幕快照 2017-03-31 下午2.32.52"></p>
<p>接下来具体说说回归树是如何进行特征选择生成二叉回归树的。<br>假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集</p>
<script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}</script><p>我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下：</p>
<ul>
<li>1）选择最优切分变量$j$与切分点$s$，求解<script type="math/tex; mode=display">
\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]</script>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$</li>
<li>2）用选定的对$(j,s)$划分区域并决定相应的输出值：<script type="math/tex; mode=display">
R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\}</script><script type="math/tex; mode=display">
\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2</script></li>
<li>3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。</li>
<li>4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树：<script type="math/tex; mode=display">
f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)}</script></li>
</ul>
<h2 id="三、Boosting-Decision-Tree：提升树"><a href="#三、Boosting-Decision-Tree：提升树" class="headerlink" title="三、Boosting Decision Tree：提升树"></a>三、Boosting Decision Tree：提升树</h2><h3 id="3-1-提升树模型"><a href="#3-1-提升树模型" class="headerlink" title="3.1 提升树模型"></a>3.1 提升树模型</h3><p>提升方法采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（Boosting tree）。对分类问题构建的决策树是二叉分类树，对回归问题构建决策树是二叉回归树。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。提升树模型可以表示为决策树的加法模型：</p>
<script type="math/tex; mode=display">
f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}</script><p>其中$<br>T\left(x;\varTheta_m\right)<br>$表示决策树；$\varTheta_m$为决策树的参数；$M$为树的个数。</p>
<h3 id="3-2-提升树算法"><a href="#3-2-提升树算法" class="headerlink" title="3.2 提升树算法"></a>3.2 提升树算法</h3><p>对回归问题的提升树算法来说，给定当前模型 $f_{m-1}{(x)}$只需要简单地拟合当前模型的残差。现将回归问题的提升树算法叙述如下：</p>
<ul>
<li>1）初始化$f_0{(x)}=0$</li>
<li>2）对$m=1,2,···,M$<ul>
<li>a）计算残差<script type="math/tex; mode=display">r_{mi}=y_i-f_{m-1}\left(x_i\right)\ ,\ i=1,2,···,N</script></li>
<li>b）拟合残差$r_{mi}$学习一个回归树，得到 $T\left(x;\varTheta_m\right)$</li>
<li>c）更新$f_m{(x)}=f_{m-1}{(x)}+T(x;\varTheta_m )$</li>
</ul>
</li>
<li>3）得到回归问题提升树<script type="math/tex; mode=display">
f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}</script></li>
</ul>
<p>接下来通过训练一个用于预测年龄的模型来展现算法的运行流程</p>
<ul>
<li><p>1）首先，训练集有4个人$A,B,C,D$，它们的年龄分别是$14,16,24,26$，其中$A,B$分别是高一和高三学生；$C,D$分别是应届毕业生和工作两年的员工，可用于分枝的特征包括上网时长、购物金额、上网时段和对百度知道的使用方式。如果是一棵传统的回归决策树来训练，会得到下图所示结果：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%884.29.09.png" alt="屏幕快照 2017-03-31 下午4.29.09"></p>
</li>
<li><p>2）但是如果用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只限定两棵树。我们会得到如下所示结果：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%884.30.34.png" alt="屏幕快照 2017-03-31 下午4.30.34"><br>第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。<br>第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。<br>第二棵树只有两个值1和-1，直接可分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。</p>
</li>
<li><p>3）将两棵回归树预测结果进行汇总，解释如下：</p>
<ul>
<li>A：14岁高一学生；购物较少；经常问学长问题；预测年龄A = 15 – 1 = 14</li>
<li>B：16岁高三学生；购物较少；经常被学弟问问题；预测年龄B = 15 + 1 = 16</li>
<li>C：24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24</li>
<li>D：26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26</li>
</ul>
</li>
</ul>
<p>对比初始的回归树与GBDT所生成的回归树，可以发现，最终的结果是相同的，那我们为什么还要使用GBDT呢？</p>
<ul>
<li>答案就是对模型过拟合的考虑。过拟合是指为了让训练集精度更高，学到了很多“仅在训练集上成立的规律”，导致换一个数据集后，当前规律的预测精度就不足以使人满意了。毕竟，在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。</li>
<li>在上面这个例子中，初始的回归树为达到100%精度使用了3个特征（上网时长、时段、网购金额），但观察发现，分枝“上网时长&gt;1.1h”很显然过拟合了，不排除恰好A上网1.5h, B上网1小时，所以用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的。</li>
<li>而在GBDT中，两棵回归树仅使用了两个特征（购物金额与对百度知道的使用方式）就实现了100%的预测精度，其分枝依据更合乎逻辑（当然这里是相比较于上网时长特征而言），算法在运行中也体现了“如无必要，勿增实体”的奥卡姆剃刀原理。</li>
</ul>
<h3 id="3-3-提升树实例"><a href="#3-3-提升树实例" class="headerlink" title="3.3 提升树实例"></a>3.3 提升树实例</h3><p>下表为训练数据，$x$的取值范围为区间$[0.5,10.5]$，$y$的取值范围为区间$[5.0,10.0]$，学习这个回归问题的提升树模型，考虑只用二叉树作为基函数：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%883.10.55.png" alt="屏幕快照 2017-03-31 下午3.10.55"><br>（1）步骤一：求$f_1(x)$即回归树$T_1(x)$</p>
<ul>
<li><p>1）首先通过以下优化问题：</p>
<script type="math/tex; mode=display">
\min_s\left[\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}\right]</script><p>求解训练数据的切分点$s$：</p>
<script type="math/tex; mode=display">
R_1=\left\{x|x\le s\right\},R_2=\left\{x|x>s\right\}</script><p>容易求得在$R_1$，$R_2$内部使平方误差达到最小值的$c_1,c_2$为</p>
<script type="math/tex; mode=display">
c_1=\frac{1}{N_1}\sum_{x_i\in R_1}{y_i}\\ ,\\ c_2=\frac{1}{N_2}\sum_{x_i\in R_2}{y_i}</script><p>这里$N_1,N_2$是$R_1,R_2$的样本点数。</p>
</li>
<li><p>2）具体地，求解训练数据的切分点。根据所给数据，考虑如下切分点：</p>
<script type="math/tex; mode=display">
1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</script><p>对各切分点，不难求出相应的$R_1,R_2,c_1,c_2$及</p>
<script type="math/tex; mode=display">
m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}</script><p>例如，当$s=2.5$时，</p>
<script type="math/tex; mode=display">R_1=\{1,2\}，R_2=\{3,4,···,9,10\}，c_1=5.63,c_2=7.73</script><script type="math/tex; mode=display">
m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}=12.07</script><p>遍历所有的$s$，计算$m(s)$，结果列表如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%883.34.57.png" alt="屏幕快照 2017-03-31 下午3.34.57">可知当$s=6.5$时$m(s)$达到最小值，此时</p>
<script type="math/tex; mode=display">R_1=\{1,2,···,6\},R_2=\{7,8,9,10\},c_1=6.24,c_2=8.91</script><p>所以回归树$T_1(x)$为</p>
<script type="math/tex; mode=display">
T_2\left(x\right)=\left\{\begin{matrix}
  6.24&        x<6.5\\
  8.91&        x\geqslant 6.5\\
\end{matrix}\right.</script><script type="math/tex; mode=display">f_1\left(x\right)=T_1\left(x\right)</script><p>用$f_1(x)$拟合训练数据的残差，表中$r_{2i}=y_i-f_1(x_i)$<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%883.44.51.png" alt="屏幕快照 2017-03-31 下午3.44.51"><br>平方损失误差为：</p>
<script type="math/tex; mode=display">
L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=1.93</script></li>
</ul>
<p>（2）步骤二：求$T_2(x)$，方法与求$T_1(x)$一样，只是拟合的数据是上一步得到的残差，可以得到：</p>
<script type="math/tex; mode=display">
T_2\left(x\right)=\left\{\begin{matrix}
    -0.52&        x<3.5\\
    0.22&        x\geqslant 3.5\\
\end{matrix}\right.</script><script type="math/tex; mode=display">
f_2\left(x\right)=f_1\left(x\right)+T_2\left(x\right)=\left\{\begin{matrix}
    5.72&        x<3.5\\
    6.46&        3.5\le x<6.5\\
    9.13&        x\geqslant 6.5\\
\end{matrix}\right.</script><p>用$f_2(x)$拟合训练数据的平方损失误差是</p>
<script type="math/tex; mode=display">
L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=0.79</script><p>继续迭代</p>
<script type="math/tex; mode=display">
T_3\left(x\right)=\left\{\begin{matrix}
    0.15&        x<6.5\\
    -0.22&        x\geqslant 6.5\\
\end{matrix}\right.------L\left(y,f_3\left(x\right)\right)=0.47</script><script type="math/tex; mode=display">
T_4\left(x\right)=\left\{\begin{matrix}
    -0.16&        x<4.5\\
    0.11&        x\geqslant 4.5\\
\end{matrix}------L\left(y,f_4\left(x\right)\right)=0.30\right.</script><script type="math/tex; mode=display">
T_5\left(x\right)=\left\{\begin{matrix}
    0.07&        x<6.5\\
    -0.11&        x\geqslant 6.5\\
\end{matrix}\right.------L\left(y,f_5\left(x\right)\right)=0.23</script><script type="math/tex; mode=display">
T_6\left(x\right)=\left\{\begin{matrix}
    -0.15&        x<2.5\\
    0.04&        x\geqslant 2.5\\
\end{matrix}\right.</script><script type="math/tex; mode=display">
f_6\left(x\right)=f_5\left(x\right)+T_6\left(x\right)=T_1\left(x\right)+···+T_5\left(x\right)+T_6\left(x\right)</script><script type="math/tex; mode=display">
=\left\{\begin{matrix}
    5.63&        x<2.5\\
    5.82&        2.5\le x<3.5\\
    6.56&        3.5\le x<4.5\\
    6.83&        4.5\le x<6.5\\
    8.95&        x\geqslant 6.5\\
\end{matrix}\right.</script><p>用$f_6(x)$拟合训练数据的平方损失误差是</p>
<script type="math/tex; mode=display">
L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=0.17</script><p>假设此时已满足误差要求，那么$f(x)=f_6(x)$即为所求提升树。</p>
<h2 id="四、Gradient-Boosting-Decision-Tree：梯度提升决策树"><a href="#四、Gradient-Boosting-Decision-Tree：梯度提升决策树" class="headerlink" title="四、Gradient Boosting Decision Tree：梯度提升决策树"></a>四、Gradient Boosting Decision Tree：梯度提升决策树</h2><h3 id="4-1-GBDT简介"><a href="#4-1-GBDT简介" class="headerlink" title="4.1 GBDT简介"></a>4.1 GBDT简介</h3><p>提升树利用加法模型与向前分布算法实现学习的优化过程，即是通过迭代得到一系列的弱分类器，进而通过不同的组合策略得到相应的强学习器。在GBDT的迭代中，假设前一轮得到的抢学习器为$f_{t-1}{(x)}$，对应的损失函数则为$L(y,f_{t-1}{(x)})$。因此新一轮迭代的目的就是找到一个弱分类器$h_t{(x)}$，使得损失函数$L(y,f_{t-1}{(x)}+h_t{(x)})$达到最小。</p>
<p>因此问题的关键就在于对损失函数的度量，这也正是难点所在。当损失函数是平方损失和指数损失时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化没那么容易，如绝对值损失函数和Huber损失函数。常见的损失函数及其梯度如下表所示：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%885.37.47.png" alt="屏幕快照 2017-03-31 下午5.37.47">那我们怎么样才能找到一种通用的拟合方法呢？</p>
<p>针对这一问题，Freidman提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值</p>
<script type="math/tex; mode=display">
-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}</script><p>作为回归问题中提升树算法的残差的近似值（与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例，拟合一个回归树），这就是梯度提升决策树。</p>
<h3 id="4-2-GBDT算法步骤"><a href="#4-2-GBDT算法步骤" class="headerlink" title="4.2 GBDT算法步骤"></a>4.2 GBDT算法步骤</h3><p>算法步骤如下：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%885.44.57.png" alt="屏幕快照 2017-03-31 下午5.44.57"><br>接下来对上图中的算法步骤进行详细解释：</p>
<ul>
<li>1）初始化弱分类器，估计使损失函数极小化的一个常数值，此时树仅有一个根结点<script type="math/tex; mode=display">
f_0\left(x\right)=arg\min_c\sum_{i=1}^N{L\left(y_i,c\right)}</script></li>
<li>2）对迭代轮数$1,2,···,M$<ul>
<li>a）对$i=1,2,···,N$，计算损失函数的负梯度值在当前模型的值，将它作为残差的估计。即<script type="math/tex; mode=display">
r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}</script>对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。    </li>
<li>b）对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj}，j=1,2,···,J$</li>
<li>c）对$j=1,2,···,J$计算<script type="math/tex; mode=display">
c_{mj}=arg\min_c\sum_{x_i\in R_{mj}}{L\left(y_i,f_{m-1}\left(x_i\right)+c\right)}</script>即利用线性搜索估计叶结点区域的值，使损失函数极小化</li>
<li>d）更新回归树<script type="math/tex; mode=display">
f_m\left(x\right)=f_{m-1}\left(x\right)+\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}</script></li>
</ul>
</li>
<li>3）得到输出的最终模型<script type="math/tex; mode=display">
\hat{f}\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}}</script></li>
</ul>
<h2 id="五、关于GBDT的一些问题"><a href="#五、关于GBDT的一些问题" class="headerlink" title="五、关于GBDT的一些问题"></a>五、关于GBDT的一些问题</h2><h3 id="5-1-为什么xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？"><a href="#5-1-为什么xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？" class="headerlink" title="5.1 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？"></a>5.1 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？</h3><p><a href="https://www.zhihu.com/question/45487317" target="_blank" rel="noopener">https://www.zhihu.com/question/45487317</a></p>
<h2 id="六、参考资料"><a href="#六、参考资料" class="headerlink" title="六、参考资料"></a>六、参考资料</h2><p><a href="http://www.jianshu.com/p/005a4e6ac775" target="_blank" rel="noopener"><code>GBDT：梯度提升决策树</code></a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> GBDT </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（6）：AdaBoost]]></title>
      <url>/2017/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost/</url>
      <content type="html"><![CDATA[<h2 id="一、集成学习"><a href="#一、集成学习" class="headerlink" title="一、集成学习"></a>一、集成学习</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>所谓集成学习（ensemble learning），是指通过构建多个弱学习器，然后结合为一个强学习器来完成分类任务。并相较于弱分类器而言，进一步提升结果的准确率。严格来说，集成学习并不算是一种分类器，而是一种学习器结合的方法。</p>
<a id="more"></a>
<p>下图显示了集成学习的整个流程：首次按产生一组“个体学习器”，这些个体学习器可以是同质的（homogeneous）（例如全部是决策树），这一类学习器被称为基学习器（base learner），相应的学习算法称为“基学习算法”；集成也可包含不同类型的个体学习器（例如同时包含决策树和神经网络），这一类学习器被称为“组件学习器”（component learner）。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-30%20%E4%B8%8A%E5%8D%8810.25.48.png" alt="屏幕快照 2017-03-30 上午10.25.48"></p>
<p>集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，它基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，直观一点理解，就是我们平时所说的“三个臭皮匠，顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。</p>
<h3 id="1-2-集成学习的条件"><a href="#1-2-集成学习的条件" class="headerlink" title="1.2 集成学习的条件"></a>1.2 集成学习的条件</h3><p>当然，这种通过集成学习来提高学习器（这里特指分类器）的整体泛化能力也是有条件的：</p>
<ul>
<li>首先，分类器之间应该具有差异性，即要有“多样性”。很容易理解，如果使用的是同一个分类器，那么集成起来的分类结果是不会有变化的。‘</li>
<li>其次，每个个体分类器的分类精度必须大于0.5，如果$p&lt;0.5$那么随着集成规模的增加，分类精度会下降；但如果是大于0.5的话，那么最后最终分类精度是可以趋于1的。</li>
</ul>
<p>因此，要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。</p>
<h3 id="1-3-集成学习的分类"><a href="#1-3-集成学习的分类" class="headerlink" title="1.3 集成学习的分类"></a>1.3 集成学习的分类</h3><p>当前，我们可以立足于通过处理数据集生成差异性分类器，即在原有数据集上采用抽样技术获得多个训练数据集来生成多个差异性分类器。根据个体学习器的生成方式，目前集成学习方法大致可分为两大类：第一类是个体学习器之间存在强依赖关系、必须串行生成的序列化方法，这种方法的代表是“Boosting”；第二类是个体学习器间不存在强依赖关系、可同时生成的并行化方法，它的代表是“Bagging”和“Random Forest”</p>
<ul>
<li><strong>Bagging</strong>：通过对原数据进行有放回的抽取，构建出多个样本数据集，然后用这些新的数据集训练多个分类器。因为是有放回的采用，所以一些样本可能会出现多次，而其他样本会被忽略。该方法是通过降低基分类器方法来改善泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会降低。</li>
<li><strong>Boosting</strong>：提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。</li>
<li><strong>Bagging与Boosting的区别：</strong><ul>
<li>二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。</li>
<li>bagging是减少variance，而boosting是减少bias。Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行。</li>
</ul>
</li>
</ul>
<h2 id="二、AdaBoost算法"><a href="#二、AdaBoost算法" class="headerlink" title="二、AdaBoost算法"></a>二、AdaBoost算法</h2><h3 id="2-1-AdaBoost算法思想"><a href="#2-1-AdaBoost算法思想" class="headerlink" title="2.1 AdaBoost算法思想"></a>2.1 AdaBoost算法思想</h3><p>对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确地分类规则（强分类器）容易得多。提升算法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p>
<p>这样，对提升方法来说，有两个问题需要回答：一是在每一轮如果改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。对于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题就被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。</p>
<p>AdaboostBoost的算法的框架如下图所示<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-30%20%E4%B8%8A%E5%8D%8811.23.58.png" alt="屏幕快照 2017-03-30 上午11.23.58"><br>具体来说，整个AdaBoost算法包括以下三个步骤：</p>
<ul>
<li>1）<strong>初始化训练样本的权值分布。</strong>如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：$1/N$。</li>
<li>2）<strong>训练弱分类器。</strong>具体训练过程中，如果某个样本已经被准确地分类，那么在构造下一个训练集中，它的权值就会被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本被用于训练下一个分类器，整个训练过程如果迭代地进行下去，使得分类器在迭代过程中逐步改进。</li>
<li>3）将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中权重较大，否则较小。得到最终分类器。</li>
</ul>
<h3 id="2-2-AdaBoost算法流程"><a href="#2-2-AdaBoost算法流程" class="headerlink" title="2.2 AdaBoost算法流程"></a>2.2 AdaBoost算法流程</h3><p>现在叙述AdaBoost算法。假定给定一个二类分类的训练数据集</p>
<script type="math/tex; mode=display">T=\{(x_1,y_1),(x_2,y_2),···,(x_n,y_n)\}</script><p>其中$y_i$属于二分类的标记组合，即$y_i\in\{+1,-1\}$，AdaBoost算法利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成一个强分类器。</p>
<p><strong>步骤一：</strong>首先，初始化训练数据的权值分布。假设每一个训练样本最开始时都被赋予相同的权值：$1/N$，即每个训练样本在基本分类器的学习中作用相同，这一假设保证步骤一能够在原始数据上学习基本分类器$G_1{(x)}$，数学化的语言表示为：</p>
<script type="math/tex; mode=display">
D_1=\left(w_{11},w_{12},···,w_{1i},···,w_{1N}\right)\ ,\ w_{1i}=\frac{1}{N}\ ,i=1,2,···,N</script><p><strong>步骤二：</strong>AdaBoost反复学习基本分类器，在每一轮$m=1,2,···,M$顺次执行下列操作：</p>
<ul>
<li>1）使用当前权值分布为$D_m$的训练数据集，学习得到基分类<script type="math/tex; mode=display">
G_m\left(x\right):\chi\rightarrow\left\{-1,+1\right\}</script></li>
<li>2）计算上一步得到的基分类器$G_m{(x)}$在训练数据集上的分类误差率$e_m$为<script type="math/tex; mode=display">
e_m=P\left(G_m\left(x\right)\ne y_i\right)=\frac{\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}}{\sum_{i=1}^N{w_{mi}}}=\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}</script>这里$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum_{i=1}^N{w_{mi}=1}$。这表明，$G_m{(x)}$在加权的训练数据集上的分类误差率是被$G_m{(x)}$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m{(x)}$的分类误差率的关系。</li>
<li>3）计算$G_m$前面的权重系数$a_m$，该系数表示$G_m$在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类器中所占的权值，系数计算公式如下：<script type="math/tex; mode=display">
a_m=\frac{1}{2}\log\frac{1-e_m}{e_m}</script>这里的对数是自然对数，由表达式可知，当$e_m≤\frac{1}{2}$时，$a_m≥0$，并且$a_m$随着$e_m$的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而$e_m≥\frac{1}{2}$则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。</li>
<li>4）更新训练数据集的权值分布为下一轮作准备<script type="math/tex; mode=display">
D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,</script>其中<script type="math/tex; mode=display">
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N</script>我们也可以写成：<script type="math/tex; mode=display">
w_{m+1,i}=\left\{\begin{matrix}
  \frac{w_{mi}}{Z_m}e^{-a_m}\ ,&        G_m\left(x_i\right)=y_i\\
  \frac{w_{mi}}{Z_m}e^{a_m}\ ,&        G_m\left(xi\right)\ne y_i\\
\end{matrix}\right.</script>由此可知，被基本分类器$G_m{(x)}$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值$e^{2a_m}=\frac{e_m}{1-e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。<br>这里我们还引入了一个规范化因子，它的作用在于使$D_{m+1}$成为一个概率分布。具体公式为<script type="math/tex; mode=display">Z_m=\sum_{i=1}^N{w_{mi}}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N</script>重复步骤二中的1至4步骤，得到一系列的权重参数$a_m$和基分类器$G_m$。</li>
</ul>
<p>步骤三：将上一步得到的基分类器根据权重参数线性组合</p>
<script type="math/tex; mode=display">
f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}</script><p>得到最终分类器$G_{(x)}$</p>
<script type="math/tex; mode=display">
G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)</script><p>线性组合$f(x)$实现了$M$个基本分类器的加权表决。系数$a_m$表示了基本分类器$G_m{(x)}$的重要性，这里，所有的$a_m$之和并不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p>
<h3 id="2-3-AdaBoost算法的一个实例"><a href="#2-3-AdaBoost算法的一个实例" class="headerlink" title="2.3 AdaBoost算法的一个实例"></a>2.3 AdaBoost算法的一个实例</h3><p>下图为给定的训练样本，假设$Y\in \{+1,-1\}$，且弱分类器由$x<v$或$x>v$产生（$v$为阈值，目的在于使分类器在训练样本上的分类误差率最低），接下来我们就要使用AdaBoost算法得到一个强分类器。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-30%20%E4%B8%8B%E5%8D%882.03.20.png" alt="屏幕快照 2017-03-30 下午2.03.20"><br> 首先，初始化训练数据的权值分布，得到：</v$或$x></p>
<script type="math/tex; mode=display">D_1=(w_{11},w_{12},w_{1,10})    ,  w_{1i}=\frac{1}{10},i=1,2,····,10</script><p> 在此基础上，开始M轮迭代。<br> 根据X和Y的对应关系，要把这10个数据分为两类，一类是1，一类是-1，根据数据的特点发现：$（0，1，2，6，7，8）$对应的类是1，$(3,4,5,9)$对于的类是-1，抛开孤独的9不说，$(0,1,2),(3,4,5),(6,7,8)$这是3类不同的数据，分别对应的类是$(1,-1,1)$,直观上推测可知，可以找到对应的数据分界点，比如$2.5、5.5、8.5$，将这几类数据分成两类。</p>
<p>1.第一次迭代（m=1）:</p>
<ul>
<li>1）在第一次迭代时，已知$w_{1i}=\frac{1}{10}$，经过计算可得：在权值分布为$D_1$的训练数据上，阈值$v$取2.5或8.5时分类误差率为0.3，取5.5时分类误差率为0.4，遵循分类误差率最低原则，从2.5或8.5 中任意选取一个阈值，这里选取2.5，故基本分类器为<script type="math/tex; mode=display">G_1\left(x\right)=\left\{\begin{matrix}1&x<2.5\\-1&x>2.5\\\end{matrix}\right.</script><ul>
<li>2）$G_1{(x)}$在训练集上的误差率：<script type="math/tex; mode=display">e_1=P(G_1{(x_i)≠y_i})=0.3</script></li>
<li>3) 根据$e_1$计算得到$G_1{(x)}$的系数：<script type="math/tex; mode=display">a_1=\frac{1}{2}\log\frac{1-e_1}{e_1}=0.4236</script>这个系数就代表$G_1{(x)}$在最终的分类函数中所占的权值。</li>
<li>4）更新训练数据的权值分布，用于下一轮迭代<script type="math/tex; mode=display">
D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,</script>其中<script type="math/tex; mode=display">
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N</script>由此得到$D_2=(0.0715,0.0715,0.715,0.0715,0.0715,0.715,0.166,0.166,0.166,0.0715)$根据$D_2$可知，分对的样本权重由0.1下降到了0.0715，分错的样本$(6,7,8)$的权值由0.1上升至0.166。<br>此时分类函数为$f_1{(x)}=0.4236G_1{(x)}$，第一个分类器$sign[f_1{(x)}]$在训练样本上有三个误分类点（第一轮的误分类点即第一个基分类器的误分类点）。</li>
</ul>
</li>
</ul>
<p>2.第二次迭代（m=2）:</p>
<ul>
<li>1）在上一轮迭代中，我们获知了新一轮的权重分布$D_2$，在此基础上，经过计算可得，阈值$v$是8.5时分类误差率最低，因此第二个基本分类器为<script type="math/tex; mode=display">G_2\left(x\right)=\left\{\begin{matrix}1&x<8.5\\-1&x>8.5\\\end{matrix}\right.</script></li>
<li>2）误差率：<script type="math/tex; mode=display">e_2=P(G_2{(x_i)≠y_i})=0.0715×3=0.2143</script></li>
<li>3）$G_2{(x)}$的系数为:<script type="math/tex; mode=display">a_2=\frac{1}{2}\log\frac{1-e_2}{e_2}=0.6496</script></li>
<li>4）更新训练样本的权值分布，得到<br>$D_3=(0.0455,0.0455,0.0455,0.1667，0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，相较于$D_2$，被分错的样本3，4，5的权值变大，其他被分对的样本的权值变小。经过第二轮迭代后，分类函数为$f_2{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}$，第二个分类器为$sign[f_2{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}]$。将10个样本点依次带入到第二个分类器中，可得到此时依然有着3个误分类点$(3,4,5)$，为此需要进行下一轮迭代。</li>
</ul>
<p>3.第三次迭代（m=3）:</p>
<ul>
<li>1）在上一轮迭代中，我们获知了新一轮的权重分布$D_3$，在此基础上，经过计算可得，阈值$v$是5.5时分类误差率最低，因此第三个基本分类器为<script type="math/tex; mode=display">
G_3\left(x\right)=\left\{\begin{matrix}{}
  1&        x>5.5\\
  -1&        x<5.5\\
\end{matrix}\right.</script></li>
<li>2）误差率:<script type="math/tex; mode=display">e_3=P(G_3{(x_i)≠y_i})=0.0455×4=0.1820</script></li>
<li>3）$G_3{(x)}$的系数为<script type="math/tex; mode=display">a_3=\frac{1}{2}\log\frac{1-e_3}{e_3}=0.7514</script></li>
<li>4）更新训练样本的权值分布，得到<br>$D_4=（0.125，0.125，0.125，0.102，0.102，0.102，0.065，0.065，0.065，0.125）$此时分类函数为$f_3{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}+0.7514G_3{(x)}$，第三个分类器为$sign[f_3{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}+0.7514G_3{(x)}]$，同样将10个样本点依次代入第三个分类器中，可发现没有误分类点，全部样本点已正确分类，因此停止迭代。算法运行完毕。</li>
</ul>
<p>从上述过程可以发现，如果某些样本被分错，那么它们在下一轮迭代中的权重将会被增大，同时，其它被分错的样本在下一轮迭代中的权值将会被减小。就这样，分错样本权值增大，分对样本权值变小。而每一轮的迭代中，总是选取让误差率最低的阈值来设计基本分类器，因此误差率$e$(所有被$G_m{(x)}$误分类样本的权值之和)在迭代中将不断降低。</p>
<h2 id="三、AdaBoost算法的训练误差分析"><a href="#三、AdaBoost算法的训练误差分析" class="headerlink" title="三、AdaBoost算法的训练误差分析"></a>三、AdaBoost算法的训练误差分析</h2><p>AdaBoost最基本的性质是它能在学习过程中不断减小训练误差，即在训练数据集上的分类误差率。关于这个问题有下面的定理</p>
<h3 id="3-1-AdaBoost的训练误差界"><a href="#3-1-AdaBoost的训练误差界" class="headerlink" title="3.1 AdaBoost的训练误差界"></a>3.1 AdaBoost的训练误差界</h3><p>首先，给出AdaBoost的训练误差界的定理：</p>
<script type="math/tex; mode=display">
\frac{1}{N}\sum_{i=1}^N{I\left(G\left(x_i\ne y_i\right)\right)}\le\frac{1}{N}\sum_i{\exp\left(-y_if\left(x_i\right)\right)=\prod_m{Z_m}}</script><p>其中</p>
<script type="math/tex; mode=display">
G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)</script><script type="math/tex; mode=display">
Z_m=\sum_{i=1}^N{w_{mi}\exp\left(-a_my_iG_m\left(x_i\right)\right)}</script><script type="math/tex; mode=display">
f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}</script><p>证明如下</p>
<ul>
<li>1）当$G(x_i)≠y_i$时，$y_if(x_i)&lt;0$，因而$exp(-y_if(x_i))≥1$。由此直接推导出前半部分。</li>
<li>2）后半部分的推导需要用到<script type="math/tex; mode=display">
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N</script><script type="math/tex; mode=display">
w_{mi}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right)=Z_mw_{m+1,i}</script>推导如下：<script type="math/tex; mode=display">
\frac{1}{N}\sum_i{\exp\left(-y_if\left(x_i\right)\right)}</script><script type="math/tex; mode=display">
=\frac{1}{N}\sum_i{\exp\left(-\sum_{m=1}^M{a_my_iG_m\left(x_i\right)}\right)}</script><script type="math/tex; mode=display">
=\sum_i{w_{1i}\prod_{m=1}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}</script><script type="math/tex; mode=display">
=Z_1\sum_i{w_{2i}\prod_{m=2}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}</script><script type="math/tex; mode=display">
=Z_1Z_2\sum_i{w_{3i}\prod_{m=3}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}</script><script type="math/tex; mode=display">
=Z_1Z_2···Z_{M-1}\sum_i{w_{Mi}\exp\left(-a_My_iG_m\left(x_i\right)\right)}</script><script type="math/tex; mode=display">
=\prod_{m=1}^M{Z_m}</script>这一定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。</li>
</ul>
<h3 id="3-2-二类分类问题AdaBoost的训练误差界"><a href="#3-2-二类分类问题AdaBoost的训练误差界" class="headerlink" title="3.2 二类分类问题AdaBoost的训练误差界"></a>3.2 二类分类问题AdaBoost的训练误差界</h3><p>对于二分类问题，有如下结果：</p>
<script type="math/tex; mode=display">
\prod_{m=1}^M{Z_m}=\prod_{m=1}^M{\left[2\sqrt{e_m\left(1-e_m\right)}\right]}=\prod_{m=1}^M{\sqrt{\left(1-4\gamma_{m}^{2}\right)}}\le\exp\left(-2\sum_{m=1}^M{\gamma_{m}^{2}}\right)</script><p>这里</p>
<script type="math/tex; mode=display">
\gamma_m=\frac{1}{2}-e_m</script><p>证明：</p>
<ul>
<li>1）首先，等式部分<script type="math/tex; mode=display">
Z_m=\sum_{i=1}^N{w_{mi}\exp\left(-a_my_iG_m\left(x_i\right)\right)}</script><script type="math/tex; mode=display">
=\sum_{y_i=G_m\left(x_i\right)}{w_{mi}\exp\left(-a_m\right)}+\sum_{y_i\ne G_m\left(x_i\right)}{w_{mi}\exp\left(a_m\right)}</script><script type="math/tex; mode=display">
=\left(1-e_m\right)e^{-a_m}+e_me^{a_m}</script><script type="math/tex; mode=display">
=2\sqrt{e_m\left(1-e_m\right)}</script><script type="math/tex; mode=display">
=\sqrt{1-4\gamma_{m}^{2}}</script></li>
<li>2）不等式部分，先由$e^x$和$\sqrt{(1-x)}$在$x=0$处的泰勒展开式<script type="math/tex; mode=display">
e^x=1+x+\frac{1}{2}x^2+···+\frac{1}{n!}x^n+O\left(x^n\right)</script><script type="math/tex; mode=display">
\left(1-x\right)^{\frac{1}{2}}=1-\frac{1}{2}x-\frac{1}{8}x^2+···+O\left(x^n\right)</script>推出不等式<script type="math/tex; mode=display">
\sqrt{\left(1-4\gamma_{m}^{2}\right)}\le\exp\left(-2\gamma_{m}^{2}\right)</script>进而得到。</li>
</ul>
<h3 id="3-3-推论"><a href="#3-3-推论" class="headerlink" title="3.3 推论"></a>3.3 推论</h3><p>由上述两个定理推出，如果存在$\gamma &gt;0$，对所有$m$有$\gamma_m\geqslant\gamma$，则</p>
<script type="math/tex; mode=display">
\frac{1}{N}\sum_{i=1}^N{I\left(G\left(x_i\right)\ne y_i\right)}\le\exp\left(-2M\gamma^2\right)</script><p>这个结论表明在此条件下Adaboost的训练误差是以指数速率下降的。</p>
<h2 id="四、AdaBoost算法的数学推导"><a href="#四、AdaBoost算法的数学推导" class="headerlink" title="四、AdaBoost算法的数学推导"></a>四、AdaBoost算法的数学推导</h2><p>AdaBoost算法还有另一个解释，AdaBoost算法可以被认为模型是加法模型，损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。</p>
<h3 id="4-1-前向分布算法"><a href="#4-1-前向分布算法" class="headerlink" title="4.1 前向分布算法"></a>4.1 前向分布算法</h3><p>在推导之前，先敲定几个概念：</p>
<ul>
<li>加法模型（additive model）:<script type="math/tex; mode=display">
f\left(x\right)=\sum_{m=1}^M{\beta_mb\left(x;\gamma_m\right)}</script>其中$b\left(x;\gamma_m\right)$为基函数，$\beta_m$为基函数的系数。</li>
<li>损失函数极小化：在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小<script type="math/tex; mode=display">
\underset{\beta_m,\gamma_m}{\min}\sum_{i=1}^N{L\left(y_i,\sum_{m=1}^M{\beta_mb\left(x_i;\gamma_m\right)}\right)}</script></li>
<li>前向分布算法（forward stagewise algorithm）：该算法的基本思路为：由于学习的是加法模型，如果可以从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（即损失函数极小化表达式），那么就可以简化优化的复杂度。具体地，每一步只需要优化如下损失函数：<script type="math/tex; mode=display">
\underset{\beta ,\gamma}{\min}\sum_{i=1}^N{L\left(y_i,\sum_{m=1}^M{\beta b\left(x;\gamma\right)}\right)}</script></li>
</ul>
<h3 id="4-2-基于前向分布算法的AdaBoost推导"><a href="#4-2-基于前向分布算法的AdaBoost推导" class="headerlink" title="4.2 基于前向分布算法的AdaBoost推导"></a>4.2 基于前向分布算法的AdaBoost推导</h3><p>前向分布算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器$f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}$，它由基本分类器$G_m{(x)}$及其系数$a_m$组成。<br>前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。<br>下面证明前向分步算法的损失函数是指数函数$<br>L\left(y,f\left(x\right)\right)=\exp\left[-yf\left(x\right)\right]<br>$时，其学习的具体操作等价于AdaBoost算法学习的具体操作。</p>
<p>假设经过$m-1$轮迭代前向分步算法已经得到$f_{m-1}{(x)}$：</p>
<script type="math/tex; mode=display">
f_{m-1}\left(x\right)=f_{m-2}\left(x\right)+a_{m-1}G_{m-1}\left(x\right)</script><script type="math/tex; mode=display">
=a_1G_1\left(x\right)+a_2G_2\left(x\right)+···+a_{m-1}G_{m-1}\left(x\right)</script><p>在$m$轮迭代得到$a_m，G_m{(x)}和f_m{(x)}$，表示为</p>
<script type="math/tex; mode=display">
f_m\left(x\right)=f_{m-1}\left(x\right)+a_mG_m\left(x\right)</script><p>此时参数$a_m$和$G_m{(x)}$均未知。因此，我们的目标是要得到最小化损失函数，通过最小化损失函数来得到模型中所需要的参数。而在AdaBoost算法中，每一轮都需要更新样本的权值参数，故而在每一轮的迭代中需要加工损失函数极小化，然后据此得到每个样例的权重更新参数。这样在每轮的迭代过程中只需要将当前基函数在训练集上的损失函数最小，最终使得$f_m{(x)}$在训练样本上的指数损失最小。<br>极小化损失函数为：</p>
<script type="math/tex; mode=display">
\left(a_m,G_m\left(x\right)\right)=arg\min_{a,G}\sum_{i=1}^N{\exp\left[-y_i\left(f_{m-1}\left(x_i\right)+aG\left(x_i\right)\right)\right]}</script><p>我们先假定$G_1{(x)},G_2{(x)},···,G_{m-1}{(x)}$和$a_1,a_2,···,a_{m-1}$已知，求解$G_m{(x)}$和$a_m$。<br>可以将上式表示为</p>
<script type="math/tex; mode=display">
\left(a_m,G_m\left(x\right)\right)=arg\min_{a,G}\sum_{i=1}^N{\bar{w}_{mi}\exp\left[-y_iaG\left(x_i\right)\right]}</script><p>其中</p>
<script type="math/tex; mode=display">
\bar{w}_{mi}=\exp\left[-y_if_{m-1}\left(x_i\right)\right]</script><p>因为$\bar{w}_{mi}$既不依赖$a$也不依赖于$G$，所以与最小化无关。但它依赖于$f_{m-1}{(x)}$，随着每一次迭代而发生改变。<br>现证使上式达到最小的$a_m$和$G_m(x)$就是AdaBoost算法所得到的$a_m$和$G_m(x)$。求解可分为两步</p>
<ul>
<li><p>1）首先，求$G_m(x)$。对任意的$a&gt;0$，使指数损失函数最小的$G(x)$由下式得到：</p>
<script type="math/tex; mode=display">G_{m}\left(x\right)=arg\min_G\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}</script><p>此分类器$G_m^*(x)$即为AdaBoost算法的基本分类器$G_m{(x)}$，因为它是使第$m$轮加权训练数据分类误差率最小的基本分类器。</p>
</li>
<li><p>2）然后，求$a_m$</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N{\bar{w}_{mi}\exp\left[-y_iaG\left(x_i\right)\right]}
=\sum_{y_i=G_m\left(x_i\right)}{\bar{w}_{mi}e^{-a}}+\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}e^a}</script><script type="math/tex; mode=display">
=e^a\sum_{y_i\ne G_m\left(x_i\right)}^{}{\bar{w}_{mi}}-e^{-a}\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}}+e^{-a}\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}}+e^{-a}\sum_{y_i=G_m\left(x_i\right)}{\bar{w}_{mi}}</script><script type="math/tex; mode=display">
=\left(e^a-e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}+e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}</script><p>将已求得的$G_m{(x)}$代入上式，对$a$求导并使导数为0，即得到使其损失函数最小的$a$。设</p>
<script type="math/tex; mode=display">
g\left(a\right)=\left(e^a-e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}+e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}</script><p>求导，并令其为0</p>
<script type="math/tex; mode=display">
\frac{\partial g\left(a\right)}{\partial a}=\left(e^a+e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}-e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}=0</script><p>得到</p>
<script type="math/tex; mode=display">
\left(e^a+e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}=e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}</script><script type="math/tex; mode=display">
\left(e^{2a}+1\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}=\sum_{i=1}^N{\bar{w}_{mi}}</script><script type="math/tex; mode=display">
\left(e^{2a}+1\right)\frac{\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}}{\sum_{i=1}^N{\bar{w}_{mi}}}=1</script><p>令</p>
<script type="math/tex; mode=display">
e_m=\frac{\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}}{\sum_{i=1}^N{\bar{w}_{mi}}}</script><p>$e_m$是分类误差率，得到</p>
<script type="math/tex; mode=display">
e_m\left(e^{2a}+1\right)=1</script><p>最终得到使损失函数最小的$a$</p>
<script type="math/tex; mode=display">
a_m^*=\frac{1}{2}\log\frac{1-e_m}{e_m}</script><p>这就是之前我们的权重系数$a_m$的来源。<br>最后来看一下每一轮样本权值的更新。由</p>
<script type="math/tex; mode=display">
f_m\left(x\right)=f_{m-1}\left(x\right)+a_mG_m\left(x\right)</script><p>以及</p>
<script type="math/tex; mode=display">
\bar{w}_{mi}=\exp\left[-y_if_{m-1}\left(x\right)\right]</script><p>可得</p>
<script type="math/tex; mode=display">
\bar{w}_{m+1,i}=\exp\left[-y_if_m\left(x\right)\right]=\exp\left[-y_i\left(f_{m-1}\left(x\right)+a_mG_m\left(x\right)\right)\right]</script><script type="math/tex; mode=display">
=\bar{w}_{mi}\exp\left[-y_ia_mG_m\left(x\right)\right]</script><p>从这一步中我们可以看到，这与开篇中所提到的AdaBoost的算法流程中的权重系数$(w_{m+1,i})$仅相差一个规范化因子$Z_m$，因而是等价的。</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> AdaBoost </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（5）：随机森林]]></title>
      <url>/2017/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
      <content type="html"><![CDATA[<h2 id="一、基本原理"><a href="#一、基本原理" class="headerlink" title="一、基本原理"></a>一、基本原理</h2><p>顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p>
<p>我们可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个特征中选择m个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。</p>
<a id="more"></a>
<p>下图为随机森林算法的示意图：<br><img src="http://omu7tit09.bkt.clouddn.com/f69ecd5658ba9557cf97e45f18d19cfd.png" alt=""></p>
<p>随机森林算法有很多优点：</p>
<ul>
<li>在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合</li>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>
<li>可生成一个$Proximities=（p_{ij}）$矩阵，用于度量样本之间的相似性： $p_{ij}=a_{ij}/N$,$ a_{ij}$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数</li>
<li>在创建随机森林的时候，对generlization error使用的是无偏估计</li>
<li>训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量</li>
<li>在训练过程中，能够检测到feature间的互相影响</li>
<li>容易做成并行化方法</li>
<li>实现比较简单</li>
</ul>
<h2 id="二、随机森林的生成"><a href="#二、随机森林的生成" class="headerlink" title="二、随机森林的生成"></a>二、随机森林的生成</h2><h3 id="2-1-生成步骤"><a href="#2-1-生成步骤" class="headerlink" title="2.1 生成步骤"></a>2.1 生成步骤</h3><p>步骤如下：</p>
<ul>
<li>1）如果训练集大小为$N$，对于每棵树而言，随机且有放回地从训练集中抽取$N$个训练样本（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本</li>
<li>2）如果每个样本的特征维度为$M$，指定一个常数$m$，且$m$&lt;$M$，随机地从$M$个特征中选取$m$个特征子集，每次树进行分裂时，从这$m$个特征中选择最优的；</li>
<li>3）每棵树都尽可能最大程度地生长，并且没有剪枝过程。</li>
</ul>
<h3 id="2-2-影响分类效果的参数"><a href="#2-2-影响分类效果的参数" class="headerlink" title="2.2 影响分类效果的参数"></a>2.2 影响分类效果的参数</h3><p>随机森林的分类效果（即错误率）与以下两个因素有关：</p>
<ul>
<li>1）森林中任意两棵树的相关性：相关性越大，错误率越大</li>
<li>2）森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<h3 id="2-3-袋外误差率"><a href="#2-3-袋外误差率" class="headerlink" title="2.3 袋外误差率"></a>2.3 袋外误差率</h3><p>如何选择最优的特征个数m，要解决这个问题，我们主要依据计算得到的袋外错误率oob error（out-of-bag error）。</p>
<p>对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1-\frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1−\frac{1}{m})^m$，当$m→∞$<br>时，$(1−\frac{1}{m})m→\frac{1}{e}≃0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。</p>
<p>袋外错误率（oob error）计算方式如下：</p>
<ul>
<li>1）对每个样本计算它作为oob样本的树对它的分类情况</li>
<li>2）以简单多数投票作为该样本的分类结果</li>
<li>3）最后用误分个数占样本总数的比率作为随机森林的oob误分率</li>
</ul>
<h2 id="三、随机采样与完全分裂"><a href="#三、随机采样与完全分裂" class="headerlink" title="三、随机采样与完全分裂"></a>三、随机采样与完全分裂</h2><p> 在建立每一棵决策树的过程中，有两点需要注意，分别是采样与完全分裂。</p>
<h3 id="3-1-随机采样"><a href="#3-1-随机采样" class="headerlink" title="3.1 随机采样"></a>3.1 随机采样</h3><p>首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。</p>
<h4 id="3-1-1-有放回抽样的解释"><a href="#3-1-1-有放回抽样的解释" class="headerlink" title="3.1.1 有放回抽样的解释"></a>3.1.1 有放回抽样的解释</h4><p>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。</p>
<h4 id="3-1-2-对Bagging的改进"><a href="#3-1-2-对Bagging的改进" class="headerlink" title="3.1.2 对Bagging的改进"></a>3.1.2 对Bagging的改进</h4><p>随机森林对Bagging的改进就在于随机采用的不同，即以下两点：</p>
<ul>
<li>1）Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；</li>
<li>2）bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Random forest效果比bagging效果好！</li>
</ul>
<h3 id="3-2-完全分裂"><a href="#3-2-完全分裂" class="headerlink" title="3.2 完全分裂"></a>3.2 完全分裂</h3><p>之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。</p>
<h2 id="四、随机森林的变体"><a href="#四、随机森林的变体" class="headerlink" title="四、随机森林的变体"></a>四、随机森林的变体</h2><p>也可以使用SVM、Logistic回归等其他分 类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。</p>
<p>比如回归问题，图中离散点为臭氧(横轴)和温度(纵轴)的关系，试拟合变化曲线，记原始数据为D，长度为N(即图中有N个离散点)</p>
<p>算法过程为：</p>
<ul>
<li>1）做100次bootstrap，每次得到的数据Di，Di的长度为N</li>
<li>2）对于每一个Di，使用局部回归(LOESS)拟合一条曲线(图 中灰色线是其中的10条曲线)</li>
<li>3）将这些曲线取平均，即得到红色的最终拟合曲线</li>
<li>4）显然，红色的曲线更加稳定，并且没有过拟合明显减弱<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-08%20%E4%B8%8B%E5%8D%882.31.19.png" alt="屏幕快照 2017-03-08 下午2.31.19"></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 随机森林 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（4）：决策树]]></title>
      <url>/2017/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>本文结合李航博士的《统计学习方法》与周志华老师的《机器学习》决策树部分，《统计学习方法》重理论的证明推导，《机器学习》注重讲解算法的特点与扩展。</p>
<a id="more"></a>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是$if-then$规则的集合，也可认为是定义在特征空间与类空间上的条件概率分布。下图是一个简单的决策树示例：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-10%20%E4%B8%8B%E5%8D%887.43.24.png" alt="屏幕快照 2017-04-10 下午7.43.24"></p>
<p>决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。</p>
<p>一个性能良好的决策树，是一个与训练数据矛盾较小的决策树，同时又具有很好地泛化能力。言外之意就是说，好的决策树不仅对训练样本有很好的分类效果，对于测试集也有较低的误差率。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。</p>
<h2 id="一、决策树模型的两种解释"><a href="#一、决策树模型的两种解释" class="headerlink" title="一、决策树模型的两种解释"></a>一、决策树模型的两种解释</h2><h3 id="1-1-决策树模型"><a href="#1-1-决策树模型" class="headerlink" title="1.1 决策树模型"></a>1.1 决策树模型</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部结点表示一个特征或属性，叶节点表示一个类。</p>
<h4 id="1-1-1-决策树与if-then规则"><a href="#1-1-1-决策树与if-then规则" class="headerlink" title="1.1.1 决策树与if-then规则"></a>1.1.1 决策树与if-then规则</h4><p>可以将决策树看成一个if-then规则的集合。即由决策树的根结点到叶节点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。</p>
<p>决策树的路径或其对应的if-then规则集合的重要性质：互斥且完备（每一个实例都被一条路径或一条规则所覆盖，且只被一条路径或一条规则所覆盖，这里的覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件）</p>
<h4 id="1-1-1-决策树与条件概率分布"><a href="#1-1-1-决策树与条件概率分布" class="headerlink" title="1.1.1 决策树与条件概率分布"></a>1.1.1 决策树与条件概率分布</h4><p>决策树还表示给定特征条件下类的条件概率分布，它定义在特征空间的一个划分。将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个单元。</p>
<p>假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(X|Y)$,各叶结点上的条件概率往往偏向于某一个类，即属于某一类的概率越大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>
<h2 id="二、特征选择"><a href="#二、特征选择" class="headerlink" title="二、特征选择"></a>二、特征选择</h2><h3 id="2-1-特征选择问题"><a href="#2-1-特征选择问题" class="headerlink" title="2.1 特征选择问题"></a>2.1 特征选择问题</h3><p>若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。</p>
<h3 id="2-2-信息增益"><a href="#2-2-信息增益" class="headerlink" title="2.2 信息增益"></a>2.2 信息增益</h3><h4 id="2-2-1-熵"><a href="#2-2-1-熵" class="headerlink" title="2.2.1 熵"></a>2.2.1 熵</h4><p>在信息论与概率统计中，熵表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为</p>
<script type="math/tex; mode=display">
P\left( X=x_i \right) =p_i,i=1,2,···,n</script><p>则随机变量$X$的熵定义为</p>
<script type="math/tex; mode=display">
H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}</script><p>若$p_i$等于0，定义$0log0=0$，熵的单位为比特或者纳特。</p>
<h4 id="2-2-2-条件熵"><a href="#2-2-2-条件熵" class="headerlink" title="2.2.2 条件熵"></a>2.2.2 条件熵</h4><p>$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望</p>
<script type="math/tex; mode=display">
H\left( Y|X \right) =\sum_{i=1}^n{p_iH\left( Y|X=x_i \right)}</script><p>经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和条件经验熵。</p>
<h4 id="2-2-3-信息增益"><a href="#2-2-3-信息增益" class="headerlink" title="2.2.3 信息增益"></a>2.2.3 信息增益</h4><p>信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即</p>
<script type="math/tex; mode=display">
g\left( D,A \right) =H\left( D \right) -H\left( D|A \right)</script><p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<p>于是我们可以应用信息增益准则来选择特征，信息增益表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。对数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。</p>
<h4 id="2-2-4-信息增益算法"><a href="#2-2-4-信息增益算法" class="headerlink" title="2.2.4 信息增益算法"></a>2.2.4 信息增益算法</h4><p>根据信息增益准则的特征选择方法为对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>
<p>在描述算法前，先对符号进行说明：<br>设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$,$k=1,2,···,K$,$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,···,a_n}$,根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,···,D_n$,$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$,即$D_{ik}=D_i\cap{C_k}$,$D_{ik}$为$D_{ik}$的样本个数。</p>
<p>具体算法步骤如下：</p>
<ul>
<li>1）计算数据集$D$的经验熵$H(D)$<script type="math/tex; mode=display">H\left( D \right) =-\sum_{k=1}^K{\frac{|C_k|}{|D|}\log _2\frac{|C_k|}{|D|}}</script></li>
<li>2）计算特征$A$对数据集$D$的经验条件熵$H(D|A)$<script type="math/tex; mode=display">H\left( D|A \right) =\sum_{i=1}^n{\frac{|D_i|}{|D|}H\left( D_i \right)}=-\sum_{i=1}^n{\frac{|D_i|}{|D|}\sum_{k=1}^K{\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}}}</script></li>
<li>3）计算信息增益<script type="math/tex; mode=display">g\left( D,A \right) =H\left( D \right) -H\left( D|A \right)</script></li>
</ul>
<h3 id="2-3-信息增益比"><a href="#2-3-信息增益比" class="headerlink" title="2.3 信息增益比"></a>2.3 信息增益比</h3><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p>
<p>信息增益比表示特征$A$对训练数据集$D$的信息增益比。$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即</p>
<script type="math/tex; mode=display">
g_R\left( D,A \right) =\frac{g\left( D,A \right)}{H_A\left( D \right)}</script><h3 id="2-4-基尼系数"><a href="#2-4-基尼系数" class="headerlink" title="2.4 基尼系数"></a>2.4 基尼系数</h3><p>分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为</p>
<script type="math/tex; mode=display">
Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}</script><p>若样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即</p>
<script type="math/tex; mode=display">
D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1</script><p>则在特征A的条件下，集合$D$的基尼指数定义为</p>
<script type="math/tex; mode=display">
Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)</script><p>基尼系数Gini(D)表示集合$D$的不确定性，表示经A=a分割后集合D的不确定性。基尼系数越大，样本集合的不确定性越大，与熵类似。</p>
<p>从下图可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-12-12%20%E4%B8%8B%E5%8D%889.39.47.png" alt="屏幕快照 2016-12-12 下午9.39.47"></p>
<h2 id="三、决策树的生成"><a href="#三、决策树的生成" class="headerlink" title="三、决策树的生成"></a>三、决策树的生成</h2><h3 id="3-1-ID3算法"><a href="#3-1-ID3算法" class="headerlink" title="3.1 ID3算法"></a>3.1 ID3算法</h3><p>ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地建构决策树。</p>
<p>其具体方法为：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。但是ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。</p>
<p>其算法步骤如下：</p>
<ul>
<li>1） 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$;</li>
<li>2）若$A=\varnothing $,则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$;</li>
<li>3） 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$;</li>
<li>4） 如果$A_g$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$</li>
<li>5） 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成树$T$,返回$T$</li>
<li>6） 对第i个子结点，以$D_i$为训练集,以$A-{A_g}$为特征集，递归地调用（1）~（5），得到子树$T_i$，返回$T$。</li>
</ul>
<h3 id="3-2-C4-5"><a href="#3-2-C4-5" class="headerlink" title="3.2 C4.5"></a>3.2 C4.5</h3><p>与ID3算法相似，C4.5算法对ID3算法进行了改进，C4.5在生成的过程中，用信息增益比来选择特征</p>
<h3 id="3-3-CART"><a href="#3-3-CART" class="headerlink" title="3.3 CART"></a>3.3 CART</h3><p>分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART是在给定输入随机变量X条件下输出变量Y的条件概率分布的学习方法。它假定决策树是二叉树，内部取值为“是”（左分支）和“否”（右分支）。<br>它的基本步骤为</p>
<ul>
<li>1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。</li>
<li>2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这是用损失函数最小作为剪枝的标准。</li>
</ul>
<h4 id="3-3-1-分类树"><a href="#3-3-1-分类树" class="headerlink" title="3.3.1 分类树"></a>3.3.1 分类树</h4><p>对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。</p>
<p>具体算法步骤如下：</p>
<ul>
<li>1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值$a$，根据样本点对$A=a$的测试为”是”或者“否”将D分割为$D_1$和$D_2$两部分，计算其基尼系数。</li>
<li>2）在所有可能的特征A以及他们所有可能的切分点$a$中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</li>
<li>3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。</li>
<li>4）生成CART决策树</li>
</ul>
<h4 id="3-3-2-回归树"><a href="#3-3-2-回归树" class="headerlink" title="3.3.2 回归树"></a>3.3.2 回归树</h4><p>首先看一个简单的回归树生成实例：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-31%20%E4%B8%8B%E5%8D%882.32.52.png" alt="屏幕快照 2017-03-31 下午2.32.52"></p>
<p>接下来具体说说回归树是如何进行特征选择生成二叉回归树的。<br>假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集</p>
<script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}</script><p>我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下：</p>
<ul>
<li>1）选择最优切分变量$j$与切分点$s$，求解<script type="math/tex; mode=display">
\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]</script>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$</li>
<li>2）用选定的对$(j,s)$划分区域并决定相应的输出值：<script type="math/tex; mode=display">
R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\}</script><script type="math/tex; mode=display">
\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2</script></li>
<li>3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。</li>
<li>4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树：<script type="math/tex; mode=display">
f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)}</script></li>
</ul>
<h2 id="四、决策树的剪枝"><a href="#四、决策树的剪枝" class="headerlink" title="四、决策树的剪枝"></a>四、决策树的剪枝</h2><h3 id="4-1-剪枝"><a href="#4-1-剪枝" class="headerlink" title="4.1 剪枝"></a>4.1 剪枝</h3><p>决策树的过拟合指的是学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化，即剪枝（从已生成的树上裁剪调一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型）。</p>
<p>设树$T$的叶结点个数为$|T|$,$t$是树$T$的叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,···,K$，$H_t(T)$为叶结点$t$上的经验熵，$a≥0$为参数，则决策树学习的损失函数可以定义为</p>
<script type="math/tex; mode=display">{ C }_{ a }(T)=\sum _{ t=1 }^{ |T| }{ { N }_{ t }{ H }_{ t }(T)+a } |T|</script><p>其中经验熵为</p>
<script type="math/tex; mode=display">{ H }_{ t }(T)=-\sum _{ k }{ \frac { { N }_{ tk } }{ N_{ t } } log\frac { { N }_{ tk } }{ { N }_{ t } }  }</script><p>在损失函数中，将右端第一项记作</p>
<script type="math/tex; mode=display">
C\left( T \right) =\sum_{t=1}^{|T|}{N_tH_t\left( T \right) =-\sum_{t=1}^{|T|}{\sum_{k=1}^K{N_{tk}\log \frac{N_{tk}}{N_t}}}}</script><p>这时有</p>
<script type="math/tex; mode=display">
C_a\left( T \right) =C\left( T \right) +a|T|</script><p>其中，$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$a≥0$控制两者之间的影响。较大的$a$促使选择较简单的模型，较小的$a$促使选择较复杂的模型。$a=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。</p>
<p>决策树生成只考虑了通过信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。此损失函数的极小化等价于正则化的极大似然估计，即利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p>
<h3 id="4-2-CART剪枝"><a href="#4-2-CART剪枝" class="headerlink" title="4.2 CART剪枝"></a>4.2 CART剪枝</h3><p>CART剪枝算法从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测<br>其具体步骤如下：</p>
<ul>
<li>1）首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个字数序列$\left\{ T_0,T_1,T_{2,….,}T_n \right\}$;</li>
</ul>
<p>在剪枝过程中，计算子树的损失函数：</p>
<script type="math/tex; mode=display">
C_a\left( T \right) =C\left( T \right) +a|T|</script><p>其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼系数），$|T|$为子树的叶结点个数，$a≥0$为参数，$C_a(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。</p>
<p>对固定的$a$，一定存在使损失函数$C_a(T)$最小的子树，将其表示为$T_a$。$T_a$在损失函数$C_a(T)$最小的意义下是最优的，且是唯一的。$a$大的时候，最优子树$T_a$偏小；当$a$小的时候，最优子树$T_a$偏大。极端情况，$a=0$时，整体树是最优的。当$a\rightarrow \infty$，根结点组成的单结点树是最优的。</p>
<p>Breiman等人证明：可以用递归地方法对树进行剪枝。将$a$从小增大，$0=a_0&lt;a_1&lt;…..a_n&lt;+\infty$产生一系列的区间$[a_i,a_{i+1})$,$i=0,1,…,n$;剪枝得到的子树序列对应着区间$<br>a\in \left[ a_i,a_{i+1} \right)<br>$，$i=0,1,2,…,n$的最优子树序列为$<br>\left\{ T_0,T_1,T_2,…,T_n \right\}<br>$，序列的子树是嵌套的。</p>
<p>具体地，从整体树$T_0$开始剪枝，对$T_0$的人以内部结点$t$，以$t$为单结点树的损失函数是</p>
<script type="math/tex; mode=display">
C_a\left( t \right) =C\left( t \right) +\alpha</script><p>以$t$为根结点的子树$T_t$的损失函数是</p>
<script type="math/tex; mode=display">C_a(T_t)=C(T_t)+a|T_t|</script><p>当$a=0$及$a$充分小时，有不等式</p>
<script type="math/tex; mode=display">C_a\left( T_t \right)小于C_a\left( T \right)</script><p>当$a$增大时，在某一$a$有</p>
<script type="math/tex; mode=display">C_a(T_t)等于C_a(t)</script><p>当$a$再增大时，有不等式</p>
<script type="math/tex; mode=display">C_a(T_t)大于C_a(T)</script><p>只要$\alpha =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$ ，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。</p>
<p>为此，对$T_0$中的每一个内部结点$t$，计算</p>
<script type="math/tex; mode=display">
g\left( t \right) =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}</script><p>它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$a_1$，$T_1$为区间$[a_1,a_2)$的最优子树。</p>
<p>如此剪枝下去，直至得到根结点。在这一过程中，不断得增加$a$的值，产生新的区间。</p>
<ul>
<li>2）在剪枝得到的子树序列$T_0,T_1,…,T_n$中通过交叉验证选取最优子树$T_a$</li>
</ul>
<p>具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,…,T_n$都对应一个参数$a_1,a_2,…,a_n$。所以当最优子树$T_k$确定时，对应的$a_k$也就确定了，即得到最由决策树$T_a$.</p>
<h2 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h2><p><code>李航《统计学习方法》</code><br><code>周志华《机器学习》</code></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 决策树 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（6）：胡乱记]]></title>
      <url>/2017/01/15/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%886%EF%BC%89%EF%BC%9A%E8%83%A1%E4%B9%B1%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h3 id="其一：人"><a href="#其一：人" class="headerlink" title="其一：人"></a>其一：人</h3><p>女人的风度在于表达她对自己的看法，以及界定别人对待她的分寸。她的风度从姿态、声音、见解、表情、服饰、品味和选定的场合上体现出来——实际上，她所作的一切，无一不为她的风度增色。……男性观察女性；女性注意自己被别人观察。这不仅决定了大多数的男女关系，还决定了女性自己的内在关系，女性自身的观察者是男性，而被观察者为女性。这样被动主动的交互构成了男人们最基本的窥淫欲和恋物癖的现实表现以及女人在这个环境下被预设为被男人品赏的艺术品的尴尬局面。</p>
<a id="more"></a>
<p>知识分子对人民的引导应该是一个缓慢感化逐渐深入的过程，一开始就让老百姓穿高端的品牌，吃喝上等的烟酒是不切实际的，他们穿着不合适，吃进去还反胃，最后还弄个“文化强行入侵”的罪名，如此悲剧来源于知识分子自以为是的旨趣，该杜绝。阳春白雪对下里巴人所持有的态度不应是嫌弃，该是怜悯。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%286%29.jpeg" alt="a"></p>
<p>我们都是时代的弄潮儿，这不错，但是必须要清楚地认识到，我们弄得潮有大也有小，也有压根儿就不知道怎么弄潮，你不该为之抱怨，这是历史的必然，总会有潜藏的智慧不被挖掘，总会有好马遇不到伯乐。但是，于自身来说，你不该愤世骇俗，你得用尽全力去弄点潮出来，加把劲，再加把劲，说不定就弄出大潮来了嘞。</p>
<p>其实我们真的太年轻，年轻到不懂得原来爱情到了考虑现实的时候，到了和金钱权利挂钩之后，就再也不是原来的摸样的了。所以，趁自己还没有沾染那些东西的时候，好好地珍惜这份来之不易的爱情。如果你要考虑现实，那就去考虑吧，跟着自己的心走。每个人确实需要更长远的眼光，要看到远方的事情，但是青春真的就那么就一眨眼就不在了，有多少人做着自己喜欢的事情，爱着真正爱的人，过着内心喜欢和向往的生活？有多少人在不停忙碌着，却不知是为了什么？有多少人在混混沌沌着，也不知是为了什么？有太多的牵绊，有太多的不舍，有太多的不甘心。待到我们放下这些所谓的不舍、牵绊与不甘心，也正是我们真正活着的时候。一个人永远守护另一个人是不可能的。希望你记住我，记住我曾经这样存在过。记忆这东西总有些不可思议。身临其境的时候，几乎未曾意识到那片风景，未曾觉得它有什么撩人情怀之处，但到了十几年之后我们可能还无法忘怀。我老是介于“不充分”和“完全不够”之间，我总是感到饥渴，真想拼着劲儿得到一次爱。容许我百分之百的任性。村上春树讲：“ 把人生当做饼干罐就可以了。饼干罐不是装了各种各样的饼干，喜欢的和不喜欢的都在里面吗？如果先一个劲儿的挑你喜欢吃的，那么剩下的就全是不大喜欢的。每次遇到麻烦我就这样想，先把这个应付过去，往下就好办了。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%289%29.jpeg" alt=""></p>
<p>计划实在是一样满足虚荣心的好玩意儿，你欢腾于欣赏顺利达成的自己并且毫不自禁的开始畅想那种虚无的满足感，这样的力量实在是软弱的，毫无生机的，既然没实质的胜利，那你就是个十足的阿q，精神胜利的恶魔在肆虐你，让你分不清方向，你依旧在狂笑，不带一点点对自己的歉意。</p>
<p>所谓的有力不取决于肉体所能承载的负重和劳累，每天听到上千种声音，每天走6000步，每天看到1000种颜色，每天接触不一样滋味的空气，我的器官接受了周围的一切顺其自然的，你想看到的或者想避开的，都会一溜烟的在你神经的深处划过轨迹，记住的便是你的选择和潜意识里面想要的得到的，所以珍惜你记住的同时缅怀你没记住的。</p>
<h3 id="其二：物"><a href="#其二：物" class="headerlink" title="其二：物"></a>其二：物</h3><p>电影的发展不是绝对的，而是在一盘散沙里面掏出金来，时不时又混点泥沙进去，往复循环，进步的地方我们看的很明显，但是退步的趋势也是在很大一部分的元素里面可以显而易见的。看电影是一个艺术门道，主观的感觉是用来解决温饱的，那么深刻地剖析就是让你开始挑剔一部好电影的“色香味”，才可以真的把整个电影的精髓挖掘出来。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%287%29.jpeg" alt=""></p>
<p>谁都有你想不到的难题，困扰于心的在别人看来都是些鸡毛蒜皮的小事。在这里可以学到一个受用的经验：当缠绕着你的那些忧思无法被自己打败以及翻越的时候，把视线放远些，不要局限于一个角度，你看到的就不会是墙角的草，而是大草原的葱茏，那是何其美妙的世界。当然，这需要有一定的阅历（阅读，经历很重要），去扩展你的经历簿。</p>
<p>如果把马蜂窝移到自己的居室，帮里头的蜜蜂精心准备好繁茂生长的花朵，恰到好处的阳光，各种生命的迹象，以至于让他们察觉不到自己身处异境，即使有人类在他们周围逡巡，也不会折回或做出抵抗。让这样的环境持续尽量久一些，蜜蜂也会在自己毫无察觉的情况下发生细微的变化，那是依赖于环境的惯性适应。强行改变人类的行为亦是如此，扭曲变形，失去重心没有防线，离开安全区域的不安与彷徨，偏离轨道与甚者再也无可复原。</p>
<p>对于哲学问题除非你长期为之苦思冥想，否则你根本说不清到底是些什么问题。要对哲学史有很好的说明，你必须竭尽所能从其”内部“看清各个哲学问题，设身处地地进入你所讨论的哲学家们的内心世界。你必须弄清那些问题对为之乐此不疲的哲学家意味着什么，弄清哲学家们始终关注的焦点。不如此”艰辛“是不可能写出真正的思想史的。另外，除非你自己专注于相关领域并进行深入研究，否则你无法写出他人在该领域艰难跋涉的历史。意识形态的历史，严格来说，只有那些热衷于意识形态问题并懂得如何思考意识形态问题的人才能写。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%288%29.jpeg" alt=""><br>哲学解决的是观念之间、词语之间或表述方法之间的冲突产生出来的各种疑难。不同于经验问题。关于生活的目的、善和恶、自由和必然、客观性和相对性等的一系列问题，既不能考查阅最高级的辞典来解决，也不能用经验方法或数学推理方法来解决。设身处地地进入思想家们的内心和世界观是必要的，移情也是不可或缺的，尽管这样做面临证据不足和不确定性，乃至困难重重。诸如在研究马克思时，应该力图使自己像马克思本人在柏林、在巴黎、在布鲁塞尔和在伦敦那样，思考它的各种概念、范畴及其德语词汇。他们的思想是怎样产生的？在什么特定的时间、地点、社会条件下产生的？他们的思想可能很多人都有同感，但毕竟那是属于他们自己的。你必须不断反问自己，是什么东西让他们烦恼？什么东西使他们对这些问题苦苦思索？他们的理论或著作是怎么样在他们头脑中成熟的？人们不能完全抽象地超历史地谈论各种思想；但是，人们也不能孤立地仅在具体的历史环境中来描述各种思想，好像这些思想在他们的框架之外没有任何意义似的。这是一种复杂的、含糊不清的、需要借助心理学视野以及丰富想象力的研究工作，他不可能获得什么必然性的结论、在多数情况下，只能达到高度的持之有故和言之有理，达到理智能力的首尾一贯和清楚明白，还有独创性和有效性。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28456751&auto=1&height=66"></iframe>


]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 电影 </tag>
            
            <tag> 哲学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（3）：逻辑斯谛回归]]></title>
      <url>/2017/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h2 id="一、逻辑斯谛分布"><a href="#一、逻辑斯谛分布" class="headerlink" title="一、逻辑斯谛分布"></a>一、逻辑斯谛分布</h2><p>介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数：</p>
<a id="more"></a>
<script type="math/tex; mode=display">
F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}}</script><script type="math/tex; mode=display">
f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2}</script><p>式中，$\mu$为位置参数，$\gamma&gt;0 $为形状参数。逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足</p>
<script type="math/tex; mode=display">
F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2}</script><p>曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%888.55.00.png" alt="屏幕快照 2017-04-04 下午8.55.00"></p>
<h2 id="二、逻辑斯谛回归模型"><a href="#二、逻辑斯谛回归模型" class="headerlink" title="二、逻辑斯谛回归模型"></a>二、逻辑斯谛回归模型</h2><p>线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个：</p>
<ul>
<li>1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值；</li>
<li>2）预测结果受样本噪声的影响比较大。</li>
</ul>
<h3 id="2-1-LR模型表达式"><a href="#2-1-LR模型表达式" class="headerlink" title="2.1 LR模型表达式"></a>2.1 LR模型表达式</h3><p>LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即</p>
<script type="math/tex; mode=display">
h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}</script><p>其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$</p>
<h3 id="2-2-理解LR模型"><a href="#2-2-理解LR模型" class="headerlink" title="2.2 理解LR模型"></a>2.2 理解LR模型</h3><h4 id="2-2-1-对数几率"><a href="#2-2-1-对数几率" class="headerlink" title="2.2.1 对数几率"></a>2.2.1 对数几率</h4><p>一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是：</p>
<script type="math/tex; mode=display">
logit\left(p\right)=\log\frac{p}{1-p}</script><p>对LR而言，根据模型表达式可以得到：</p>
<script type="math/tex; mode=display">
\log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx</script><p>即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型</p>
<h4 id="2-2-2-函数映射"><a href="#2-2-2-函数映射" class="headerlink" title="2.2.2 函数映射"></a>2.2.2 函数映射</h4><p>除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。</p>
<p>考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。</p>
<p>线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。</p>
<p>LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。</p>
<h4 id="2-2-3-概率解释"><a href="#2-2-3-概率解释" class="headerlink" title="2.2.3 概率解释"></a>2.2.3 概率解释</h4><p>LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。</p>
<p>这里从事件、变量以及结果的角度给予解释。</p>
<p>我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？</p>
<p>一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。</p>
<p>那么对于第$i$个样本，概率公式表示如下：</p>
<script type="math/tex; mode=display">P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0  |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}</script><p>将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率：</p>
<script type="math/tex; mode=display">P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)}))^{1-y(i)}</script><p>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为：</p>
<script type="math/tex; mode=display">
P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}</script><p>通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。</p>
<h2 id="三、模型参数估计"><a href="#三、模型参数估计" class="headerlink" title="三、模型参数估计"></a>三、模型参数估计</h2><h3 id="3-1-Sigmoid函数"><a href="#3-1-Sigmoid函数" class="headerlink" title="3.1 Sigmoid函数"></a>3.1 Sigmoid函数</h3><p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-06%20%E4%B8%8B%E5%8D%8811.55.11.png" alt="屏幕快照 2017-04-06 下午11.55.11"></p>
<p>上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。</p>
<p>此外非常重要的，sigmoid函数求导后为：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-07%20%E4%B8%8A%E5%8D%8812.00.18.png" alt="屏幕快照 2017-04-07 上午12.00.18"><br>以下的推导中会用到，带来了很大的便利。</p>
<h3 id="3-2-参数估计推导"><a href="#3-2-参数估计推导" class="headerlink" title="3.2 参数估计推导"></a>3.2 参数估计推导</h3><p>上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。参数在整个样本空间的似然函数可表示为：</p>
<script type="math/tex; mode=display">
L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right)</script><script type="math/tex; mode=display">
=\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)}</script><script type="math/tex; mode=display">
=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}</script><p>为了方便参数求解，对这个公式取对数，可得对数似然函数：</p>
<script type="math/tex; mode=display">
l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)}</script><script type="math/tex; mode=display">
=\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}</script><p>最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。<br>先不考虑累加和，我们针对每一个参数$w_j$求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right)</script><script type="math/tex; mode=display">
=\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx</script><script type="math/tex; mode=display">
=\left(y-h_{\theta}\left(x\right)\right)x_j</script><p>最后，通过扫描样本，迭代下述公式可求得参数：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}</script><p>其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。</p>
<p>以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。</p>
<h3 id="3-3-分类边界"><a href="#3-3-分类边界" class="headerlink" title="3.3 分类边界"></a>3.3 分类边界</h3><p>知道如何求解参数后，我们看一下模型得到的最后结果是什么样的。<br>假设我们的决策函数为：</p>
<script type="math/tex; mode=display">y^∗=1, \ \ if \ \ P(y=1|x)>0.5</script><p>选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。<br>很容易看出，当$\theta ^Tx&gt;0$时，$y=1$，否则$y=0$。$\theta ^Tx=0$是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间（kernel trick），而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性曲线（通过特征映射）。<br><img src="http://omu7tit09.bkt.clouddn.com/15009922005058.png" alt=""><br>左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换$[x_1,x_2]=&gt;[x_1,x_2,x_1^2,x_2^2,x_1x_2]$后的空间是线性可分的，对应的原始空间中分类边界为一条椭圆曲线。</p>
<p>不过，通常使用的kernel都是隐式的，也就是找不到显式地把数据从低维映射到高维的函数，而只能计算高维空间中的数据点的内积。在这种情况下，logistic regression模型就不能再表示成$w^Tx+b$的形式（原始形式primal form），而只能表示成$\sum_ia_i<x_i,x>+b$的形式（对偶形式dual form）。忽略b，则原始形式的模型蚕食只有$w$，只需要一个数据点那么多的存储量；而对偶形式的模型不仅需要存储各个$a_i$，还要存储训练数据$x_i$本身，这个存储量就大了。</x_i,x></p>
<p>SVM也具有原始形式和对偶形式，相比之下，SVM的对偶形式是稀疏的，即只有支持向量的$a_i$才非零，才需要存储相应的$x_i$，所以，在非线性可分的情况下，SVM用的更多。</p>
<h2 id="四、延伸"><a href="#四、延伸" class="headerlink" title="四、延伸"></a>四、延伸</h2><h3 id="4-1-生成模型与判别模型"><a href="#4-1-生成模型与判别模型" class="headerlink" title="4. 1 生成模型与判别模型"></a>4. 1 生成模型与判别模型</h3><p>逻辑回归是一种判别模型，表现为直接对条件概率$P(y|x)$建模，而不关心背后的数据分布$P(x,y)$。而高斯贝叶斯（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算属于各个类别的后验概率，即：</p>
<script type="math/tex; mode=display">p(y|x)=\frac{P(x|y)P(y)}{\sum P(x|y)P(y)}</script><p>通常假设$P(x|y)$是高斯分布，$P(y)$是多项式分布，相应的参数可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到：</p>
<script type="math/tex; mode=display">log\frac{P(y=1|x)}{P(y=0|x)}=log\frac{P(x|y=1)}{P(x|y=0)}+log\frac{P(y=1)}{P(y=0)}=-\frac{(x-\mu_1)^2}{2\sigma_1^2}+\frac{(x-\mu_0)^2}{2\sigma_0^2}+\theta_0</script><p>如果$\sigma_1=\sigma_0$，二次项会抵消，我们得到一个简单的线性关系：</p>
<script type="math/tex; mode=display">log\frac{P(y=1|x)}{P(y=0|x)}=\theta^Tx</script><p>上式进一步可以得到：</p>
<script type="math/tex; mode=display">P(y=1|x)=\frac{e^{\theta^Tx}}{1+e^{e^Tx}}=\frac{1}{1+e^{-\theta^Tx}}</script><p>可以看到，这个概率和逻辑回归中的形式是一样的，这种情况下高斯贝叶斯和LR会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。</p>
<h3 id="4-2-多分类"><a href="#4-2-多分类" class="headerlink" title="4.2 多分类"></a>4.2 多分类</h3><p>如果$y$不是在$[0,1]$中取值，而是在$K$个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当$K$个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果$K$个类别是互斥的，即 $y=i$ 的时候意味着 $y$ 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax 函数来对概率建模，具体形式如下：</p>
<script type="math/tex; mode=display">P(y=i|x,\theta)=\frac{^{e^{\theta^T_ix}}}{\sum_j^K e^{\theta_j^Tx}}$$而决策函数为：$$y^*=argmax_iP(y=i|x,\theta )</script><p>对于的损失函数为</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{N}\sum_i^N\sum_j^KP(y_i=j)log\frac{e^{\theta_i^Tx}}{\sum e^{\theta_k^Tx}}</script><p>类似的，我们也可以通过梯度下降或其他高阶方法来求解该问题，这里不再赘述。</p>
<h3 id="4-3-应用"><a href="#4-3-应用" class="headerlink" title="4.3 应用"></a>4.3 应用</h3><p>这里以预测用户对品类的购买偏好为例，介绍一下美团是如何用逻辑回归解决工作中问题的。该问题可以转换为预测用户在未来某个时间段是否会购买某个品类，如果把会购买标记为1，不会购买标记为0，就转换为一个二分类问题。我们用到的特征包括用户在美团的浏览，购买等历史信息，见下表：<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-25 下午11.18.05.png" alt="屏幕快照 2017-07-25 下午11.18.05"><br>其中提取的特征的时间跨度为30天，标签为2天。生成的训练数据大约在7000万量级（美团一个月有过行为的用户），我们人工把相似的小品类聚合起来，最后有18个较为典型的品类集合。如果用户在给定的时间内购买某一品类集合，就作为正例。有了训练数据后，使用Spark版的LR算法对每个品类训练一个二分类模型，迭代次数设为100次的话模型训练需要40分钟左右，平均每个模型2分钟，测试集上的AUC也大多在0.8以上。训练好的模型会保存下来，用于预测在各个品类上的购买概率。预测的结果则会用于推荐等场景。<br><img src="http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-25 下午11.18.05.png" alt="屏幕快照 2017-07-25 下午11.18.05"><br><img src="http://omu7tit09.bkt.clouddn.com/11.png" alt="11"></p>
<p>由于不同品类之间正负例分布不同，有些品类正负例分布很不均衡，我们还尝试了不同的采样方法，最终目标是提高下单率等线上指标。经过一些参数调优，品类偏好特征为推荐和排序带来了超过1%的下单率提升。</p>
<p>此外，由于LR模型的简单高效，易于实现，可以为后续模型优化提供一个不错的baseline，我们在排序等服务中也使用了LR模型。</p>
<p>逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。</p>
<h3 id="4-4-LR与SVM"><a href="#4-4-LR与SVM" class="headerlink" title="4.4 LR与SVM"></a>4.4 LR与SVM</h3><p>两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。</p>
<p>两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html" target="_blank" rel="noopener"><code>对线性回归，logistic回归和一般回归的认识</code></a></p>
<p><a href="https://tech.meituan.com/intro_to_logistic_regression.html" target="_blank" rel="noopener">Logistic Regression 模型简介</a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑斯谛回归 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（5）：离别记]]></title>
      <url>/2017/01/10/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%885%EF%BC%89%EF%BC%9A%E7%A6%BB%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>离别的仪式以其该有的苍白的方式展开了，一个拥抱，一声保重，一段你们来时走过的路，它竭力地想被赋予更多的力量与情感，那么看吧，看看在我们心灵藏匿最深的地方，那里骚动起了真诚而又岌岌可危的猛兽，在故意掩藏的角落里缠绵、撕咬、交姌。你可以尽情嗅到生命的律动，几近沸腾的血液在体内加快步伐，血泵察觉到了仪式的情绪，它已经完全失去控制，它发了疯似的让灵动的双眼淌着热泪，身体仿佛接受了呼啸的寒风开始战栗起来，此刻即将分别，每一个鲜活的生命就要前往这个国度的各个角落，那里的星辰大海，下一次见面的时候请一定告诉我。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%281%29.jpeg" alt="d"><br>生活总需要特定的仪式来配合鼓舞，四年前我们仪式着来，四年后仪式着走，这无关乎虚荣，也绝不是空穴矫情，站在这个时间的关口，回望过去的日子，它不温不火，但也一定不平凡。当年踏上这片土地å，驾着云轻吟功与名，如今洒脱地背着旗帜四处宣告这四年的狂热与藏匿，我们汲汲求索，生活让我们的心灵焦灼躁动，却反而褪出一种光泽来，它们每一样都隐秘而伟大，孕育着一股神奇的力量。</p>
<p>我们曾如孩童般痴笑着欢送旧时光，如酒神般渴求下一秒思想的闪耀，过去哪怕是闯入一块新天地的边境，也要花光我们所有的力气，迟来的矍铄如同一杯烈酒，蓄满无畏与重生的快感，愿我们承受得住这些深渊，一直自命不凡地在里面挣扎，生命不止，它最好一刻也不要停止，也愿我们每一次触碰这个星球时都可以感受到有一种声音在呼喊，那声音在遥远的未来回响，也发轫于不朽的丰碑与偶像存留的尘土。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg.jpeg" alt="d"></p>
<p>我们即将面临一次非同寻常、却令人困倦的旅程，我们今天就要乘上一列所谓的失控列车。没有人可以诚恳地向我们展示前方有什么，而那些落在后面的人也会因接续我们这一毫无秩序的开端而爱莫能助。但我们必须要承认，这段旅途有去无回，我们可以宽慰自身的也只能是这样的思想：无论遇到什么不开心的事情，无论是哪一个车站都会一闪而过，那只是影片的一个人微妙的段落，列车绝不会在一个车站停留地太久。今天我们一起驻守的这块地方，即将成为过去的一站，我们只能挥手道别，在它还保持着正常的模样之前，在它还没有成为一张照片之前，就让我们怀着我们所有的温情再看它一眼，那也是在打量我们的过去。</p>
<p>先生们，我只是吝啬地留了一个苦涩的脸庞在你们记忆里，而你们在我的视野里留下了一个个鲜活的梦。篮球梦、炸金花梦、LOL梦，亦或是妹子梦、富帅梦、伟光正梦，它们都即将循着自己的路子熠熠生辉，都端正着自己的仪式带着未完成的梦离开吧，在812留下的痕迹就让我们在年迈的时候再来品味，倘若你们还记得床底的乌烟瘴气与泛黄纯白色纸巾，就算是重走了一回青春。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1476106&auto=1&height=66"></iframe>

]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 笑忘录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（2）：线性回归]]></title>
      <url>/2017/01/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h2 id="一、线性回归模型"><a href="#一、线性回归模型" class="headerlink" title="一、线性回归模型"></a>一、线性回归模型</h2><p>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。</p>
<a id="more"></a>
<p>我们可以有这样的模型表达：</p>
<script type="math/tex; mode=display">y=\theta_0+\theta_1x_1+\theta_2x_2+···+\theta_nx_n</script><p>其中，$x_1,x_2,···,x_n$表示自变量（特征分量），$y$表示因变量，$\theta_i$表示对应自变量（特征）的权重，$\theta_0$是偏倚项（又称为截距）。</p>
<p>对于参数$\theta$，在物理上可以解释为：在自变量（特征）之间相互独立的前提下，$\theta_i$反映自变量$x_i$对因变量$y$的影响程度，$\theta_i$越大，说明$x_i$对结果$y$的影响越大。因此，我们可以通过每个自变量（特征）前面的参数，可以很直观的看出那些特征分量对结果的影响比较大。</p>
<p>如果令$x_0=1,y=h_\theta{(x)}$，可以将上述模型写成向量形式，即：</p>
<script type="math/tex; mode=display">
h_\theta\left(x\right)=\sum_{i=0}^n{\theta_ix_i}=\theta^Tx</script><p>其中$\theta=(\theta_0,\theta_1,···,\theta_n)，x=(1,x_1,x_2,···,x_n)$均为向量，$\theta^T$为$\theta$的转置。</p>
<p>在上述公式中，假设特征空间与输入空间$x$相同。准确地讲，模型表达式要建立的是特征空间与结果之间的关系。在一些应用场合中，需要将输入空间映射到特征空间中，然后建模，定义映射函数为$\varPhi\left(x\right)$，因此我们可以把公式写成更通用的表达公式：</p>
<script type="math/tex; mode=display">
h_\theta\left(x\right)=\theta^T\varPhi\left(x\right)</script><p>特征映射相关技术，包括特征哈希、特征学习、$Kernel$等。</p>
<h2 id="二、目标函数"><a href="#二、目标函数" class="headerlink" title="二、目标函数"></a>二、目标函数</h2><h3 id="2-1-目标函数"><a href="#2-1-目标函数" class="headerlink" title="2.1 目标函数"></a>2.1 目标函数</h3><p>上面的公式的参数向量$\theta$是$n+1$维的，每个参数的取值是实数集合，也就是说参数向量$\theta$在$n+1$维实数空间中取值结果有无穷种可能。</p>
<p>那么，如何利用一个规则或机制帮助我们评估求得的参数$\theta$，并且使得线性模型效果最佳呢？直观地认为，如果求得参数$\theta$线性求和后，得到的结果$h_\theta{(x)}$与真实值$y$之差越小越好。</p>
<p>这时我们需要映入一个函数来衡量$h_\theta{(x)}$表示真实值$y$好坏的程度，该函数称为损失函数（loss function，也称为错误函数）。数学表示如下：</p>
<script type="math/tex; mode=display">
J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^n{\left(\left(h_\theta\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right)^2}</script><script type="math/tex; mode=display">
\min_\theta J\left(\theta\right)</script><p>这个损失函数用的是$x^{(i)}$的预测值$h_\theta{(x^{(i)})}$与真实值$y^{(i)}$之差的平方和。如果不考虑诸如过拟合等其他问题，这就是我们需要优化的目标函数。</p>
<h3 id="2-2-目标函数的概率解释"><a href="#2-2-目标函数的概率解释" class="headerlink" title="2.2 目标函数的概率解释"></a>2.2 目标函数的概率解释</h3><p>一般地，机器学习中不同的模型会有相应的目标函数。而回归模型（尤其是线性回归类）的目标函数通常用平方损失函数来作为优化的目标函数（即真实值与预测值之差的平方和）。为什么要选用误差平方和作为目标函数呢？答案可以从概率论中的中心极限定理、高斯分布等知识中找到。</p>
<h4 id="2-2-1-中心极限定理"><a href="#2-2-1-中心极限定理" class="headerlink" title="2.2.1 中心极限定理"></a>2.2.1 中心极限定理</h4><p>目标函数的概率解释需要用到中心极限定理。中心极限定理本身就是研究独立随机变量和的极限分布为正态分布的问题。</p>
<p><strong>中心极限定理的公式表示为：</strong><br>设$n$个随机变量$X_1,X_2,···,X_n$相互独立，均具有相同的数学期望与方差，即$E(X_i)=\mu ;D(X_i)=\sigma^2$，令$Y_n$为随机变量之和，有</p>
<script type="math/tex; mode=display">Y_n=X_1+X_2+···+X_n</script><script type="math/tex; mode=display">
Z_n=\frac{Y_n-E\left(Y_n\right)}{\sqrt{D\left(Y_n\right)}}=\frac{Y_n-n\mu}{\sqrt{n}\sigma}\rightarrow N\left(0,1\right)</script><p>称随机变量$Z_n$为$n$个随机变量$X_1,X_2,···,X_n$的规范和。</p>
<p><strong>它的定义为：</strong><br>设从均值为$\mu$、方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本，当$n$充分大时，样本均值的抽样分布$\frac{Y_n}{n}$近似服从于均值为$\mu$、方差为$\sigma^2$的正态分布。</p>
<h4 id="2-2-2-高斯分布"><a href="#2-2-2-高斯分布" class="headerlink" title="2.2.2 高斯分布"></a>2.2.2 高斯分布</h4><p>假设给定一个输入样例$x^{(i)}$根据公式得到预测值$\theta^Tx^{(i)}$与真实值$y^{(i)}$之间存在误差，即为$\varepsilon^{\left(i\right)}$。那么，它们之间的关系表示如下：</p>
<script type="math/tex; mode=display">
y^{\left(i\right)}=\theta^Tx^{\left(i\right)}+\varepsilon^{\left(i\right)}</script><p>而这里假设误差$\varepsilon^{\left(i\right)}$服从标准高斯分布是合理的。</p>
<p><strong>解释如下：</strong></p>
<p>回归模型的最终目标是通过函数表达式建立自变量$x$与结果$y$之间的关系，希望通过$x$能较为准确地表示结果$y$。而在实际的应用场合中，很难甚至不可能把导致$y$的所有变量（特征）都找出来，并放到回归模型中。那么模型中存在的$x$通常认为是影响结果$y$最主要的变量集合（又称为因子，在ML中称为特征集）。根据中心极限定理，把那些对结果影响比较小的变量（假设独立同分布）之和认为服从正态分布是合理的。</p>
<p>可以用一个示例来说明误差服从高斯分布是合理的：</p>
<p>$Andrew Ng$的课程中第一节线性回归的例子中，根据训练数据建立房屋的面积$x$与房屋的售价$y$之间的函数表达。<br>它的数据集把房屋面积作为最为主要的变量。除此之外我们还知道房屋所在的地段（地铁、学区、城区、郊区），周边交通状况，当地房价、楼层、采光、绿化面积等等诸多因素会影响房价。</p>
<p>实际上，因数据收集问题可能拿不到所有影响房屋售价的变量，可以假设多个因素变量相互独立，根据中心极限定理，认为变量之和服从高斯分布。即：</p>
<script type="math/tex; mode=display">
\epsilon^{\left(i\right)}=y^{\left(i\right)}-\theta^Tx^{\left(i\right)}</script><p>那么$x$和$y$的条件概率可表示为：</p>
<script type="math/tex; mode=display">
p\left(y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)</script><h4 id="2-2-3-极大似然估计与损失函数极小化等价"><a href="#2-2-3-极大似然估计与损失函数极小化等价" class="headerlink" title="2.2.3 极大似然估计与损失函数极小化等价"></a>2.2.3 极大似然估计与损失函数极小化等价</h4><p>根据上述公式估计得到一条样本的结果概率，模型的最终目标是希望在全部样本上预测最准，也就是概率积最大，这个概率积就是似然函数。优化的目标函数即为似然函数，表示如下：</p>
<script type="math/tex; mode=display">
\max_\theta L\left(\theta\right)=\prod_{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)</script><p>对$L(x)$取对数，可得对数似然函数：</p>
<script type="math/tex; mode=display">
\max_\theta l\left(\theta\right)=-m\log\sqrt{2\pi}\sigma -\frac{1}{2\sigma^2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}</script><p>由于$n,\sigma$都为常数，因此上式等价于</p>
<script type="math/tex; mode=display">
\min_\theta\frac{1}{2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}</script><p>我们可以发现，经过最大似然估计推导出来的待优化的目标函数与平方损失函数是等价的。因此可以得出结论：</p>
<p>线性回归误差平方损失极小化与极大似然估计等价。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是一个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。</p>
<p>那上面为什么是条件概率$p(y|x;\theta)$呢？因为我们希望预测值与真实值更接近，这就意味着希望求出来的参数$\theta$，在给定输入$x$的情况下，得到的预测值等于真实值得可能性越大越好。而$\theta$，$x$均为前提条件，因此用条件概率$p(y|x;\theta)$表示。即$p(y|x;\theta)$越大，越能说明估计的越准确。当然也不能一味地只有该条件函数，还要考虑拟合过度以及模型的泛化能力问题。</p>
<h2 id="三、参数估计"><a href="#三、参数估计" class="headerlink" title="三、参数估计"></a>三、参数估计</h2><p>如何调整参数$\theta$使得$J(\theta)$取得最小值？方法有很多，这里介绍几种比较经典的方法，即最小二乘法、梯度下降法以及牛顿法。</p>
<h3 id="3-1-最小二乘法"><a href="#3-1-最小二乘法" class="headerlink" title="3.1 最小二乘法"></a>3.1 最小二乘法</h3><h4 id="3-1-1-目标函数的矩阵形式"><a href="#3-1-1-目标函数的矩阵形式" class="headerlink" title="3.1.1 目标函数的矩阵形式"></a>3.1.1 目标函数的矩阵形式</h4><p>将$m$个$n$维样本组成矩阵$X$：</p>
<script type="math/tex; mode=display">
\left(\begin{matrix}
    1&        x_{1}^{\left(1\right)}&        x_{1}^{\left(2\right)}&        ···&        x_{1}^{\left(n\right)}\\
    1&        x_{2}^{\left(1\right)}&        x_{2}^{\left(2\right)}&        ···&        x_{2}^{\left(n\right)}\\
    ···&        ···&        ···&        &        \\
    1&        x_{m}^{\left(1\right)}&        x_{m}^{\left(2\right)}&        ···&        x_{m}^{\left(n\right)}\\
\end{matrix}\right)</script><p>则目标函数的矩阵形式为</p>
<script type="math/tex; mode=display">
J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^m{\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^2}=\frac{1}{2}\left(X\theta -y\right)^T\left(X\theta -y\right)</script><h4 id="3-1-2-最小二乘法求解"><a href="#3-1-2-最小二乘法求解" class="headerlink" title="3.1.2 最小二乘法求解"></a>3.1.2 最小二乘法求解</h4><p>对$\theta$求导，梯度（矩阵求导）：</p>
<script type="math/tex; mode=display">
\nabla_\theta J\left(\theta\right)=\nabla_\theta\left(\frac{1}{2}\left(X\theta-y\right)^T\left(X\theta-y\right)\right)</script><script type="math/tex; mode=display">
=\nabla_\theta\left(\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^Ty-y^Ty\right)\right)</script><script type="math/tex; mode=display">
=\frac{1}{2}\left(2X^TX\theta-X^Ty-\left(y^TX\right)^T\right)</script><script type="math/tex; mode=display">
=X^TX\theta-X^Ty</script><p>令其为零，求得驻点：</p>
<script type="math/tex; mode=display">
\theta=\left(X^TX\right)^{-1}X^Ty</script><h3 id="3-2-梯度下降法"><a href="#3-2-梯度下降法" class="headerlink" title="3.2 梯度下降法"></a>3.2 梯度下降法</h3><p>梯度下降法是按下面的流程进行的：</p>
<ul>
<li>1）首先对$\theta$赋值，这个值可以是随机的，也可是让$\theta$是一个全零的向量；</li>
<li>2）改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。</li>
</ul>
<p>为了更清楚，给出下面的图：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.02.35.png" alt="屏幕快照 2017-04-04 下午6.02.35"></p>
<p>这是一个表示参数$\theta$与目标函数$J(\theta)$的关系图，红色的部分是表示$J(\theta)$有比较高的取值，我们需要的是，能够让$J(\theta)$的值尽量的低。也就是深蓝色的部分。$\theta_0$和$\theta_1$表示$\theta$向量的两个维度。</p>
<p>在上面提到梯度下降法的第一步是给$\theta$一个初值，假设随机给的初值是在图上的十字点。然后我们将$\theta$按照梯度下降的方向进行调整，就会使得$J(\theta)$往更低的方向进行变化，如图所示，算法的结束将是在$\theta$下降到无法继续下降为止。</p>
<p>当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，比如下面这张图中描述的就是一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法会在很大程度上被初始点的选择影响而陷入局部最小点。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.03.23.png" alt="屏幕快照 2017-04-04 下午6.03.23"></p>
<p>下面对于目标函数$J(\theta)$求偏导数：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta_j}J\left(\theta\right)=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)^2</script><script type="math/tex; mode=display">
=2·\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)\frac{\partial}{\partial\theta_j}\left(h_{\theta}\left(x\right)-y\right)</script><script type="math/tex; mode=display">
=\left(h_{\theta}\left(x\right)-y\right)x_j</script><p>下面是更新的过程，也就是$\theta_i$会向着梯度最小的方向进行减少。$\theta$表示更新之前的值，$a$表示步长，也就是每次按照梯度减少的方向变化多少，由于求得是极小值，因此梯度方向是偏导数的反方向，结果为</p>
<script type="math/tex; mode=display">
\theta :=\theta_j+a\left(h_{\theta}\left(x\right)-y\right)x_j</script><p>一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量$\theta$，每一维分量$\theta_i$都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管他是全局的还是局部的。</p>
<p>在对目标函数$J(\theta)$求偏导时，可以用更简单的数学语言（倒三角表示梯度）进行描述：</p>
<script type="math/tex; mode=display">
\nabla_{\theta}J=\left[\begin{array}{c}
    \frac{\partial}{\partial\theta_0}J\\
    ···\\
    ···\\
    \frac{\partial}{\partial\theta_n}J\\
\end{array}\right]</script><script type="math/tex; mode=display">
\theta :=\theta +a\nabla_{\theta}J</script><p>将梯度下降法应用到线性回归有三种方式：批处理梯度下降法、随机梯度下降法。</p>
<h4 id="3-2-1-批量梯度下降法（BGD）"><a href="#3-2-1-批量梯度下降法（BGD）" class="headerlink" title="3.2.1 批量梯度下降法（BGD）"></a>3.2.1 批量梯度下降法（BGD）</h4><p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.36.09.png" alt="屏幕快照 2017-04-04 下午6.36.09"></p>
<p>可以看出，参数$\theta$的值每更新一次都要遍历样本集中的所有的样本，得到新的$\theta_j$，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。注意到，虽然梯度下降法易受到极小值的影响，但是一般的线性规划问题只有一个极小值，所以梯度下降法一般可以收敛到全局的最小值。例如，$J$是二次凸函数，则梯度下降法的示意图为：</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.40.55.png" alt="屏幕快照 2017-04-04 下午6.40.55"></p>
<p>图中，一圈上表示目标函数的函数值类似于地理上的等高线，从外圈开始逐渐迭代，最终收敛全局最小值。</p>
<h4 id="3-2-2-随机梯度下降算法（SGD）"><a href="#3-2-2-随机梯度下降算法（SGD）" class="headerlink" title="3.2.2 随机梯度下降算法（SGD）"></a>3.2.2 随机梯度下降算法（SGD）</h4><p><img src="http://omu7tit09.bkt.clouddn.com/2222.png" alt="屏幕快照 2017-04-04 下午6.43.06"><br>在这个算法中，我们每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.51.43.png" alt="屏幕快照 2017-04-04 下午6.51.43"><br>批量梯度下降法，每更新一次，需要用到样本集中的所有样本；随机梯度下降法，每更新一次，只用到训练集中的一个训练样本，所以一般来说，随机梯度下降法能更快地使目标函数达到最小值（新样本的加入，随机梯度下降法有可能会使目标函数突然变大，迭代过程中在变小。所以是在全局最小值附近徘徊，但对于实际应用俩说，误差完全能满足要求）。另外，对于批量梯度下降法，如果样本集增加了一些训练样本，就要重新开始迭代。由于以上原因，当训练样本集较大时，一般使用随机梯度下降法。</p>
<h2 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h2><p><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html" target="_blank" rel="noopener"><code>对线性回归，logistic回归和一般回归的认识</code></a></p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习算法系列（1）：K近邻]]></title>
      <url>/2017/01/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AK%E8%BF%91%E9%82%BB/</url>
      <content type="html"><![CDATA[<h2 id="一、K近邻算法"><a href="#一、K近邻算法" class="headerlink" title="一、K近邻算法"></a>一、K近邻算法</h2><p>K近邻算法简单、直观。首先给出一张图，根据这张图来理解最近邻分类器。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-23%20%E4%B8%8B%E5%8D%883.26.59.png" alt="屏幕快照 2017-03-23 下午3.26.59"></p>
<p>根据上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形或者红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。</p>
<p>我们常说，物以类聚，人以群分，判别一个人是一个什么样的人，常常可以从他身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于那一类数据么，好说，从他的另据下手。但一次性看多少个邻居呢？从上图中，你还可以看到：</p>
<ul>
<li>如果K=3,绿色圆点最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li>
<li>如果K=5,绿色圆点的最近5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色这个待分类点属于蓝色的正方形一类。</li>
</ul>
<p>于此，我们看到，KNN算法为给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。</p>
<p>K近邻法算法步骤如下：</p>
<p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),···，（x_N,y_N）}$，其中，$x_i$是实例的特征向量，$y_i$是实例的类别；新实例的特征向量$x$</p>
<p>输出：新实例$x$所属的类别$y$</p>
<ul>
<li>1)根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这k个点的$x$领域记作$N_k(x)$;</li>
<li>2)在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$:</li>
</ul>
<script type="math/tex; mode=display">
y=arg\underset{c_j}{\max}\sum_{x_i\in N_k\left( x \right)}^{}{I\left( y_i=c_i \right) \ \ ,\ \ i=1,2,···,N;j=1,2,···,K}</script><p>其中$I$为指示函数，即当$y_i=c_i$时为1，否则为0.</p>
<h2 id="二、K近邻模型"><a href="#二、K近邻模型" class="headerlink" title="二、K近邻模型"></a>二、K近邻模型</h2><h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>k近邻法中，当训练集、距离度量、K值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。</p>
<h3 id="2-2-距离度量"><a href="#2-2-距离度量" class="headerlink" title="2.2 距离度量"></a>2.2 距离度量</h3><p>特征空间中两个实例点的距离可以反映出两个实力点之间的相似性程度。K近邻模型的特征空间一般是N维实数向量空间，使用的距离可以是欧式距离，也可以是其他距离。</p>
<ul>
<li>欧氏距离：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中</li>
</ul>
<script type="math/tex; mode=display">
d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( x_i-y_i \right) ^2}}</script><ul>
<li><p>曼哈顿距离：我们可以定义曼哈顿距离的正式意义为$L1$距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投射的距离总和。</p>
<p>  通俗来讲，想想你在曼哈顿要从一个十字路口开车到另一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,此即曼哈顿距离名称的来源，同时，曼哈顿距离也称为城市街区距离。</p>
<script type="math/tex; mode=display">
d\left( x,y \right) =\sum_{i=1}^n{|x_i-y_i|}</script></li>
<li><p>切比雪夫距离：</p>
<script type="math/tex; mode=display">
d\left( x,y \right) =\underset{k\rightarrow \infty}{\lim}\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}}</script></li>
<li><p>闵可夫斯基距离：它不是一种距离，而是一组距离的定义。</p>
<script type="math/tex; mode=display">
d\left( x,y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}}</script><p>  当p=1时，即曼哈顿距离<br>  当p=2时，即欧式距离<br>  当$p\rightarrow \infty$，即切比雪夫距离</p>
</li>
<li><p>标准化欧氏距离：对样本集先进行标准化$<br>\hat{x}_i=\frac{x_i-\bar{x}}{s}$经过简单的推导就可以得到来标准化欧氏距离。</p>
<script type="math/tex; mode=display">
d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( \frac{x_i-y_i}{s} \right) ^2}}</script></li>
<li><p>夹角余弦：几何中夹角余弦可用来衡量两个向量方向的相似度，机器学习中借用这一概念来衡量向量之间的相似度。</p>
</li>
</ul>
<script type="math/tex; mode=display">
\cos \left( \theta \right) =\frac{a·b}{|a|·|b|}</script><h3 id="2-3-K值的选择"><a href="#2-3-K值的选择" class="headerlink" title="2.3 K值的选择"></a>2.3 K值的选择</h3><p>K值得选择会对K近邻法的结果产生重大影响。</p>
<p>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值得减小就意味着整体模型变得复杂，容易发生过拟合（容易受到训练数据的噪声而产生的过拟合的影响）。</p>
<p>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远的训练实例也会对预测器作用，使预测发生错误，且K值得增大就意味着整体的模型变得简单。</p>
<p>如果K=N。那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的（最近邻列表中可能包含远离其近邻的数据点），如下图所示。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-03-23%20%E4%B8%8B%E5%8D%883.27.22.png" alt="11"><br>在实际应用中，K值一般取一个比较小的数值。通常采用交叉验证法来选取最优的K值（经验规则：K一般低于训练样本数的平方根）。</p>
<h3 id="2-4-分类决策规则"><a href="#2-4-分类决策规则" class="headerlink" title="2.4 分类决策规则"></a>2.4 分类决策规则</h3><p>K近邻法中的分类决策规则往往是多数表决，即由输入实例的K个邻近的训练实例中的多数类决定输入实例的类。</p>
<h2 id="三、K近邻的优缺点"><a href="#三、K近邻的优缺点" class="headerlink" title="三、K近邻的优缺点"></a>三、K近邻的优缺点</h2><h3 id="3-1-优点"><a href="#3-1-优点" class="headerlink" title="3.1 优点"></a>3.1 优点</h3><ul>
<li>简单、易于理解、易于实现、无需估计参数、无需训练。</li>
<li>适合对稀有事件进行分类（如大概流式率很低时，比如0.5%，构造流失预测模型）；</li>
<li>特别适合多酚类问题，如根据基因特征来判断其功能分类，KNN比SVM的表现要好。</li>
</ul>
<h3 id="3-2-缺点"><a href="#3-2-缺点" class="headerlink" title="3.2 缺点"></a>3.2 缺点</h3><ul>
<li>懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢。</li>
<li>可解释性较差，无法给出决策树那样的规则。</li>
<li>当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。</li>
<li>KNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找K个近邻），懒惰的后果，构造模型很简单，但在测试样本分类地系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量。</li>
<li>决策树和基于规则的分类器都是积极学习<code>eager learner</code>的例子，因为一旦训练数据可用，它们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为消极学习法<code>lazy learner</code>。最近邻分类器就是这样的一种方法。</li>
</ul>
<h2 id="四、python代码实现"><a href="#四、python代码实现" class="headerlink" title="四、python代码实现"></a>四、python代码实现</h2><h3 id="4-1-K-近邻算法简单示例"><a href="#4-1-K-近邻算法简单示例" class="headerlink" title="4.1 K-近邻算法简单示例"></a>4.1 K-近邻算法简单示例</h3><p><code>KNN算法</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">Created on Feb 22  2017</div><div class="line">KNN</div><div class="line">@author: plushunter</div><div class="line">"""</div><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"><span class="keyword">import</span> operator</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNNClassifier</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,k=<span class="number">3</span>)</span>:</span></div><div class="line">        self._k=k</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calDistance</span><span class="params">(self,inputX,trainX)</span>:</span></div><div class="line">        dataSetSize=trainX.shape[<span class="number">0</span>]</div><div class="line">        <span class="comment"># tile for array and repeat for matrix in Python</span></div><div class="line">        diffMat=tile(inputX,(dataSetSize,<span class="number">1</span>))-trainX</div><div class="line">        sqDiffMat=diffMat**<span class="number">2</span></div><div class="line">        <span class="comment"># take the sum of difference from all dimensions,axis=0是按列求和,axis=1 是按行求和</span></div><div class="line">        sqDistances=sqDiffMat.sum(axis=<span class="number">1</span>)</div><div class="line">        distances=sqDistances**<span class="number">0.5</span></div><div class="line">        <span class="comment"># argsort returns the indices that would sort an array.argsort函数返回的是数组值从小到大的索引值</span></div><div class="line">        <span class="comment"># http://www.cnblogs.com/100thMountain/p/4719503.html</span></div><div class="line">        <span class="comment"># find the k nearest neighbours</span></div><div class="line">        sortedDistIndicies = distances.argsort()</div><div class="line">        <span class="keyword">return</span> sortedDistIndicies</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_classify</span><span class="params">(self,sample,trainX,trainY)</span>:</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> isinstance(sample,ndarray) <span class="keyword">and</span> isinstance(trainX,ndarray) <span class="keyword">and</span> isinstance(trainY,ndarray):</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                sample=array(sample)</div><div class="line">                trainX=array(trainX)</div><div class="line">                trainY=array(trainY)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">               <span class="keyword">raise</span> TypeError(<span class="string">"numpy.ndarray required for trainX and .."</span>)</div><div class="line">        sortedDistIndicies=self._calDistance(sample,trainX)</div><div class="line">        classCount=&#123;&#125;<span class="comment">#create the dictionary</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self._k):</div><div class="line">            label=trainY[sortedDistIndicies[i]]</div><div class="line">            classCount[label]=classCount.get(label,<span class="number">0</span>)+<span class="number">1</span></div><div class="line">            <span class="comment">#get(label,0) : if dictionary 'classCount' exist key 'label', return classCount[label]; else return 0</span></div><div class="line">        sorteditem=sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">#operator.itemgetter(1) can be substituted by 'key = lambda x: x[1]'</span></div><div class="line">        <span class="keyword">return</span> sorteditem[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self,inputX,trainX,trainY)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(inputX,ndarray) <span class="keyword">and</span> isinstance(trainX,ndarray) \</div><div class="line">            <span class="keyword">and</span> isinstance(trainY,ndarray):</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                inputX = array(inputX)</div><div class="line">                trainX = array(trainX)</div><div class="line">                trainY = array(trainY)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">              <span class="keyword">raise</span> TypeError(<span class="string">"numpy.ndarray required for trainX and .."</span>)</div><div class="line">        d = len(shape(inputX))</div><div class="line">        results=[]</div><div class="line">        <span class="keyword">if</span> d == <span class="number">1</span>:</div><div class="line">            result = self._classify(inputX,trainX,trainY)</div><div class="line">            results.append(result)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(inputX)):</div><div class="line">                result = self._classify(inputX[i],trainX,trainY)</div><div class="line">                results.append(result)</div><div class="line">        <span class="keyword">return</span> results</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    trainX = [[<span class="number">1</span>,<span class="number">1.1</span>],</div><div class="line">              [<span class="number">1</span>,<span class="number">1</span>],</div><div class="line">              [<span class="number">0</span>,<span class="number">0</span>],</div><div class="line">              [<span class="number">0</span>,<span class="number">0.1</span>]]</div><div class="line">    trainY = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</div><div class="line">    clf=KNNClassifier(k=<span class="number">3</span>)</div><div class="line">    inputX = [[<span class="number">0</span>,<span class="number">0.1</span>],[<span class="number">0</span>,<span class="number">0</span>]]</div><div class="line">    result = clf.classify(inputX,trainX,trainY)</div><div class="line">    <span class="keyword">print</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#output which type these belongs to</span></div><div class="line">/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/KNN.py</div><div class="line">[<span class="string">'B'</span>, <span class="string">'B'</span>]</div><div class="line"></div><div class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></div></pre></td></tr></table></figure>
<h3 id="4-2-KNN实现手写识别系统"><a href="#4-2-KNN实现手写识别系统" class="headerlink" title="4.2 KNN实现手写识别系统"></a>4.2 KNN实现手写识别系统</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">Created on Mar 22  2017</div><div class="line">KNN: Hand Writing</div><div class="line">@author: plushunter</div><div class="line">"""</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</div><div class="line"><span class="keyword">import</span> KNN</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></div><div class="line">    returnVect = zeros((<span class="number">1</span>, <span class="number">1024</span>))</div><div class="line">    fr = open(filename)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">        lineStr = fr.readline()</div><div class="line">        <span class="comment"># n = len(lineStr)</span></div><div class="line">        <span class="comment"># if not n:</span></div><div class="line">        <span class="comment">#     continue</span></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">            returnVect[<span class="number">0</span>, <span class="number">32</span> * i + j] = int(lineStr[j])</div><div class="line">    <span class="keyword">return</span> returnVect</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(filedir)</span>:</span></div><div class="line">    FileList = listdir(filedir)</div><div class="line">    m = len(FileList)</div><div class="line">    X = zeros((m,<span class="number">1024</span>))</div><div class="line">    Y = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">        fileNameStr = FileList[i]</div><div class="line">        classNumStr = int(fileNameStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">        Y.append(classNumStr)</div><div class="line">        X[i, :] = img2vector(filedir + <span class="string">"/"</span> + fileNameStr)</div><div class="line">    <span class="keyword">return</span> X,Y</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handWritingClassTest</span><span class="params">(inputX,inputY,trainX,trainY)</span>:</span></div><div class="line">    cls=KNN.KNNClassifier(k=<span class="number">3</span>)</div><div class="line">    error=<span class="number">0.0</span></div><div class="line">    result = cls.classify(inputX,trainX,trainY)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(result)):</div><div class="line">        <span class="keyword">if</span> result[i] != inputY[i]:</div><div class="line">            error+=<span class="number">1</span></div><div class="line">    precision_rate =<span class="number">1</span>- error /len(inputY)</div><div class="line">    <span class="keyword">print</span> precision_rate</div><div class="line">    <span class="comment"># return errorRate</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    trainDir = <span class="string">"digits/trainingDigits"</span></div><div class="line">    testDir = <span class="string">"digits/testDigits"</span></div><div class="line">    trainX,trainY = loadDataSet(trainDir)</div><div class="line">    inputX,inputY = loadDataSet(testDir)</div><div class="line">    handWritingClassTest(inputX,inputY,trainX,trainY)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    main()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#output precision_rate</span></div><div class="line">/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/HandWriting.py</div><div class="line"><span class="number">0.988372093023</span></div><div class="line"></div><div class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> K近邻 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（4）：二零一七第一天]]></title>
      <url>/2017/01/01/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E9%9B%B6%E4%B8%80%E4%B8%83%E7%AC%AC%E4%B8%80%E5%A4%A9/</url>
      <content type="html"><![CDATA[<h3 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h3><p>二零一六就像是一出异乎寻常的戏码，在戏里，我扮演了一个再寻常不过的角色，狼狈得被汹涌的剧情推搡着向前，迷失在混乱的舞台，舞台中央的鲜花与热泪，在光环底下肆意飞扬，那个失落的孩子站在舞台边呆呆凝视，视力模糊，它们在晃动，狂妄的审视，或者只是在给生活与艺术授予赞词。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2813%29.jpeg" alt=""></p>
<p>如何才可以抓住风？<br>如何才可以让仅存的模样愈加沉醉？<br>如何让这伟大的世界变得安静啊？</p>
<p>我想要回感受力，我察觉到它们正离我而去，嗅觉不再敏锐，耳旁嘈杂不堪，视觉里容不下美，身体的每一个部位都在宣告，你不要再挣扎，你已结束兴盛的时代，那个王朝早已落幕，你只能徘徊在硝烟四起的边疆了罢。</p>
<p>我不甘啊，我还那么年轻。</p>
<h3 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h3><p>二零一七呐，有好多期待，我要带着不甘重新上路了。</p>
<p>第一件，物。<br>神奇的物，帮助我们和这个曼妙的世界沟通。它们总能贴合我们的感官，让那些险些失去的感觉聚焦，消沉从而变得细腻可触，欢喜也可以沉浸许久，它们与我们的身体交融。<br><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2812%29.jpeg" alt=""><br>私人物品就像一种符号，为其在和自己关系密切的现实世界与社交网络中的身份提供具象化的证据。它是生活故事的标签，这些故事依附于私人物品铺展开来，串联起一个人的过去、现在与未来。</p>
<p>携带着与气质相符的物，让物呈现你的身体与灵魂吧，又何尝不可呢，不要说话，闭上眼睛，不要透露心声，关上襟怀。物不是其它，它就是你。</p>
<p>第二件，人。<br>怎么会有那么多可爱的人呢？我历数不过来了，我默默看着他们走过、停留、消逝，然后有一天，又重新出现，谁知道是哪一天，谁又知道会出现在哪个场景里，我唯一可以做的，便是等待着随便哪一种未来。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/222.jpg" alt=""></p>
<p>有的人，因为音乐而靠近<br>有的人，因为日常运作而汇合<br>有的人，因为城市的穿梭而擦肩而过<br>有的人，你永远遇见不到，但它们都有繁盛的心灵花园，你要学会自己去探望。</p>
<p>第三件，事。</p>
<p>我做不好很多事情，它们有的躺在我的计划簿子里，有的早已糜烂，有的做了一半就没有兴致了，有的也因为心有余而力不足，变得苍白烂尾。能做的就那么多，唯一希望的便是可以做的更好一些。</p>
<p>忽然觉得计划这玩意儿真神奇，它跨越时间区间去规定通往未来之路，让未来似乎变得清晰可触，在计划里，每个人都是自恋的，他们看到了更加完好的自己，从这种虚假的完备换取了瞬时的慰藉，每次向计划簿里添加精致的清单之时，便会看到镜像中的自己，绚丽地燃烧。而慢慢地，热情燃烧殆尽，才发觉，自己偏移的太远。</p>
<h3 id="其三"><a href="#其三" class="headerlink" title="其三"></a>其三</h3><p>二零一七要做更多有趣的事啊，生活雅致还得有。</p>
<p>计算机真是一个让人着迷的东西，还有很多的未知领域需要探索，让好奇心与求知欲都尽情地来驱动自己吧。</p>
<p>机器学习同样让人痴迷，16年下半年的时间支离破碎的，都没有系统的梳理下，作为今年的主线吧。</p>
<p>你好久不读书了，清单正在消沉，逐渐变得索然无味，当你再次拾起的时候，我相信它们会踊跃呼唤你的名。</p>
<p>美食、音乐、旅行，这些美妙生活的配方，没有它们，可能就没有那些人了，你也失去了让感受力生根发芽的机会。</p>
<p>我还是很瘦，每次在健身房的镜子前看着自己的样子，一脸嫌弃。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2815%29.jpeg" alt=""></h2><p>分享真是是一件让人振奋的事，一个人的日志，可以直接通往他的心灵花园，所以很多人选择把它藏起来，那是属于自己的东西。很多人只与自己说真话，虽然看起来他和很多人对话；也有很多人不与别人说话，它们还没有准备好，它们害怕话一说出口就会变得浅薄无趣，就索性让自己保有最温热的部分了。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=441116264&auto=1&height=66"></iframe>

]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 笑忘录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（3）：享书记]]></title>
      <url>/2016/12/24/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%883%EF%BC%89%EF%BC%9A%E4%BA%AB%E4%B9%A6%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h3 id="小序"><a href="#小序" class="headerlink" title="小序"></a>小序</h3><p>现在书就在我的手上，约摸有个十来本，我想说说我接下来和他们即将擦出的火花。</p>
<p>对，书还没放上书架，买来只是随意堆在床边，还没有带来和井然有序俱来的那种轻微无聊的单调滋味，我还没有准备好把淘来的书展示给朋友们看，就像是没有精心剪辑的影片，那种随意拼凑出来的画面固然有吸引力，但是我会赋予更多关于藏书的意义（每本书的每一册都有其命运）。 </p>
<a id="more"></a>
<h3 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h3><p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%283%29.jpeg" alt="11"><br>从我接手一本书那一时刻起，我便承担起了这本书的美妙的命运，或者说这本书将在我的手上重生，这会是一次有价值的邂逅，和自己产生共鸣的那一刻我便认定了这孩子，他让我同样获取了孩童心态，我就像是个孩子一样抚摸每一页，感受那种苍老或幽静的感觉，我为他取一个专属于我与他之间的名字，遇到一个好的主人成为你存在在这个世界上的荣耀，你可以安详的躺在我的书架上，享受散发你香味的漫长的一生，哪一天或许我心血来潮了，我会来看看你，或者和你深入的交往沟通，你需要理解我占有你时陶醉的心情，我也会因为复活你的生命沾沾自喜，于你而言，就像是复活了一个时代，那是我最深层次的动机。</p>
<p>我享受这样的快感——我将你们锁入我的圈子，永久成为我无法拒绝的暧昧对象。每一个关于你的回忆和念头，便是我的所有财富的基座，支架和锁钥，我打开每一个味觉嗅觉视觉触觉神经细胞，探索你的每一个细节，你的精髓我会暗自藏在心里，由此引发的灵感仿佛可以透过你们看到遥远的过去，出版那一天的情形，那位伟岸的作者在字里行间留下的最捉摸不透的暗号，装帧师傅设计封面时所被浸透的头脑风暴，你以前的那个主人如何把你捧在手心私密的对话。</p>
<h3 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h3><p>该是遇到怎样的多舛而繁华的命运才可以到我手上，我感谢每一位呵护你照顾你帮助你与你彻夜畅谈的每一位先人们，现在我是“老者”，你会陪伴我走向我最寂寥最繁荣的日子。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/111.jpg" alt="11"></p>
<p>我或许不会再回头去看你，就如曾经有个庸人赞美一番阿那托尔·法朗士的书斋，最后问了一个常见的 问题：“法朗士先生，这些书您都读过了吗？”回答是足以说明问题的： “还不到十分之一。不过我想您并不是每天都用您的塞弗尔瓷器吧？”</p>
<p>我想你安安静静的躺在我帮你精心设计的书架上，和周围的那些你的兄弟姐妹们和谐的交谈，就像是智者之间毫无遮拦的对话，那是你们伟大的灵魂有缘的碰撞和融合，又偶尔，我会来和你们打个招呼，或和你们中的某一位共处出在小屋子中，一盏台灯， 一杯清茶，度过一个思想火花四溅的午后。</p>
<p>我突然敞开怀笑了，哦， 藏书者的一大乐事，散逸人的一大福祉。我愿无声无息，无誉无毁躲在施比兹韦格的“书虫”面具后。没有人比这样的人更有富足感 了，因为附体的神灵或是精怪，会让爱着你们的人——与你们保持着最为亲密的拥有关系。就像是你们活在我的精神世界里面一样，我不会忍心舍弃你们而去。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=139381&auto=1&height=66"></iframe>







]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 阅读 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（2）：疯子]]></title>
      <url>/2016/12/20/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E7%96%AF%E5%AD%90/</url>
      <content type="html"><![CDATA[<h3 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h3><p>二十一世纪的第十三个年头，顺势来到的一天，圣索菲亚教堂前，游客不多，一些人交流，一些人快步，一些人，像我一样，对着教堂说话，像是对着自己。</p>
<p>丹尼斯大街与路易斯大街交汇的十字路口中央，高空中悬着一个雕塑，底下刷成灰色的塑像，凝视状。街边的的花满楼里，歌妓一刻不停，欢歌轻舞，北上的旅人在肉欲慰藉里且度今宵。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2811%29.jpeg" alt=""><br>一个神经病突然发出声，左边，右边，上头，后面，四周的人头朝向了一个声音，他开始搜索眼神，轻率，讶异；唇角，僵硬，微微收拢；线条轮廓，百无聊赖的平淡，妇女的乳房变得苍白，持续的缓慢的最终鲜明确凿的凸现：抑郁寡欢。</p>
<p>格格不入，对峙，退却。他捂住耳朵，周围的表情幻化成压抑的声源，他没有停止搜寻的迹象，那是一件危险的事。</p>
<p>那是两天前发生的事，他来到了夜幕下的哈尔滨。</p>
<h3 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h3><p>为了躲开原本那座城市刺眼的眼神，他才开始北上。几天前，他读毕一本书，八百余页，一整天，于是就再也看不清路人了，他没有变瞎，慢跑到紫禁城红墙下，已是深夜，红墙看不明颜色，但他知道颜色恰巧到中古悠然，也知道这是最契合这座城市意蕴的地方，他不准备停留，沿着府右街过景山前街，在南北池子大街的交汇处的东华门处停下，路上一对夜归的老夫妻互相搀扶前行，凝视十分钟，他没有意识到自己正对着他们傻笑，老夫妻被吓得赶忙加快脚步，一瘸一拐，他对着空白的街景继续傻笑。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/1111.jpg" alt=""></p>
<p>十分钟时间不长，对他的外在来讲也不长，只是意识流开始缠上身，前两分钟，唐宋时代的风俗人情，三四分钟；来自柏拉图的洞穴的幻影浮现；五六分钟，霸王别姬，哪吒传奇；六七分钟，竖排繁体的《脂砚斋重评石头记》，优雅笃定的当下感，博尔赫斯在图书馆，沈从文的边城；最后三分钟，《洛丽塔》，《人间失格》，《一个陌生女人的来信》，《2666》，《荒原狼》，《树上的男爵》，《瓦尔登湖》，《伊豆的舞女》，《到灯塔去》，《都柏林人》，若即若离。</p>
<p>他累了，找个旅馆睡下，一头扎进了一个没有亮光的胡同，那是一家老店。沦落为蜗居在老城区角落的廉价旅馆，早已徒有虚名。窄小巷子中的灰白色混凝土小楼，如同所有以临时心态搭建的建筑，苟且度日。接待处服务员，胖而迟钝的中年妇女，磕瓜子看着面前的电视屏幕，麻木的表情。走廊上铺陈一长条红色地毯，清洗。睡觉对他来说还不算是一件煎熬的事，可以在十分钟内很快睡去。夜深了。</p>
<h3 id="其三"><a href="#其三" class="headerlink" title="其三"></a>其三</h3><p>这是他常会有的那几天，业已习惯了这样的日子，例如还有几次，他一个人乘着火车，火车往南开，去的是一个中部城市，城市很大。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/123.jpeg" alt=""></p>
<p>火车上，乘客不多。一些时间说话，一些时间睡觉，一些时间喝水与看望黑夜，一些时间思考不着边际的问题。10个小时后，火车抵达值夏的城市。下车，出地道。出站口两扇敞开木门，一角灰蓝色天空。微风缭绕。广场上出租汽车和三轮车颇显冷落，生意寥寥。在清晨的光景里，这个城略显闹腾，低矮旧楼被雨水洗刷成暗色，路边耸立广告牌上，词汇带有时光倒退30年的落伍气息。他的精神一振，知道来到正确的地方，稍许加快了脚步，往前。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=26961949&auto=1&height=66"></iframe>

]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[笑忘录（1）：第二十二封情书]]></title>
      <url>/2016/12/05/%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9A%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B0%81%E6%83%85%E4%B9%A6/</url>
      <content type="html"><![CDATA[<p>还记得上一次给你写信是你从南方回到北方的那个夜晚，我激动地关掉播放到一半的电影，颇似少女祈祷般，浸润在回忆过往的文字中，静静地等待着那个北方姑娘，回到属于她的地方。正是那天，我感觉到我在这里的生活渐渐有了生机。现在，距离你来的那一天已经过去快三年了，我已记不清我们有过多少次争吵，但回想起来那些模糊的场景，却总觉还是温暖的，那正是我们共同度过寻常生活的印记。</p>
<p>昨天我们又争吵了。</p>
<a id="more"></a>
<p>回想这些天发生在我们之间的争吵、不合拍、嫌弃、甚至厌恶，我不知道有一些情感是否来的真实，但它确实从我们内心生发出来了，回头看那些生涩的言语，都让我倍感无助，感觉到心情的起伏已经无法被理智控制，就像河水猛兽般汹涌的侵蚀自己敏感却又极力克制的心，无法逆转，无法平复，根本无法像平常那样和周围的人说话。我总会在想：爱情是不痛苦的，它是纯快乐，不该掺进别的，尤其不该掺进痛苦，记得一首外国诗这样说：“啊！“爱情”！他们大大的误解了你！他们说你的甜蜜是痛苦，当你丰富的果实比任何果实都甜蜜。”</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/timg%20%2817%29.jpeg" alt=""></p>
<p>然而，我们真切的在爱情里尝到过痛苦，撕心裂肺的感觉。面对离别，我们却只能远望对方的背影渐渐离去；面对不理解，脑海里只会倍增对对方的不满；如若行为与本人价值观不符，便会用自己的意志控制对方，使对方的自由关进牢笼；如若争辩，我们也异常坚定的秉持自己的立场，尝试使用任何办法说服，如果对方的乖张让你感到舒适，战斗才停歇下来，否则就是一场无穷无尽的无聊的口舌之争，对，我们也常常使用这个套数让对方处于劣势（我不想在这里和你发生这些无聊的口舌之争）；服软的那一方看似更爱对方一些，不服输的总是多爱自己一些。</p>
<p>我们都在做一些我们自己也无法理解的事，我们沉浸在不理性，肆意泼洒的情绪里，我们在戏剧舞台上疯狂地表演，我们虚伪地接受爱情荒诞可笑的模样。有了爱，我们便有了最长的触角，伸向我们不曾触及的外部世界，伸向早已麻木倦怠的感官，但同时也把我们最柔软的部分显露在对方面前，大部分时间它得到了铠甲的庇护，而总有一天会有数不尽的痛触，让你质问爱情的真面目。</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-11%20%E4%B8%8B%E5%8D%8810.32.31.png" alt=""></p>
<p>“什么是爱，爱是恒久忍耐，爱是彼此包容，爱是相对付出”出自圣经，高中的时候，和班上的同学你问我答，说得好轻巧，但当注入真实体验的时候，我们已经开始怀疑，我们所经历的爱情，到底是不是真正的爱情了。在现实的情境与企盼的盛景有落差时，我常有幻灭的感觉，爱情难道不是那样吗？爱情不该是这样的，它不会辜负良苦用心的我们的吧？</p>
<p>罗兰巴特对爱情的不同阶段的场景有这么一段描述：</p>
<p>尽管恋人的表述仅仅是纷纭的情境，他们骚动起来全无秩序可言，不比在屋子里胡飞乱舞的苍蝇的轨迹更有规律，我还是能——至少是在回忆或者想像中——给爱情的发展找出一定的规律来。爱情的旅程似乎分为三个阶段（或者三幕戏）：首先是一见钟情，是闪电般的“迷上”“被俘虏”（我被一个形象迷住了）；然后便是一连串的相逢（约会、电话、情书、短途旅行），在此期间，我如痴如醉地发掘着情偶的完美，也就是说，对象与我的欲望之间那种完全出乎我意料的契合：这是初时的柔情，田园诗一般的光阴。在这幸福时光之后便是“一连串”恋爱的麻烦——持续不断的痛苦、创伤、焦虑、忧愁、怨恨、失望、窘迫还有陷阱——我们都成了里面的困兽，老是提心吊胆，生怕爱情衰退，怕这衰退不仅会毁了对方和我，还会毁了当初的缘分，那种神奇的情投意合。从这漫长的隧道中走出来，我又能重见天日了：这也许是因为我成功的找到了解决了不幸爱情的辩证出路——维持爱情、但脱离梦幻，冷静现实的面对它。）</p>
<p><img src="http://omu7tit09.bkt.clouddn.com/IMG_2015%202.jpg" alt=""></p>
<p>我想我们正在经历如他所述的爱情低谷，我们眼前一片黑暗，对方的形象也因此而模糊、歪曲。爱情在这个年纪注定是不能冷静的，在荷尔蒙的鼓动下，它露出了狂傲不羁的模样，我们的一半是孩子，一半又竭力挣脱这样的形象。但总有一天会磨合出一个更为安静的环境供爱情生长，愿爱情如我们所期盼的那样，我们会脱离梦境、从那漫长的黑暗隧道中走出来，如你所说：内心笃定，爱则长久。我们为了彼此，做最好的自己，然后留给岁月。在长久的陪伴中，我们越来越相似于彼此，直到连微笑的弧度都一样。直到看着越来越像彼此的自己埋怨不起来。我想，这就是我们的爱情，它来源的没有任何预兆，也没有任何可以消失的理由。它并不逊色生离死别的爱情，相比之下更多了青春奋斗的最美好的回忆。</p>
<p>晚安。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=330217&auto=1&height=66"></iframe>

]]></content>
      
        <categories>
            
            <category> 笑忘录 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 笑忘录 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习算法系列（20）：Kaggle 数据挖掘比赛经验分享（陈成龙）]]></title>
    <url>%2F2017%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8820%EF%BC%89%EF%BC%9AKaggle%20%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[简介Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。笔者从 2013 年开始，陆续参加了多场 Kaggle上面举办的比赛，相继获得了 CrowdFlower 搜索相关性比赛第一名（1326支队伍）和 HomeDepot 商品搜索相关性比赛第三名（2125支队伍），曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。笔者目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。此文分享笔者在参加数据挖掘比赛过程中的一点心得体会。 一、Kaggle基本介绍Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。在 Kaggle 上，企业或者研究机构发布商业和科研难题，悬赏吸引全球的数据科学家，通过众包的方式解决建模问题。而参赛者可以接触到丰富的真实数据，解决实际问题，角逐名次，赢取奖金。诸如 Google，Facebook，Microsoft 等知名科技公司均在 Kaggle 上面举办过数据挖掘比赛。2017年3月，Kaggle 被 Google CloudNext 收购。 1.1 参赛方式可以以个人或者组队的形式参加比赛。组队人数一般没有限制，但需要在 Merger Deadline 前完成组队。为了能参与到比赛中，需要在 Entry Deadline 前进行至少一次有效提交。最简单地，可以直接提交官方提供的 Sample Submission。关于组队，建议先单独个人进行数据探索和模型构建，以个人身份进行比赛，在比赛后期（譬如离比赛结束还有 2~3 周）再进行组队，以充分发挥组队的效果（类似于模型集成，模型差异性越大，越有可能有助于效果的提升，超越单模型的效果）。当然也可以一开始就组好队，方便分工协作，讨论问题和碰撞火花。 Kaggle 对比赛的公正性相当重视。在比赛中，每个人只允许使用一个账号进行提交。在比赛结束后 1~2 周内，Kaggle 会对使用多账号提交的 Cheater 进行剔除（一般会对 Top 100 的队伍进行 Cheater Detection）。在被剔除者的 Kaggle 个人页面上，该比赛的成绩也会被删除，相当于该选手从没参加过这个比赛。此外，队伍之间也不能私自分享代码或者数据，除非在论坛上面公开发布。 比赛一般只提交测试集的预测结果，无需提交代码。每人（或每个队伍）每天有提交次数的限制，一般为2次或者5次，在 Submission 页面会有提示。 1.2 比赛获奖Kaggle 比赛奖金丰厚，一般前三名均可以获得奖金。在最近落幕的第二届 National Data Science Bowl 中，总奖金池高达 100W 美刀，其中第一名可以获得 50W 美刀的奖励，即使是第十名也能收获 2.5W 美刀的奖金。获奖的队伍需要在比赛结束后 1~2 周内，准备好可执行的代码以及 README，算法说明文档等提交给 Kaggle 来进行获奖资格的审核。Kaggle 会邀请获奖队伍在 Kaggle Blog 中发表 Interview，来分享比赛故事和经验心得。对于某些比赛，Kaggle 或者主办方会邀请获奖队伍进行电话/视频会议，获奖队伍进行 Presentation，并与主办方团队进行交流。 1.3 比赛类型从 Kaggle 提供的官方分类来看，可以划分为以下类型（如下图1所示）： Featured：商业或科研难题，奖金一般较为丰厚； Recruitment：比赛的奖励为面试机会； Research：科研和学术性较强的比赛，也会有一定的奖金，一般需要较强的领域和专业知识； Playground：提供一些公开的数据集用于尝试模型和算法； Getting Started：提供一些简单的任务用于熟悉平台和比赛； In Class：用于课堂项目作业或者考试。 从领域归属划分：包含搜索相关性，广告点击率预估，销量预估，贷款违约判定，癌症检测等。 从任务目标划分：包含回归，分类（二分类，多分类，多标签），排序，混合体（分类+回归）等。 从数据载体划分：包含文本，语音，图像和时序序列等。 从特征形式划分：包含原始数据，明文特征，脱敏特征（特征的含义不清楚）等。 1.4 比赛流程一个数据挖掘比赛的基本流程如下图2所示，具体的模块我将在下一章进行展开陈述。 这里想特别强调的一点是，Kaggle 在计算得分的时候，有Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。用于计算 Public LB 和 Private LB 的数据有不同的划分方式，具体视比赛和数据的类型而定，一般有随机划分，按时间划分或者按一定规则划分。 这个过程可以概括如下图3所示，其目的是避免模型过拟合，以得到泛化能力好的模型。如果不设置 Private LB（即所有的测试数据都用于计算 Public LB），选手不断地从 Public LB（即测试集）中获得反馈，进而调整或筛选模型。这种情况下，测试集实际上是作为验证集参与到模型的构建和调优中来。Public LB上面的效果并非是在真实未知数据上面的效果，不能可靠地反映模型的效果。划分 Public LB 和 Private LB 这样的设置，也在提醒参赛者，我们建模的目标是要获得一个在未知数据上表现良好的模型，而并非仅仅是在已知数据上效果好。 二、数据挖掘比赛流程从上面图2可以看到，做一个数据挖掘比赛，主要包含了数据分析，数据清洗，特征工程，模型训练和验证等四个大的模块，以下来一一对其进行介绍。 2.1 数据分析数据分析可能涉及以下方面： 分析特征变量的分布 特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。 特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。 分析目标变量的分布 目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模（在这种情况下，需要对预测结果进行逆变换）。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。 目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样（Stratified Sampling）。 分析变量之间两两的分布和相关度 可以用于发现高相关和共线性的特征。 通过对数据进行探索性分析（甚至有些情况下需要肉眼观察样本），还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。 2.2 数据清洗数据清洗是指对提供的原始数据进行一定的加工，使得其方便后续的特征抽取。其与特征抽取的界限有时也没有那么明确。常用的数据清洗一般包括： 数据的拼接 提供的数据散落在多个文件，需要根据相应的键值进行数据的拼接。 特征缺失值的处理 特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受 outlier 的影响； 特征值为离散值：使用众数代替 文本数据的清洗 在比赛当中，如果数据包含文本，往往需要进行大量的数据清洗工作。如去除HTML 标签，分词，拼写纠正, 同义词替换，去除停词，抽词干，数字和单位格式统一等。 2.3 特征工程有一种说法是，特征决定了效果的上限，而不同模型只是以不同的方式或不同的程度来逼近这个上限。这样来看，好的特征输入对于模型的效果至关重要，正所谓”Garbage in, garbage out”。要做好特征工程，往往跟领域知识和对问题的理解程度有很大的关系，也跟一个人的经验相关。特征工程的做法也是Case by Case，以下就一些点，谈谈自己的一些看法。 2.3.1 特征变换主要针对一些长尾分布的特征，需要进行幂变换或者对数变换，使得模型（LR或者DNN）能更好的优化。需要注意的是，Random Forest 和 GBDT 等模型对单调的函数变换不敏感。其原因在于树模型在求解分裂点的时候，只考虑排序分位点。 2.3.2 特征编码对于离散的类别特征，往往需要进行必要的特征转换/编码才能将其作为特征输入到模型中。常用的编码方式有 LabelEncoder，OneHotEncoder（sklearn里面的接口）。譬如对于”性别”这个特征（取值为男性和女性），使用这两种方式可以分别编码为$\{0,1\}$和$\{[1,0], [0,1]\}$。 对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理： 统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优； 统计每个 ID 特征的一些统计量（譬如历史平均点击率，历史平均浏览率）等代替该 ID 取值作为特征，具体可以参考 Avazu 点击率预估比赛第二名的获奖方案； 参考 word2vec 的方式，将每个类别特征的取值映射到一个连续的向量，对这个向量进行初始化，跟模型一起训练。训练结束后，可以同时得到每个ID的Embedding。具体的使用方式，可以参考 Rossmann 销量预估竞赛第三名的获奖方案(entron/entity-embedding-rossmann) 对于 Random Forest 和 GBDT 等模型，如果类别特征存在较多的取值，可以直接使用 LabelEncoder 后的结果作为特征。 2.4 模型训练与验证2.4.1 模型选择在处理好特征后，我们可以进行模型的训练和验证。 对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式； 对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好； 数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模，具体可以参考2.5.2节 Stacking 部分。 2.4.2 调参和模型验证对于选定的特征和模型，我们往往还需要对模型进行超参数的调优，才能获得比较理想的效果。调参一般可以概括为以下三个步骤： 训练集和验证集的划分。根据比赛提供的训练集和测试集，模拟其划分方式对训练集进行划分为本地训练集和本地验证集。划分的方式视具体比赛和数据而定，常用的方式有： 随机划分：譬如随机采样 70% 作为训练集，剩余的 30% 作为测试集。在这种情况下，本地可以采用 KFold 或者 Stratified KFold 的方法来构造训练集和验证集。 按时间划分：一般对应于时序序列数据，譬如取前 7 天数据作为训练集，后 1 天数据作为测试集。这种情况下，划分本地训练集和验证集也需要按时间先后划分。常见的错误方式是随机划分，这种划分方式可能会导致模型效果被高估。 按某些规则划分：在 HomeDepot 搜索相关性比赛中，训练集和测试集中的 Query 集合并非完全重合，两者只有部分交集。而在另外一个相似的比赛中（CrowdFlower 搜索相关性比赛），训练集和测试集具有完全一致的 Query 集合。对于 HomeDepot 这个比赛中，训练集和验证集数据的划分，需要考虑 Query 集合并非完全重合这个情况，其中的一种方法可以参考第三名的获奖方案。 指定参数空间。在指定参数空间的时候，需要对模型参数以及其如何影响模型的效果有一定的了解，才能指定出合理的参数空间。譬如DNN或者XGBoost中学习率这个参数，一般就选 0.01 左右就 OK 了（太大可能会导致优化算法错过最优化点，太小导致优化收敛过慢）。再如 Random Forest，一般设定树的棵数范围为 100~200 就能有不错的效果，当然也有人固定数棵数为 500，然后只调整其他的超参数。 按照一定的方法进行参数搜索。常用的参数搜索方法有，Grid Search，Random Search以及一些自动化的方法（如 Hyperopt）。其中，Hyperopt 的方法，根据历史已经评估过的参数组合的效果，来推测本次评估使用哪个参数组合更有可能获得更好的效果。有关这些方法的介绍和对比，可以参考文献 [2]。 2.4.3 适当利用Public LB的反馈在2.4.2节中我们提到本地验证（Local Validation）结果，当将预测结果提交到 Kaggle 上时，我们还会接收到 Public LB 的反馈结果。如果这两个结果的变化趋势是一致的，如 Local Validation 有提升，Public LB 也有提升，我们可以借助 Local Validation 的变化来感知模型的演进情况，而无需靠大量的 Submission。如果两者的变化趋势不一致，需要考虑2.4.2节中提及的本地训练集和验证集的划分方式，是否跟训练集和测试集的划分方式一致。 另外，在以下一些情况下，往往 Public LB 反馈亦会提供有用信息，适当地使用这些反馈也许会给你带来优势。如图4所示，(a)和(b)表示数据与时间没有明显的关系（如图像分类），(c)和(d)表示数据随时间变化（如销量预估中的时序序列）。(a)和(b)的区别在于，训练集样本数相对于 Public LB 的量级大小，其中(a)中训练集样本数远超于 Public LB 的样本数，这种情况下基于训练集的 Local Validation 更可靠；而(b)中，训练集数目与 Public LB 相当，这种情况下，可以结合 Public LB 的反馈来指导模型的选择。一种融合的方式是根据 Local Validation 和 Public LB 的样本数目，按比例进行加权。譬如评估标准为正确率，Local Validation 的样本数为$N_l$，正确率为$A_l$；Public LB 的样本数为 $N_p$，正确率为 $A_p$。则可以使用融合后的指标：$（N_l A_l + N_p A_p）/(N_l + N_p)$，来进行模型的筛选。对于(c)和(d)，由于数据分布跟时间相关，很有必要使用 Public LB 的反馈来进行模型的选择，尤其对于(c)图所示的情况。 2.5 模型集成如果想在比赛中获得名次，几乎都要进行模型集成（组队也是一种模型集成）。关于模型集成的介绍，已经有比较好的博文了，可以参考 [3]。在这里，我简单介绍下常用的方法，以及个人的一些经验。 2.5.1 Averaging 和 Voting直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。 2.5.2 Stacking图5展示了使用 5-Fold 进行一次 Stacking 的过程（当然在其上可以再叠加 Stage 2, Stage 3 等）。其主要的步骤如下： 数据集划分。将训练数据按照5-Fold进行划分（如果数据跟时间有关，需要按时间划分，更一般的划分方式请参考3.4.2节，这里不再赘述）； 基础模型训练 I（如图5第一行左半部分所示）。按照交叉验证（Cross Validation）的方法，在训练集（Training Fold）上面训练模型（如图灰色部分所示），并在验证集（Validation Fold）上面做预测，得到预测结果（如图黄色部分所示）。最后综合得到整个训练集上面的预测结果（如图第一个黄色部分的CV Prediction所示）。 基础模型训练 II（如图5第二和三行左半部分所示）。在全量的训练集上训练模型（如图第二行灰色部分所示），并在测试集上面做预测，得到预测结果（如图第三行虚线后绿色部分所示）。 Stage 1 模型集成训练 I（如图5第一行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集，按照步骤 2 可以得到 Stage 1模型集成的 CV Prediction。 Stage 1 模型集成训练 II（如图5第二和三行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集和步骤 3 中得到的 Prediction 当作新的测试集，按照步骤 3 可以得到 Stage 1 模型集成的测试集 Prediction。此为 Stage 1 的输出，可以提交至 Kaggle 验证其效果。 在图5中，基础模型只展示了一个，而实际应用中，基础模型可以多种多样，如SVM，DNN，XGBoost 等。也可以相同的模型，不同的参数，或者不同的样本权重。重复4和5两个步骤，可以相继叠加 Stage 2, Stage 3 等模型。 2.5.3 BlendingBlending 与 Stacking 类似，但单独留出一部分数据（如 20%）用于训练 Stage X 模型。 2.5.4 Bagging Ensemble SelectionBagging Ensemble Selection [5] 是我在 CrowdFlower 搜索相关性比赛中使用的方法，其主要的优点在于可以以优化任意的指标来进行模型集成。这些指标可以是可导的（如 LogLoss 等）和不可导的（如正确率，AUC，Quadratic Weighted Kappa等）。它是一个前向贪婪算法，存在过拟合的可能性，作者在文献 [5] 中提出了一系列的方法（如 Bagging）来降低这种风险，稳定集成模型的性能。使用这个方法，需要有成百上千的基础模型。为此，在 CrowdFlower 的比赛中，我把在调参过程中所有的中间模型以及相应的预测结果保留下来，作为基础模型。这样做的好处是，不仅仅能够找到最优的单模型（Best Single Model），而且所有的中间模型还可以参与模型集成，进一步提升效果。 2.6 自动化框架从上面的介绍可以看到，做一个数据挖掘比赛涉及到的模块非常多，若有一个较自动化的框架会使得整个过程更加的高效。在 CrowdFlower 比赛较前期，我对整一个项目的代码架构进行了重构，抽象出来特征工程，模型调参和验证，以及模型集成等三大模块，极大的提高了尝试新特征，新模型的效率，也是我最终能斩获名次的一个有利因素。这份代码开源在 Github 上面，目前是 Github 有关 Kaggle 竞赛解决方案的 Most Stars，地址链接。 其主要包含以下部分： 模块化特征工程 接口统一，只需写少量的代码就能够生成新的特征； 自动将单独的特征拼接成特征矩阵。 自动化模型调参和验证 自定义训练集和验证集的划分方法； 使用 Grid Search / Hyperopt 等方法，对特定的模型在指定的参数空间进行调优，并记录最佳的模型参数以及相应的性能。 自动化模型集成 对于指定的基础模型，按照一定的方法（如Averaging/Stacking/Blending 等）生成集成模型。 三、Kaggle竞赛方案盘点3.1 图像分类到目前为止，Kaggle 平台上面已经举办了大大小小不同的赛事，覆盖图像分类，销量预估，搜索相关性，点击率预估等应用场景。在不少的比赛中，获胜者都会把自己的方案开源出来，并且非常乐于分享比赛经验和技巧心得。这些开源方案和经验分享对于广大的新手和老手来说，是入门和进阶非常好的参考资料。以下笔者结合自身的背景和兴趣，对不同场景的竞赛开源方案作一个简单的盘点，总结其常用的方法和工具，以期启发思路。 3.1.1 图像分类National Data Science Bowl 3.1.2 任务详情随着深度学习在视觉图像领域获得巨大成功，Kaggle 上面出现了越来越多跟视觉图像相关的比赛。这些比赛的发布吸引了众多参赛选手，探索基于深度学习的方法来解决垂直领域的图像问题。NDSB就是其中一个比较早期的图像分类相关的比赛。这个比赛的目标是利用提供的大量的海洋浮游生物的二值图像，通过构建模型，从而实现自动分类。 3.1.3 获奖方案1st place:Cyclic Pooling + Rolling Feature Maps + Unsupervised and Semi-Supervised Approaches。值得一提的是，这个队伍的主力队员也是Galaxy Zoo行星图像分类比赛的第一名，其也是Theano中基于FFT的Fast Conv的开发者。在两次比赛中，使用的都是 Theano，而且用的非常溜。方案链接：Classifying plankton with deep neural networks 2nd place：Deep CNN designing theory + VGG-like model + RReLU。这个队伍阵容也相当强大，有前MSRA 的研究员Xudong Cao，还有大神Tianqi Chen，Naiyan Wang，Bing XU等。Tianqi 等大神当时使用的是 CXXNet（MXNet 的前身），也在这个比赛中进行了推广。Tianqi 大神另外一个大名鼎鼎的作品就是 XGBoost，现在 Kaggle 上面几乎每场比赛的 Top 10 队伍都会使用。方案链接：National Data Science Bowl 17th place：Realtime data augmentation + BN + PReLU。方案链接：ChenglongChen/caffe-windows 3.1.4 常用工具 Theano: Welcome – Theano 0.9.0 documentation Keras: Keras Documentation Cuda-convnet2: akrizhevsky/cuda-convnet2 Caffe: Caffe | Deep Learning Framework CXXNET: dmlc/cxxnet MXNet: dmlc/mxnet 3.2 销量估计3.2.1 任务名称Walmart Recruiting – Store Sales Forecasting 3.2.2 任务详情Walmart 提供 2010-02-05 到 2012-11-01 期间的周销售记录作为训练数据，需要参赛选手建立模型预测 2012-11-02 到 2013-07-26 周销售量。比赛提供的特征数据包含：Store ID, Department ID, CPI，气温，汽油价格，失业率，是否节假日等。 3.2.3 获奖方案1st place：Time series forecasting method: stlf + arima + ets。主要是基于时序序列的统计方法，大量使用了 Rob J Hyndman 的 forecast R 包。方案链接：Walmart Recruiting – Store Sales Forecasting2nd place：Time series forecasting + ML: arima + RF + LR + PCR。时序序列的统计方法+传统机器学习方法的混合，方案链接：Walmart Recruiting – Store Sales Forecasting16th placeFeature engineering + GBM。方案链接：ChenglongChen/Kaggle_Walmart-Recruiting-Store-Sales-Forecasting 3.2.4 常用工具 R forecast package: https://cran.r-project.org/web/packages/forecast/index.html R GBM package: https://cran.r-project.org/web/packages/gbm/index.html 3.3 搜索相关性3.3.1 任务名称CrowdFlower Search Results Relevance 3.3.2 任务详情比赛要求选手利用约几万个 (query, title, description) 元组的数据作为训练样本，构建模型预测其相关性打分 {1, 2, 3, 4}。比赛提供了 query, title和description的原始文本数据。比赛使用 Quadratic Weighted Kappa 作为评估标准，使得该任务有别于常见的回归和分类任务。 3.3.3 获奖方案1st place：Data Cleaning + Feature Engineering + Base Model + Ensemble。对原始文本数据进行清洗后，提取了属性特征，距离特征和基于分组的统计特征等大量的特征，使用了不同的目标函数训练不同的模型（回归，分类，排序等），最后使用模型集成的方法对不同模型的预测结果进行融合。方案链接：ChenglongChen/Kaggle_CrowdFlower 3.3.4 常用工具 NLTK: Natural Language Toolkit Gensim: gensim: topic modelling for humans XGBoost: dmlc/xgboost RGF: baidu/fast_rgf 3.4 点击率预估I3.4.1 任务名称Criteo Display Advertising Challenge 3.4.2 任务详情经典的点击率预估比赛。该比赛中提供了7天的训练数据，1 天的测试数据。其中有13 个整数特征，26 个类别特征，均脱敏，因此无法知道具体特征含义。 3.4.3 获奖方案1st place：GBDT 特征编码 + FFM。台大的队伍，借鉴了Facebook的方案 [6]，使用 GBDT 对特征进行编码，然后将编码后的特征以及其他特征输入到 Field-aware Factorization Machine（FFM） 中进行建模。方案链接：Display Advertising Challenge | Kaggle 3rd place：Quadratic Feature Generation + FTRL。传统特征工程和 FTRL 线性模型的结合。方案链接：Display Advertising Challenge | Kaggle 4th place：Feature Engineering + Sparse DNN 3.4.4 常用工具 Vowpal Wabbit: JohnLangford/vowpal_wabbit XGBoost: dmlc/xgboost LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines 3.5 点击率预估II3.5.1 任务名称Avazu Click-Through Rate Prediction 3.5.2 任务详情点击率预估比赛。提供了 10 天的训练数据，1 天的测试数据，并且提供时间，banner 位置，site, app, device 特征等，8个脱敏类别特征。 3.5.3 获奖方案1st place：Feature Engineering + FFM + Ensemble。还是台大的队伍，这次比赛，他们大量使用了 FFM，并只基于 FFM 进行集成。方案链接：Click-Through Rate Prediction | Kaggle 2nd place：Feature Engineering + GBDT 特征编码 + FFM + Blending。Owenzhang（曾经长时间雄霸 Kaggle 排行榜第一）的竞赛方案。Owenzhang 的特征工程做得非常有参考价值。方案链接：owenzhang/kaggle-avazu 3.5.4 常用工具 LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines XGBoost: dmlc/xgboost 四、参考资料[1] Owenzhang 的分享： Tips for Data Science Competitions [2] Algorithms for Hyper-Parameter Optimization [3] MLWave博客：Kaggle Ensembling Guide [4] Jeong-Yoon Lee 的分享：Winning Data Science Competitions [5] Ensemble Selection from Libraries of Models [6] Practical Lessons from Predicting Clicks on Ads at Facebook 五、结语作为曾经的学生党，十分感激和庆幸有 Kaggle 这样的平台，提供了不同领域极具挑战的任务以及丰富多样的数据。让我这种空有满（yi）腔（xie）理（wai）论（li）的数据挖掘小白，可以在真实的问题场景和业务数据中进行实操练手，提升自己的数据挖掘技能，一不小心，还能拿名次，赢奖金。如果你也跃跃欲试，不妨选一个合适的任务，开启数据挖掘之旅吧。 转载自知乎：Kaggle 数据挖掘比赛经验分享]]></content>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark大数据分析（1）：Spark简介]]></title>
    <url>%2F2017%2F06%2F01%2FSpark%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BC%881%EF%BC%89%EF%BC%9ASpark%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[一、Spark概述1.1 Spark简介Spark是一个快速、通用、可扩展的大数据分析引擎。 它作为大数据计算平台的后起之秀，在2014年打破了Hadoop保持的基准排序（Sort Benchmark）纪录，使用206个节点在23分钟的时间里完成了100TB数据的排序，而Hadoop则是使用2000个节点在72分钟的时间里完成同样数据的排序。也就是说，Spark仅使用了十分之一的计算资源，获得了比Hadoop快3倍的速度。新纪录的诞生，使得Spark获得多方追捧，也表明了Spark可以作为一个更加快速、高效的大数据计算平台。 1.2 Spark的历史Apache Spark最初是在2009年由加州伯克利大学（UC Berkeley）AMP lab实验室开发，实验室中的一些研究人员曾经用过Hadoop MapReduce，他们发现MapReduce在迭代计算和交互计算的任务上表现地效率低下。因此，Spark从一开始就是为交互式查询和迭代算法所设计的，同时还支持内存式存储和高效的容错机制。 2009年，关于Spark的研究论文在学术会议上发表，同年Spark项目正式诞生。其后不久，相比于MapReduce，Spark在某些任务上已经获得了10~20倍的性能提升。 Spark最早的一部分用户来自加州伯克利分校的其他研究小组，其中比较著名的有Mobile Millennium。作为机器学习领域的研究项目，它们利用Spark来监控并预测旧金山湾区的交通拥堵情况，仅仅过了短短的一段时间，许多外部机构也开始使用Spark。如今，像 Verizon Verizon、 NBC、Yahoo、 Spotify……都是Spark的使用者，其源码贡献者有IBM、Oracle、DataStax、BlueData、Cloudera……。 Spark源码托管在Github中，截至2016年3月，共有超过800名来自200多家不同公司的开发人员贡献了15000次代码提交，可见Spark的受欢迎程度。 此外，每年举办的全球Spark顶尖技术人员峰会Spark Summit，吸引了使用Spark的一线技术公司及专家汇聚一堂，共同探讨目前Spark在企业的落地情况及未来Spark的发展方向和挑战。Spark Summit的参会人数从2014年的不到500人暴涨到2015年的2000多人，足以反映Spark社区的旺盛人气。 如今Spark已经受到了国内外各大公司的青睐，如腾讯、淘宝、百度、亚马逊等公司均不同程度地使用了Spark来构建大数据分析应用，并应用到实际的生产环境中。相信在将来，Spark会在更多的应用场景中发挥重要作用。 1.3 Spark的优点 运行速度快：Spark使用先进的DAG（Directed Acyclic Graph，有向无环图）执行引擎，以支持循环数据流与内存计算，与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上；而基于硬盘的运算也要快10倍以上。 易用性：Spark支持使用Scala、Java、Python和R语言进行编程，简洁的API设计有助于用户轻松构建并行程序，并且可以通过Spark Shell进行交互式编程； 通用性：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（通过Spark SQL）、实时流处理（通过Spark Streaming）、机器学习（通过Spark MLlib）和图计算（通过Spark GraphX），这些不同类型的处理都可以在同一个应用中无缝使用。 可融合性：Spark可以非常方便地与其他的开源产品进行融合。如Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cas-sandra等，不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架。 二、Spark生态系统在实际应用中，大数据处理主要包括以下三个类型： 复杂的批量数据处理：时间跨度通常在数十分钟到数小时之间； 基于历史数据的交互式查询：时间阔度通常在数十秒到数分钟之间； 基于实时数据流的数据处理：时间跨度通常在数百毫秒到数秒之间。 目前已有很多相对成熟的开源软件用于处理以上三种情境，比如，可以利用Hadoop MapReduce来进行批量数据处理，可以用Lmpala来进行交互式查询（lmpala与Hive相似，但底层引擎不同，提供了实时交互式SQL查询），对于流式数据处理可以采用开源流计算框架Storm。一些企业可能只会涉及其中部分应用场景，只需部署相应软件即可满足业务需求，但是，对于互联网公司而言，通常会同时存在以上三种场景，就需要同时部署三种不同的软件，这样做难免会带来一些问题： 不同场景之间的输入输出不能做到无缝共享，通常需要进行数据格式转换 不同的软件需要不同的开发和维护团队，带来了较高的使用成本； 比较难以对同一个集群中的各个系统进行统一的资源协调和分配。 Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统，既能够提供内存计算框架，也可以支持SQL即时查询、实时流式计算、机器学习和图计算等。因此Saprk所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理。 现在，Saprk生态系统已经成为伯克利数据分析软件栈BDAS（Berkeley Data Analytics Stack）的重要组成部分。BDAS的架构如图所示，从中可以看出，Spark专注于数据的处理分析，而数据的存储还是要借助于Hadoop分布式文件系统HDFS、Amazon S3等来实现的。因此，Spark生态系统可以很好地实现与Hadoop生态系统的兼容，使得现有Hadoop应用程序可以非常容易地迁移到Spark系统中。Spark的生态系统主要包含了Spark Core、SparkSQL、Spark Streaming、MLlib和GraphX等组件，各个组件的具体功能如下： Spark SQL实现了基于Spark的交互式查询。 Spark Streaming实现了实时流处理； MLlib实现了很多机器学习算法； GraphX实现了图计算；2.1 Spark CoreSpark Core实现了Spark的基本功能，包含任务调度、内存计算、故障恢复、部署模式、存储管理等。Spark Core中还包含了对弹性分布式数据集（Resilient distributed dataset）的API定义，RDD表示分布在多个计算节点上可以并行操作的元素集合，是Spark主要的编程抽象。Spark Core提供了创建和操作这些集合的多个API。 2.2 Spark SQLSpark SQL是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用SQL或HQL来查询数据。 Spark SQL允许开发人员直接处理RDD，同时也可查询Hive、HBase等外部数据源： Spark SQL的一个重要特点是支持开发者将SQL和传统的RDD编程的数据操作方式相结合，不论是使用Python、Java还是Scala，开发者都可以在单个的应用中同时使用SQL和复杂的数据分析。 通过与Spark所提供的丰富的计算环境进行如此紧密的结合，Spark SQL得以从其他开源数据仓库工具中脱颖而出。Spark SQL是在Spark 1.0中被引入的。 在Spark SQL之前，加州大学伯克利分销曾经尝试修改Apache Hive以使其运行在Spark上，当时的项目叫做Shark。现在由于Spark SQL与Spark引擎和API的结合更紧密，Shark已经被Spark SQL所取代。 2.3 Spark StreamingSpark Streaming基于Spark Core实现了可扩展、高吞吐和容错的的实时数据流处理。 现在支持的数据源有Kafka、Flume、Twitter、ZeroMQ、Ki-nesis、HDFS、S3和TCP套接字。处理后的结果可以存储到HDFS、Database或者Dashboard中。 Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark，也就是把Spark Streaming的输入数据按照批处理尺寸（如1秒）分成一段一段的数据（Stream），每一段数据都转换成Spark中的RDD，然后将Spark Streaming中对DStream的转换操作变为针对Spark中的RDD的转换操作，将RDD经过操作变成中间结果保存在内存中。 2.4 MLlibMLlib是Spark对常用的机器学习功能的程序库，MLlib提供了很多机器学习算法，包括分类、回归、聚类、协同过滤、降维（dimensionality reduction）等，还提供了模型评估、数据导入等额外的支持功能。MLlib还提供了一些更底层的机器学习元素，包括一个通用的梯度下降优化算法。其中：与分类和回归相关的算法包括SVM、逻辑回归、线性回归、朴素贝叶斯分类、决策树等；聚类实现了K-means、高斯混合（Gaus-sian mixture）、Power Iteration Clustering（PIC）、Latent Dirichlet Allocation（LDA）和Streaming版本的K-means；协同过滤实现了交替最小二乘法（Alternating Least Square，ALS）；降维实现了Singular Value De-composition（SVD）和Principal ComponentAnalysis（PCA）；频繁模式挖掘（frequent pat-tern mining）实现了FP-growth。 2.5 GraphXGraphX是用来操作图（比如社交网络的朋友关系图）的程序库，可以进行并行的图计算，提供了关于图和图并行计算的API，集ETL、试探性分析和迭代式的图计算于一体，并且在不失灵活性、易用性和容错性的前提下获得了很好的性能。与Spark Streaming和Spark SQL类似，扩展了Spark的RDD API，能用来创建一个顶点和边都包含任意属性的有向图。GraphX还支持针对图的各种操作（比如进行分割的subgraph和操作所有顶点的mapVertices），以及一些常用图算法（比如PageRank和三角计数） 三、Spark运行架构3.1 几个基本概念RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是一个分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型； DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系； Executor：是运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据； 应用：用户编写的Spark应用程序 作业：一个作业包含多个RDD及作用于相应RDD上的各种操作； 阶段：是作业的基本调度单元，一个作业会分为多组任务，每组任务被称为“阶段”，或者也被称为“任务集” 3.2 Spark架构设计如下图所示，Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）。其中，集群资源管理器可以是Spark自带的资源管理器，也可以是YARN或Mesos等资源管理框架。 与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：一是利用多线程来执行具体地任务（Hadoop MR采用的是进程模型），减少任务的启动开销；二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时，就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO的性能。 总体而言，如下图所示，在Spark中，一个应用（Application）由一个任务控制节点（Driver）和若干个作业（Job）构成，一个作业由多个阶段（Stage）构成，一个阶段由多个任务（task）组成。当执行一个应用时，任务控制节点会向集群管理器（Cluster Manager）申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行任务，运行结束后，执行结果会返回给任务控制节点，或者写到HDFS或者其他数据库中。 3.3 Spark运行基本流程如下图所示，Spark的基本运行流程如下： 1）当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个Spark Context，由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请、任务的分配和监控等。Spark Context会向资源管理器注册并申请运行Executor的资源； 2）资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上； 3）Spark Context根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器（DAGScheduler）进行解析，将DAG分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（Task Scheduler）进行处理； 4）任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。 总体而言，Spark运行架构具有以下特点： 1）每个应用都有自己专属的Executor进程，并且该进程在应用运行期间一直驻留。Executor进程以多线程的方式运行任务，减少了多进程任务频繁的启动开销，使得任务执行变得非常高效和可靠； 2）Spark运行过程与资源管理器无关，只要能够获取Exector进程并保持通信即可； 3）Executor上有一个BlockManager存储模块，类似于键值存储系统（把内存和磁盘共同作为存储设备），在处理迭代计算任务时，不需要把中间结果写入到HDFS等文件系统，而是直接放在这个存储系统上，后续有需要时就可以直接读取；在交互式查询场景下，也可以把表提前缓存到这个存储系统上，提高读写IO性能； 4）任务采用了数据本地性和推测执行等优化机制。数据本地性是尽量将计算移到数据所在的节点上进行，即“计算向数据靠拢”，因为移动计算比移动数据所占的网络资源要少得多。而且，Spark采用了延时调度机制，可以在更大的程度上实现执行过程优化。比如，拥有数据的节点当前正被其他的任务占用，那么，在这种情况下是否需要将数据移动到其他的空闲节点呢？答案是不一定。因为，如果经过预测发现当前节点结束当前任务的时间要比移动数据的时间还要少，那么，调度就会等待，直到当前节点可用。 参考资料：《Spark快速大数据分析》《大数据技术与原理》]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（19）：机器学习性能评价指标]]></title>
    <url>%2F2017%2F05%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[一、分类问题的评价指标1.1 混淆矩阵对一个二分类问题，将实例分成正类（postive）或者负类（negative），但在实际分类中，会出现以下四种情况： True Positive（真正，TP）：将正类预测为正类数 True Negative（真负，TN）：将负类预测为负类数 False Positive（假正，FP）：将负类预测为正类数 False Negative（假负，FN）：将正类预测为负类数 从下图可以直观的看出四者的关系： 混淆矩阵（Confusion matrix）又被称为错误矩阵，它是一种特定的矩阵来呈现算法性能的可视化呈现。其每一列代表预测值，每一行代表的是实际的类别，这个名字来源于他是否可以非常容易的表明多个类别是否有混淆（也就是一个class被预测为另一个class）混淆矩阵的$i$行$j$列是列别$i$被分为类别$j$的样本个数。 1.2 精确率、召回率与F1值 精确率（precision rate）定义为：$$P=\frac{TP}{TP+FP}$$这里需要注意的是精确率（precision）和准确率（accuracy）是不一样的$$ACC=\frac{TP+TN}{TP+TN+FP+FN}$$ 在非平衡数据的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击），ACC也达到了99%以上，这就没有意义了。 召回率（Recall rate）定义为：$$R=\frac{TP}{TP+FN}$$ 此外，还有F1值，它是精确率和召回率的调和均值，即$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$$$F_1=\frac{2TP}{2TP+FP+FN}$$精确率与召回率都很高时，$F_1$值也会很高。 1.4 通俗理解通俗来讲，精确率是针对我们的预测结果而言的，他表示的是预测为正的样本中有多少是对的，那么预测为正就有两种可能了，一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP）。 而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN）。 在信息搜索领域，精确率和召回率又被称为查准率和查全率$$查准率=\frac{检索出的相关信息量}{检索出的信息总量}$$$$查全率=\frac{检索出的相关信息量}{系统中的相关信息总量}$$ 1.5 ROC曲线ROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具（飞机、船舰），也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、犯罪心理学领域中，而且最近在机器学习（machine learning）和数据挖掘（data mining）领域也得到了很好的发展。 下图是一个ROC曲线的示例图。 在这个ROC曲线的示例图中，横坐标为false positive rate(FPR)，纵坐标为true positive rate（TPR）。由混淆矩阵可得到横纵轴的计算公式。 1）$TPR=\frac{TP}{TP+FN}$ 代表分类器预测的正类中实际正实例占所有正实例的比例。直观上代表能将正例分对的概率。 2）$FPR=\frac{FP}{FP+TN}$ 代表分类器预测的正类中实际负实例占所有负实例的比例。直观上代表将负类错分为正例的概率。 假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着更多的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点（0，0），阈值最小时，对应坐标点（1，1）。 接下来我们考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。 下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。 如何绘制ROC曲线呢？ 假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，“class”一栏表示每个测试样本真正的标签（P表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。 接下来，我们从高到低，依次将“Score”值作为阈值的threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第四个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图： 1.6 AUCAUC（Area under Curve）指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观地评价分类器的好坏，值越大越好。 The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. 首先AUC是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。 以下是根据AUC判断分类器优劣的标准： 1）AUC=1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数场合，不存在完美的分类器。 2）0.5&lt;AUC&lt;1，优于随机猜测。这个分类器妥善设定阈值的话，能有预测价值。 3）AUC=0.5，跟随机猜测一样（如丢硬币），模型没有预测价值。 4）AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 那我们为什么使用ROC曲线呢？ 既然已经有那么多的评价标准，为何还要使用ROC和AUC曲线呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现非平衡数据的现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： 在上图中，a和c为ROC曲线，b和d为Precision-Recall曲线。a和b展示的是分类器在原始测试集（正负样本分布平衡）的结果，c和d是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall则变化较大。 二、回归问题的评价指标2.1 平均绝对误差平均绝对误差MAE（Mean Absolute Reeor）又被称为L1范数损失（L1-norm loss）：$${\rm MAE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}|y_i-\hat{y}_i|$$ 2.2 平均平方误差平均平方误差MSE（Mean Squared Error）又被称为L2范数损失（L2-norm loss）:$${\rm MSE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}(y_i-\hat{y}_i)^2$$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>评价指标</tag>
        <tag>精确率</tag>
        <tag>召回率</tag>
        <tag>ROC</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（10）：DMC—卷积神经网络分享]]></title>
    <url>%2F2017%2F05%2F19%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[链接：卷积神经网络]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>Batch Normalization</tag>
        <tag>dropout</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（9）：Batch Normalization]]></title>
    <url>%2F2017%2F05%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9ABatch%20Normalization%2F</url>
    <content type="text"><![CDATA[batch normalization(Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的创新之一。实际上它并不是一个优化算法，而是一个自适应的重新参数化 的方法，试图解决训练非常深层模型的困难。Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift机器学习领域有一个很重要的假设：iid独立同分布假设，就是假设训练数据和测试数据满足相同分布，这是通过训练数据训练出来的模型能够在测试集上获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。 一、Internal covariate shift首先给出covariate shift的定义：模型实例集合中的输入值X的分布总是变化，违背了idd独立通同布假设。 深度学习网络包含很多隐层的网络结构，在训练过程中参数会不断发生改变，导致后续每一层输入的分布也面临着covariate shift，也就是在训练过程中，隐层的输入分布总是发生改变，这就是所谓的Internal covariate shift，Internal指的是深层网络的隐层，covariate shift发生在深度神经网络内部，就被称作Internal covariate shift。 在DNN的实验中，对数据进行预处理时，例如白化或者zscore，甚至是简单的减均值操作都是可以加速收敛的。为什么减均值、白化可以加快训练，作如下分析： 首先，图像数据的每一维一般都是0~255之间的数字，因此数据点智慧落在第一象限，而且图像数据具有很强的相关性，比如第一个灰度值为30，比较黑，那它旁边的一个像素值一般不会超过100，否则给人的感觉就像噪声一样。由于强相关性，数据点仅会落在第一象限的小区域内，形成类似第一个图的狭长分布。 其次，神经网络模型在初始化的时候，权重W都是随机采样生成的，一般都是零均值，因此起初的拟合y=Wx+b，基本过原点附近，如图b红色虚线。因此，网络需要经过多次迭代学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。更何况，这里只是个二维的演示，数据占据四个象限中的一个，但如果是几百、几千、上万维呢？而且数据在第一象限也只是占了很小的一部分区域而已，可想而知若不对数据进行预处理带来了多少运算资源的浪费，而且大量的数据外分割面在迭代时很可能会在刚进入数据中是就遇到了一个局部最优，导致overfit的问题。如果我们对输入数据先作减均值操作，如图c，数据点就不再只分布在第一象限，这是一个随机分界面落入数据分布的概率增加了$2^n$倍，大大加快学习。更进一步的，我们对数据再进行去相关操作，例如PCA和ZCA白化，数据不再是一个狭长的分布，随机分界面有效的概率就又大大增加了，使得数据更加容易区分，这样又会加快训练，如图d。 不过计算协方差的特征值太耗时也太耗空间，一般最多只用到z-score处理，即每一维减去自身均值，再除以自身标准差，这样能使数据点在每维上具有相似的宽度，可以起到增大数据分布范围，进而使更多随机分界面有意义的作用。 二、Batch Normalization2.1 直观解释Batch Normalization的基本思想其实很直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B,U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或者正值），所以这导致反向传播的时候低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。 但是这里有个问题，如果都通过Batch Normalization，那么不就跟把非线性函数替换成线性函数效果相同了？我们知道，如果是多层的线性函数变换，其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到零均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型的表达能力。BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者由移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动，让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。从而保证整个网络的capacity。 2.2 算法过程假设对于一个深层神经网络来说，其中两层结构如下：要对每个隐藏神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WY+B激活值获得之后，非线性函数变换之前，其图示如下：对Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：$$\hat{x}^{\left(k\right)}=\frac{x^{\left(k\right)}-E\left[x^{\left(k\right)}\right]}{\sqrt{var\left[x^{\left(k\right)}\right]}}$$要注意，这里t层某个神经元的$x(k)$不是指原始输入，就是说不是$t-1$层每个神经元的输出，而是$t$曾这个神经元的激活$x=WU+B$，这里的$U$才是$t-1$层神经元的输出。还有一点，上述公式中用到了均值和方差，在理想情况下均值和方差是针对整个数据集的，但显然这是不现实的，因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。这个变换就是：某个神经元对应的原始的激活$x$减去Mini-Batch内$m$个激活$x$求得的均值$E(x)$并除以求得的方差$Var(x)$来进行转换。 上文说过经过这个变换后某个神经元的激活$x$形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大梯度，增强反向传播信息流行性，加快训练收敛速度。但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这俩个参数是通过训练来学习的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scle和shift操作，这其实是变换的反操作：$$y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}$$其整个算法流程如下： 2.3 推理过程BN在训练的时候可以根据Mini-Batch数据里可以得到的统计量，那就想其他办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，知识因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。 决定了获得统计量的数据范围，那么接下来的问题就是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：$$E\left[x\right]\gets E_{\beta}\left[\mu_{\beta}\right]$$$$Var\left[x\right]\gets\frac{m}{m-1}E_{\beta}\left[\sigma_{\beta}^{2}\right]$$有了均值和方差，每个隐藏神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算BN进行变换了，在推理过程中进行BN采取如下方式：$$y=\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}·x+\left(\beta -\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}\right)$$这个公式其实和训练时$$y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}$$是等价的，通过简单的合并计算推导就可以得出这个结论。在实际运行时，按照这种变体形式可以减少计算量，因为对每一个隐节点来说：$\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}$和$\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}$都是固定值，这样两个值可以实现算好存起来，在推理的时候直接用就行了，比原始的公式每一步骤都少了出发的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。 2.4 参数训练以上是对算法原理的讲述，在反向传导的时候，我们需要求最终的损失函数对$\gamma$和$\beta$两个参数的导数，还要求损失函数对Wx+b中x的导数，一遍使误差继续向后传播。几个主要的公式如下，主要用到了链式法则。 三、Experiments作者在文章中也做了很多实验对比，这里简要说明两个： 下图a说明，BN可以加速训练。图b和c分别展示了训练过程中输入数据分布的变化情况。 下表是一个实验结果的对比，需要注意的是在使用BN的过程中，算法对sigmoid激活函数的提升非常明显，解决了困扰学术界十几年的sigmoid过饱和的问题，但sigmoid在分类问题上确实没有ReLU好用，大概是因为sigmoid的中间部分太“线性”了，不像ReLU一个很大的转折，在拟合复杂非线性函数的时候可能没那么高效。 四、算法优势论文中罗列了Batch Normalization的很多作用，一一列举如下： 1）可以使用很高的学习率。如果每层的scale不一致，实际上每层需要的学习率是不一样的，同一层不同维度的scale往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization 2）移除或使用较低的dropout。dropout是常用的防止overfitting的方法，而导致overfitting的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfitting现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%~50%相比，可以大大提高训练速度。 3） 降低L2权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，现在用了Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。 4）取消Local Response Normalization层。 由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。 5）减少图像扭曲的使用。 由于现在训练epoch数降低，所以要对输入数据少做一些扭曲，让神经网络多看看真实的数据。 说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（8）：激活函数]]></title>
    <url>%2F2017%2F05%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的activation function，传入到下一层神经元；再经过该层神经元的activate function，继续往下传递，如此循环往复，直到输出层。其中的激活函数的主要作用是提供网络的非线性建模能力，使得神经网络有足够的capacity来抓取复杂的pattern，在各个领域取得state-of-the-art的结果。 现在假设一个神经网络中仅包含线性激励和全连接运算，那么该网络仅仅能够表达线性映射，即使增加网络的深度也依旧还是线性映射，即输出都是输入的线性组合，失去了隐藏层存在的意义，难以有效建模实际环境中非线性分布的数据。加入非线性激活函数之后，深度学习网络可以逼近任意函数，具备了分层的非线性映射学习能力。加拿大蒙特利尔大学的Bengio教授在 ICML 2016 的文章中给出了激活函数的定义：激活函数是映射 h:R→R，且几乎处处可导。从定义来看，几乎所有连续可导函数都可以用作激活函数。但目前常见的多是分段线性和具有指数形状的非线性函数。 显而易见，activation function在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的activation function不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。 一、软饱和与硬饱和激活函数Bengio 教授等将具有 1）在定义域内处处可导 2）两侧导数逐渐趋近于0，即$\lim_{x\rightarrow\infty}f’\left(x\right)=0$。的激活函数定义为软饱和激活函数。 与极限的定义类似，饱和也分为左饱和与右饱和，左侧软饱和为：$$\lim_{x\rightarrow -\infty}f’\left(x\right)=0$$右侧软饱和为：$$\lim_{x\rightarrow +\infty}f’\left(x\right)=0$$ 与软饱和激活函数相对的是硬饱和激活函数，即：$$f’(x)=0, 当|x|&gt;c,c为常数$$同理，应饱和也分为左饱和与右饱和，左侧硬饱和为：$$f’(x)=0, 当-x&gt;c, c为正数$$右侧硬饱和为：$$f’(x)=0, 当x&gt;c, c为正数$$ 二、sigmoidsigmoid非线性函数的数学公式为：$$\sigma\left(x\right)=\frac{1}{1+e^{-x}}$$函数图像及梯度函数图像如下所示： 它将输入实数值“挤压”到0-1范围内。更具体地说，很大的负数变成0，很大的正数变成1。它是便于求导的平滑函数，其导数为$\sigma(x)(1-\sigma(x))$，这是它的优点。sigmoid 在定义域内处处可导，且两侧导数逐渐趋近于0，即：$\lim_{x\rightarrow\infty}f’\left(x\right)=0$ 然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有三个主要缺点： 1）梯度消失。Sigmoid 的软饱和性，使得深度神经网络在二三十年里一直难以有效的训练，是阻碍神经网络发展的重要原因。具体地，我们知道优化神经网络的方法是Back Propagation，即导数的反向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。sigmoid反向传导的梯度包含了一个f’(x) 因子（sigmoid关于输入的导数），因此一旦输入落入饱和区，f’(x) 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。一般来说， sigmoid 网络在 5 层之内就会产生梯度消失现象。我们也可以在图中看出原因，主要在于两点：(1) 在上图中容易看出，当$\sigma(x)$中x较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 (2) Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。梯度消失问题至今仍然存在，但被新的优化方法有效缓解了，例如DBN中的分层预训练，Batch Normalization的逐层归一化，Xavier和MSRA权重初始化等代表性技术。 2）Sigmoid函数的输出不是Zero-centered的。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数(比如在$f=w^Tx+b$中每个元素都x&gt;0),那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降（如下图所示）。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 3）幂运算相对耗时：相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在ReLU函数中，需要做的仅仅是一个thresholding，相对于幂运算来讲会快很多。 三、tanhtanh非线性函数的数学公式为：$$tanh x = \frac{e^x-e^{-x}}{e^x+e^{-x}}$$函数图像及梯度函数图像如下所示：如上图所示，计算可以知道：$tanh(x)=2sigmoid(2x)-1$，它其实是一个简单放大的sigmoid神经元，和sigmoid神经元一样，也具有软饱和性。但是和sigmoid神经元不同的是，它解决了zero-centered的输出问题，因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。然而，gradient vanishing的问题和幂运算的问题仍然存在。Xavier在文献[]中分析了sigmoid与tanh的饱和现象及特点，具体见原论文。此外，文献[]中提到了tanh网络的收敛速度要比sigmoid块。因为tanh的输出均值比sigmoid更接近0，SGD会更接近natural gradient（一种二次优化技术），从而降低所需的迭代次数。 四、ReLUReLU非线性函数的数学公式为：$$ReLU(x)=max(0,x)$$函数图像及梯度函数图像如下所示：虽然2006年Hinton教授提出通过分层无监督预训练解决深层网络训练困难的问题，但是深度网络的直接监督式训练的最终突破，最主要的原因是新型激活函数ReLU。它有以下几大优点： 1）解决了gradient vanishing问题：ReLU在$x0$时导数为1，所以，ReLU能够在$x&gt;0$时保持梯度不衰减，从而缓解梯度消失问题。 2）计算速度非常快。对比sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，ReLU可以简单地通过对一个矩阵进行阈值计算得到。 3）收敛速度非常快。相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。下图是从 Krizhevsky 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。 4）ReLU另外一个性质是提供神经网络的稀疏表达能力，在Bengio教授的Deep Sparse Rectifier Neural Network[6]一文中被认为是ReLU带来网络性能提升的原因之一。但后来的研究发现稀疏性并非性能提升的必要条件，文献 RReLU [9]也指明了这一点。 PReLU[10]、ELU[7]等激活函数不具备这种稀疏性，但都能够提升网络性能。本文作者在文章[8]中给出了一些实验比较结果。首先，在cifar10上采用NIN网络，实验结果为 PReLU &gt; ELU &gt; ReLU，稀疏性并没有带来性能提升。其次，在 ImageNet上采用类似于[11] 中model E的15 层网络，实验结果则是ReLU最好。为了验证是否是稀疏性的影响，以 LReLU [12]为例进一步做了四次实验，负半轴的斜率分别为1，0.5，0.25, 0.1，需要特别说明的是，当负半轴斜率为1时，LReLU退化为线性函数，因此性能损失最大。实验结果展现了斜率大小与网络性能的一致性。综合上述实验可知，ReLU的稀疏性与网络性能之间并不存在绝对正负比关系。 ReLU也有几个缺点： 1）Dead ReLU Problem。随着训练的推进，部分输入会落入硬饱和区，某些神经元可能永远不会被激活，这个ReLU单元在训练中将不可逆转的死亡，导致相应的参数永远不能被更新，使得数据多样化丢失。这种现象被称为“神经元死亡”。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 2）偏移现象。即输出均值恒大于零。偏移现象和Dead ReLU Problem会共同影响网络的收敛性。 尽管存在上述几个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！ 五、Leaky ReLULeaky ReLU非线性函数的数学公式为：$$f(x)=max(0.01x,x)$$函数图像及梯度函数图像如下所示：人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为0.01x而非0。理论上来说，Leaky ReLU拥有ReLU的所有优点，外加不会有Dead ReLU problem，但是在实际操作中，并没有完全证明Leaky ReLU总是好于ReLU。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。 六、PReLUParametric ReLU非线性函数的数学公式为：$$f(x)=max(\alpha x,x)$$PReLU是ReLU和LReLU的改进版本，具有非饱和性。与LReLU相比，PReLU中的负半轴斜率$\alpha$由back propagation学习而非固定。原文献建议初始化$\alpha$为0.25，不采用正则。 虽然PReLU 引入了额外的参数，但基本不需要担心过拟合。例如，在cifar10+NIN实验中， PReLU比ReLU和ELU多引入了参数，但也展现了更优秀的性能。所以实验中若发现网络性能不好，建议从其他角度寻找原因。 与ReLU相比，PReLU收敛速度更快。因为PReLU的输出更接近0均值，使得SGD更接近natural gradient。证明过程参见原文[10]。 七、RReLU数学形式与PReLU类似，但RReLU[9]是一种非确定性激活函数，其参数是随机的。这种随机性类似于一种噪声，能够在一定程度上起到正则效果。作者在cifar10/100上观察到了性能提升。 综上，ReLU家族讲完了，总结如下图： 其中表格为在cifar10上采用NIN网络的实验结果。 八、MaxoutMaxout[13]是ReLU的推广，其发生饱和是一个零测集事件（measure zero event）。正式定义为：$$max(w_1^Tx+b_1,w^T_2+b_2,···,w_n^Tx+b_n)$$Maxout网络能够近似任意连续函数，且Maxout是对ReLU和leaky ReLU的一般化归纳，当$w_2,b_2,···,w_n,b_n$为0时，退化为ReLU。其实，Maxout的思想在视觉领域存在已久。例如，在HOG特征里有这么一个过程：计算三个通道的梯度强度，然后在每一个像素位置上，仅取三个通道中梯度强度最大的数值，最终形成一个通道。这其实就是Maxout的一种特例。 所以Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和，能够缓解梯度消失），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。 九、ELUELU（Exponential Linear Units）非线性函数的数学公式为：$$f(x)=max(0,x)+\alpha·min(0,exp(x)-1)$$函数图像及梯度函数图像如下所示：ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，并有自身的特点，罗列如下： 1）右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱和能够燃ELU对输入变换或噪声更加鲁棒。 2）ELU的输出均值接近于零，即zero-centered，所以收敛速度更快。经ELU的作者实验，ELU的收敛性质的确优于ReLU和PReLU。在cifar10上，ELU 网络的loss 降低速度更快；在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛 。 它的一个小问题在于计算量稍大，类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。 十、Noisy Activation FunctionsBengio教授在ICML2016提出了一种激活策略[1]，可用于多种软饱和激活函数，例如sigmoid和tanh。当激活函数发生饱和时，网络参数还能够在两种动力下继续更新：正则项梯度和噪声梯度。引入适当的噪声能够扩大SGD的参数搜索范围，从而有机会跳出包河区。在激活函数中引入噪声的更早工作可追溯到[5]，但文献[5]的工作并不考虑噪声引入的时间和大小。本篇的特点在于，只在饱和区引入噪声，且噪声量与饱和程度相关（原式与泰勒展开式一次项之差$\delta$）。算法1中g表示sigmoid，用于归一化$\delta$。注意，ReLU的$\delta$恒为0，无法直接加噪声，所以作者把噪声加在了输入上。 CReLUMPELU十一、小结建议用ReLU非线性函数。但是要注意初始化和learning rate的设置，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。 参考资料[1] Gulcehre, C., et al., Noisy Activation Functions, in ICML 2016. 2016.[2] Glorot, X. and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS 2010.[3] LeCun, Y., et al., Backpropagation applied to handwritten zip code recognition. Neural computation, 1989. 1(4): p. 541-551.[4] Amari, S.-I., Natural gradient works efficiently in learning. Neural computation, 1998. 10(2): p. 251-276.[5] Nair, V. and G.E. Hinton. Rectified linear units improve Restricted Boltzmann machines. ICML 2010.[6] Glorot, X., A. Bordes, and Y. Bengio. Deep Sparse Rectifier Neural Networks.AISTATS 2011.[7] Djork-Arné Clevert, T.U., Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). ICLR 2016.[][9] Xu, B., et al. Empirical Evaluation of Rectified Activations in Convolutional Network. ICML Deep Learning Workshop 2015.[10] He, K., et al. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ICCV 2015.[11] He, K. and J. Sun Convolutional Neural Networks at Constrained Time Cost. CVPR 2015.[12] Maas, A.L., Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. in ICML 2013.[13] Goodfellow, I.J., et al. Maxout Networks. ICML 2013..]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（7）：神经网络的优化方法]]></title>
    <url>%2F2017%2F05%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、Gradient Descent [Robbins and Monro, 1951, Kiefer et al., 1952]机器学习中，梯度下降法常用来对相应的算法进行训练。常用的梯度下降法包含三种不同的形式，分别是BGD、SGD和MBGD，它们的不同之处在于我们在对目标函数进行梯度更新时所使用的样本量的多少。 以线性回归算法来对三种梯度下降法进行比较。一般线性回归函数的假设函数为：$$h_{\theta}=\sum_{j=0}^n{\theta_jx_j}$$（即有n个特征）对应的损失函数为$$L\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^m{\left(h\left(x_i\right)-y_i\right)^2}$$下图即为一个二维参数$\theta _0$和$\theta _1$组对应的损失函数可视化图像： 1.1 BGD（Batch Gradient Descent）批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，其数学形式如下： 1）对上述的损失函数求偏导：$$\frac{\partial L\left(\theta\right)}{\partial\theta_j}=-\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)}x_{j}^{\left(i\right)}$$ 2）由于是最小化损失函数，所以按照每个参数$\theta$的梯度负方向来更新每个$\theta$：$$\theta_{j}^{‘}=\theta_j+\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}}$$其伪代码如下： 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 从上面的公式可以看到，它得到的是全局最优解，但是每迭代一步，都要用到训练集所有的数据，若样本数目$m$很大，那么迭代速度会大大降低。其优缺点如下： 优点：全局最优解；易于并行实现； 缺点：当样本量很大时，训练过程会很慢 1.2 SGD（Stochastic Gradient Descent）由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。对每个样本的损失函数对$\theta$求偏导得到对应的梯度，来更新$\theta$：$$\theta_{j}^{‘}=\theta_j+\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}$$具体的伪代码形式为 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已将将$\theta$迭代到最优解了，对比上面的批量梯度下降，迭代一次不可能最优，如果迭代十次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。其优缺点如下： 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 从迭代次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图表示如下： 1.3 MBGD（Mini-batch Gradient Descent）从上述的两种梯度下降法可以看出，其各自均有优缺点，那么能否在两种方法的性能之间取得一个折中呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。 下面的伪代码中，我们每轮迭代的mini-batches设置为50： 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 1.4 梯度下降算法的局限虽然梯度下降算法效果很好，并且被广泛的使用，但它存在着一些需要解决的问题： 1）首先选择一个合适的学习速率很难。若学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么会阻碍收敛，即在极值点附近振荡 2）学习速率调整（又称学习速率调度，Learning rate schedules）试图在每次更新过程中，改变学习速率，如模拟退火按照预先设定的调度算法或者当相邻的迭代中目标变化小于一个阈值时候减小学习速率。但是梯度下降算法的调度和阈值需要预先设置，无法对数据集特征进行自适应。 3）模型所有的参数每次更新都是使用相同的学习速率。如果我们的数据很稀疏并且我们的特征出现的次数不同，我们可能不会希望所有的参数以某种相同的幅度进行更新，而是针对很少出现的特征进行一次大幅度更新。 4）在神经网络中常见的极小化highly non-convex error functions的一个关键挑战是避免步入大量的suboptimal local minima。Dauphin等人认为实践中的困难来自saddle points而非local minima。这些saddle points（鞍点）经常被一个相等误差的平原包围，导致SGD很难摆脱，因为梯度在所有方向都近似于0。 二、Momentum这是一种启发式算法。形式如下：$$v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta\right)$$$$\theta =\theta -v_t$$我们用物理上的动能势能转换来理解它。即物体在这一时刻的动能=物体在上一时刻的动能+上一时刻的势能差。由于有阻力和转换时的损失，所以两者都乘以一个系数。 就像一个小球从坡上向下滚，当前的速度取决于上一时刻的速度和势能的改变量。 这样在更新参数时，除了考虑到梯度以外，还考虑了上一时刻参数的历史变更幅度。例如，参数上一次更新幅度较大，并且梯度也较大，那么在更新时是不是得更加猛烈一些了。这样的启发式算法，从直观感知上确实有道理。 下面两张图直观的展示了Momentum算法，其中绿色箭头表示上一时刻参数的变更幅度，红色箭头表示梯度，两者向量叠加即得到蓝色箭头即真实的更新幅度。 三、NAG（Nesterov accelerated gradient） [Nesterov, 1983]还是以上面小球的例子来看，momentum方式下小球完全是盲目被动的方式滚下的。这样有个缺点就是在邻近最优点附近是控制不住速度的。我们希望小球可以预判后面的“地形”，要是后面地形还是很陡峭，那就继续坚定不移地大胆走下去，不然的话就减缓速度。 当然，小球自己也不知道真正要走到哪里，这里以$$\theta - \gamma v_{t-1}$$作为下一个位置的近似，将动量的公式更改为：$$v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta - \gamma v_{t-1}\right)$$$$\theta =\theta -v_t$$相比于动量方式考虑的是上一时刻的动能和当前点的梯度，而NAG考虑的是上一时刻的梯度和近似下一点的梯度，这使得它可以先往前探探路，然后慎重前进。 Hinton的slides是这样给出的：其中两个blue vectors分别理解为梯度和动能，两个向量和即为momentum方式的作用结果。 而靠左边的brown vector是动能，可以看出它那条blue vector是平行的，但它预测了下一阶段的梯度是red vector，因此向量和就是green vector，即NAG方式的作用结果。 momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法 四、学习率退火训练深度网络的时候，可以让学习率随着时间退火。因为如果学习率很高，系统的动能就过大，参数向量就会无规律地变动，无法稳定到损失函数更深更窄的部分去。对学习率衰减的时机把握很有技巧：如果慢慢减小，可能在很长时间内只能浪费计算资源然后看着它混沌地跳动，实际进展很少；但如果快速地减少，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有三种方式： 1）随步数衰减：每进行几个周期就根据一些因素降低学习率。通常是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的十分之一。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 2）指数衰减。数学公式是$\alpha=\alpha_0e^{-kt}$，其中$\alpha_0,k$是超参数，$t$是迭代次数（也可以使用周期作为单位）。 3）$1/t$衰减的数学公式是$\alpha=\alpha_0/(1+kt)$，其中$\alpha_0,k$是超参数，t是迭代次数。 在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。但如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 五、自适应学习率方法5.1 Adagrad [Duchi et al., 2011]之前的方法中所有参数在更新时均使用同一个Learning rate。而Learning rate调整是一个非常耗费计算资源的过程，所以如果能够自适应地对参数进行调整的话，就大大降低了成本。在Adagrad的每一个参数的每一次更新中都使用不同的learning rate。这样的话，令第$t$步更新时对第$i$个参数的梯度为$$g_{t,i}=\nabla_{\theta}J\left(\theta_j\right)$$参数的更新的一般形式为：$$\theta_{t+1,i}=\theta_{t,i}-\eta g_{t,i}$$如上所述，Adagrad的差异之处正是在于learning rate不同于其他，将learning rate改为如下：$$\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}·g_{t,i}$$实质上是对学习率形成了一个约束项regularizer：$\frac{1}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}$，${\sum_{i=0}^t{\left(g^i\right)^2}}$是对直至t次迭代的梯度平方和的累加和，$\epsilon $是一个防止分母为0的很小的平滑项。不用平方根操作，算法性能会变差很多 我们可以将到累加的梯度平方和放在一个对角矩阵中$G_t\in\mathbb{R}^{d×d}$中，其中每个对角元素$(i,i)$是参数$\theta_i$到时刻$t$为止所有时刻梯度的平方之和。由于$G_t$的对角包含着所有参数过去时刻的平方之和，我们可以通过在$G_t$和$g_t$执行element-wise matrix vector mulitiplication来向量化我们的操作：$$\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t$$ 优点：Adagrad让学习速率自适应于参数，在前期$g_t$较小的时候，regularizer较大，能够放大梯度；后期$g_t$较大的时候，regularizer较小，能够约束梯度；因为这一点，它非常适合处理稀疏数据。Dean等人发现Adagrad大大地提高了SGD的鲁棒性并在谷歌的大规模神经网络训练中采用了它进行参数更新，其中包含了在Youtube视频中进行猫脸识别。此外，由于低频词（参数）需要更大幅度的更新，Pennington等人在GloVe word embeddings的训练中也采用了Adagrad。 缺点：由公式可以看出，仍依赖于人工设置一个全局学习率；$\eta$设置过大的话，会使得regularizer过于敏感，对梯度的调节太大；中后期，分母上梯度平方的累加将会越来越大，使得梯度为0，训练提前结束。 5.2 RMSprop [Hinton]RMSprop是一个没有公开发表的适应性学习率方法，它是Hinton在他的课上提出的一种自适应学习速率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。它用了一种很简单的方式修改了Adagrad方法，让它不过于激进而过早停止学习。具体说来就是，它使用了一个梯度平方的滑动平均，仍然是基于梯度的大小来对每个权重的学习率进行修改，效果不错。但是和Adagrad不同的是，其更新不会让学习率单调变小。 下图展示了RMSprop的计算过程，其中$\alpha$是一个超参数，常用的值是[0.9,0.99,0.999]： 5.3 Adadelta [Zeiler, 2012]Adadelta是Adagrad的一种扩展，以缓解Adagrad学习速率单调递减问题的算法。Adadelta不是对过去所有时刻的梯度平方进行累加，而是将累加时刻限制在窗口大小为的$w$区间。 但梯度累加没有采用简单的存储前$w$个时刻的梯度平方，而是递归地定义为过去所有时刻梯度平方的decaying average$E[g^2]_t$。$t$时刻的running average仅仅依赖于之前average和当前的梯度：$$E\left[g^2\right]_t=\gamma E\left[g^2\right]_{t-1}+\left(1-\gamma\right)g_{t}^{2}$$类似momentum term，我们将$\gamma$取值在0.9附近。简介起见，我们从参数更新向量$\Delta\theta_t$角度重写普通SGD的参数更新：$$\Delta\theta_t=-\eta ·g_{t,i}$$$$\theta_{t+1}=\theta_t+\Delta\theta_t$$Adagrad中我们推导的参数更新向量现在就以下述形式出现：$$\Delta \theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t$$现在我们简单地将对角矩阵替换为过去时刻梯度平方的decaying average $E[g^2]_t$：$$\Delta \theta_t=-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\odot g_t$$由于分母是root mean squared (RMS) error criterion of the gradient，则上面公式可以替换为：$$\Delta \theta_t=-\frac{\eta}{RMS[g]_t}$$作者发现（和SGD，Momentum或者Adagrad一样）上述更新中的单元不匹配，即只有部分参数进行更新，也就是参数和更新应该有着相同的hypothetical units。为了实现这个目的，他们首先定义了另外一个exponentially decaying average，这一次对更新参数的平方进行操作，而不只是对梯度的平方进行操作： $$E[\Delta\theta^2]_t=\gamma·E[\Delta\theta^2]_t+(1-\gamma)\Delta\theta^2$$参数更新中的root mean squared error则为：$$RMS[\Delta\theta]_t=\sqrt{E[\Delta\theta^2]_t+\epsilon}$$将以前的更新规则中的学习速率替换为参数更新的RMS，则得到Adadelta更新规则:$$\Delta\theta_t=-\frac{RMS[\Delta\theta]_t}{RMS[g]_t}·g_t$$$$\theta_{t+1}=\theta_t+\Delta\theta$$由于Adadelta更新规则中没有了学习速率这一项，我们甚至都不用对学习速率进行设置。 5.4 Adam [Kingma and Ba, 2014]Adaptive Moment Estimation (Adam)是另外一种对每个参数进行自适应学习速率计算的方法，除了像Adadelta和RMSprop一样保存去过梯度平方和的exponentially decaying average外，Adam还保存类似momentum一样过去梯度的exponentially decaying average。它看起来像是RMSProp的动量版。 $$m_t = \beta_1·m_{t-1}+(1-\beta)·g_t$$$$v_t = \beta_2·v_{t-1}+(1-\beta_2)·g^2_t$$$m_t$和$v_t$分别是分别是梯度的一阶矩（均值）和二阶距（偏方差）的估计，由于$m_t$和$v_t$由全零的向量来初始化，Adam的作者观察到他们会被偏向0，特别是在initial time steps或decay rates很小的时候（即$\beta_1$和$\beta_2$都接近于1）,于是他们通过计算bias-corrected一阶矩和二阶矩的估计低消掉偏差。$$\hat{m}=\frac{m}{1-\beta_{1}^{t}}$$$$\hat{v}=\frac{v}{1-\beta_{2}^{t}}$$然后使用上述项和Adadelta和RMSprop一样进行参数更新，可以得到Adam的更新规则：$$\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v}+\epsilon}}\hat{m}$$ Adam的完整更新过程如下图所示，其中它推荐默认设置$\alpha=0.001,\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$，在实际操作中，推荐将Adam作为默认的算法，一般而言跑起来比RMSProp要好一些。但也可以试试SGD+Nesterov动量。 六、算法可视化下面两幅动画让我们直观感受一些优化算法的优化过程。 在第一幅动图中，我们看到他们随着时间推移在损失表面的轮廓（contours of a loss surface）的移动。注意到Adagrad、Adadelta和RMSprop几乎立刻转向正确的方向并快速收敛，但是Momentum和NAG被引导偏离了轨道。这让我们感觉就像看滚下山的小球。然而，由于NAG拥有通过远眺所提高的警惕，它能够修正他的轨迹并转向极小值。 第二幅动图中为各种算法在saddle point（鞍点）上的表现。所谓saddle point也就是某个维度是positive slope，其他维度为negative lope。前文中我们已经提及了它给SGD所带来的困难。注意到SGD、Momentum和NAG很难打破对称，虽然后两者最后还是逃离了saddle point。然而Adagrad, RMSprop, and Adadelta迅速地沿着negative slope下滑。 七、二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下：$$\displaystyle x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)$$ 这里$Hf(x)$是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。 然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，L-BFGS使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。 然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。 实践时在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。 参考资料[1] Kiefer, J., Wolfowitz, J., et al. (1952). Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, 23(3):462–466.[2] Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376.[3] Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.[4] Hinton. Neural Networks for Machine Learning[5] Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method.[6] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization.[7] CS231n Convolutional Neural Networks for Visual Recognition.[8] Sebastian Ruder. An overview of gradient descent optimization algorithms]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch深度学习系列（1）：Convolutional Neural Network]]></title>
    <url>%2F2017%2F05%2F01%2FPytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%EF%BC%9AConvolutional%20Neural%20Network%2F</url>
    <content type="text"><![CDATA[1.载入模块12345import torch import torch.nn as nnimport torchvision.datasets as dsetsimport torchvision.transforms as transformsfrom torch.autograd import Variable 其中torchvision.transforms 用于数据预处理，torchvision.datasets加载内置数据集 2.设置参数123num_epochs = 5batch_size = 100learning_rate = 0.001 迭代次数num_epochs设置为5；批处理样本数batch_size设置为100；学习率learning_rate设置为0.001。 3.加载数据集加载训练集，将MNIST数据集自动从网上下载并解压，train=true表示取出训练集部分，并变换为张量。 1234train_dataset = dsets.MNIST(root='../data/', train=True, transform=transforms.ToTensor(), download=True) 加载测试集，train=False即表示取出测试集部分，并变换为张量。 123test_dataset = dsets.MNIST(root='../data/', train=False, transform=transforms.ToTensor()) 将训练集的60000张图片划分成600份，每份100张图，用于mini-batch输入。同时将测试集的10000张图片分成100份，每份100张图。shffule=True在表示不同批次的数据遍历时，打乱顺序，反之则不打乱顺序。 123456train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 4.CNN模型（两个卷积层）1234567891011121314151617181920212223class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, padding=2),#卷积：1 input image channel, 16 output channels, 5x5 square convolution kernel，2 zero padding） nn.BatchNorm2d(16),#归一化 nn.ReLU(),#非线性激活函数ReLU nn.MaxPool2d(2))#池化层 self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)) self.fc = nn.Linear(7*7*32, 10)#全连接层，in_features, out_features, bias=True def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) out = self.fc(out) return out# 正常情况下, 我们都会用类进行封装一个网络 cnn = CNN() 5.损失函数与优化方法12criterion = nn.CrossEntropyLoss()#损失函数，这里为交叉熵optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)#优化方法，这里使用Adam 6.训练模型12345678910111213141516for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # wrap them in Variable images = Variable(images) labels = Variable(labels) # Forward + Backward + Optimize optimizer.zero_grad() outputs = cnn(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics if (i+1) % 100 == 0: print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0])) Epoch [1/5], Iter [100/600] Loss: 0.1363 Epoch [1/5], Iter [200/600] Loss: 0.0487 Epoch [1/5], Iter [300/600] Loss: 0.0688 Epoch [1/5], Iter [400/600] Loss: 0.1273 Epoch [1/5], Iter [500/600] Loss: 0.0283 Epoch [1/5], Iter [600/600] Loss: 0.0375 Epoch [2/5], Iter [100/600] Loss: 0.0398 Epoch [2/5], Iter [200/600] Loss: 0.0595 Epoch [2/5], Iter [300/600] Loss: 0.0793 Epoch [2/5], Iter [400/600] Loss: 0.0166 Epoch [2/5], Iter [500/600] Loss: 0.0235 Epoch [2/5], Iter [600/600] Loss: 0.0128 Epoch [3/5], Iter [100/600] Loss: 0.0273 Epoch [3/5], Iter [200/600] Loss: 0.0507 Epoch [3/5], Iter [300/600] Loss: 0.0384 Epoch [3/5], Iter [400/600] Loss: 0.0150 Epoch [3/5], Iter [500/600] Loss: 0.0086 Epoch [3/5], Iter [600/600] Loss: 0.0616 Epoch [4/5], Iter [100/600] Loss: 0.0243 Epoch [4/5], Iter [200/600] Loss: 0.0112 Epoch [4/5], Iter [300/600] Loss: 0.0391 Epoch [4/5], Iter [400/600] Loss: 0.0140 Epoch [4/5], Iter [500/600] Loss: 0.0324 Epoch [4/5], Iter [600/600] Loss: 0.0053 Epoch [5/5], Iter [100/600] Loss: 0.0358 Epoch [5/5], Iter [200/600] Loss: 0.0109 Epoch [5/5], Iter [300/600] Loss: 0.0066 Epoch [5/5], Iter [400/600] Loss: 0.0028 Epoch [5/5], Iter [500/600] Loss: 0.0380 Epoch [5/5], Iter [600/600] Loss: 0.0518 7.模型测试1234567891011cnn.eval()correct = 0total = 0for images,labels in test_loader: images = Variable(images) outputs = cnn(images) _,predicted = torch.max(outputs.data,1) total += labels.size(0) correct += (predicted == labels).sum()print ('Test Accuracy of model on the 10000 test images:%d %%'%(100*correct/total)) Test Accuracy of model on the 10000 test images:99 % 8.保存模型1torch.save(cnn.state_dict(),'cnn.pkl')]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（6）：递归神经网络]]></title>
    <url>%2F2017%2F04%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9A%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[上一篇我们学习了循环神经网络，它可以用来处理包含序列的信息。然而，除此之外，信息往往还存在着诸如树结构、图结构等更复杂的结构。对于这种复杂的结构。循环神经网络就无能为力了。本文学习一种更为强大、复杂的神经网络：递归神经网络（Recursive Neural NetWork，RNN），以及它的训练算法BPTS（Back Propagation Through Structure）。顾名思义，递归神经网络可以处理诸如树、图这样的递归网络。 一、递归神经网络的定义因为神经网络的输入层单元个数是固定的，因此必须用循环或者递归的方式来处理长度可变的输入。循环神经网络实现了前者，通过将长度不定的输入分割为等长度的小块，然后再依次的输入到网络中，从而实现了神经网络对变长输入的处理。一个典型的例子是，当我们处理一句话的时候，我们可以把一句话看作是词组成的序列，然后，每次向循环神经网络输入一个词，如此循环直至整句话输入完毕，循环神经网络将产生对应的输出。如此，我们就能处理任意长度的句子了。如下图所示：然而，有时候把句子看作是词的序列是不够的，比如下面这句话“两个外语学院的学生”：上图显示了这句话的两个不同的语法解析树。可以看出这句话有歧义，不同的语法解析树则对应了不同的意思。一个是『两个外语学院的/学生』，也就是学生可能有许多，但他们来自于两所外语学校；另一个是『两个/外语学院的学生』，也就是只有两个学生，他们是外语学院的。为了能够让模型区分出两个不同的意思，我们的模型必须能够按照树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。 递归神经网络可以把一个树、图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。也就是说，如果两句话（尽管内容不容）它的意思是相似的，那么把它们分别编码后的两个向量的距离也更近；反之，如果两句话的意思截然不同，那么编码后的距离则更远。如下图所示： 从上图我们可以看到，递归神经网络将所有的词、句都映射到一个2维向量空间中。句子“the country of my birth”和句子“the place where I was born”的意思是非常接近的，所以表示它们的两个向量在向量空间中的距离很近。另外两个词“Germany”和“France”因为表示的都是地点，它们的向量与上面两句话的向量的距离，就比另外两个表示时间的词“Monday”和“Tuesday”的向量的距离近得多。这样，通过向量的距离，就得到了一种语义的表示。 上图还显示了自然语言可组合的性质：词可以组成句、句可以组成段落、段落可以组成篇章，而更高层的语义取决于底层的语义以及它们的组合方式。递归神经网络是一种表示学习，它可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。比如上面这个例子，递归神经网络把句子”the country of my birth”表示为二维向量[1,5]。有了这个『编码器』之后，我们就可以以这些有意义的向量为基础去完成更高级的任务（比如情感分析等）。如下图所示，递归神经网络在做情感分析时，可以比较好的处理否定句，这是胜过其他一些模型的：在上图中，蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如”intelligent humor”是正面评价，而”care about cleverness wit or any other kind of intelligent humor”是中性评价。我们可以看到，模型能够正确的处理doesn’t的含义，将正面评价转变为负面评价。 尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用循环神经网络处理句子，那么我们可以直接把句子作为输入。然而，如果我们用递归神经网络处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于递归神经网络能够带来的性能提升，这个投入是不太划算的。 二、递归神经网络的前向计算接下来，我们详细介绍一下递归神经网络是如何处理树/图结构的信息的。在这里，我们以处理树型信息为例进行介绍。 递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示： $c_1$和$c_2$分别是表示两个子节点的向量，$p$是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。我们用矩阵$W$表示这些连接上的权重，它的维度将是$d×2d$，其中，$d$表示每个节点的维度。父节点的计算公式可以写成：$$p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\\end{array}\right]+b\right)$$在上式中，tanh是激活函数（当然也可以用其它的激活函数），是偏置项，它也是一个维度为的向量。 然后，我们把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，我们将得到根节点的向量，我们可以认为它是对整棵树的表示，这样我们就实现了把树映射为一个向量。在下图中，我们使用递归神经网络处理一棵树，最终得到的向量$p_3$，就是对整棵树的表示：举个例子，我们使用递归神将网络将”两个外语学校的学生”映射为一个向量，如下图所示：最后得到的向量$p_3$就是对整个句子”两个外语学校的学生”的表示。由于整个结构是递归的，不仅仅是根节点，事实上每个节点都是以其为根的子树的表示。比如，在左边的这棵树中，向量$p_2$是短语”外语学院的学生”的表示，而向量$p_1$是短语”外语学院的”的表示。 $$p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\\end{array}\right]+b\right)$$该式就是递归神经网络的前向计算算法，它和全连接神经网络没有什么区别，只是在输入的过程中需要根据输入的树结构依次输入每个子节点。 需要特别注意的是，递归神经网络的权重$W$和偏置项$b$在所有节点都是共享的。 三、递归神经网络的训练递归神经网络的训练算法和循环神经网络类似，两者不同之处在于，前者需要将残差$\selta$从根节点反向传播到各个子节点，而后者是将残差$\delta$从当前时刻$t_k$反向传播到初始时刻$t_1$。 下面，我们介绍适用于递归神经网络的训练算法，也就是BPTS算法。 3.1 误差项的传递首先，我们先推导将误差从父节点传递到子节点的公式，如下图：定义$\delta_p$为误差函数E相对于父节点$p$的加权输入$net_p$的导数，即：$$\delta_p=\frac{\partial E}{\partial net_p}$$设$net_p$是父节点的加权输入，则$$net_p=W\left[\begin{array}{c} c_1\\ c_2\\\end{array}\right]+b$$在上述式子里，$net_p、c_1、c_2$都是向量，而$W$是矩阵。为了看清楚它们的关系，我们将其展开：$$\left[\begin{array}{c} net_{p1}\\ net_{p2}\\ ···\\ net_{pn}\\\end{array}\right]=\left[\begin{matrix} w_{p1c11}&amp; w_{p1c12}&amp; ···&amp; w_{p1c21}···\\ w_{p2c11}&amp; w_{p2c12}&amp; ···&amp; w_{p2c21}···\\ ···&amp; ···&amp; ···&amp; ···\\ w_{pnc11}&amp; w_{pnc12}&amp; ···&amp; w_{pnc21}···\\\end{matrix}\right]\left[\begin{array}{c} net_{c11}\\ net_{c12}\\ ···\\ net_{c21}\\ net_{c22}\\ ···\\\end{array}\right]$$在上面的公式中，$p_i$表示父节点$p$的第i个分量；$c_{1i}$表示子节点的第i个分量；$c_{2i}$表示$c_2$子节点的第$i$个分量；$w_{p_ic_{jk} }$表子节点$c_j$的第k个分量到父节点p的第i个分量的权重。根据上面展开后的矩阵乘法形式，我们不难看出，对于子节点$c_{jk}$来说，它会影响父节点所有的分量。因此，我们求误差函数E对$c_{jk}$的导数时，必须用到全导数公式，也就是：$$\frac{\partial E}{\partial c_{jk}}=\sum_i{\frac{\partial E}{\partial net_{p_i}}\frac{\partial net_{p_i}}{\partial c_{jk}}}=\sum_i{\delta_{p_i}w_{p_ic_{jk}}}$$有了上式，我们就可以把它表示为矩阵形式，从而得到一个向量化表达：$$\frac{\partial E}{\partial c_j}=U_j\delta_p$$其中，矩阵$U_j$是从矩阵$W$中提取部分元素组成的矩阵。其单元为$u_{j_{ik}}=w_{p_k}c_{ji}$上式看上出可能有点抽象，从下图，我们可以直观的看到$U_j$到底是啥。首先我们把$W$矩阵拆分为两个矩阵$W_1$和$W_2$，如下图所示： 显然，子矩阵$W_1$和$W_2$分别对应子节点$c_1$和$c_2$的到父节点$p$权重。则矩阵$U_j$为：$$U_j=W_j^T$$也就是说，将误差项反向传递到相应子节点$c_j$的矩阵$U_j$就是其对应权重矩阵$W_j$的转置。 现在，我们设$net_{c_j}$是子节点$c_j$的加权输入，$f$是子节点$c$的激活函数，则：$$c_j=f(net_{c_j})$$这样，我们得到：$$\delta_{c_j}=\frac{\partial E}{\partial net_{c_j}}=\frac{\partial E}{\partial c_j}\frac{\partial c_j}{\partial net_{c_j}}=W_{j}^{T}\delta_p°f’\left(net_{c_j}\right)$$如果我们将不同子节点$c_j$对应的误差项$\delta_{c_j}$连接成一个向量$$\delta_c=\left[\begin{array}{c} \delta_{c_1}\\ \delta_{c_2}\\\end{array}\right]$$那么，上式可以写成$$\delta_c=W^T\delta_p°f’\left(net_c\right)$$它就是将误差项从父节点传递到其子节点的公式。注意上式中的$net_c$也是将两个子结点的加权输入$net_{c_1}$和$net_{c_2}$连在一起的向量。有了传递一层的公式，我们就不难写出逐层传递的公式。上图是在树型结构中反向传递项的全景图，反复应用上式，在已知$\delta_p^{(3)}$的情况下，我们不难算出$\delta_p^{(1)}$为：$$\delta^{\left(2\right)}=W^T\delta_{p}^{\left(3\right)}°f’\left(net^{\left(2\right)}\right)$$$$\delta_{p}^{\left(2\right)}=\left[\delta^{\left(2\right)}\right]_p$$$$\delta^{\left(1\right)}=W^T\delta_{p}^{\left(2\right)}°f’\left(net^{\left(1\right)}\right)$$$$\delta_{p}^{\left(1\right)}=\left[\delta^{\left(1\right)}\right]_p$$在上面的公式中$$\delta^{\left(2\right)}=\left[\begin{array}{c} \delta_{c}^{\left(2\right)}\\ \delta_{p}^{\left(2\right)}\\\end{array}\right]$$,$\left[\delta^{\left(2\right)}\right]_p$表示取向量$\delta^{(2)}$属于节点p的部分。 3.2 权重梯度的计算根据加权输入的计算公式：$$net_p^{(l)}=Wc^{(l)}+b$$其中，$net_p^{(l)}$表示第$l$层的父节点的加权输入，$c^{(l)}$表示第$l$层的子节点。W是权重矩阵，$b$是偏置项，将其展开可得：$$net_{p_j}^{l}=\sum_i{w_{ji}c_{i}^{l}}+b_j$$那么，我们可以求得误差函数在第$l$层对权重的梯度为：$$\frac{\partial E}{\partial w_{ji}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial w_{ji}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}·c_{i}^{\left(l\right)}$$上式是针对一个权重项$w_{ji}$的公式，现在需要把它扩展为对所有的权重项的公式。我们可以把上式写成写成矩阵的形式（在下面的公式中，m=2n）:$$\frac{\partial E}{\partial W^{\left(l\right)}}=\delta^{\left(l\right)}·\left(c^{\left(l\right)}\right)^T$$这就是第$l$层权重项的梯度计算公式。我们知道，由于权重$W$是在所有层共享的，所以和循环神经网络一样，递归神经网络的最终权重梯度是各个层权重梯度之和。即：$$\frac{\partial E}{\partial W}=\sum_l{\frac{\partial E}{\partial W^{\left(l\right)}}}$$和循环神经网络一样，递归神经网络最终梯度之和是各层梯度之和。 接下来，我们求偏置项$b$的梯度计算公式。先计算误差函数对第$l$层偏置项$b^{(l)}$的梯度：$$\frac{\partial E}{\partial b_{j}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial b_{j}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}$$把上式扩展为矩阵的形式：$$\frac{\partial E}{\partial b^{\left(l\right)}}=\delta_{p}^{\left(l\right)}$$最终的偏置项梯度是各个层偏置项梯度之和，即：$$\frac{\partial E}{\partial b}=\sum_l{\frac{\partial E}{\partial b^{\left(l\right)}}}$$ 3.3 权重更新如果使用梯度下降优化算法，那么权重更新公式为：$$W\gets W+\eta\frac{\partial E}{\partial W}$$其中，$\eta$是学习速率常数。把之前的式子代入上式，即可完成权重的更新。同理，偏置项的更新公式为：$$b\gets b+\eta\frac{\partial E}{\partial b}$$同样把之前求得式子代入上式，即可完成偏置项的更新。 这就是递归神经网络的训练算法BPTS。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>递归神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（5）：长短时记忆网络（LSTM）]]></title>
    <url>%2F2017%2F04%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要预测的词位置之间的间隔很小，而RNN可以学会使用较近距离的信息。 但是到了一个更加复杂的场景，假设我们试着预测“I grew up in France……I speak fluent French”中最后的词，从这句话的信息来看，下一个词很有可能是一种语言的名字，但具体到是哪种语言，我们就需要在与之距离较远的“I grew up in France”中得到。这说明相关信息与当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。 当然，在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以通过调参来解决，但是在实践中，RNN肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，它们发现一些使训练RNN变得非常困难的相当根本的原因。 既然找到了问题的原因，那我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年的时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出长短时记忆网络，一举解决了这个问题。 二、LSTM的核心思想Long Short Term网络，一般就叫做LSTM，是一种特殊的RNN变体，它可以学习长期依赖信息。LSTM由Hochreiter和Schmidhuber在1997年提出，并在近期被Alex Graves进行了改良和推广。在很多问题上，LSTM都取得了相当巨大的成功，并得到了广泛的使用。LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认属性，而非需要付出很大的代价才能获得的能力！所有的RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。LSTM同样是这样的结构，但是其中重复的模块拥有一个不同的结构。不同于单一神经网络层，这里有四个以非常特殊的方式进行交互的小器件。图中每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，比如向量的和，而黄色的矩阵就是学习到的神经网络层。 LSTM的关键在于细胞（Cell），水平线在细胞内贯穿运行。细胞类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在水平线上很容易保持不变。 LSTM通过精心设计“门”结构来去除或者增加信息到Cell上。门是一种让信息选择式通过的方法（过滤器）。它们包含一个sigmoid神经网络层和一个pointwise乘法操作。 Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过” 三、LSTM的前向计算LSTM用两个门来控制单元状态Cell的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态$c_t-1$有多少保留到当前时刻$c_t$；另一个是输入门（input gate），他决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$。LSTM用输出门（output gate）来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。 3.1 遗忘门我们先看一下遗忘门：$$f_t=\sigma(W_f·[h_{t-1,x_t}]+b_f)$$上式中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。若输入的维度是$d_x$，隐藏层的维度是$d_h$，单元状态的维度是$d_c$（通常$d_c=d_h$），则遗忘门的权重矩阵$W_f$维度是$d_c×(d_h+d_x)$。事实上，权重矩阵$W_f$都是两个矩阵拼接而成的：一个是$W_{fh}$，它对应着输入项$h_{t-1}$，其维度为$d_c×d_h$；一个是$W_{fx}$，它对应着输入项$x_t$，其维度为$d_c×d_x$。$W_f$可以写为：$$\left[W_f\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\\end{array}\right]=\left[\begin{matrix} W_{fh}&amp; W_{fx}\\\end{matrix}\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\\end{array}\right]=W_{fh}·h_{t-1}+W_{fx}x_t$$ 所以总结一下，遗忘门的作用为控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。其数学公式可以写作：$$f_t = sigmoid(W_{fx}·x_t+W_{fh}·h_{t-1}+b_i)$$其计算图示如下： 3.2 输入门接下来看输入门：$$i_t=\sigma(W_i·[h_{t-1},x_t]+b_i)$$上式中，$W_i$是输入们的权重矩阵，$b_i$是输入门的偏置项。下图表示了输入门的计算： 接下来，我们计算用于描述当前输入的单元状态$\tilde{c}_t$，它是根据上一次的输出和本次输入来计算的：$$\tilde{c}_t=\tan\textrm{h}\left(W_c·\left[h_{t-1},x_t\right]+b_c\right)$$下图是$\tilde{c}_t$的计算：现在，我们计算当前时刻的单元状态$c_t$。它是由上一次的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\tilde{c}_t$按元素乘以输入门$i_t$，再将两个积加和产生的：$$c_t=f_t°c_{t-1}+i_t°\tilde{c}_t$$下图是$c_t$的计算图示：这样，我们就把LSTM关于当前的记忆$\tilde{c}_t$和长期的记忆$c_{t-1}$组合在一起，形成了新的单元状态$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。 3.3 输出门下面，我们要看看输入们，它控制了长期记忆对当前输出的影响：$$o_t=\sigma(W_o·[h_{t-1},x_t]+b_o)$$下图表示输出门的计算：LSTM最终的输出，是由输出门和单元状态共同确定的：$$h_t=o_t°\tan\textrm{h}\left(c_t\right)$$下图表示LSTM最终输出的计算： 四、LSTM的训练LSTM的训练算法仍然是反向传播算法，它主要有下面三个步骤： 1）前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$五个向量的值。 2）反向计算每个神经元的误差项$\delta$值。与循环神经网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿着时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。 3）根据相应的误差项，计算每个权重的梯度。 首先，我们队推导中用到的一些公式、符号做一下必要的说明。 接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。它们的导数分别为：$$\sigma\left(z\right)=y=\frac{1}{1+e^{-z}}$$$$\sigma ‘\left(z\right)=y\left(1-y\right)$$$$\tan\textrm{h}\left(z\right)=y=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$$$\tan\textrm{h’}\left(z\right)=1-y^2$$从上面可以看出，sigmoid和tanh函数的导数都是原函数的函数。这样，我们一旦计算原函数的值，就可以用它来计算出导数的值。 LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$，以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$，因为权重矩阵的两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵$W_f、W_i、W_c、W_o$都会被写成分开的两个矩阵：$W_{fh}、W_{fx}、W_{ih}、W_{ix}、W_{oh}、W_{ox}、W_{ch}、W_{cx}$。 我们解释一下按元素乘$o$符号。当$o$作用于两个向量时，运算如下：$$a°b=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\\end{array}\right]°\left[\begin{array}{c} b_1\\ b_2\\ ···\\ b_n\\\end{array}\right]=\left[\begin{array}{c} a_1b_1\\ a_2b_2\\ ···\\ a_nb_n\\\end{array}\right]$$当$o$作用于一个向量和一个矩阵时，运算如下：$$a°X=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\\end{array}\right]°\left[\begin{matrix} x_{11}&amp; x_{12}&amp; ···&amp; x_{1n}\\ x_{21}&amp; x_{22}&amp; ···&amp; x_{2n}\\ ···&amp; ···&amp; ···&amp; ···\\ x_{n1}&amp; x_{n2}&amp; ···&amp; x_{nn}\\\end{matrix}\right]=\left[\begin{matrix} a_1x_{11}&amp; a_1x_{12}&amp; ···&amp; a_{1n}x_{1n}\\ a_2x_{21}&amp; a_2x_{22}&amp; ···&amp; a_2x_{2n}\\ ···&amp; ···&amp; ···&amp; ···\\ a_nx_{n1}&amp; a_nx_{n2}&amp; ···&amp; a_nx_{nn}\\\end{matrix}\right]$$当$o$作用于两个矩阵时，两个矩阵对应位置的元素相乘。按元素乘可以再某些情况下简化矩阵和向量的运算。例如，当一个对角矩阵右乘一个矩阵时，相当于用对角矩阵的对角线组成的向量按元素乘那个矩阵：$diag[a]·X=a °X$当一个行向量右乘一个对角矩阵时，相当于这个行向量按元素乘那个矩阵对角线组成的向量：$$a^T·diag[b]=a°b$$上面这俩点，在后续推导中会多次用到。 在t时刻，LSTM的输出值为$h_t$。我们定义t时刻的误差项$\delta_t$为：$$\delta_t=\frac{\partial E}{\partial h_t}$$注意，这里假设误差项是损失函数对输出值的导数，而不是对加权输入$net^l$的导数。因为LSTM有四个加权输入，分别对应$f_t、i_t、c_t、o_t$，我们希望往上一层传递一个误差项而不是四个。但我们仍然要定义出这四个加权输入，以及他们对应的误差项。 $$$net_{f,t}=W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_f$$$$net_{i,t}=W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_i$$$$net_{\tilde{c},t}=W_c\left[h_{t-1},x_t\right]+b_c=W_{ch}h_{t-1}+W_{cx}x_t+b_c$$$$net_{o,t}=W_o\left[h_{t-1},x_t\right]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o$$$$\delta_{f,t}=\frac{\partial E}{\partial net_{f,t}}$$$$\delta_{i,t}=\frac{\partial E}{\partial net_{i,t}}$$$$\delta_{\tilde{c},t}=\frac{\partial E}{\partial net_{c,t}}$$$$\delta_{o,t}=\frac{\partial E}{\partial net_{o,t}}$$ 4.1 误差项沿时间的反向传播沿时间反向传导误差项，就是要计算出$t-1$时刻的误差项$\delta_{t-1}$。$$\delta_{t-1}^{T}=\frac{\partial E}{\partial h_{t-1}}$$$$=\frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}$$$$=\delta_{t}^{T}\frac{\partial h_t}{\partial h_{t-1}}$$我们知道，$\frac{\partial h_t}{\partial h_{t-1}}$是一个jacobian矩阵。如果隐藏层$h$的维度是N的话，那么它就是一个$N×N$矩阵。为了求出它，我们列出$h_t$的计算公式：$$c_t=f_t°c_{t-1}+i_t°\tilde{c}_t$$$$h_t=o_t°\tan\textrm{h}\left(c_t\right)$$显然，$o_t、f_t、i_t、\tilde{c}_t$都是$h_{t-1}$的函数，那么，利用全导数公式可得： 4.2 将误差项传递到上一层4.3 权重梯度的计算五、LSTM的变体—GRU（Gated Recurrent Unit）前面我们讲了一种最为普通的LSTM，事实上LSTM存在很多变体，许多论文中的LSTM都或多或少的不太一样。只要遵守几个关键点，就可以根据需求设计需要的Gated RNNS。在众多的LSTM变体中，GRU也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。 GRU对LSTM做了两个大改动： 1）将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）$z_t$和重置门（Reset Gate）$r_t$。 2）将单元状态与输出合并为一个状态：$h$ GRU的前向计算公式为：$$z_t=\sigma(W_z·[h_{t-1},x_t])$$$$r_t=\sigma(W_r·[h_{t-1},x_t])$$$$\tilde{h}_t=\tan\textrm{h}\left(W·\left[r_t°h_{t-1},x_t\right]\right)$$$$h=\left(1-z_t\right)°h_{t-1}+z_t°\tilde{h}_t$$下图是GRU的示意图：GRU的训练算法比LSTM相对也要简单一些 当然还有很多其他的变体，如 Gers &amp; Schmidhuber (2000) 提出的LSTM变体增加了“peephole connection”；另一种变体使用coupled 遗忘和输入门对遗忘和需要的信息一同做出决定。Yao, et al. (2015) 提出的Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的Clockwork RNN。 但Greff, et al. (2015)给出了流行变体的比较，结论是它们基本上是一样的。Jozefowicz, et al. (2015) 则在超过一万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>长短时记忆网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（4）：循环神经网络（RNN）]]></title>
    <url>%2F2017%2F04%2F23%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[之前学习了全连接神经网络和卷积神经网络，以及它们的训练与应用。它们都只能单独的去处理单个的输入，且前后的输入之间毫无关系。但是，在一些任务中，我们需要更好的去处理序列的信息，即前后的输入之间存在关系。比如，在理解一整句话的过程中，孤立理解组成这句话的词是不够的，我们需要整体的处理由这些词连接起来的整个序列；当我们处理视频时，我们也不能单独地仅仅分析每一帧，而要分析这些帧连接起来的整个序列。这就引出了深度学习领域中另一类非常重要的神经网络：循环神经网络（Recurrent Neural Network）。 一、语言模型RNN是在自然语言处理领域中最先使用的，如RNN可以为语言模型来建模。那么，何为语言模型？ 自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型（Statistical Language Model）。它最先由贾里尼克提出。 我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词，比如下面这句： 我昨天上学迟到了，老师批评了____。 我们给电脑展示了这句话前面这些词，然后让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是“我”，而不可能是“小明”，甚至是“吃饭”。语言模型的出发点很简单：一个句子是否合理，就看看它的可能性大小如何。 语言模型有很多用处，比如在语音转文本（STT）的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最有可能的。当然，它同样也可以用在图像到文本的识别中（OCR技术）。 在使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3.它的含义是，假设一个词出现的概率只和前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词： 我 昨天 上学 迟到 了 ， 老师 批评 了 ____。 如果用2-Gram进行建模，那么电脑在预测时，只会看到前面的“了”，然后，电脑会在语料库中，搜索“了”后面最有可能的一个词。不管最后电脑选的是不是“我”，这个模型看起来并不是那么靠谱，因为“了”前面的一大堆实际上丝毫没起作用。如果是3-Gram模型呢，会搜索“批评了”后面最有可能的词，看齐俩感觉比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息“我”，远在9个词之前！ 似乎我们可以不断提升N的值，比如4-Gram、9-Gram·······。实际上，这个想法是没有实用性的。在实际应用中最多的是N=3的三元模型，更高阶的模型就很少使用了。主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即$O(|V|^N)$，这里$|V|$是一种语言词典的词汇量，一般在几万到几十万个。然后，使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即$O(|V|^{N-1})$。因此N不能太大。当N从1到2，再从2到3时，模型的效果上升显著。而当模型从3到4时，效果的提升就不是很显著了，而资源的耗费增加却非常快，所以，除非是不惜资源为了做到极致，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语言搜索系统，使用的是四元模型，该模型存储于500台以上的Google服务器中。 RNN就解决了N-Gram的缺陷，它在理论上可以往前看（往后看）任意多个词。 二、基本循环神经网络开始前，我们先回顾一下，简单的MLP三层神经网络模型：其中x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。 再看下图中一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成。我们可以看到，循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。如果我们把上面的图展开，循环神经网络也可以画成下面这个样子： 现在看起来就清楚不少了，这个网络在t时刻接收到输入$X_t$之后，隐藏层的值是$S_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$。我们可以使用下面的公式来表示循环神经网络的计算方法：$$o_t=g(Vs_t)$$$$s_t=f(Ux_t+Ws_{t-1})$$式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，f是激活函数。 从上面的公式可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。 若反复把式2代入带式1，我们将得到：$$o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1}))$$$$=g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2 })))$$$$=g(Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3 }))))$$从上面可以看出，循环神经网络的输出值$o_t$，是受前面历次输入值$x_{t}$、$x_{t-1}$、$x_{t-2} …$的影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。 三、双向循环神经网络对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的，但是如果我们也看到了后面的词是“一部新手机”，那么横线上的词填“买”的概率就大很多了。 而这个在单向循环神经网络是无法建模的，因此我们需要双向循环神经网络，如下图所示： 我们先考虑$y-2$的计算，从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个$A’$参与反向计算。最终的输出值$y_2$取决于$A_2$和$A_2’$，其计算方法为：$$y_2=g(VA_2+V’A_2’)$$$A_2$和$A_2’$则分别计算：$$A_2=f(WA_1+Ux_2)$$$$A_2’=f(W’A_3’+U’x_2 )$$现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_{t-1}$有关；反向计算时，隐藏层的值$s_t’$与$s_{t+1}’$有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法：$$o_t=g(Vs_t+V’s_t’)$$$$s_t=f(Ux_t+Ws_{t-1 })$$$$s_t’=f(U’x_t+W’s_{t+1}’)$$从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说$U$和$U’$、$W$和$W’$、$V$和$V’$都是不同的权重矩阵。 四、深度循环神经网络前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示： 我们把第$i$个隐藏层的值表示为$s_t^{(i)}、s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为：$$o_t=g(V^{(i)}s_t^{(i)}+V^{‘(i)}s_t^{‘(i)})$$$$s_t^{(i)}=f(U^{(i)}s_t^{i-1}+W^{(i)})$$$$s_t^{‘(i)}=f(U^{‘(i)}s_t^{‘(i-1)}+W^{‘(i)}s_{t+1}’)$$$$···$$$$s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1})$$$$s_t^{‘(1)}=f(U^{‘(1)}x_t+W^{‘(1)}s_{t+1}’)$$ 五、循环神经网络的训练算法：BPTTBPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤： 1）前向计算每个神经元的输出值； 2）反向计算每个神经元的误差项$\delta_j$，它是误差函数E对神经元$j$的加权输入$net_j$的偏导数； 3）计算每个权重的梯度。 4）最后再用随机梯度下降算法更新权重。 循环层如下图所示： 5.1 前向计算使用前面的式2对循环层进行前向计算：$$s_t=f(Ux_t+Ws_{t-1})$$注意，上面的$s_t、x_t、s_{t-1}$都是向量，用黑体字表示；而$U、V$是矩阵，用大写字母表示。向量的下标表示时刻，例如，$s_t$表示在$t$时刻向量$s$的值。 我们假设输入向量x的维度是$m$，输出向量的维度是$n$，则矩阵$U$的维度是$n×m$，矩阵$W$的维度是$n×n$。下面是上式展开成矩阵的样子，看起来更直观一点：$$\left[\begin{array}{c} s_{1}^{t}\\ s_{2}^{t}\\ ·\\ s_{n}^{t}\\\end{array}\right]=f\left(\left[\begin{matrix} u_{11}&amp; u_{12}&amp; ··&amp; u_{1m}\\ u_{21}&amp; u_{22}&amp; ··&amp; u_{2m}\\ ·&amp; ·&amp; ·&amp; ·\\ u_{n1}&amp; u_{n2}&amp; ···&amp; u_{nm}\\\end{matrix}\right]\left[\begin{array}{c} x_1\\ x_2\\ ···\\ x_m\\\end{array}\right]+\left[\begin{matrix} w_{11}&amp; w_{12}&amp; ···&amp; w_{1n}\\ w_{21}&amp; w_{22}&amp; ···&amp; w_{2n}\\ ··&amp; ··&amp; ···&amp; ··\\ w_{n1}&amp; w_{n2}&amp; ···&amp; w_{nn}\\\end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\\end{array}\right]\right)$$在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，$s_j^t$表示向量$s$的第$j$个元素在$t$时刻的值。$u_{ji}$表示输入层第$i$个神经元到循环层第$j$个神经元的权重。$w_{ji}$表示循环层第$t-1$时刻的第$i$个时刻的第$j$个神经元的权重。 5.2 误差项的计算BTPP算法将第$l$层$t$时刻的误差项$\delta _t^l$值沿两个方向传播，一个方向是其传递到上一层网络，得到$\delta _t^{l-1}$，这部分只和权重矩阵$U$有关；另一个方向是将其沿着时间线传递到初始$t_1$时刻，得到$\delta_1^l$，这部分只和权重矩阵$W$有关。 我们用向量$net_j$表示神经元在$t$时刻的加权输入，因为：$$net_j=Ux_t+Ws_{t-1}$$$$s_{t-1}=f(net_{t-1})$$因此：$$\frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}$$我们用$a$表示列向量，用$a^T$表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵：$$\frac{\partial net_t}{\partial s_{t-1}}=\left[\begin{matrix} w_{11}&amp; w_{12}&amp; ···&amp; w_{1n}\\ w_{21}&amp; w_{22}&amp; ···&amp; w_{2n}\\ ···&amp; ···&amp; ···&amp; ···\\ w_{n1}&amp; w_{n2}&amp; ···&amp; w_{nn}\\\end{matrix}\right]=W$$上式第二项也是一个jacobian矩阵：$$\frac{\partial s_{t-1}}{\partial net_{t-1}}=\left[\begin{matrix} \frac{\partial s_{1}^{t-1}}{\partial net_{1}^{t-1}}&amp; \frac{\partial s_{1}^{t-1}}{\partial net_{2}^{t-1}}&amp; ···&amp; \frac{\partial s_{1}^{t-1}}{\partial net_{n}^{t-1}}\\ \frac{\partial s_{2}^{t-1}}{\partial net_{1}^{t-1}}&amp; \frac{\partial s_{2}^{t-1}}{\partial net_{2}^{t-1}}&amp; ···&amp; \frac{\partial s_{2}^{t-1}}{\partial net_{n}^{t-1}}\\ ···&amp; ···&amp; ···&amp; ···\\ \frac{\partial s_{n}^{t-1}}{\partial net_{1}^{t-1}}&amp; \frac{\partial s_{n}^{t-1}}{\partial net_{2}^{t-1}}&amp; ···&amp; \frac{\partial s_{n}^{t-1}}{\partial net_{n}^{t-1}}\\\end{matrix}\right]$$$$=\left[\begin{matrix} f’\left(net_{1}^{t-1}\right)&amp; 0&amp; ···&amp; 0\\ 0&amp; f’\left(net_{2}^{t-1}\right)&amp; 0&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\ 0&amp; 0&amp; 0&amp; f’\left(net_{n}^{t-1}\right)\\\end{matrix}\right]$$$$=diag\left[f’\left(net_{t-1}\right)\right]$$最后，将俩项合在一起，可得：$$\frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}=W·diag\left[f’\left(net_{t-1}\right)\right]$$上式描述了将$\delta$沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻$k$的误差项$\delta_k$：$$\delta_{k}^{T}=\frac{\partial E}{\partial net_k}=\frac{\partial E}{\partial net_t}·\frac{\partial net_t}{\partial net_{t-1}}·\frac{\partial net_{t-1}}{\partial net_{t-2}}···\frac{\partial net_{k+1}}{\partial net_k}$$$$=\delta_{t}^{T}Wdiag\left[f’\left(net_{t-1}\right)\right]Wdiag\left[f’\left(net_{t-2}\right)\right]···Wdiag\left[f’\left(net_k\right)\right]$$$$=\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f’\left(net_i\right)\right]}$$这个就是将误差项沿着时间反向传播的算法。 循环层将误差项反向传递到上一层网络，与普通的全连接层是完全一样的，在此简要描述一下：循环曾的加权输入$net^l$与上一层的加权输入$net^{l-1}$关系如下：$$net^l_t=Ua_t^{l-1}+Ws_{t-1}$$$$a_t^{l-1}=f^{l-1}(net_t^{l-1})$$上式中$net_t^l$是第$l$层神经元的加权输入（假如第$l$是循环层）；$net_t^{l-1}$是$l-1$层神经元的加权输入；$a_t^{l-1}$是第$l-1$层神经元的输出；$f^{l-1}$是第$l-1$层的激活函数。$$\frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}}=\frac{\partial net_{t}^{l}}{\partial a_{t}^{l-1}}\frac{\partial a_{t}^{l-1}}{\partial net_{t}^{l-1}}=U ·diag\left[f’^{l-1}\left(net_{t}^{l-1}\right)\right]$$所以$$\delta_{t}^{l-1}=\frac{\partial E}{\partial net_{t}^{l-1}}=\frac{\partial E}{\partial net_{t}^{l}}\frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}}$$$$=\delta_{t}^{l}·U·diag\left[f’^{l-1}\left(net_{t}^{l-1}\right)\right]$$上式就是将误差项传递到上一层算法。 5.3 权重梯度的计算接下来是BPTT算法的最后一步：计算每个权重的梯度。首先我们计算误差函数$E$对权重矩阵$W$的梯度$$\frac{\partial E}{\partial W}$$ 上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻$t$循环层的输出值$s_t$，以及误差项$\delta_t$。 我们只要知道了任意一个时刻的误差项$\delta_t$，以及上一个时刻循环层的输出值$s_{t-1}$，就可以按照下面的公式求出权重矩阵在$t$时刻的梯度：$$\nabla_{w_t}E=\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}&amp; \delta_{1}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}&amp; \delta_{2}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{2}^{t}s_{n}^{t-1}\\ ···&amp; ···&amp; ···&amp; ···\\ \delta_{n}^{t}s_{1}^{t-1}&amp; \delta_{n}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{n}^{t}s_{n}^{t-1}\\\end{matrix}\right]$$上式中，$\delta_i^t$表示$t$时刻误差项向量的第$i$个分量；$s_i^{t-1}$表示$t-1$时刻循环层第$i$个神经元的输出值。 下面我们简单推导一下上式。 我们知道$$net_t=Ux_t+Ws_{t-1}$$$$\left[\begin{array}{c} net_{1}^{t}\\ net_{2}^{t}\\ ·\\ net_{n}^{t}\\\end{array}\right]=Ux_t+\left[\begin{matrix} w_{11}&amp; w_{12}&amp; ···&amp; w_{1n}\\ w_{21}&amp; w_{22}&amp; ···&amp; w_{2n}\\ ···&amp; ···&amp; ···&amp; ···\\ w_{n1}&amp; w_{n2}&amp; ···&amp; w_{nn}\\\end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\\end{array}\right]$$$$=Ux_t+\left[\begin{array}{c} w_{11}s_{1}^{t-1}+w_{12}s_{2}^{t-1}+···+w_{1n}s_{n}^{t-1}\\ w_{21}s_{1}^{t-1}+w_{22}s_{2}^{t-1}+···+w_{2n}s_{n}^{t-1}\\ ···\\ w_{n1}s_{1}^{t-1}+w_{n2}s_{2}^{t-1}+···+w_{nn}s_{n}^{t-1}\\\end{array}\right]$$因为对$W$求导与$Ux_t$无关，我们不加考虑。现在，我们考虑对权重项$w_{ji}$求导。通过观察上式我们可以看到$w_{ji}$只与$net^t_j$有关，所以：$$\frac{\partial E}{\partial w_{ji}}=\frac{\partial E}{\partial net_{j}^{t}}\frac{\partial net_{j}^{t}}{\partial w_{ji}}=\delta_{j}^{t}s_{i}^{t-1}$$按照这个规律就可以生成梯度矩阵$\nabla_{w_t}E$了。 我们已经求得权重矩阵$W$在$t$时刻的梯度$\nabla_{w_t}E$，最终的梯度$\nabla_{w_t}E$是各个时刻的梯度之和：$$=\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}&amp; \delta_{1}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}&amp; \delta_{2}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{2}^{t}s_{n}^{t-1}\\ ···&amp; ···&amp; ···&amp; ···\\ \delta_{n}^{t}s_{1}^{t-1}&amp; \delta_{n}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{n}^{t}s_{n}^{t-1}\\\end{matrix}\right]+···+\left[\begin{matrix} \delta_{1}^{1}s_{1}^{0}&amp; \delta_{1}^{1}s_{2}^{0}&amp; ···&amp; \delta_{1}^{1}s_{n}^{0}\\ \delta_{2}^{1}s_{1}^{0}&amp; \delta_{2}^{1}s_{2}^{0}&amp; ···&amp; \delta_{2}^{1}s_{n}^{0}\\ ···&amp; ···&amp; ···&amp; ···\\ \delta_{n}^{1}s_{1}^{0}&amp; \delta_{n}^{1}s_{2}^{0}&amp; ···&amp; \delta_{n}^{1}s_{n}^{0}\\\end{matrix}\right]$$这就是计算循环曾权重矩阵$W$的梯度的公式。 前面介绍了权重梯度的计算方法，看上去比较直观。但为什么最终的梯度的是各个时刻的梯度之和呢？我们前面只是直接用了这个结论，实际上这里面是有道理的。 我们从这个式子开始：$$net_t=Ux_t+Wf(net_{t-1})$$因为$Ux_t$与$W$完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为$W$和$f(net_{t-1})$都是$W$的函数，所以，对其求偏导得到：$$\frac{\partial net_t}{\partial W}=\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+W\frac{\partial f\left(net_{t-1}\right)}{\partial W}$$我们最终需要计算的是$$\nabla_WE=\frac{\partial E}{\partial W}=\frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial W}=\delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}$$我们先计算加号左边的部分。$\frac{\partial W}{\partial W}$是矩阵对矩阵求导，其结果是一个四维张量（tensor），如下所示：$$\frac{\partial W}{\partial W}=\left[\begin{matrix} \frac{\partial w_{11}}{\partial W}&amp; \frac{\partial w_{12}}{\partial W}&amp; ···&amp; \frac{\partial w_{1n}}{\partial W}\\ \frac{\partial w_{21}}{\partial W}&amp; \frac{\partial w_{22}}{\partial W}&amp; ···&amp; \frac{\partial w_{2n}}{\partial W}\\ ···&amp; ···&amp; ···&amp; ···\\ \frac{\partial w_{n1}}{\partial W}&amp; \frac{\partial w_{n2}}{\partial W}&amp; ···&amp; \frac{\partial w_{nn}}{\partial W}\\\end{matrix}\right]$$$$=\left[\begin{matrix} \left[\begin{matrix} \frac{\partial w_{11}}{\partial w_{11}}&amp; ···&amp; \frac{\partial w_{11}}{\partial w_{1n}}\\ ···&amp; ···&amp; ···\\ \frac{\partial w_{11}}{\partial w_{n1}}&amp; ···&amp; \frac{\partial w_{11}}{\partial w_{nn}}\\\end{matrix}\right]&amp; ···&amp; ···&amp; \left[\begin{matrix} \frac{\partial w_{1n}}{\partial w_{11}}&amp; ···&amp; \frac{\partial w_{1n}}{\partial w_{1n}}\\ ···&amp; ···&amp; ···\\ \frac{\partial w_{1n}}{\partial w_{n1}}&amp; ···&amp; \frac{\partial w_{1n}}{\partial w_{nn}}\\\end{matrix}\right]\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ \left[\begin{matrix} \frac{\partial w_{n1}}{\partial w_{11}}&amp; ···&amp; \frac{\partial w_{n1}}{\partial w_{1n}}\\ ···&amp; ···&amp; ···\\ \frac{\partial w_{n1}}{\partial w_{n1}}&amp; ···&amp; \frac{\partial w_{n1}}{\partial w_{nn}}\\\end{matrix}\right]&amp; ···&amp; ···&amp; \left[\begin{matrix} \frac{\partial w_{nn}}{\partial w_{11}}&amp; ···&amp; \frac{\partial w_{nn}}{\partial w_{1n}}\\ ···&amp; ···&amp; ···\\ \frac{\partial w_{nn}}{\partial w_{n1}}&amp; ···&amp; \frac{\partial w_{nn}}{\partial w_{nn}}\\\end{matrix}\right]\\\end{matrix}\right]$$$$=\left[\begin{matrix} \left[\begin{matrix} 1&amp; 0&amp; ···&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\ ···&amp; ···&amp; ·&amp; ···\\ 0&amp; 0&amp; 0&amp; 0\\\end{matrix}\right]&amp; \left[\begin{matrix} 0&amp; 1&amp; 0&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\ ···&amp; ···&amp; ···&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\\end{matrix}\right]&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\\end{matrix}\right]$$接下来，我们知道$s_{t-1=f(net_{t-1})}$，它是一个列向量。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量$\delta_t^T$，最终得到一个矩阵：$$\delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)=\delta_{t}^{T}\left[\begin{matrix} \left[\begin{matrix} 1&amp; 0&amp; ···&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\ ···&amp; ···&amp; ·&amp; ···\\ 0&amp; 0&amp; 0&amp; 0\\\end{matrix}\right]&amp; \left[\begin{matrix} 0&amp; 1&amp; 0&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\ ···&amp; ···&amp; ···&amp; 0\\ 0&amp; 0&amp; ···&amp; 0\\\end{matrix}\right]&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\\end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\\end{array}\right]$$$$=\left[\begin{matrix} \delta_{1}^{t}&amp; \delta_{2}^{t}&amp; ···&amp; \delta_{n}^{t}\\\end{matrix}\right]\left[\begin{matrix} \left[\begin{array}{c} s_{1}^{t-1}\\ 0\\ ···\\ 0\\\end{array}\right]&amp; \left[\begin{array}{c} s_{2}^{t-1}\\ 0\\ ···\\ 0\\\end{array}\right]&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\ ···&amp; ···&amp; ···&amp; ···\\\end{matrix}\right]$$$$=\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}&amp; \delta_{1}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}&amp; \delta_{2}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{2}^{t}s_{n}^{t-1}\\ ···&amp; ···&amp; ···&amp; ···\\ \delta_{n}^{t}s_{1}^{t-1}&amp; \delta_{n}^{t}s_{2}^{t-1}&amp; ···&amp; \delta_{n}^{t}s_{n}^{t-1}\\\end{matrix}\right]$$$$=\nabla_{W_t}E$$接下来，我们计算加号右边的部分：$$\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}=\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W}$$$$=\delta_{t}^{T}Wf’\left(net_{t-1}\right)\frac{\partial net_{t-1}}{\partial W}$$$$=\delta_{t}^{T}\frac{\partial net_t}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W}$$$$=\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W}$$我们得到了如下递推公式：$$\nabla_WE=\frac{\partial E}{\partial W}=\nabla_{W_t}E+\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W}$$$$=\nabla_{W_t}E+\nabla_{W_{t-1}}E+\delta_{t-2}^{T}\frac{\partial net_{t-2}}{\partial W}$$$$=\nabla_{W_t}E+\nabla_{W_{t-2}}E+···+\nabla_{W_1}E$$$$=\sum_{k=1}^t{\nabla_{W_k}E}$$ 与权重矩阵$W$类似，我们可以得到权重矩阵$U$的计算方法。$$\nabla_{U_t}E=\left[\begin{matrix} \delta_{1}^{t}x_{1}^{t}&amp; \delta_{1}^{t}x_{2}^{t}&amp; ···&amp; \delta_{1}^{t}x_{m}^{t}\\ \delta_{2}^{t}x_{1}^{t}&amp; \delta_{2}^{t}x_{2}^{t}&amp; ···&amp; \delta_{2}^{t}x_{m}^{t}\\ ···&amp; ···&amp; ···&amp; ···\\ \delta_{n}^{t}x_{1}^{t}&amp; \delta_{n}^{t}x_{2}^{t}&amp; ···&amp; \delta_{n}^{t}x_{m}^{t}\\\end{matrix}\right]$$它是误差函数在$t$时刻对权重矩阵$U$的梯度。和权重矩阵$W$一样，最终的梯度也是各个时刻的梯度之和：$$\nabla_UE=\sum_{i=1}^t{\nabla_{U_i}E}$$具体地证明与上述类似。 六、梯度爆炸与梯度消失不幸的是，实践中前面介绍的集中RNNs并不能很好地处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能再较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。 为什么RNN会产生梯度爆炸和梯度消失问题呢？我们根据下式来分析。之前推导过程中得到：$$\delta_{k}^{T}=\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f’\left(net_i\right)\right]}$$$$||\delta_{k}^{T}||\le ||\delta_{t}^{T}||\prod_{i=k}^{t-1}{||W||||diag\left[f’\left(net_i\right)\right]||}$$$$\le ||\delta_{t}^{T}||\left(\beta_W\beta_f\right)^{t-k}$$上式的$\beta$定义为矩阵的模的上界。因为上式是一个指数函数，如果$t-k$很大的话（也就是向前看得很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于$\beta$大于1还是小于1）。 通常来说，梯度爆炸更容易处理一些。因为梯度爆炸时，我们的程序会收到NaN的错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。 梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题： 1）合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。 2）使用ReLu代替sigmoid和tanh作为激活函数。 3）使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrenr Unit（GRU），这是最流行的做法。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（3）：卷积神经网络]]></title>
    <url>%2F2017%2F04%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别领域的重要突破都是卷积神经网络取得的。它在 2012 年崭露头角，Alex Krizhevsky 凭借它们赢得了那一年的 ImageNet 挑战赛（大体上相当于计算机视觉的年度奥林匹克），他把分类误差记录从 26% 降到了 15%，在当时震惊了世界。自那之后，大量公司开始将深度学习用作服务的核心。Facebook 将神经网络用于自动标注算法、谷歌将它用于图片搜索、亚马逊将它用于商品推荐、Pinterest 将它用于个性化主页推送、Instagram 将它用于搜索架构。打败李世石的AlphaGo也用到了这种网络。本文将详细介绍卷积神经网络的结构以及它的训练算法 一、初识卷积神经网络1.1 全连接神经网络与卷积神经网络全连接神经网络之所以不太适合图像识别任务，主要有三个方面的问题： 1）参数数量太多：考虑一个输入为1000×1000像素的图片（100万像素，现在已经不能算大图了），输入层有100万个节点。假设第一个隐藏层有100个节点（这个数量并不多），那么仅这一层就有（1000×1000+1）×100=1亿 的参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此其扩展性很差。显而易见，这种全连接方式效率低下，大量的参数也很快会导致过拟合。 2）没有利用像素之间的位置信息。对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系就比较小了。若一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同对待，这不符合实际情况。当我们完成每个连接权值的学习之后，最终可能会发现，有大量的权值，它们的值都是很小的也就是这些连接其实都是无关紧要的。努力学习大量并不重要的权值，这样的学习必将是非常低效的。比如下面这个喵星人：我们的任务就是想识别这只猫，按照之前的做法就是把这幅图片转换成一个一维向量，然后作为神经网络的输入。对于人眼的物体识别来说，虽然对人眼识别物体的原理并没有研究明白，但绝不是通过把物体转换为一维向量再做识别的。一张图片必然存在着一定的位置关系，比如猫的鼻子下面有嘴巴、鼻子上面有眼睛，这些都是很明确的位置关系，但要是转换成了一维向量，这些位置关系就被掩盖了。 3）网络层数限制：我们知道网络层数越多，其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过三层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。 那卷积神经网络是怎么解决全连接神经网络的这些问题的呢？主要有以下几个方面。 1.2 激活函数——Relu最近几年卷积神经网络中，激活函数往往不选择sigmoid或者tanh函数，而是选择relu函数。relu函数的定义是：$$f\left(x\right)=\max\left(x,0\right)$$Relu函数图像如下所示： Relu函数作为激活函数，有以下几大优势： 1）速度快：采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。而Relu函数其实就是一个max（x,0），整个过程的计算量节省很多。 2）减轻梯度消失问题：回忆一下计算梯度的公式$\nabla =\sigma^,\delta x$。其中$\sigma ^,$是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，没经过一层sigmoid神经元，梯度就要乘上一个$\sigma ^, $。从下图可以看出，$\sigma^,$函数最大值是$\frac{1}{4}$。因此，乘一个$\sigma ^,$会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而Relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面Relu的表现强于sigmoid。使用Relu激活函数可以让你训练更深的网络。 3）通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%~30%的激活率时是比较理想的。因为Relu函数在输入小于0的时候是完全不激活的，因此可以获得一个更低的激活率。 1.3 局部感受野（local receptive fields）在之前的全连接神经网络中，一个样例的输入被转换为一个一维向量。但在一个卷积网络中，把输入看作是一个按照28×28排列的正方形，或者当有颜色通道的时候，比如28x28x3，就是宽高都是28，且有3个颜色通道。比如下图就代表了一个输入 然后，我们通常把输入像素连接到一个隐藏层的神经元，但和全连接神经网络那样每个输入都连接一个隐藏层神经元不同的是，这里我们只是把输入图像进行局部的连接。 如此不断地重复，构建起第一个隐藏层。注意如果我们有一个28×28的输入图像，5×5的局部感受野，那么隐藏层中就会有24×24个神经元。这是因为在抵达抵达最右边或最底部的输入图像之前，我们只能把局部感受野向右或向下移动23个神经元。 如上图所示，把图中间的那个看作是可以“滑动的窗口”，他的作用是和输入相应的“感受域”下的像素做运算得到新的值。这个运算就是“卷积”运算了。图上面有详细的运算过程。实际上就是每个相应元素的值相乘，然后把得到的都加起来。这个窗口的本质是其中的数字和一个偏置构成的，通常就把这个窗口叫做滤波器或者卷积核。上图是对于一个颜色通道的输入做卷积操作，但通常是三个颜色通道。中间那个“窗口”是可以滑动的，每次的滑动步长可以人为指定。 1.4 共享权值与偏置（Shared weights and biases）权值共享是指在一个模型的多个函数中使用相同的参数。 在传统的神经网络中，当计算一层的输出时，权值矩阵的每一个元素只使用一次， 当它乘以输入的一个元素后就再也不会用到了。而在卷积神经网络中，我们对24×24的隐藏层神经元的每一个使用相同的权重和偏置，这样可以很好地使用图像的平移不变性（例如稍稍移动一副猫的图像，它仍然是一副猫的图像）。因为这个原因，我们有时候把输入层到隐藏层的映射称为一个特征映射。把定义特征映射的权重称为共享权重，把以这种方式定义特征映射的偏置称为共享偏置。共享权值和偏置通常被称为一个卷积核或者滤波器。共享权值和偏置有一个很大的优点就是，它大大较少了参与卷积网络的参数，它的平移不变性将会使训练更快，有助于我们使用卷积层建立深度网络。 1.5 池化（pooling）在连续的卷积层之间会周期性地插入一个池化层。经过池化层前后，发生的变化如下图所示： 它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。上图是一个MAX Pooling的过程，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 二、卷积神经网络的层首先，让我们对卷积神经网络有一个感性的认识，下图就是一个卷积神经网络的示意图：如上图所示，一个神经网络由若干卷积层（CONV）、Pooling层（POOL）、全连接层（FC）组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为：$$INPUT\rightarrow\left[\left[CONV\right]\times N\rightarrow POOL\right]\times M\rightarrow\left[FC\right]\times K$$也就是N个卷积层叠加，然后叠加一个Pooling层（可选），重复这个结构M次，最后叠加K个全连接层。 对于上图来说，该卷积神经网络的架构为：$$INPUT\rightarrow\left[\left[CONV\right]\times 1\rightarrow POOL\right]\times 2\rightarrow\left[FC\right]\times 2$$也就是$N=1,M=2,K=2$ 从中我们可以发现卷积神经网络和全连接神经网络的层结构有很大不同。全连接网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。 我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而他的深度为1。接着第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。实际上这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。 在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。 最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。 至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。 2.1 卷积层卷积层的参数是一些可学习的滤波器集合（卷积核）构成，滤波器的宽度和高度一般不大，深度与其输入数据保持一致。见下图： 在上面的过程中，原图像是32×32×3的图像，我们有一个滤波器（卷积核）为5×5×3，5×5的宽高相比起32×32来说，不怎么大，深度3和输入数据保持一致。一个卷积核在原图像上滑动，可以生成一个activation map，这里有6个不同的卷积核，得到的6个不同的activation map分别表示诸如边缘特征、形状特征等特征图，将这些activation map映射在深度方向上层叠起来就生成了输出数据。所以在用了6个过滤器（卷积层）之后，我们可以得到28×28×6的激活图。对各个activation map的直观感受可以看下图，其中每一个activation map代表着不同层次的特征。 2.1.1 卷积层输出值的计算我们使用一个简单的例子来讲述如何计算卷积，然后，抽象出卷积层的一些重要概念和计算方法。 假设有一个5×5的图像，使用一个3×3的滤波器进行卷积，想得到3×3的Feature Map，如下所示：为了清楚地描述卷积的计算过程，我们首先对图像的每个像素进行编号，用$x_{i,j}$表示图像的第$i$行第$j$列元素；对filter的每个权重进行编号，用$w_{m,n}$表示第$m$行第$n$列权重，用$w_b$表示filter的偏置项；对Feature Map的每个元素进行编号，用$a_{i,j}$表示Feature Map的第$i$行第$j$列元素；用$f$表示激活函数（此处使用Relu函数作为激活函数）。然后使用下列公式计算卷积：$$a_{i,j}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+i,n+j}}}+w_b\right)$$例如，对于Feature Map的左上角元素$a_{0,0}$来说，其卷积计算方法为：$$a_{0,0}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+0,n+0}}}+w_b\right)=Relu\left(4\right)=4$$按照这个公式可以依次计算出Feature Map中所有的值，下面的动画显示了整个Feature Map的计算过程： 上面的计算过程中，步幅（stride）为1。当然步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：这里我们可以看到，当把步幅设置为2时，Feature Map就变成了2×2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。我们设卷积前的图像宽度为$N$，步幅为$S$，filter的边长为$F$，卷积后Feature Map宽度为$N_f$，如图所示则它们之间的关系为：$$N_f=\frac{N-F}{S}+1 $$但是这样持续卷积运算下去会出现一些问题，比如下图： 每经过一个filter，得到的激活图就会小一些，要是经过好几个，会导致最后消失殆尽。所以我们通过零填充（zero padding）的方法在原始图像周围补上几圈0，将补上的圈数设为$P$，则改写我们之前的关系式为：$$N_f=\frac{N+2P-F}{S}+1 $$这里$P$乘以2是加了一圈之后两侧都加了1。例如上方那幅图中，$N=5$，$F=3$，$S=1$，我们想保持卷积前后的尺寸保持不变，即$N_f=N=5$，则零填充的$P$为：$$P=\frac{(N_f-1)S+F-N}{2}=\frac{(5-1)×1+3-5}{2}=1$$ 到此我们讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。如果卷积前的图像深度为$D$，那么相应的filter的深度也必须为$D$。我们扩展一些之前的式子，得到深度为$D$的卷积计算公式：$$a_{i,j}=f\left(\sum_{d=0}^{D-1}{\sum_{m=0}^{M-1}{\sum_{n=0}^{N-1}{w_{d,m,n}x_{d,m+i,n+j}}}}+w_b\right)$$该式中，$D$为深度；$w_{d,m,n }$表示filter的第d层第m行第n列的权重；$a_{d,i,j }$表示图像的第d层第i行第j列像素。我们前面还曾提到，每个卷积层可以有多个filterr。每个filter和原始图像进行卷积之后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度和卷积层的filter个数是相同的。 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到7×7×3的输入，经过两个3×3×3filter的卷积，其步幅为2，得到了3×3×2的输出，另外我们也会看到下图的Zero Padding是1，也就是在输入元素的周围补了一圈0。Zero Padding对图像边缘部分的特征提取是很有帮助的。 以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连（卷积计算规则），且filter的权值对于上一层所有神经元都是一样的。对于包含两个3×3×3的filter的卷积层来说，其参数数量仅有$(3×3×3+1)×2=56$个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 2.1.2 用卷积公式来表达卷积层计算之前计算卷积层输出的式子很繁冗，最好可以简化一下 2.2 池化层（Pooling layer）Pooling层的主要作用就是通过下采样，去掉Feature Map中不重要的样本，进一步减少参数数量，降低了计算成本，而且可以控制过拟合（overfitting）。池化层并不会对Feature map的深度有影响，即还是会保持原来的深度。 Pooling的方法很多，最常用的是Max Pooling，它实际上就是在$n×n$的样本中取最大值，作为采样后的样本值。下图是2×2 Max Pooling： 此外，还有平均池化（average pooling）和L2-norm池化。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。池化层背后的直观推理是：一旦我们知道了原始输入中一个特定的特征，它与其他特征的相对位置就比它的绝对位置更重要。 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃池化层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用池化层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无池化层的结构不太可能扮演重要的角色。 2.3 归一化层在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。 2.4 全连接层全连接层输出值的计算经网络和全连接神经网络是一样的，这里就不再赘述了。 三、卷积神经网络的训练和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权值。训练算法依然是反向传播算法。 我们知道神经网络和反向传播那一节中介绍的反向传播算法，它的整个算法分为三个基本步骤： 1）前向计算每个神经元的输出值$a_j$（$j$表示网络的第$j$个神经元，以下同）； 2）反向计算每个神经元的误差项$\delta_j$，$\delta_j$在有的文献中也叫作敏感度（sensitivity）。它实际上是网络的损失函数$E_d$对神经元加权输出$net_j$的偏导数，即$\delta_j=\frac{\partial E_d}{\partial net_j}$； 3）计算每个神经元连接权重$w_{ij}$的梯度（$w_{ij}$表示从神经元$i$连接到神经元$j$的权重，公式为$\frac{\partial E_d}{\partial w_{ji}}=a_i\delta_j$）,其中，$a_i$表示神经元$i$的输出。 4）根据梯度下降法更新每个权重即可 对于卷积神经网络，由于涉及到局部连接、下采样等操作，影响到了第二部误差项$\delta$的具体计算方法，而权值共享影响了第三步权重$w$的梯度的计算方法。接下来，我们分别介绍卷积层和池化层的训练算法。 3.1 卷积层的训练3.2 Pooling层的训练]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（18）：方差偏差权衡（Bias-Variance Tradeoff）]]></title>
    <url>%2F2017%2F04%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8818%EF%BC%89%EF%BC%9A%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E6%9D%83%E8%A1%A1%EF%BC%88Bias-Variance%20Tradeoff%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、定义1.1 感性解释Bias和Variance是针对Generalization（泛化、一般化）来说的。在机器学习中，我们用训练数据集学习一个模型，我们通常会定义一个损失函数（Loss Function），然后将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（即将训练出来的模型运用于预测集），单纯地将训练数据集的Loss最小化，并不能保证解决更一般的问题时模型仍然是最优的，甚至不能保证模型是可用的。这个训练数据集的Loss与一般化的数据集（预测数据集）的Loss之间的差异就叫做Generalization error。 而Generalization error又可以细分为Random Error、Bias和Variance三个部分。 首先需要说的是随机误差。它是数据本身的噪声带来的，这种误差是不可避免的。 其次如果我们能够获得所有可能的数据集合，并在这个数据集合上将Loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以真实模型一定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。 Bias和Variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距（除去随机误差）。 Bias描述的是对于测试数据集，“用所有可能的训练数据集训练出的所有模型的输出预测结果的期望”与“真实模型”的输出值（样本真实结果）之间的差异。简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就是复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)。 Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。 在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。 1.2 图示解释下图将机器学习任务描述为一个打靶的活动：根据相同算法、不同训练数据集训练出的模型，对同一个样本进行预测；每个模型作出的预测相当于是一次打靶。 左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。 右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。 左下角的示例表示偏差大而方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。 右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。 再看一个来自PRML的例子： 这是一个曲线拟合的问题，对同分布的不同数据集进行了多次的曲线拟合，左边表示方差（variance），右边表示偏差（bias），绿色是真实值函数。$In \lambda$表示的是模型的复杂度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低的时候，方差是很小的，但是偏差很大；但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了，所以方差就很大，但此时偏差就很小了。 1.3 数学解释排除人为的失误，人们一般会遇到三种误差来源：随机误差、偏差和方差。 首先需要说明的是随机误差。随机误差是数据本身的噪声带来的，这种误差是不可避免的。一般认为随机误差服从高斯分布，记作$\varepsilon ~N\left(0,\sigma_{\varepsilon}\right)$。因此，若有变量$y$作为预测值，以及$X$作为自变量（协变量），那么我们将数据背后的真实规律$f$记作$$y=f(X)+\epsilon$$偏差和方差则需要在统计上做对应的定义。 偏差（Bias）描述的是通过学习拟合出来的结果的期望，与真实结果之间的差距，记作$$Bias\left(X\right)=E\left[\hat{f}\left(X\right)\right]-f\left(X\right)$$ 方差（Variance）即为统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作$$E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2$$ 以均方误差为例，有如下推论：$$Err\left(X\right)=E\left[\left(y-\hat{f}\left(X\right)\right)^2\right]\\=E\left[\left(f\left(X\right)+\varepsilon -\hat{f}\left(X\right)\right)^2\right]$$$$=\left(E\left[\hat{f}\left(X\right)\right]-f\left(X\right)\right)^2+E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2+\sigma_{\varepsilon}^{2}$$$$=Bias^2+Variance+Random Error$$ 二、如何Tradeoff2.1 最佳平衡点假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习）。在训练的起始，Bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大的差异。 而随着训练过程的进行，Bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能的训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了了我们在很多其他的数据集上就无法获得很好地效果，也就是所谓的Overfitting（过拟合）。 考虑到模型误差是偏差与方差的加和，因此我们可以绘制出这样的图像。 图中的最优位置，实际上是Total Error曲线的拐点。我们知道，连续函数的拐点意味着此处一阶导数的值为0。即$$\frac{d\left(Total Error\right)}{d\left(Complexity\right)}=\frac{d\left(Bias+Variance\right)}{d\left(Complexity\right)}=\frac{d\left(Bias\right)}{d\left(Complexity\right)}+\frac{d\left(Variance\right)}{d\left(Complexity\right)}=0$$这个公式给出了寻找最优平衡点的数学描述。若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于欠拟合；若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合。 3.2 过拟合与欠拟合的外在表现尽管有了上述的数学表述，但是在现实环境中，有时候我们很难计算模型的偏差与方差。因此，我们需要通过外在表现，判断模型的拟合状态：是欠拟合还是过拟合。 同样地，在有限的训练数据集中，不断增加模型的复杂度，意味着模型会尽可能多地降低在训练集上的误差。因此在训练集上，不断地增加模型的复杂度，训练集上的误差会一直下降。 我们把数据分为三个部分：训练数据集、验证数据集、测试数据集。 因此，我们可以绘制出这样的图像。在上图左边区域，训练集与验证集的误差都很高，这块区域的偏差比较高。在右边区域，在验证集上误差很高，但是在训练集上偏差很低，这块区域的方差比较高。我们希望在中间的区域得到一个最优平衡点。 所以，偏差较高（欠拟合）有以下两个特征： 1）训练集误差很高 2）验证集误差和训练集误差差不多大 方差较高（过拟合） 1）训练集误差较低 2）非常高的验证集误差 3.3 如何处理欠拟合与过拟合有了以上的分析，我们就能比较容易地判断模型所处的拟合状态。接下来，我们可以参考Ng提供的处理模型欠拟合与过拟合的一般方法了。 当模型处于欠拟合状态时，根本的办法是增加模型的复杂度。我们一般有以下一些办法： 1）增加模型迭代次数； 2）训练一个复杂度更高的模型：比如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM 3）获取更多的特征以供训练使用：特征少，对模型信息的刻画就不足够了 4）降低正则化权重：正则化正是为了限制模型的灵活度（复杂度）而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，根本的办法是降低模型的复杂度。我们一般有以下一些办法： 1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。 2）减少特征数量 3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Tradeoff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（17）：非平衡数据处理]]></title>
    <url>%2F2017%2F04%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、Introduction常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到不平衡的数据集，比如广告点击预测（点击转化率一般都很小）、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测等等。 对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很高。 但对于极不均衡的分类问题，比如仅有1%的人是坏人，99%的人是好人，最简单的分类模型就是将所有人都划分为好人，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。 在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。 处理不平衡数据，可以从两方面考虑：一是改变数据分布，从数据层面使得类别更为平衡； 二是改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。 本部分对数据层面的一些方法做一个介绍，改变数据分布的方法主要是重采样： 1）过采样：增加少数类样本的数量 2）欠采样：减少多数类样本的数量 3）综合采样：将过采样和欠采样结合 二、过采样2.1 随机过采样采样算法通过某一种策略改变样本的类别分布，以达到将不平衡分布的样本转化为相对平衡分布的样本的目的，而随机采样是采样算法中最简单也最直观易懂的一种方法。 随机过抽样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下： 1.首先在少数类$S_{min}$集合中随机选中一些少数类样本 2.然后通过复制所选样本生成样本集合$E$ 3.将它们添加到$S_{min}$中来扩大原始数据集从而得到新的少数类集合$S_{min-new}$ $S_{min}$中的总样本数增加了 $|E| $个新样本，且$S_{min-new}$ 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。 重复样本过多，容易造成分类器的过拟合 2.2 SMOTE算法(Synthetic Minority Oversampling Technique)在合成抽样技术方面，Chawla NY等人提出的SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。 SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集$S_{min}$ $\subset$ $S$，对于每一个样本$x_i$$\subset$ $S_{min}$使用K-近邻法，其中K-近邻被定义为考虑$S_{min}$中的K个元素本身与$x_i$的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。算法流程如下： 1）对于少数类中的每一个样本$(x_i)$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到K近邻； 2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其K近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$； 3）对于每一个随机选出的近邻$\tilde{x}$，分别与原样本按照如下的公式构建新的样本:$x_{new}=x+rand\left(0,1\right)\times\left(\tilde{x}-x\right)$ 2.3 Borderline-SMOTE算法原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。算法流程如下： 1.首先，对于每个$x_{i}$$\subset$$S_{min}$确定一系列K-近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN}$$\subset$$S$； 2.然后，对每个样本$x_{i}$，判断出最近邻样本集中属于多数类样本的个数，即：|$S_{i-kNN}\cap$$S_{maj}$|； 3.最后，选择满足下面不等式的$x_{i}$:$\frac{k}{2}$&lt;|$S_{i-kNN}$$\cap$$S_{maj}$|&lt;$k$,将其加入危险集$DANGER$， 对危险集中的每一个样本点（最容易被错分的样本），采用普通的$SMOTE$算法生成新的少数类样本。 三、欠采样3.1 随机欠采样减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。 1）首先我们从$S_{maj}$中随机选取一些多数类样本$E$ 2）将这些样本从$S_{maj}$中移除，就有|$S_{maj-new}|=|S_{maj}-|E$| 优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。 3.2 Tomek Links方法定义：Tomek links被定义为相反类最近邻样本之间的一对连接。 符号约定：给定一个样本对$\left(x_i,x_j\right)$，其中$x_{i}$ $\in$ $S_{maj}$，$x_{j}$ $\in$ $S_{min}$，记$d\left(x_i,x_j\right)$是样本$x_i$和$x_j$之间的距离 公式表示：如果不存在任何样本$x_k$，使得$d\left( x_i,x_k \right)$ &lt;$d\left( x_i,x_j \right)$ ，那么样本对$\left(x_i,x_j\right)$被称为Tomek Links 使用这种方法，如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声要么它们都在两类的边界上。所以Tomek Links一般有两种用途：在欠采样中：将Tomek Links中属于是多数类的样本剔除；在数据清洗中，将Tomek Links中的两个样本都剔除。 3.3 NearMiss方法NearMiss方法是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助KNN，总结起来有以下几类： 1）NearMiss-1：在多数类样本中选择与最近的三个少数类样本的平均距离最小的样本 2）NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本 3）NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本 NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。 NearMiss-1方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离。 NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。 实验结果表明得到NearMiss-2的不均衡分类性能最优。 四、Informed UnderstandingInformed欠抽样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中有一些集成（ensemble）的想法，主要有两种方法，分别是EasyEnsemble算法和BalanceCascade算法。 4.1 EasyEnsemble算法它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本$S_{maj}$，通过$n$次有放回抽样生成$n$份子集，少数类样本$S_{min}$分别和这$n$份样本合并训练AdaBoost分类器，这样可以得到$n$个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为N，少数类样本为P，算法流程如下： EasyEnsemble的想法是多次随机欠抽样，尽可能全面地涵盖所有信息，算法特点是利用boosting减小偏差（Adaboost）、bagging减小方差（集成分类器）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。 4.2 BalanceCascade算法EasyEnsemble算法训练的子过程是独立的，BalanceCascade则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法流程如下： BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。 五、综合采样目前为止我们使用的重采样方法几乎都是只针对某一类样本：对多数类样本欠采样，对少数类样本过采样。也有人提出将欠采样和过采样综合的方法，解决样本类别分布不平衡和过拟合问题，本部分介绍其中的SMOTE+Tomek Links和SMOTE+ENN。 5.1 SMOTE+Tomek LinksSMOTE+Tomek Links方法的算法流程非常简单： 1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2.剔除T中的Tomek Links对 普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合。 Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题，下图红色加号为SMOTE产生的少数类样本，可以看到，红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决。 由于第一步SMOTE方法已经很好地平衡了类别分布，因此在使用Tomek Links对的时候考虑剔除所有的Tomek Links对。 5.2 SMOTE+ENNSMOTE+ENN方法和SMOTE+Tomek Links方法的想法和过程都是很类似的： 1）利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2）对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非平衡数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在线编程系列：剑指offer（1-5）]]></title>
    <url>%2F2017%2F04%2F16%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B1-5%2F</url>
    <content type="text"><![CDATA[一、二维数组中的查找1.1 题目描述：在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1.2 思路： 利用二维数组由上到下，由左到右递增的规律，那么选取右上角或者左下角的元素a[row][col]与target进行比较， 当target小于元素array[row][col]时，那么target必定在元素a所在行的左边,即col-=1； 当target大于元素a[row][col]时，那么target必定在元素a所在列的下边,即row+=1； 1.3 代码实现1234567891011121314151617# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row_0 = len(array) col_0 = len(array[0]) row = 0 col = col_0-1 while row &lt; row_0 and col &gt;=0: if target == array[row][col]: return True elif target &gt; array[row][col]: row +=1 else: col -=1 return False 二、替换空格2.1 题目描述请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 2.2 思路 Python replace() 方法把字符串中的 old（旧字符串） 替换成 new(新字符串)，如果指定第三个参数max，则替换不超过 max 次。 replace()方法语法：str.replace(old, new[, max]) 参数old – 将被替换的子字符串。new – 新字符串，用于替换old子字符串。max – 可选字符串, 替换不超过 max 次 2.3 代码实现1234567 -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): # write code here s = s.replace(' ','%20') return s 三、从头到尾打印链表3.1 题目描述输入一个链表，从尾到头打印链表每个节点的值。 3.2 思路Python 中的 list 并不是我们传统（计算机科学）意义上的列表，这也是其 append 操作会比 insert 操作效率高的原因。传统列表——通常也叫作链表（linked list）——通常是由一系列节点（node）来实现的，其每一个节点（尾节点除外）都持有一个指向下一个节点的引用。 其简单实现： 1234class Node: def __init__(value, next=None): self.value = value self.next = next 接下来，我们就可使用链表的结构来组织所有节点了。 123&gt;&gt;&gt; L = None('a', Node('b', Node('c', Node('d'))))&gt;&gt;&gt; L.next.next.value'c' 这是所谓的单向链表，双向链表的各节点还需要持有一个指向前一节点的引用。 3.3 代码实现123456789101112131415# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here list = [] while listNode: list.append(listNode.val) listNode = listNode.next return list[::-1] 四、重建二叉树4.1 题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 4.2 解题思路已知前序、中序、求后序遍历例如：前序遍历: GDAFEMHZ 中序遍历: ADEFGHMZ 画树求法： 1）根据前序遍历的特点，我们知道根结点为G 2）观察中序遍历ADEFGHMZ。其中root节点G左侧的ADEF必然是root的左子树，G右侧的HMZ必然是root的右子树。 3）观察左子树ADEF，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为D。 4）同样的道理，root的右子树节点HMZ中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 5）观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。该步递归的过程可以简洁表达如下： 1、确定根,确定左子树，确定右子树。2、在左子树中递归。3、在右子树中递归。4、打印当前根。 那么，我们可以画出这个二叉树的形状：那么，根据后序的遍历规则，我们可以知道，后序遍历顺序为：AEFDHZMG 4.3 代码实现1234567891011121314151617181920# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None elif len(pre) == 1: return TreeNode(pre[0]) else: Node = TreeNode(pre[0]) index = tin.index(pre[0]) Node.left = self.reConstructBinaryTree(pre[1:index+1],tin[:index]) Node.right = self.reConstructBinaryTree(pre[index+1:],tin[index+1:]) return Node 五、用两个栈实现队列5.1 题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 5.2 解题思路栈是一个非常常见的数据结构，它在计算机领域中被广泛应用，比如操作系统会给每个线程创建一个栈来存储函数调用时各个函数的参数、返回地址及临时变量等。栈的特点是后进先出，即最后被压入（push）栈的元素会第一个被弹出（pop）。 队列是另外一种很重要的数据结构。和栈不同的是，队列的特点是先进先出，即第一个进入队列的元素将会第一个出来。 栈和队列虽然是针锋相对的两个数据结构，但有意思的是他们却相互联系。 通过一个具体的例子来分析往队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空，再向stack1压入b和c，此时stack1中的元素有{a,b,c}，其中c处于栈顶，而stack2仍然是空的。 因为a是最先进的，最先被删除的元素应该是a，但a位于栈低。我们可以把stack1中的元素逐个弹出并压入stack2，元素在stack2的顺序正好和原来在stack1的顺序相反因此经过三次弹出stack1和压入stack2操作之后，stack1为空，而stack2的元素是{c,b,a}，这时就可以弹出stack2的栈顶a了，随后弹出stack2中的b和c，而这个过程中stack1始终为空. 从上面的分析我们可以总结出删除一个元素的步骤：当stack2中不为空时，在stack2的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入之后就处于stack2的顶端了，又可以直接弹出。 5.3 代码实现123456789101112131415# -*- coding:utf-8 -*-class Solution: def __init__(self): self.stack1=[] self.stack2=[] def push(self, node): # write code here self.stack1.append(node) def pop(self): # return xx if self.stack2==[]: while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop() return self.stack2.pop()]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（2）：神经网络MNIST实战]]></title>
    <url>%2F2017%2F04%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMNIST%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[一、数据集与任务介绍MNIST数据集是一个基本的手写字体识别数据集，该数据原本是包含60000个训练图像和10000个测试图像，但这里我们事先对数据进行了划分，从训练样本中抽取10000个数据作为验证集，所以处理后的数据集包含50000个训练样本（training data）、10000个验证样本（validation data）10000个测试样本（test data），都是28乘以28的分辨率。 我们可以先将数据集从GitHub上Clone下来： 1➜ fig git:(master) ✗ git clone https://github.com/lisa-lab/DeepLearningTutorials 可以从这个链接了解对该数据集的加载和处理。 这里任务就是构建神经网络来实现对于MNIST数据集的手写字体识别分类。从任务和输入就能够得到大概的网络结构： 损失函数为平方误差损失函数，激活函数为sigmoid函数。 二、读取数据读取数据由mnist_loader.py这个文件实现。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding: utf-8 -*-"""mnist_loader~~~~~~~~~~~~A library to load the MNIST image data. For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``. In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code."""#### Libraries# Standard libraryimport cPickleimport gzip# Third-party librariesimport numpy as np#从数据集中载入数据def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data)#改变数据集的格式def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data)def vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network.""" e = np.zeros((10, 1)) e[j] = 1.0 return e 2.1 load_data函数12345def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) load_data()函数的主要作用就是解压数据集，然后从数据集中把数据取出来。取出来之后的几个变量代表的数据的格式分别如下： training_data：是一个由两个元素构成的元组。其中一个元素是测试图片集合，是一个50000✖️784的numpy ndarray（其中50000行就是样本个数，784列就是一个维度，即一个像素）；第二个元素就是一个测试图片的标签集，是一个50000✖️1的Numpy ndarray，其中指明了每一个样本是什么数字，通俗来说就是这个样子：validation_data 和 test_data 的结构和上面的training_data是一样的，只是数量不一样，这两个是10000行。 2.2 load_data_wrapper()函数1234567891011121314def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) 之前的load_data返回的格式虽然很漂亮，但是并不是非常适合我们这里计划的神经网络的结构，因此我们在load_data的基础上使用load_data_wrapper（）函数来进行一点点适当的数据集变换，使得数据集更加适合我们的神经网络训练。 以训练集的变换为例。对于training_inputs来说，就是把之前的返回的training_data[0]，即第一个元素的所有样例都放到一个列表中，简单的来说如下所示： 同样可以知道training_labels的样子为： 然后training_data为zip函数组合，那么training_data为一个列表，其中每个元素是一个元组，二元组又有一个training_inputs和一个training_labels的元素组合而成，如下图： 同理可以推出其他数据的形状。 三、神经网络代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# -*- coding: utf-8 -*-import randomimport numpy as npclass Network(object): #初始化神经网络 def __init__(self, sizes): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.""" self.num_layers = len(sizes)#神经网络层数 self.sizes = sizes#储存各层神经元个数的列表 self.biases = [np.random.randn(y, 1) for y in sizes[1:]]#随机初始化偏置 self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]#随机初始化权重 #前向传播算法 def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a #随机梯度下降（训练数据，迭代次数，小样本数量，学习率，是否有测试集（默认为无）） def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" if test_data: n_test = len(test_data)#若有测试集，则计算其大小 n = len(training_data)#训练集大小 #迭代过程 for j in xrange(epochs): # shuffle() 方法对训练集随机排序 random.shuffle(training_data) # mini_batch是列表中切割之后的列表 mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format(j, self.evaluate(test_data), n_test) else: print "Epoch &#123;0&#125; complete".format(j) def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.""" #存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #mini_batch中的一个实例调用梯度下降得到各个参数的偏导 for x, y in mini_batch: 从一个实例得到的梯度 delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] #每一个mini_batch更新一下参数 self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] #反向传播（对于每一个实例） def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" # 存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # 前向过程 activation = x #存储所有的激活值，一层一层的形式 activations = [x] # list to store all the activations, layer by layer #存储所有的中间值（weighted sum） zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # 反向过程 # 输出层error delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. # 非输出层 for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) # 输出层cost函数对于a的导数 def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)#### Miscellaneous functions#sigmoid函数def sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))#sigmoid函数的导数def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 四、结果比较将隐藏层设为30层，随机梯度下降的迭代次数为30次，小批量数量大小为10，学习速率为3.0 123456789101112131415In [77]: import mnist_loaderIn [78]: import networkIn [80]: training_data,validation_data,test_data = mnist_loader.load_data_wrapper()In [81]: net = network.Network([784,30,10])In [82]: net.SGD(training_data,30,10,3.0,test_data = test_data)Epoch 0: 8185 / 10000Epoch 1: 8363 / 10000Epoch 2: 8404 / 10000Epoch 3: 8447 / 10000······Epoch 25: 9492 / 10000Epoch 26: 9494 / 10000Epoch 27: 9468 / 10000Epoch 28: 9504 / 10000Epoch 29: 9507 / 10000 经过30次迭代之后，神经网络的识别率为95%左右。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（1）：神经网络与反向传播算法]]></title>
    <url>%2F2017%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、神经元首先我们从最简单的神经网络——神经元讲起，以下即为一个神经元（Neuron）的图示： 这个神经元是一个以$x_1,x_2,···,x_K$以及截距$b$为输入值的运算单元，其输出为$$\alpha =\sigma\left(w^Ta+b\right)=\sigma\left(w_1a_1+w_2a_2+···+w_Ka_K+b\right)$$其中$w$为权值项，$b$为偏置项，函数$\sigma$被称为“激活函数”。之前在学习感知机的时候，我们知道感知机的激活函数是阶跃函数；而当我们说神经元的时，激活函数往往选择sigmoid函数或tanh函数。激活函数的作用就是将之前加法器输出的函数值$z$进行空间映射，如下图所示： 可以看出，这个单一神经元的输入输出的映射关系其实就是一个逻辑回归（logistic regression）。 关于sigmoid阶跃函数的性质，在逻辑回归中已经了解过了，有一个等式我们会用到：$f^,(z)=f(z)(1-f(z))$。现在我们简要看一下双曲正切函数（tanh）。它的表达式为：$$f\left(z\right)=\tan\textrm{h}\left(z\right)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$它们图像为tanh（z）函数是sigmoid函数的一种变体，它的取值范围为[-1,1]，而不是sigmoid函数的[0,1]，它的导数为$f^，(z)=1-(f(z))^2$ 二、神经网络模型2.1 神经网络模型所谓神经网络就是将许多神经元联结在一起，这样，一个神经元的输出就可以是另一神经元的输入。例如，下图就是一个简单的神经网络： 我们使用圆圈来表示神经网络的输入，标上”+1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右边的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，如此命名是因为我们不能在训练样本中观测到它们的值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不算在内），三个隐藏单元及一个输出单元。 我们用$n_l$来表示神经网络的层数，本例中$n_l=3$，我们将第$l$层记为$L_l$，于是$L_1$是输入层，输出层是$L_{nl}$。本例神经网络有参数$(W,b)=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$，其中$W_{ij}^{(l)}$是第$l$层第$j$个单元与第$l+1$层第$i$单元之间的联接参数（其实就是连接线上的权重，注意标号前后顺序），$b_i^{(l)}$是第$l+1$层第$i$个单元的偏置项。偏置单元没有输入，即没有单元连向偏置单元，它们总是输出$+1$。同时，我们用$s_l$表示第$l$层的节点数（偏置单元不计在内）。 我们用$a_i^{(l)}$表示第$l$层第$i$个单元的激活值（输出值）。当$l=1$时，$a_i^{(1)}=x_i$，也就是第$i$个输入值（输入值的第$i$个特征）。对于给定参数集合$W,b$，我们的神经网络就可以按照函数$h_{W,b}{(x)}$来计算结果。本例中神经网络的计算步骤如下：$$a_{1}^{\left(2\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(1\right)}\right)$$$$a_{2}^{\left(2\right)}=f\left(W_{21}^{\left(1\right)}x_1+W_{22}^{\left(1\right)}x_2+W_{23}^{\left(1\right)}x_3+b_{2}^{\left(1\right)}\right)$$$$a_{3}^{\left(2\right)}=f\left(W_{31}^{\left(1\right)}x_1+W_{32}^{\left(1\right)}x_2+W_{33}^{\left(1\right)}x_3+b_{3}^{\left(1\right)}\right)$$$$h_{w,b}\left(x\right)=a_{1}^{\left(3\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(2\right)}\right)$$ 2.2 具体举例接下来举一个具体的例子来说明这个过程，我们先给神经网络的每个单元写上编号。 图中，输入层有三个节点，我们将其依次编号为1，2，3；隐藏层的4个节点，编号依次为4，5，6，7；最后输出层的两个节点编号为8，9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有链接。比如我们可以看到隐藏层的节点4，它和输入层的三个节点1，2，3之间都有连接，其连接上的权重分别为$w_{41},w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？ 为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1，2，3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是向量$\vec{x}$。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。 一旦我们有了节点1、2、3的输出值，我们就可以计算节点4的输出值$a_4$：$$a_4=f(\vec{w}·\vec{x})=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})$$其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41},w_{42},w_{43} $分别为节点1、2、3到节点4连接的权重，在给权值$w_{ij}$编号时，我们把目标节点的编号$i$放在前面，把源节点的编号$i$放在后面。 同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$:$$y_1=f(\vec{w}·\vec{x})=f(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})$$同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\\end{array}\right]$时，神经网络的输出向量$\vec{y}=\left[\begin{array}{c} y_1\\ y_2\\\end{array}\right]$。这里我们也看到，输出向量的维度和输出层神经元个数相同。 2.3 神经网络的矩阵表示神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。首先我们把隐藏层4个节点的计算依次排列出来：$$a_4=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})$$$$a_5=f(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})$$$$a_6=f(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})$$$$a_7=f(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})$$接着，定义神经网络的输入向量$\vec{x}$和隐藏层每个节点的权重向量$\vec{w_j}$。令$$\vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\ 1\\\end{array}\right]$$$$\vec{w_4}=[w_{41},w_{42},w_{43},w_{4b}]$$$$\vec{w_5}=[w_{51},w_{52},w_{53},w_{5b}]$$$$\vec{w_6}=[w_{61},w_{62},w_{63},w_{6b}]$$$$\vec{w_7}=[w_{71},w_{72},w_{73},w_{7b}]$$代入之前的一组式子，得到$$a_4=f(\vec{w_4}·\vec{x})$$$$a_5=f(\vec{w_5}·\vec{x})$$$$a_6=f(\vec{w_6}·\vec{x})$$$$a_7=f(\vec{w_7}·\vec{x})$$现在，我们把上述计算$a_4,a_5,a_6,a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示他们的计算了。令$$\vec{a}=\left[\begin{array}{c} a_4\\ a_5\\ a_6\\ a_7\\\end{array}\right]$$$$\vec{W}=\left[\begin{array}{c} \vec{w_4}\\ \vec{w_5}\\ \vec{w_6}\\ \vec{w_7}\\\end{array}\right]=\left[\begin{matrix} w_{41}&amp; w_{42}&amp; w_{43}&amp; w_{4b}\\ w_{51}&amp; w_{52}&amp; w_{53}&amp; w_{5b}\\ w_{61}&amp; w_{62}&amp; w_{63}&amp; w_{6b}\\ w_{71}&amp; w_{72}&amp; w_{73}&amp; w_{7b}\\\end{matrix}\right]$$$$f\left(\left[\begin{array}{c} x_1\\ x_2\\ ··\\ ··\\\end{array}\right]\right)=\left[\begin{array}{c} f\left(x_1\right)\\ f\left(x_2\right)\\ f\left(x_3\right)\\ ···\\\end{array}\right]$$代入前面的一组式子，得到$$\vec{a}=f(W·\vec{x})$$在上式中，$f$是激活函数，在本例中为sigmoid函数；$W$是某一层的权重矩阵；$\vec{x}$是某层的输入向量；$\vec{a}$是某层的输出向量。它说明了神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数。 每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重举证分别为$W_1,W_2,W_3,W_4$,每个隐藏层的输出分别是$\vec{a_1},\vec{a_2} ,\vec{a_3} $，神经网络的输入为$\vec{x}$，神经网络的输入为$\vec{y}$，如下图所示： 则每一层的输出向量的计算可以表示为：$$\vec{a_1}=f(W_1·\vec{x})$$$$\vec{a_2}=f(W_2·\vec{a_1})$$$$\vec{a_3}=f(W_3·\vec{a_2})$$$$\vec{y}=f(W_4·\vec{a_3})$$这就是神经网络输出值的计算方法。 三、反向传导算法3.1 损失函数与正则化项假设我们有一个固定样本集$\{(x^{(1)},y^{(1)}),···,(x^{(m)},y^{(m)})\}$,它包含$m$个样本。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例$(x,y)$，其代价函数为：$$J(W,b;x,y)=\frac{1}{2}||h_{W,b}{(x)}-y||^2$$这是一个平方误差损失函数。对于包含$m$个样本的数据集，我们可以定义整体的损失函数为：$$J\left(W,b\right)=\left[\frac{1}{m}\sum_{i=1}^m{J\left(W,b;x^{\left(i\right)},y^{\left(j\right)}\right)}\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ji}^{\left(l\right)}\right)^2}}}$$$$=\left[\frac{1}{m}\sum_{i=1}^m{\frac{1}{2}}\parallel h_{W,b}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\parallel^2\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ij}^{\left(l\right)}\right)^2}}}$$以上关于$J(W,b)$定义中的第一项是均方误差项，第二项是一个正则化项，也叫权重衰减项，其目的就是减小权重的幅度，防止过度拟合。权重衰减参数$\lambda$用于控制公式中两项的相对重要性。需要注意的是，$J(W,b;x,y)$是针对单个样本计算得到的方差代价函数；$J(W,b)$是整体样本代价函数，它包含权重衰减项。 3.2 反向传播算法反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt剔除感知机算法将近30年之后才被发明和普及的。接下来，我们用链式求导法则来推导反向传播算法。 按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。 假设我们的参数集合为$\theta =\{w_1,w_2,···,b_1,b_2···\}$，设初始参数为$\theta^0$，将损失函数$L(\theta)$分别对参数求导：$$\nabla L\left(\theta\right)=\left[\begin{array}{c} \partial L\left(\theta\right)/\partial w_1\\ \partial L\left(\theta\right)/\partial w_2\\ ···\\ \partial L\left(\theta\right)/\partial b_1\\ \partial L\left(\theta\right)/\partial b_2\\ ···\\\end{array}\right]$$计算$\nabla L(\theta^0)$，参数更新$$\theta ^1=\theta ^0-\eta \nabla L(\theta^0)$$计算$\nabla L(\theta ^1)$，参数更新$$\theta ^2=\theta ^1-\eta \nabla L(\theta^1)$$因为推导过程需要用到链式法则，具体如下图所示：我们定义整体损失函数为：$$L\left(\theta\right)=\sum_{n=1}^N{C^n\left(\theta\right)}$$对参数$w$求偏导：$$\frac{\partial L\left(\theta\right)}{\partial w}=\sum_{n=1}^N{\frac{\partial C^n\left(\theta\right)}{\partial w}}$$因此我们只需要求出单个样例的偏导数，就可以推导出整体损失函数的偏导数。根据链式法则，对于某一个节点，如下所示： $$\frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}$$容易得到$$\partial z/\partial w_1=x_1$$$$\partial z/\partial w_2=x_2$$我们可以利用前向传导的方法计算出所有层的$\frac{\partial z} {\partial w}$ 我们已经求出了整个偏导数的左半部分，接下来看右半部分，即$\frac{\partial C}{\partial z}$。 根据链式法则得到：$$\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$$对于$\frac{\partial a}{\partial z}$，我们知道就是激活函数对加法器的偏导，知道了激活函数便知道了$\frac{\partial a}{\partial z}$，我们设其求导结果为$\partial ‘ (z)$，因为$z$在前向传播中已经确定，所以$\partial ‘ (z)$其实是一个常数。接下来看$\frac{\partial C}{\partial a}$ 根据链式求导法则$$\frac{\partial C}{\partial a}=\frac{\partial z’}{\partial a}\frac{\partial C}{\partial z’}+\frac{\partial z’’}{\partial a}\frac{\partial C}{\partial z’’}$$易知$\frac{\partial z’}{\partial a}$即为权值，而$\frac{\partial C}{\partial z’}$假设其已知，则我们可以得到$$\frac{\partial C}{\partial z}=\sigma ‘\left(z\right)\left[w_3\frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}\right]$$ 而对于$\frac{\partial C}{\partial z}$的求导，我们需要区分输出层和隐藏层两种情况： 第一种情况。如果已经是输出层了，如下图所示 我们可以直接求得。 第二种情况。如果还处于隐藏层，我们可以根据上述算法不断递归的计算$\partial C /\partial z$，直到抵达输出层。 最后总结一下，我们根据前向传播算法求得所有的$\frac{\partial z}{\partial w}$，根据反向传播算法求得所有的$\frac{\partial C}{\partial z}$（需要用到前向传播算法求得的$\frac{\partial a}{\partial z}$，即$\sigma ‘\left(z\right)$）。这样就可以用更新公式对参数进行迭代更新了。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>反向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人大应统学长倾心经验贴]]></title>
    <url>%2F2017%2F04%2F04%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E5%AD%A6%E9%95%BF%E5%80%BE%E5%BF%83%E7%BB%8F%E9%AA%8C%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[2016人大考研总算是落下了帷幕，大半年来的努力终于落定。在北京的春天里，窗外飘着杨絮，我在图书馆的沙发上码下这难忘时光的的锤炼与凝结。 初试成绩：总分392（政治68、英语73、数学三121、统计学130） 复试成绩：总分292（英语笔试40、英语面试40、专业课笔试77、专业课面试135） 复习起始：初试： 8月10日-12月26日 | 复试： 2月20日-3月11日 日常安排 6:30-7:30 | 起床洗漱早餐、七点半准时到海洋楼坐定，考研四个小伙伴轮流占座。 7:30-8:30 | 扇贝单词打卡一小时（这个习惯坚持到了考研最后一天还是觉得蛮给力的） 8:30-11:00 | 数学（雷打不动的看书做题） 11:00-11:30 | 午餐（十一点去食堂吃饭总会看到一个法兰西女孩，就像西西里的美丽传说里的莫妮卡一样风姿卓越，哈哈哈！食欲大增） 11:30-13:00 | 数学（消化午餐的同时还能做做数学，多享受） 13:00-14:00 | 午睡（这个是每日必备良药，不午睡下午的效率等于0） 14:00-17:00 | 英语（有几天没午睡，这段时间就会是睡过去的） 17:00-17:30 | 晚餐（晚餐的时候见不到那个法兰西女孩了，食欲大减） 17:30-20:00 | 政治（每天两个半小时足够了） 20:00-23:00 | 专业课（因为跨专业、这个是真的有很努力在学） 当然每天都不会完完全全的严丝合缝的来，到后期就是固定这个模式了，考研的四个小伙伴也是在不断地磨合中才基本保持了一致的计划，抱团的作用就在于此，当你投入不进去的时候，自然会有外力来催促你，除非你的个人毅力可以抵制该死的懒癌，你求知的欲望超过你对舒适的渴求，否则，建议抱团。 一、考研专业选择本科学的工科，学的不精，但对计算机有接触，算是有点编程的底子，慢慢的了解到大数据专业，自己也摸索着学习了一些有关数据挖掘的模型算法，蛮感兴趣的。最近在拜读吴军博士的《数学之美》，科普下这个领域，更深入的也正处于摸索阶段，之前在人大大数据老胡学长那里要到了几本大块头的书，英文版，头疼中，坚持啃完。当然敲代码的日子我想应该就会换来蓬头垢面的自己吧。 对这种跨领域的东西特别着迷，就像最近博弈论的发展方向，复旦韦森教授总结道：“沿着道德哲学、政治哲学把休谟、卢梭、康德到罗尔斯的思想和理论程式化，致力于回复经济学的亚当·斯密的古典传统，即从审慎推理（prudential reasoning——即目前经济学家所言的理性最大化推理）和道德推理（moral reasoning）两个维度研究人们的经济与社会行为，并从中折射出制度的伦理维度和道德基础来。在这个研究向量上的经济学家主要有已过世的诺奖得主哈森伊、宾默尔以及萨金。笔者也发现，由一些谙熟现代博弈论分析工具的一些当代思想家——如美国加州大学洛杉矶分校的人类学家Rob Boyd，美国麻省大学的经济学家Herbert Gintis，慕尼黑大学的经济学家Ernst Fehr，以及麻省理工学院的著名博弈论大师弗登博格（DrewFudenberg）等，最近从文化、互惠合作（reciprocity）、利己和利他行为的产生及其在社会选择中作用等相关领域的探索非常值得我们注意。笔者这里贸然推断，这些文化人类学、经济学和博弈论的跨学科的合作研究，也许在不久的将来会汇合哈森伊、宾默尔以及萨金在伦理与社会选择探索向量上的已有理论探索。这一研究向量的理论从任何当今一门社会科学的学科视角来看均无疑代表了人类认识社会和自身的目前最前沿思考，且与经济学的制度分析的最深层基础密切相关联（这是我把他们的工作也视作为经济学的制度分析的主要理由）。笔者目前甚至乐观地估计，如果在这一研究向量中以哈森伊、宾默尔所代表的“经济学—伦理学—政治哲学—博弈论的交叉分析”与另一方面的“文化人类学—经济学—博弈论”的跨学科研究汇融起来的话，这又将会在二十一世纪谱写并演奏出一首宏大与辉煌的“理论交响曲”，从而极大地推进人类对自身所处的社会和人本身的认识和理解。” 之所以摘录那么长一段，是因为个人对于像韦森教授的文本话语中所展现出来的厚重的学术穿透力，像何怀宏教授这样的对正义美好的生活崇尚、像香港中文大学周保松教授对自由民主的理性而温柔的认可与支持、像钱理群教授对生命的关切和对知识的渴求所呈现的高尚的个人品格尤为佩服，他们的精神与智慧必将永远的接续下去，徜徉于他们的文字中，我最为真切感受到的莫过于他们所严肃实践的一种生活理念，这是一种热切的关怀、是一种直抵生命深处的质问、亦是一种现实世界中的身体力行的实践思想，它们似乎与主流思维相悖，这种影响逐渐在我们这一辈的学子中发生，生根发芽，我们开始从无数年被灌输出来的满脑子“绝对真理”中走出来，开始接受世界的丰富性，这即是自由的开端。感谢他们这样的勇士。 二、考研院校选择本科在北京，这个学校唯一让我眷恋的就是，他可能是全中国最幸福的院校，从学校出发去北大人大，半小时车程，清华徒步即可，那时候挚爱哲学，跑去北大人大哲学院蹭课，每个老师的课都会零零碎碎的听一点，北大戴锦华老师、人大周濂老师的课尤为喜爱。北大国家发展研究院（ccer）的课也去蹭了一学期，那时候汪丁丁的新政治经济学和行为经济学，第一次去上课的时候，斗胆的坐在第一排，捧着那本融合了脑神经学、认知心理学、经济学、哲学、政治学、社会学、伦理学、宗教和神秘主义的《新政治经济学》，扑面而来的知识模块，令人头晕目眩，结果便是全程睡过去的，实在是对不住老师，万圣书园刘苏里称道他或许是我们这个时代少有的文艺复兴式的知识人，考研完了再次拜读老师的书，书中呈现出来的渊博之魅让人瞠目结舌。北京最让人眷恋的便是这种藏匿在深处的知识图景，学识对撞的读书会、严肃学术的研讨会、陶冶人心的音乐会、感同身受的小剧场话剧，纵然周围是雾霾缭绕，但当你挖掘到内核，或许就再也不想离不开。对北京的这种念想让我毅然决然的选择了北京的研究生院，但考研之前，还是对高校的应用统计专业略微地进行了考查。 人大：经济统计、精算、医学统计、国民经济这几块人大是老字号了，在全国都名列前茅。但最让我热血沸腾的是近两年刚开办的五校协同交叉培养大数据方向专业硕士，对于这个平台，袁卫老师有很赞的叙述：“5所学校参与培养，就是出于学科交叉的考虑。中国人民大学统计学院的学科、专业设置是综合的、应用的，理论和应用兼而有之，应用领域涉及卫生、健康、经济、社会、管理等，总体实力较强。而北京大学和中国科学院大学，大家都知道，他们在计算机、数学和统计理论研究方面相当强，掌握大数据分析技术的前沿。中央财经大学和首都经贸大学是财经类为主的院校，这两所学校侧重于应用人才的培养，特别是面向经济、管理、社会这样的领域。他们和很多行业企业、金融机构有着密切联系。这5所高校分别属于教育部直属高校、中国科学院的高校和地方高校3种类型，各有特色，优势互补，能够建成一个很好的、学科交叉的人才培养协同体。”所以，心里其实早就锁定这个了。 清华：招生人数太少，信息严重不对称，个人实力不济，就没怎么考虑。 北大：之前在网上看到北大444哥考应用统计的雄文，看得我是荷尔蒙暴增，一冲动就买来了所有的参考书，结果发现，专业课的难度自己很难把握，继续默哀。而且北大分数线实在高，几乎每年都在390左右。学长应该是毕业了，感谢他，因为我的很多经验都会参考他，当时是打印出来一个字一个字拜读。给出他的经验贴链接吧：我的444分北大考研成功经验谈 其他：考研帮有个帖子，比较全面的分析了全国各考研院校的应用统计专业，大家可以参考下：应用统计全国高校分析 三、各科复习方法3.1 英语复习用书： 扇贝APP 张剑考研真题黄皮书 张剑阅读理解150篇 王江涛高分写作 复习方案： 单词：没有用很厚的单词书，感觉会压垮自己，所以选择了app来背，一开始使用的是新东方的乐词，但是亲测效果不佳，后来经研友推荐上了扇贝的船，从此每天早上就开始打卡背诵，保证一个小时的量。特别要注意的是背单词必须要保持连续性，不能中间隔开半个月或者一个月搁置在那里而不背单词了，每天背，这是一个积少成多的过程，我就是一个反面例子，中间有段时间特别抵触背单词，荒了快一个月，结果就是做阅读的时候单词在耳边就是想不起来意思。到考研前要达到的效果就是看到一个单词就立马反应过来，背上三四遍之后就比较省力了，有的时候一小时可以背诵七百个，当然这是建立在前期不断反复不断反复的基础上，基本每个单词app都是依照记忆曲线帮你安排任务，所以也不用担心会漏背或者背不熟。 完形填空、新题型、翻译：这三部分分值占的比较小，基本上每个人都可以得到个基本分数，所以就没太花时间在这上面，只做了历年真题里面的这些部分，然后看黄皮书的分析，掌握一些技巧即可。 阅读： 张剑黄皮书系列的真题基本上是人手一册，真题的研究对于阅读来说是至关重要的，至少要保证两遍以上的联系和琢磨，网上都说英语考80分以上的都是真题研究了四遍五遍的，这是有一定的道理的，阅读的正确与你对命题老师的出题思路的熟悉程度正相关，基本上每一类题都会有特定的规律性，只有当你顺应了老师们的思维模式，在琢磨选项的正误时你都可以类似于套圈子一样套进去，这正是真题的重要性所在，我们可以接受这种思维模式训练的第一手资料就是真题，任何模拟题都无法与之比拟。当然不是说模拟题不需要，在这里推荐张剑的150篇，还是比较贴近真题的，但他的功效仅仅在于提高对新篇章文本的适应性，在考场上难免会遇到和历年真题风格不一致的文章，这就是模拟题的优势所在，这本书基本涵盖了考研英语阅读出现率比较高的话题，你可以拿它当做拓宽英语话语体系的佐料。我当时是花了大约一个月的时间来做这本书，每天下午四十分钟左右做两篇阅读，其他的时间研究前一天做的那两篇，一直循环下去。这里还要说下怎么研究模拟题和真题。拿到一篇文章，按照你的方法做完，然后就是挨个查单词，分析长难句，挨个解读选项和分析，自己从文中找出依据来，最好自己搭建一个文章的框架。 作文：今年的作文幸亏了王江涛，一开始心存侥幸想背个模板就完事了，后来越来越不安，觉得模板作文我难以驾驭的好，如果成篇的万能句型上去，老师必然不买账，估计就是个低分的下场。当然如果真的可以把一个模板变通到炉火纯青，那也一样OK。但对于我这样四级飘过的，六级未过的naïve青年来说怎么会有那么好的英语修养呢，所以最最后一个月算是逼着自己按照王道长的指示安安分分的背诵了8篇范文，滚瓜烂熟，倒背如流，只能这样。道长说了，英语作文看的是你的语言功底，只要和主题搭上边了，没啥语法错误，词汇量足够，还能写出漂亮的句型来，那就不会差到哪里去。所以，背诵历年真题就是最好的办法，踏踏实实的背诵、默写、仿写，王江涛在书里说的都很清楚。 友情提醒：再一次华丽丽的证明了自己的心理素质是多么不堪一击，哈哈，我边上的童鞋老抖腿，以至于我总是有意无意的要观察他的抖腿频率，这让我几近奔溃，没举报老师怪我太善良，结果就是做阅读完全不在状态，基本上就靠语感在做了，我谢谢他全家，我不造其他人被影响到没，怪只怪自己心理素质就像一块薄冰，一碰就会碎。故，学弟学妹们在考场上一定要杜绝这种危害社会健康的事情发生，一经发现，向朝阳群众学习，立马举报上级。 3.2 政治考了68，不敢拿出来献丑，对于这种被人戏谑的学科，其实是拒绝介绍经验的，先说几个好玩的政治段子：“靠别人，你永远是右倾投降主义，靠自己你才是工农武装割据”、“如果全世界都对你恶语相加，我愿对你说上一世纪社会主义核心价值观”、“你我之间本无缘分，全靠党的章程死撑”、“别低头！GDP会掉！别落泪！资本主义会笑”、“想和你谈一场弘扬社会主义正能量的恋爱，你却要我好好做自己的中国梦”。茶余饭后看点段子还是蛮有意思的。 复习用书：（按出场顺序排列） 《思想政治理论考研大纲解析》 肖秀荣《命题人1000题》 风中劲草《冲刺背诵核心考点》 肖秀荣《命题人冲刺八套卷》 启航《20天20题》 肖秀荣《命题人形势与政策》 肖秀荣《终极预测四套卷》、任汝芬《最后四套卷》、任燕翔《考前预测4套卷》 复习方案： 9月5日-10月10日：九月份考研大纲解析发布，开始快马加鞭的看，一块让人厌恶的砖头，不过你还是得静下心来去啃，尝试着一字一句的过，网上流传着一张图，内容是这样的：恩格斯问大胡子马克思先生：“你在干嘛呢”。马克思心平气和的回答道：“管他呢，反正又不是我背”。中国学生对其抵触的心理可见一斑。但其实真正的马克思先生的思想很可贵，以至于20世纪法国解构主义哲学大师德里达在他的著作《马克思的幽灵》中写道：“不能没有马克思，没有马克思，没有对马克思的记忆，没有马克思的遗产，也就没有将来；无论如何得有某个马克思，得有他的才华，至少得有他的某种精神。现在该维护马克思的幽灵们了。”只是在这个极权盛行的国度，马克思变成了鬼魂，笼罩这苍茫的大地，最后沦落为官方口腔，这才是知识分子真正的悲哀，依附于权力而放弃说理。言归正传，大纲解析的脉络其实是很清晰的，看不懂也没有大碍，拿出肖秀荣先生的《1000题》，看完大纲一个章节的内容你就要把《1000题》上对应的章节的选择题给做了，不要看书，把答案写在一张A4上，注意1000题是要做三遍的，第一遍做都会错的惨不忍睹，错了没事，切忌欺骗自己看看答案把错的题给改对了，因为学长就是这样喜欢自我欺骗的前车之鉴哈哈。马克思主义和毛中特部分或许会让你略微头疼，这种理论性质的东西充斥着新闻联播的气质，但也务必沉住气，到后面解决史纲和思修就是分分钟的事，在高中阶段谁都学过维新变法、辛亥革命之类的，学起来还可能会让你增生一些兴趣。在这里安利一部良心巨制《走向共和》，看完这部电视剧会让你对中国近代史的基本脉络有一个清晰的呈现，记得本科的时候看徐中约先生的《中国近代史》，看得我是心力交瘁，后经学长推荐才去看的这部戏，看完后再回头看那本书，不适感就下降很多。下一步要做的事很重要，就是把你做错的题目，从大纲解析里面找答案，用晨光彩色标记笔标注出来。好了这一遍下来，务必请你自己做一个粗略的知识回顾和框架的搭建，每个章节在讲什么内容，拿毛中特部分举个例子，第一章提纲挈领先介绍了马克思主义中国化的两大理论：毛爷爷思想和中特理论体系，也是同上一部分的马克思主义的衔接，然后从第二章开始，分别是第二章新民主主义革命（1919-1949），第三章社会主义改造（1949-1956），第四章社会主义建设（1956-1978），第五章和第六章插播了总依据和总任务，因为接下来的是第七章改革开放了（1978-不知道啥时候结束），再然后是考研政治的重点，第八章总布局，这章内容及其丰富，包括中特经济、政治、文化、社会、生态文明。紧接着就是祖国统一、外交国际战略，最后两章是建设中特的相关问题，总结陈词就是党好党棒棒哒领导好领导棒棒哒。你可以自己建立一个思维导图，带着这个脉络你可以顺利的进入到下一关，这些基础工作是为后期服务的。 10月10日-11月5日：第二遍重复上一步的内容，看大纲，做1000题，纠错回大纲标注。注意第二遍的时候你可以把答案写在1000题上了，然后看1000题后面的答案，把错的以及你觉得好的题的解析在题目边上标注下，要知道为什么错，举一反三。 11月6日-考研结束：刷完了两编大纲和1000题，这时候会出现一本震撼人心的资料出现—风中劲草，这本书的编排和印刷是下了一番功夫的，他的细节之处可以让你真的佩服这本书的作者，跟进大纲，条理清晰，标注分明，重点突出，考研资料中的扛鼎之作。所以，你一定要把这本书当做是你考研政治的制胜法宝，你该怎么做呢？看，一个字一个字的看，我当时就有种心态，自己可以假装看懂普鲁斯特的《追忆似水年华》，我就必须要看透杨杰先生的《风中劲草》，两个时代的回响多璀璨。直到考研之前，你也不要放下这本书，看的遍数越积越多，你就会达到你自己都意想不到的层次，就是合上书，你大概可以知道哪个知识点在那一页的哪块位置。我保持的速度大概是1天15页左右，15天一本书，到考研结束加起来看了三遍。马原毛中特部分你可以多看几遍，四遍五遍无上限。对了，这时候我还同时做了一件事，就是第一遍的时候只是把1000题上的每一道题都在风中劲草上标注，若是单选题第一题，就标注“单1”，多选类推，同时用彩色笔标注。此外，肖秀荣的《形势与政策》也出来了，买来利用空余时间看两遍。 12月15日-考研结束：各种模拟预测题纷纷登场，这里首推肖秀荣的《8套卷》和《4套卷》，可以去学校打印店购买，便宜实惠，八套卷你都要当做考试一样对待，基本上三十分钟就可以完成一套，完成一套之后看下答案分析，纠错，在风中劲草上标注，如肖秀荣第三套第1题，就标注“肖8三1”。因为之前咱们已经完成了以下任务：大纲解析两遍，1000题两遍，风中劲草两遍左右，再加上不断的标注，8套卷就是检验你复习成果的最佳试题，肖秀荣老师编的资料都棒呆，个人灰常喜欢他的讲课风格，一口流利的方言普通话，考研期间要随时关注肖老师的微博微信，都是同步的，关注一个就可以，把它发布的一些重要文件下载下来，打印研究。八套卷你可以做两遍三遍，注意要举一反三，尤其是错题。然后过几天4套卷就会隆重登场，基本是人手一套，不要迟疑，买来赶紧把客观题做了，按照之前的流程对待客观题。接下来就是万众瞩目的主观题，大家从开始到现在还没有接触过主观题，4套卷就是专门为了主观题准备的，如果你不想留太多的时间和精力在政治上（毕竟政治只有100分，数学和专业课才是重中之重），那就建议你只背诵4套卷的主观题，之前提到的启航20天20题可以翻翻，虽然好，但知识点太多，没时间应付，咱们把赌注押在肖秀荣的4套卷上，背诵的时候主要要挑关键词背诵，自己想法子变通式的背下来，切忌原封不动的机械背诵，虽然这几年老爷子押原题的能力衰弱了，但每年的知识点还是压得相当准的，当然像今年很多人说蒋中挺几乎全压中原题了，没怎么看过他的资料，不予置评。 考场上：考政治的时候觉得选择题so easy啊，做到主观题，有点懵逼了，肖大大押中的题的答案全变成了考研的题干，只能硬着头皮上了，相关的知识点全答上去了，生死未卜的赶脚，有一道家庭美德的就全靠扯了。所以，我的复习方案可以给大家敲响警钟，要是想要考高分的，主观题也是要早点准备的，尽量多参考几个考研机构的预测卷，稍微整理下答题的思路，预防真题出现一些偏题。 说在后面：考研政治的时效性特别明显，16年开了十八届五中全会，那么有关它的内容一定会是考研的重头戏，今年客观题和主观题都有一定的体现，而且比重还不低。所以，形势与政治也要多加注意，有时间就多看几遍，尤其是考前那几天，加深印象，有的关键词列点背诵。肖大大解答过考研命题人如何出题：先确定要考的知识点，然后去报纸和杂志上找相应的材料。所以，我们必须要对知识点分外敏感，在不同的模拟题中间总结出知识点来。 另外推荐几个不错的复习资料： 肖秀荣的【马克思主义基本原理概论逻辑图】，哲学的主观题就全靠这个资料来沥青脉络了。 肖秀荣的【近代史时间轴】：把近代发生的事件按照时间顺序排列，一些重要的知识点也有叙述。 肖秀荣的《知识点提要》八个附录：网上也有，可以看看背背啥的。 这样下来政治需要看的东西也蛮多的，时间要自己控制好，到了后期，专业课要背，英语作文要背，政治也要背，别被他们压垮，挺过去！ 3.3 数学三 复习资料 张宇数学三的视频课 《李永乐复习全书》大红色 《李永乐660题》 《李永乐历年真题》 40套模拟题（《张宇8套卷》、《4套卷》、《永乐6套卷》、《历年合工大最后五套卷》、《400题》） 复习方案： 8月10日-9月1日：花了将近一个月的时间来看张宇的视频，他的整个讲解的框架体系蛮成熟的，按照他的指示把笔记全部抄下来，然后自己尝试着背诵，这样下来就可以搭建起数学三的整体的脉络，知道要考的知识点和题型。我个人认为这个框架的搭建对于学习数学来说太重要了，他可以帮助你以一种高屋建瓴的视角来面对你所遇见的各种题型，而不至于迷失在茫茫的题海中无法自拔，它就像在你的脑海中植入了一份详尽的探险地图，遇见一个题，你可以将其归入体系中的某个知识点，这样的训练增加之后，对你的做题速度和准确率也会有很大的提升。一定要有这样的意识将知识归整而不是碎片化存在于你的大脑中，一个成熟的知识体系都会是如此，麻省理工大学的数学大咖林达华在讲解自己的数学体系时，必然脑海中有这样的一个完备的详尽的清晰的图景。 9月1日-10月15日：进入考研攻坚期，复习全书是必备的，因为数学是上午考，我也象征性的把复习时间安排在了上午，每天看10页左右，消化不了太多，一些原则：1、必须自己拿笔写，切忌眼高手低以为看看就会了/2、切忌还没怎么思考就看答案解析，不会做没事，自己思考的过程尤为关键，在每道题的边上写下自己的思考推算过程以及这道题的关键之处、3、琢磨好久都搞不明白的，可以询问大神研友，或者自己做个标记，以后来解决（我后来忘记我曾经有不会的题了，就是这么大马哈）。数学其实有点像练书法，一开始可能你的水准只能够临摹大师们的作品，还只能学个皮毛，但重复训练达到一定的层次之后，你会形成自己的笔法（数学思维方式），在之后对于从未涉猎的新帖（新题型）也可以驾轻熟重。 10月16日-11月10日：复习全书完了之后，一本虐人无数的660题登场，别以为他全是选择题和填空题，但他的每一道题都是精心锤炼过得，所体现的数学思想方法绝对会让你获益匪浅，他的题目的设置真的恰到好处，细细的琢磨每一道题的精髓，虽然真题是绝对达不到这样的难度的，关键的是思想方法。我大概刷了20多天，但有些题后来又忘记回过头去考虑了，这就落下不少病根， 所以建议复习的早一点，可以有更多的时间来调整自己的复习计划。 11月11日-11月20日：从光棍节那天清晨开始，我拿起了真题，花了十天时间每天完成一套卷子，因为很多题其实在你做全书的时候已经遇到过了，所以其实真题对于真实水平的评估还是有很大偏差的，对完答案，订正，错的题的解题思路思考一遍就完事，当时的分数基本保持在120-140之间，也没有太大的失常，第一，历年真题相对于之前的训练还是简单一些的，第二，平时的训练不紧张，三个小时基本上都是轻松愉快的度过的，那时候每天早上起床就盼着可以做数学真题了，就像恋爱一样。 11月20日-12月23日：接下来来到了我个人最为推崇的一种方法，就是数学套卷模拟，在这个阶段中，每做一套模拟题，就可以把所有的章节的重要内容复习一遍，尽管无法覆盖每一个知识点，但模拟题的编写还是有一定的规律的，可以让你随机的复习到一些重要的知识点。要遵守几个原则：1.三小时一套，时间到了就停止答题，然后根据答案自己批分数。2.必须严格遵守考研数学设定的考场规则，不能看书，不能交头接耳，不能询问学神研友，不能嚼口香糖。3.交叉训练法，每个老师出题的风格可能不一样，你可以先做两套张宇的，再做两套李永乐的，这样循环着做，可以增强你的适应能力，亲测有效。至于该做哪些模拟题，我推荐的都在上边写着，这个方法也是借鉴北大444哥的。为什么要建议这个办法呢，因为考研数学今年风格大变，像线性代数和概率论与数理统计的大题都是很难遇到的，那怎么办呢，就是不断地训练模拟题，不断地遇到新题，不断地提高自己的解题能力，而且这样的实战模拟也会让你开始意识到考场上的时间分配是何其重要，3个小时，挑大肉吃，有的是在太难的，抛开也无妨，考场的战略是需要在平时的训练中积累的。此外，你的书写也需要在这段时间里训练，张宇的8套卷和4套卷都提供了和考研一模一样的答题纸，不要大手大脚的乱答题，解题的条理性要注意。合工大的那几套题真的很不错，今年在考场上做高数的时候相当顺利的原因就是这些题型基本都在那15套卷子里遇到过了，所以，直到高数大题做完，我只花了1个小时20分钟，最后剩下一个半小时左右的时间来解决线代和概率大题，但是，我真是个天生的考场悲剧制造者，详情见下。 考场上： 因为考研期间基本上很少运动，打个球跑个步都是奢侈的不行，散步也成了浪费时间的活儿，后来自己也尝到恶果了，我的小心脏承受不住了，有时候会突然之间心跳加速到200多次每分钟，去校医院检查的时候医生说说是有阵发性室上性心动过速，吓得我够呛，问医生为啥，他说是我的心脏短路了，很多时候是因为焦虑不安或者过度兴奋造成的，好吧，那时候都到了考研的冲刺期，我也只能硬挺着，反正医生说没什么生命危险，然后就听到了考研的那天。在考研数学的考场上，我因为一个小时十分钟就完成了高数部分，high的不行了，一兴奋，犯病了，心跳开始砰砰砰的上去，以至于我无法正常答题，短时间治愈这个病的办法就是蹲下去深呼吸，于是我就申请去走廊自己做深蹲，然后深呼吸，深蹲深呼吸，当时真的快要奔溃了，以为这场试就这么完蛋了。深蹲深呼吸了好久，大约过了十五分钟，还是没有恢复过来，感觉真的没救了，老师也一直在边上看着我，该咋办，我的天哪，情急之下我开始捶自己的胸部，一锤倒是好了，但是背部还是有明显的不适感，总觉得有东西在怼我，就在这样的状态下，勉强答完了题，再加上线代和概率题和之前的训练风格太不一样了，我就有点崩溃了，连时间都看错了，原本是11点半结束，我却以为11点就结束了，情急之下把线代大题答得满卷子都是，感觉都看不清楚了，菩萨保佑吧，希望老师可以手下留情。所以，从我身上可以吸取的教训就是平时要注意身体，有时间就去跑跑步打打球，千万不能输在身体上。最后数学考了121分，也算是谢天谢地了。 3.4 432统计学复习用书： 贾俊平《统计学》第四版（经管类） 贾俊平《统计学》第六版（21世纪统计学系列教材） 何晓群《多元统计分析》 王燕 《时间序列分析》 何晓群《应用回归分析》 复习方案： 贾俊平《统计学》：一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架体系也比较清晰，基本上一个章节一天就OK，看了两遍，然后整理了自己的笔记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下，做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析（这俩个属于多元统计分析）都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的，幸好看了第四版。 何晓群《多元统计分析》：说实在的这本书写得像哲学，感觉是直接从英文版翻译过来的，很多表述没有那么通俗易懂。我只看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去对应可以解决的问题，不需要去死抠推导过程，这不是432需要重视的，但是对于理解还是有帮助的，有兴趣的可以推推看。16年没考这部分内容，但不能预计17年会不会考，要复习的全面一些。一些问答题都要结合历年真题自己根据课本进行总结。 王燕《时间序列分析》：这本书的编写就相对好得多，条理很清晰，思路引导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道08年学硕考过的题：有趋势有季节变动可建立的模型，写出模型形式并简要说明。可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点，只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。 何晓群《应用回归分析》:一直以为何晓群老师是个女老师，后来复试的时候才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。 关于真题：真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作用。我当时的做法就是把真题整理成八个专题，分别是：《专题一：图表展示与概括性度量》、《专题二：统计量与抽样分布》、《专题三：参数估计与假设检验》、《专题四：分类数据分析》、《专题五：方差分析与实验设计》、《专题六：回归分析》、《专题七：时间序列分析》、《专题八：多元统计分析》，学硕和专硕的历年真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像今年时序和回归的题都是曾经考过的，几乎一模一样。 关于笔记：自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思，但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知识域相对来说也会完善一些。 关于背诵：心理学中有一个广为认可的记忆机制，即：我们在记忆的时候将许多线索（诸如对一个原理的发散性理解、当时联想的事物的多样性）一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面，搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出入，但是关键词必须要锁定，大致意思要接近。 其他：大家如果有专业课的问题，可以向我询问，我尽力解答。最近也在恶补专业知识，毕竟是跨专业，害怕一进人大就被各路大神碾压，大家互相学习吧，或许我的专业素养还比不上学弟学妹呢。和考研小伙伴一起建立了一个微信群，大家伙可以加进来在群里分享应用统计的资料、讨论复习过程中遇到的难题、分享考研路上的酸甜苦辣，啥啥啥都可以。因为微信群已满100人，可以加我的个人微信：zhanghua63170140，拉学弟学妹进群。 四、QA师妹皱着眉头问： 师哥好，我也想考人大统计，本科统计，但只是普通一本。旁边人都说人大太难考，因为我是师范学校，而且我们数科院好几年没有人考到人大，感觉挺迷茫的，也不知道该不该换个学校，但是总觉得不甘心，为什么别人能考上我不能啊？ 师兄皱着眉头答： 在现实世界中，我们的决策往往会倚赖过往的历史经验，就像你所述的，你所在的师范学校的数科院没有人考入过人大统计学院，看到这么惨淡的景象，畏惧心理在所难免，既然他们已经为你趟过这条深水，且已证明这不是一条容易的路，那为何我还要继续当做下一个被湍急的河流卷走的“微弱的个体”呢？且不说投入进去的时间成本以及其他一些不可控因素给自身带来难以计数的艰难险阻，万一这一年的所有努力在成绩出来的那一刹那都付之一炬，名校梦从而化为泡影，岂不是做了一次失败的买卖吗？ 可我想告诫你的却是，人大必须要去考，而且要义无反顾的前往，不要有所畏惧，从你的描述中得知并没有太多的现实因素的阻挠，你仅仅在惧怕强大而无耻的经验施加在你身上的不能承受的阻抗。先说说你会在这条幽深曲折的路上看到哪些曼妙的风景，你或许可以涉猎到从未踏入的知识盲区，当你被无数的知识模块所充盈，你会感受到这样的缓慢累积会给你带来前所未有的愉悦与渴求欲的满足，我们时代的知识分隔已经异常凸显出来了，每一个领域会将拥有一套成熟的体系，而当你掌控着庞大细密的知识网时，你便拥有了铠甲，他将带你在众人面前展示话语的力量，那种力量就是需要这些知识来支撑的。其次，你可能可以收获几个志同道合的朋友，网络的延伸将你的诉求与宣告呈现在他们面前，就像现在我正在尝试着与你促膝长谈一样，也必定会有无数的这样的人尝试着与你建立精神上的关联与挂钩，你们摸索着同一片黑夜，也凝望着同一片蓝天，为这笃定的信念挥汗。你要坚信，总有一天，你会与你精神气质相合的那些可爱的人相聚，就像家人一样聚在一起，所以，不要抗拒孤独，那仅仅是你还没有那样强烈的遇见。最后，也是最重要的，你将迎来新的人生，你说你想考人大统计，我相信你的内心必然会有一股洪荒之力在不断地催促着你，会有一种声音在耳边呼唤你，那便是我们最大的动力，这样的声音会在你颓然之时支起你的躯体，无论什么样的生活的贫乏无趣都驱散不了这种称之为信仰的东西，只要你持续地温存这样的声音，去战胜所有的困惑与不安，正是这种不甘让我们变成一个撑起自己所有维度的勇士。是的，你会变成一个勇士，即使被现实迫害的遍体鳞伤，你也依然可以坚毅地挺立原地，然后，舔舐自己的伤口，继续热烈地往前走。那种热烈，只能自己亲手栽培，别人无法给予，你也不能凭空取得，那是一个个白天黑夜的伏案所换来的最盛大的生命花园，你要在漫长的年华里种上玫瑰、植入梧桐、嵌进宝石，让它灿烂的更彻底点吧，即使荒败了，也要在极度的繁盛中逝去。 酷酷的师弟问： 师兄，我是跨专业，感觉对专业课比较迷惘，不知如何下手，可不可以建议一个比较摸得着套路的专业课复习方案呀？ 严肃的师兄答： 432统计学复习流程建议 贾俊平《统计学》第六版+圣才《贾俊平统计学 笔记与课后习题详解》：花费20天左右的时间进行全篇阅读，不遗漏任何一个点，包括概念、公式、解释、注释、表格、图片、例题，每一个字都要盯上至少一秒钟。每看完一章节的所有内容之后，在A4纸上写下课后思考题与练习题的答案，可以翻阅课本，但必须要自己动手整理归纳或者解答一遍，切记眼高手低，能写入教科书的例题就必然会有其存在的必要性，它可以帮助你梳理课本知识，也可以帮你抓住章节重点，整理这些问答题和计算题的过程也是再一次深入理解知识点的过程，绝不可废弃之。课后的思考题和练习题的答案可以在圣才出版的《贾俊平统计学 笔记与课后习题详解》找到，我只找到了第五版对应的（一共331页），已经上传到网盘里。大家也可以参考人大配套的学习指导书，网上可以买到。这样一遍下来对这本书的框架有一定的了解，强烈建议看完每一章节之后画一张框架结构图，理清知识脉络。重点章节是”第3章：数据的图表展示“、”第4章：数据的概括性度量“（这两个章节联合起来会在真题中考察一道大题）；第6章，统计量概念和中心极限定理是重点；第7-13章，所有的都是重点。第1、2、5、14章可以粗略看一下，非重点。 贾俊平《统计学》第二遍：第二遍依旧要有如第一遍的细致程度，且在第一遍的基础上加深理解，争取可以简单的使用自己的话简述一遍，同时要开始做笔记，按照书本的结构组织笔记，且必须要把这本笔记当做是一样艺术品，用心编排、用心写字、用心画图，重点分明、内容完整、结构清晰，切不可潦草糊弄过关，这本笔记是你在日后的复习中常常会碰面的，翻阅起来可以大大地提高效率，而且看起来愉悦舒心一目了然，何乐而不为？切勿盲目求快，做笔记是一个梳理知识点、更深入理解知识点的过程，欲速则不达，抄一遍了事对理解没有丝毫的助益，下笔之前想明白这句话所指涉的是什么、是否还存留我尚未领会的含义、我能否清晰的在脑海里梳理分析流程等等等，这些都可以增益你对细节的深入探索，慢工出细活，相信我，循序渐进的来，一定会有很大的成效。同时这一遍笔记要把管理学第四版中出现的新知识补充到笔记中，其中的新知识点包括：正态性的评估、实验设计、哑变量回归、非参数统计（注：时间序列分析和多元统计分析的部分内容会在其他两本书中会详细展开，不添加进去也无妨）。 《应用回归分析》：这本书囿于时间只看了前八章，也就是到主成分回归与偏最小二乘估计这一章为止，因为在《统计学》书中已经对这部分的内容有了基本的了解，加上《应用回归分析》书中的推导也不是特别艰深，基本上每一步思路都很清晰，大家可以尝试着推导一遍，加深理解，但这并不是重点，重点在于诸如违背回归方程基本假设条件的三种情况（异方差、自相关、多重共线性）的原因、影响、诊断、处理这类偏向论述、步骤与原理的知识点，所以，如果推导有困难，也不必强求。但其实后两章的非线性回归与定性变量回归模型也比较容易理解，虽然初试考察的概率不大，但为兼顾知识结构的完整性以及复试的时候有可能被老师问及，看一下肯定是有好处的。这本《应用回归分析》也是要看一遍，再做一遍笔记，做笔记的方法与上述一样，不再赘述。 《应用时间序列分析》：这本书的条理狠清晰，大致就是平稳时间序列分析、非平稳时间序列的确定性分析和非平稳时间序列的随机性分析三块内容，考试重点在于若干个时间序列分析模型的结构与性质，譬如AR模型、MA模型、ARMA模型、ARIMA模型、指数平滑法、分解模型、含哑变量的多元回归预测模型等等，要搞清楚每一种模型所适用的时间序列类型、模型中每一个参数代表的含义以及分析的思路与步骤，其他的重点包括差分运算、一些基础的概念等。最后一章考的概率不高，但时间序列最后一章内容在时序分析所占地位是很高的（虚假回归、单位根检验、单整与协整）要是想扩充知识点，，也建议看一遍。同样，整个过程也是看一遍书，整理一遍笔记。 《多元统计分析》：聚类分析、判别分析、主成分分析、因子分析、对应分析、典型相关分析必看，后几章个人认为考的几率不大，看个人时间分配。重点考点是这些多元统计分析方法的基本思想、过程细节、重点性质之类的，历年考过因子分析、判别分析、典型相关分析的相关内容，考的概率蛮大的，绝不能弃看。过程还是一样，第一遍，看书、理解、适当推导，第二遍，做笔记，再次理解，加深印象。 八个专题整理：这是学长根据历年真理的考题分布情况总结出来的八个专题，已经在上述的复习中有所呈现，依次是：“专题一：数据的图表展示与概括性度量”；“专题二：统计量与抽样分布”；“专题三：参数估计与假设检验”；“专题四：分类数据分析”；“专题五：方差分析与实验设计”；“专题六：相关分析与回归分析”；“专题七：时间序列分析”；“专题八：多元统计分析”。几乎每一年都是在这八个专题中抽取七个专题的知识点，例如2015年432真题的排布分别为：第一题属于专题一（数据的图表展示与概括性度量）、第二题属于专题四（分类数据分析）、第三题属于专题五（方差分析与实验设计）、第四题属于专题八（多元统计分析）、第五题属于专题三（参数估计与假设检验）、第六题属于专题六（相关分析与回归分析）、第七题属于专题七（时间序列分析）。还是比较有代表性的，其他年份的分布学弟学妹们也都可以总结一下规律，有助于自主预测考题。大家可以以这个思路去归纳整理自己最终的一份笔记，八个专题，每一个专题都要有结构框架，可以借助思维导图这个工具，每一个专题包括四个部分（第一部分：这一专题的课本内容有序的整理；第二部分：重要问答题整理；第三部分：属于这一专题的真题整理（每一道题的答案一定要完整有序地整理）；第四部分：这一专题的思维导图）。整理完这份专题笔记之后时间也就剩下一个月左右，接下来的时间就是背背背，当然要理解地去背，把笔记与思维导图结合起来，背到滚瓜烂熟，背到天昏地暗，到最后阶段会特别难熬，英语作文要背，政治大题要背，专业课要背，抗住压力就是了。 可爱的小师妹问：你的专业课思维导图咋搞咯？还有专业课的复习时间怎么分配？难点不懂怎么办？师兄你有笔记吗？： 依旧严肃的师兄答： 这八个专题的思维导图我已经整理完并放置在百度云群里了，这里给出百度云链接，人大432专业课思维导图 密码: pwu9。当然思维导图要随时自己更新，如果自己觉得需要补充的，可以在思维导图上添加。要想使这个思维导图的效用最大化，就得把框架熟记在心中，在答题的时候一定会有帮助的，会让你的答案有结构有条理，列点回答更加轻松自如，不知道怎么把题目答得全面闪亮？只需把思维导图的一个个点用书本的内容或者你整理的笔记来填充就好啦。不过，话说在前头，理解才是关键。它只是一个框架工具，核心在于知识点。 时间怎么分配呢？：基本上每天我都会花三个小时复习专业课，有的时候白天数学的任务没完成，也会适当压缩专业课的时间，大致的时间安排是《统计学》阅读及笔记25天、《应用回归分析》阅读及笔记20天、《应用时间序列分析》阅读及笔记20天、《多元统计分析》阅读及笔记20天、《八个专题整理》25天、背诵30天。 难点不懂咋办办？：在多元统计分析或者时序分析中会出现一些自己无法理解的地方，诸如推导过程和计算证明，这些确实不是432统计学的考察重点，但是如果不搞清楚这些，对知识点就会感觉隔着一层迷雾，无法透彻地解析整个过程总会叫人不爽快，不求甚解是深层次理解知识点的大敌。但是，时间所迫，实在搞不懂这些玩意儿咋办呢，那就只能退而求其次，可以大概的知道这个推导是在干什么以及它在整个过程中的作用。也足够应对432统计学了。 学长你有笔记吗？：哈哈哈，到最后了学长要黄婆卖瓜自卖自夸了，简要说下学长的专业课笔记，概览链接: 密码: 5trd，笔记分为七个部分其中前六份都是学长亲手整理的，就是依据上述的复习过程一步步整理下来，并且经过了精心的排版，保证大家的用户体验一级棒。如果想深入了解资料的细部，可以私聊学长，随时等候你的到来。学长微信号：zhanghua63170140 五、写在最后这一路下来，感谢的人很多，人大陈思聪学长（见过学长本人，沉稳贴心哈哈，考研的时候不会的题去问他都会耐心解答），小杰克学长（这个群真的建设的太赞了，复试的资料是向学长要的，帮助很大），老胡学长（最近给我发了好几本砖头书，全英文版，据说是装逼神器），以及在中海洋读研的王淼淼童鞋（跨专业考统计真心不容易，磕磕碰碰了很多次，王淼淼鼎力相助；我后期整理了一个专业课思维导图，也是王淼淼给我的灵感），在幼儿园种花种草的冯涵小朋友，三个考研小伙伴大象、赫姐还有小姨妈（每次拿小姨妈开玩笑真的屡试不爽），还有强悍的北大444哥（他的经验贴真的是棒呆）。最后想感谢的是我的女朋友，她脸上的笑容总能够化解我的忧愁与不安，每次复习到夜深，想起在这座城市的另一边有那么一个人与我一同牵手向前，就会充满力量。 在本科期间，我们所接受的更多的是老师所教授的知识，而到了研究生期间，我们要准备开始制造新的知识，更高层次的目标便是对人类普遍的知识有所贡献，而达到如此境界的来源正是你对知识的渴求与不断地追索，你不再满足于单一的知识面，而渴望搭建更坚固的知识架构。我们不能功利的对待这场磨练你意志的战役，不能仅仅把它看成是获取更多外在利益的工具，虽然确实可以达到这样的效果，但这样的动力绝对不会持久的催促你往更高的知识领域探索。 很多人在中途放弃或是马马虎虎的应付，就是自己内心缺乏行动的信念支撑，倘若只是觉得考上研就一劳永逸了，就没有必要花费那么厚重的时间成本了，因为，考上研只是一个起点，更艰难的路，在前方。 祝愿学弟学妹可以在考研路上发现更多的风景，顺利考上人大。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（16）：统计学习概论]]></title>
    <url>%2F2017%2F03%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[一、统计学习1.1 特点统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。赫尔伯特·西蒙曾对学习定义为：“如果一个系统能够执行某个过程改进它的性能，这就是学习。”按照这一观点，统计学习就是计算机系统通过数据及统计方法提高系统性能的机器学习。 1.2 对象统计学习的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。数据包括存在于计算机及网络上的各种数字、文字、图像、视频、音频及它们的组合。统计学习对于数据的基本假设是同类数据具有一定###的统计规律性。 1.3 目的考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要尽可能地提高学习效率。 1.4 方法统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）、强化学习（reinforcement learning）等组成 监督学习：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据独立同分布，并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space），应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。 三要素：模型的假设空间（模型）、模型选择的准则（策略）、模型学习的算法（算法） 步骤： 1）得到一个有限的训练数据集合 2）确定包含所有可能的模型的假设空间，即学习模型的集合 3）确定模型选择的准则，即学习的策略 4）实现求解最优模型的算法，即学习的算法 5）通过学习方法选择最优模型 6）利用学习的最优模型对新数据进行预测或分析 二、监督学习2.1 定义监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做一个好的预测。它从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。 2.2 基本概念2.2.1 输入空间、特征空间与输出空间 输入空间与输出空间：输出与输出所有可能值的集合。通常输出空间远远小于输入空间 特征空间：所有特征向量存在的空间。特征空间的每一维对应于一个特征。模型都定义在特征空间上。 输入实例$x$的特征向量：$$x=\left(x^{\left(1\right)},x^{\left(2\right)},···,x^{\left(i\right)},···,x^{\left(n\right)}\right)^T$$其中$x^{(i)}$表示$x$的第$i$个特征，$x_i$表示多个输入向量的第$i$个，即$$x_i=\left(x_i^{\left(1\right)},x_i^{\left(2\right)},···,x_i^{\left(n\right)}\right)^T$$ 训练数据和测试数据由输入输出对（即样本）组成，通常表示为：$$T=\left\{\left(x_1,y_1\right),\left(x_2,y_2\right),···,\left(x_N,y_N\right)\right\}$$ 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量与输出变量均为变量序列的预测问题。 2.2.2 联合概率分布统计学习假设数据存在一定的统计规律，监督学习的基本假设为$X$和$Y$具有联合概率分布的假设，我们把训练数据与测试数据看作是依联合概率分布$P(X,Y)$独立同分布产生的。 2.2.3 假设空间监督学习的目的在于找到由输入到输出的映射模型集合中最好的一个。这个集合即假设空间。模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数$Y=f(X)$表示。 三、统计学习三要素3.1 模型模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布（概率模型）或决策函数（非概率模型）。假设空间用$\mathscr{F}$表示，它是由一个参数向量决定的决策函数族：$$\mathscr{F}=\left\{f | Y=f_{\theta}\left(X\right),\theta\in R^n\right\}$$参数向量$\theta$取值于$n$维欧式空间$R^n$，称为参数空间。也可以是一个参数向量决定的条件概率分布族：$$\mathscr{F}=\left\{P | P_{\theta}\left(Y|X\right),\theta\in R^n\right\}$$ 3.2 策略有了模型的假设空间，接下来需要考虑按照什么样的准则学习或选择最优的模型。 3.2.1 损失函数和风险函数损失函数（loss function）度量一次预测的好坏。损失函数越小，模型就越好常用的损失函数有： 0-1损失函数（0-1 loss function）：$$L=\left\{\begin{matrix}{} 1&amp; Y\ne f\left(X\right)\\ 0&amp; Y=f\left(X\right)\\\end{matrix}\right.$$ 平方损失函数（quadratic loss function）:$$L=\left(Y-f\left(X\right)\right)^2$$ 绝对损失函数（absolute loss function）：$$L=|Y-f\left(X\right)|$$ 对数损失函数（logarithmic loss function）：$$L=-\log P\left(Y|X\right)$$ 风险函数（risk function）或期望损失（expected loss）是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失。$$R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}$$我们学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$未知，风险函数不能直接计算。这样，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就沦为病态问题。但我们可以计算训练数据集的平均损失，即经验风险（empirical risk）或经验损失（empirical loss）：$$\textrm{R}_{emp}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}$$根据大数定理，当样本容量$N$趋于无穷时，经验风险趋于期望风险。自然而然想到可以使用经验风险来估计期望风险。但现实中训练样本数目很小，这种估计往往不理想，需要矫正，以此引出经验风险最小化和结构风险最小化。 3.2.2 经验风险最小化和结构风险最小化经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题：$$\underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}$$当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为：$$R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)$$其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题：$$\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)$$ 这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。 3.3 算法学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优化。如何用数值计算求解，如何保证找到全局最优化，并使求解过程高效，是一个重要的问题。 四、模型评估与模型选择4.1 训练误差与测试误差训练误差（training error）是模型关于训练数据集的平均损失：$$R_{emp}\left(\hat{f}\right)=\frac{1}{N_1}\sum_{i=1}^{N_1}{L\left(y_i,\hat{f}\left(x_i\right)\right)}$$测试误差(test error)是模型关于测试数据集的平均损失：$$R_{emp}\left(\hat{f}\right)=\frac{1}{N_2}\sum_{i=1}^{N_2}{L\left(y_i,\hat{f}\left(x_i\right)\right)}$$测试误差反映了学习方法对未知的测试数据集的预测能力，即泛化能力。 4.2 过拟合与模型选择我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。 模型选择旨在避免过拟合并提高模型的预测能力，模型选择时，不仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力。下图描述了训练误差和测试误差与模型的复杂度之间的关系：当模型复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最小值后又增大。当选择的模型复杂度过大时，过拟合现象就会发生。所以要选择复杂度适当的模型，已达到测试误差最小的目的。以此引出正则化与交叉验证。 五、正则化与交叉验证5.1 正则化5.1.1 定义模型选择的典型方法是正则化（regularzation）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下：$$\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)$$第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。 5.1.2 不同形式正则化项可以取不同的形式。例如，回归问题中，损失函数是平方误差，正则化项可以是参数向量的$L_2$范数:$$L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\frac{\lambda}{2}||w||^2$$也可以是参数向量的$L_1$范数：$$L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\lambda ||w||_1$$第一项的经验风险较小的模型可能较复杂（有多个非零参数），这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。 5.1.3 奥卡姆剃刀正则化符合奥卡姆剃刀原理，应用于模型选择时变为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有很小的先验概率，简单的模型有较大的先验概率。 5.2 交叉验证5.2.1 定义如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别是训练集（training set）用来训练模型、验证集（validation set）用于模型的选择、测试集（test set）用于最终对学习方法的评估，最终选择对验证集有最小预测误差的模型。 但是实际应用中数据不充足，所以我们采用交叉验证，它的基本思想是重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试和模型选择。 5.2.2 方法 简单交叉验证：首先随机地将已给数据分为两个部分，一部分作为训练集（70%），另一部分作为测试集（30%）；然后用训练集在各种条件下（如不同的参数个数）训练模型，从而得到不同的模型；在测试机上评价各个模型的测试误差，选出测试误差最小的模型。 S折交叉验证（S-fold cross validation）：应用最广泛。首先随即将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。 留一交叉验证leave-one-out cross validation）：S折交叉验证的特殊情形是S=N（N为给定数据集的容量），往往在数据缺乏的情况下使用 六、泛化能力6.1 泛化误差泛化能力是指由该方法学习到的模型对未知数据的预测能力。现实中常常通过测试误差来评价学习方法的泛化能力，但因为测试数据及有限，评价结果不一定可靠。 理论上，通过泛化误差来反映学习方法的泛化能力，泛化误差即用学习到的模型对未知数据预测的误差：$$R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}$$泛化误差越小，模型效果就好。泛化误差就是所学习到的模型的期望风险。 七、生成模型与判别模型7.1 判别模型判别模型由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。它关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：K近邻法、感知机、决策树、逻辑斯谛回归、最大熵模型、支持向量机、提升方法、条件随机场。 判别方法的特点： 直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率很高； 由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 7.2 生成模型生成模型由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型：$$P\left(Y|X\right)=\frac{P\left(X,Y\right)}{P\left(X\right)}$$因为模型表示了给定输入$X$产生输出$Y$的生成关系，所以被称为生成模型。典型的生成模型有：朴素贝叶斯、隐马尔科夫模型 生成方法的特点： 生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能； 生成方法的学习收敛速度快，即当样本容量增加时，学到的模型可以很快收敛于真实模型； 当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用。 八、分类问题8.1 定义在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。它从数据中学习一个分类模型或分类决策函数，即学习一个分类器，然后对新的输入进行输出的预测，即进行分类。分为多类分类和二类分类问题。 8.2 学习过程如图所示，分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。 8.3 分类准确率8.3.1 定义评价分类器性能的指标一般是分类准确率（accuracy）,即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也即损失函数是0-1损失时测试数据集上的准确率。 8.3.2 常用指标通常将关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP（正类预测为正类数）、FN（正类预测为负类书）、FP（负类预测为正类数）、TN（负类预测为负类数） 精确率：$$P=\frac{TP}{TP+FP}$$ 召回率：$$R=\frac{TP}{TP+FN}$$ $F_1$值，是精确率和召回率的调和均值，即$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$$$F_1=\frac{2TP}{2TP+FP+FN}$$精确率与召回率都很高时，$F_1$值也会很高。 8.3.3 分类方法与应用 常见分类统计方法：K近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、支持向量机、提升、贝叶斯网络、神经网络Winnow 应用：银行业务构建客户分类模型，对客户按照贷款风险大小分类；网络非法入侵检测；人脸是否出现的检测；网页分类；文本分类等。 九、标注问题9.1 定义标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。它的目的在于学习一个模型，使它能够对观测序列给出标记序列作为预测。标注问题分为学习和标注两个过程： 9.2 应用标注常用的统计学习方法有：隐马尔科夫模型、条件随机场 它在信息抽取、自然语言处理领域被广泛应用。 自然语言处理的词性标注：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。 十、回归问题10.1 定义回归模型表示输入变量和输出变量之间映射的函数，等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。可分为一元回归和多元回归，线性回归和非线性回归。它最常用的损失函数为平方损失函数，可以用最小二乘法求解。回归问题分为学习和标注两个过程： 10.2 应用股价预测：将影响股价的信息视作自变量，将股价视为因变量，将过去的数据作为训练数据，学习一个回归模型，并对未来的股价进行预测。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习概论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（15）：EM算法]]></title>
    <url>%2F2017%2F03%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8815%EF%BC%89%EF%BC%9AEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[期望最大值（Expectation Maximization，简称EM算法）是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。其主要思想就是通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数。本文将尽可能详尽地描述EM算法的原理。并结合高斯混合模型介绍EM算法是如何求解的。 一、定义EM算法是一种迭代算法，用于含有隐变量（hidden variable）的改了吧模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：E步-求期望（expectation）；M步-求极大（maximization）。故称为期望极大算法（expectation maximization），简称EM算法。 二、Jensen不等式设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f^{‘’}(x)≥0$，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的即$H≥0$，那么$f$是凸函数。如果$f^{‘’}(x)&gt;0$或$H&gt;0$，那么称$f$是严格凸函数。 $Jensen$不等式表述如下： 如果$f$是凸函数，$x$是随机变量，那么：$E[f(x)]≥f(E[x])$。特别地，如果$f$是严格凸函数，$E[f(x)]≥f(E[x])$，那么当且仅当$P(x=E[x])=1$(也就是说$x$是常量)，$E[f(x)]=f(E[x])$; 如果$f$是凹函数，$x$是随机变量，则$E[f(x)]≥f(E[x])$。当$f$是（严格）凹函数当且仅当$-f$是（严格）凸函数。 通过下面这张图，我们可以加深理解： 上图中，函数$f$是凸函数，$X$是随机变量，有0.5的概率为$a$，有0.5的概率是b（就像抛硬币一样）。$X$的期望值就是a和b的中值了，图中可以看到$E[f(x)]≥f(E[x])$成立。 三、EM思想3.1 极大似然估计EM算法推导过程中，会使用到极大似然估计参数。 极大似然估计是一种概率论在统计学的应用。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察结果，利用结果推出参数的大概值。极大似然估计建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 这里再给出求极大似然估计值的一般步骤： 1）写出似然函数； 2）对似然函数取对数，并整理； 3）求导数，令导数为0，得到似然方程； 4）解似然方程，得到的参数即为所求； 关于极大似然估计的实例，可以参考wikipedia最大似然估计条目 3.2 EM算法思想下面介绍EM算法的思想： 给定的训练样本是$x^{(1)}，x^{(2)}，···，x^{(m)}$，样例间相互独立，但每个样本对应的类别$z^{(i)}$是未知的，也即隐含变量。我们想找到每个样例隐含的类别$z$，能使得$P(x,z)$最大。$P(x,z)$的最大似然估计如下：$$l\left(\theta\right)=\sum_{i=1}^m{\log p\left(x;\theta\right)}=\sum_{i=1}^m{\log\sum_z{}p\left(x,z;\theta\right)}$$ 第一步是对极大似然函数取对数，第二步是对每个样本实例的每个可能的类别$z$求联合分布概率之和。但是直接求$\theta$一般比较困难，因为有隐藏变量$z$存在，如果$z$是一个已知的数，那么使用极大似然估计来估算会很容易。在这种$z$不确定的情形下，EM算法就派上用场了。 EM算法是一种解决存在隐变量优化问题的有效方法。对于上述情况，由于存在隐变量，不能直接最大化$l(\theta)$，我们可以不断地建立$l$的下界（E步），然后优化下界（M步），依次迭代，直至算法收敛到局部最优。这就是EM算法的核心思想，简单的归纳一下： EM算法通过引入隐变量，使用MLE进行迭代求解参数。通常引入隐含变量后会有两个参数，EM算法首先会固定其中的第一个参数，然后使用MLE计算第二个变量值；接着通过固定第二个变量，再使用MLE估计第一个变量值，依次迭代，直至收敛到局部最优解。 四、EM推导下面来推导EM算法： 对于每一个样例$i$，让$Q_i$表示该样例隐含变量$z$的某种分布，$Q_i$满足的条件是$$\sum_z{Q_i\left(z\right)=1 \ Q_i\left(z\right)\geqslant 0}$$（如果$z$是连续的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果是按照隐藏变量是男女，那么就是伯努利分布。 可以由前面阐述的内容得到下面的公式： $$\sum_i{\log p\left(x^{\left(i\right)};\theta\right)=\sum_i{\log\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}}}········（1）$$$$=\sum_i{\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}}········（2）$$$$\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}\right)}}······（3）$$ 上面三个式子中，式（1）是根据联合概率密度下某个变量的边缘密度求解的（这里把$z$当做是随机变量）。对每一个样本$i$的所有可能类别求等式右边的联合概率密度函数和，也就是得到等式左边为随机变量$x$的边缘概率密度。由于对式（1）直接求导非常困难，我们可以做一个简单的变化，将其分子分母都乘以一个相等的函数$Q_i(Z^{(i)})$，得到式（2）。那么如何从式（2）推导出式（3）呢，这就需要用到之前提到的Jensen不等式。 以下为具体的分析过程： 首先，把（1）式中的$log$函数看成是一个整体，即令$f(x)=log(x)$，因为$(log(x))^”=-1/x^{2}&lt;0$，根据定理可知其为凹函数。 再根据凹函数的Jensen不等式：$f(E[X])&gt;=E[f(x)]$。 到这里，我们可以观察到，在式（2）中，当把$log(x)$看成$f(x)$时，后边的$$\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}$$ 其实就是可以类比为离散型随机变量的期望公式。具体的求解可以参照下图中离散型随机变量的期望公式。 我们可以把$Q_i^{(z^{(i)})}$看成是相应的概率$p_i$，把$$\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}$$看作是$z^{(i)}$的函数$g(z)$，根据期望公式$E\left[g\left(x\right)\right]=\sum_{i=1}^{\infty}{g\left(x_i\right)·p_i}$可以得到：$$E\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}\right)=\sum_{z^{\left(i\right)}}{Q_i\left(z\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}}$$，把上述根据Jensen不等式整合到一起得到：$$f\left[E\left(g\left(X\right)\right)\right]=\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}$$$$\geqslant E\left[f\left(g\left(X\right)\right)\right]=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}$$这样我们就得到了式（3）。 现在我们把式（2）和式（3）的不等式次而成：似然函数$L(\theta)≥J(z,Q)$的形式，其中$z$为隐变量，那么我们可以通过不断地最大化$J$的下界，来使得$L(\theta)$不断提高，最终达到它的最大值。借助下图来解释下这个过程： 首先我们固定$\theta$，调整$Q(z)$使下界（绿色曲线）$J(z,Q)$沿着绿色虚线上升至与$L(\theta)$在此点$\theta$处相等（绿色曲线至蓝色曲线），然后固定$Q(z)$，调整$\theta$使下界$J(z,Q)$达到最大值($\theta {_t}$至$\theta _{t+1}$)，然后再固定$\theta$，调整$Q(z)$…….直到收敛到似然函数$L(\theta)$的最大值处的$\theta^*$ 这里有两个问题： 什么时候下界$J(z,Q)$与$L(\theta)$在此点$\theta$处相等？ 为什么一定会收敛？ 首先来解释下第一个问题。在Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立。而在这里，为：$$\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}=c$$对该式做个变换，将分母移到等号右边，并对所有的$z$求和，得到第一个等号；又因为前面提到的$\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)=1}$，得到第二个等号。$$\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)c}=c$$根据上面两个式子可以得到$$Q_i\left(z^{\left(i\right)}\right)=\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{\sum_z{p\left(x^{\left(i\right)},z;\theta\right)}}$$$$\\\\ =\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{p\left(x^{\left(i\right)};\theta\right)}$$$$\\\ =p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)$$到这里，我们推出了在固定参数$\theta$后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），解决了$Q(z)$如何选择的问题。此步就是EM算法的E步，目的是建立$L(\theta)$的下界。接下来的M步，目的是在给定$Q(z)$后，调整$\theta$，从而极大化$L(\theta)$的下界$J$（在固定$Q(z)$后，下界还可以调整的更大）。那么一般的EM算法的步骤如下： 第一步：初始化分布参数$\theta$； 第二步：重复E步和M步直到收敛： E步：根据参数的初始值或上一次迭代的模型参数来计算出的因变量的后验概率（条件概率），其实就是隐变量的期望值，来作为隐变量的当前估计值：$$\\\ Q_i\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)$$ M步：最大化似然函数从而获得新的参数值：$$\theta :=arg\underset{\theta}{\max}\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}}$$ 通过不断地迭代，然后就可以得到使似然函数$L(\theta)$最大化的参数$\theta$了。 接下来我们看第二个问题。上面多次说到直到收敛，那为什么一定会收敛呢？证明如下： 假定$\theta^{(t)}$和$\theta^{(t+1)}$是EM第t次和t+1次迭代后的结果。如果我们证明了$l(\theta^{(t)})≤l(\theta^{(t+1)})$，也就是说极大似然估计单调增加，那么最终我们就会得到极大似然估计的最大值。 下面来证明，选定$\theta^{(t)}$后，我们得到E步：$$\\\ Q_i^{(t)}\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)$$这一步保证了在给定$\theta^(t)$时，Jensen不等式中的等式成立，也就是$$l\left(\theta^{\left(t\right)}\right)=\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}$$然后进行M步，固定$Q_i^{(t)}(z^{(i)})$，并将$\theta^{(t)}$视作变量，对上面的$l(\theta^{(t)})$求导后，得到$\theta^{(t+1)}$,这样经过一些推导会有以下式子成立：$$l\left(\theta^{\left(t+1\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t+1\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（4）}$$$$\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（5）}$$$$=l\left(\theta^{\left(t\right)}\right)······\textrm{（6）}$$解释第（4）步，得到$\theta^{(t+1)}$时，只是最大化$l(\theta^{(t)})$，也就是$l(\theta^{(t+1)})$的下界，而没有使等式成立，要想使等式成立只有在固定$\theta$，并按E步得到$Q_i$时才能成立。况且根据我们前面得到的下式，对于所有的$Q_i$和$\theta$都成立$$l\left(\theta^{\left(t\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}$$第（5）式利用的M步的定义，M步就是将$\theta^{(t)}$调整到$\theta^{(t+1)}$，使得下界最大化。这样（5）、（6）就都证明成立了。 再结合之前那个图解释一下这几步推导： 首先（4）对所有的参数都满足，而其等式成立条件只是在固定$\theta$，并调整好$Q$时成立，而第（4）步只是固定$Q$，调整$\theta$，不能保证等式一定成立。对应到图上就是蓝色曲线的峰值与$l(\theta^{(t+1)})$的关系，要使它们相等还必须要固定$\theta$，调整好$Q$；（4）到（5）就是M步的定义，也就是固定$Q$，调整$\theta^{(t)}$至$\theta^{(t+1)}$，对应到图上即为蓝色曲线与红色曲线交点处至蓝色曲线峰值。（5）到（6）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$l(\theta)$一个特定值（这里为$\theta^{(t)}$）一样的高度，而此时发现下界仍然可以上升，因此经过$M$步后，下界又被拉升，但达不到与$l(\theta)$另外一个特定值（$\theta^{(t+1)}$）一样的高度，之后E步又将下界拉到了与这个特定值一样的高度，循环往复，直到达到最大值。 这样就证明了$l(\theta)$会单调增加。如果要判断收敛情况，可以这样来做：一种收敛方法是$l(\theta)$不再变化，还有一种就是变化幅度很小，即根据$l(\theta^{(t+1)})=l(\theta^{(t)})$的值来决定。 从前面的推导中我们知道$l(\theta)≥J(Q,\theta)$，EM也可以看做是$J$的坐标上升法，如下图所示： 图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步：固定θ，优化Q；M步：固定Q，优化θ；交替将极值推向最大。 五、EM的应用：混合高斯模型待补充 六、EM的应用：EM聚类以下的聚类图来自维基百科，可以生动的看出 待补充 七、参考资料The EM Algorithm混合高斯模型和EM算法cs229-notes8]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度面试回归记]]></title>
    <url>%2F2017%2F03%2F12%2F%E7%99%BE%E5%BA%A6%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[第一次去BAT面试数据挖掘岗实习生，面试过程中考察了很多基础的知识点，深度没有想象中的深，主要是考察的范围非常广，后来看了一些面经，这个岗位大致会考察你几个部分，分别是数据结构与算法、编码实现能力、项目实现与业务理解。慢慢地积累，一块一块地突破。其实面试关倒可以短时间内补补课就过去了，关键是笔试部分，这几天做了一些OJ，想想是任重而道远了。加油吧，少年！把面试中考察到的知识点整理如下。 一、搜索结果满意度度量 是否翻页：如果无法在第一个页面找到满意的网页，就会进行翻页操作。如果翻页了，说明搜索引擎的排序并不是自己想要的。 是否重新输入查询词： 查准率：这个相当重要，搜到的东西即使又多又快，但你想要的结果不知道要翻多少页菜能找到，那么搜索结果几乎没有意义。 相关度：请受过训练的人来评估每个引擎的前几个结果是否相关，评估时不参考结果的来源、引擎的品牌等。 速度：就是输入搜索词，得到结果的时间。很多测试告诉我们0.2秒的速度会导致用户满意度的落差，和未来使用的频率。 新鲜度：就是能爬到多新的内容，而且要有一定规模（只有新闻内容不算够新）。 怎样量化评价搜索引擎的结果质量 二、网站用户满意度指标 流量数据：在该网站消耗的流量越多，说明对该网站的需求越多。 访问时长：用户访问网站的停留时间。如果用户不喜欢网站的内容，可能稍微看一眼就关闭网页了，那么访问时长就很短;如果用户对网站的内容很感兴趣，一连看了很多内容，或者在网站停留了很长时间，访问时长就很长。较长的访问时长表明访问者与您的网站进行了较为广泛的互动。 访问页数：用户访问该网站的浏览页数。访问页数较少，说明访客进入你的网站后访问少数几个页面就离开了，满意度并不是很高。 用户是否直接跳出：指访客来到网站后，只访问了一个页面就离开网站的访问次数占总访问次数的百分比。跳出率=只访问一个页面就离开网站的访问次数/总访问次数，跳出率越低说明流量质量越好，用户对网站的内容越感兴趣。用户进入网站后的满意度会影响其进一步使用的决定。分析跳出率的优点是容易探查，分析简单。缺点是难以进一步分析原因，无法对跳出用户进行跟踪。 点击后退按钮 （最常见）；关闭浏览器 （窗口/标签）；输入一个新的URL ；什么也不做 （会话超过30分钟后） 对于单页营销的网站来说，跳出率只能是100%，因为用户只有一个页面可以访问，所以单页营销网站不必考虑这个指标。在百度搜索推广中跳出率和平均访问时长可以反映出网站推广关键词的选择是否精准，创意的撰写是否优秀，着陆页的设计是否符合用户体验。 推荐率：非常重要的满意度指标。对于很多应用或网站也可以看作是分享率。根据kano模型，产品对用户需求的满足分三个层面，基本、期望、兴奋。当产品品质高于用户期望，用户会乐于向身边的人分享。所以正常运营中的产品的用户自然推荐率可以反映满意度。 站内点击次数 三、排序算法3.1 冒泡排序 冒泡排序(bubble sort)：每个回合都从第一个元素开始和它后面的元素比较，如果比它后面的元素更大的话就交换，一直重复，直到这个元素到了它能到达的位置。每次遍历都将剩下的元素中最大的那个放到了序列的“最后”(除去了前面已经排好的那些元素)。注意检测是否已经完成了排序，如果已完成就可以退出了。 冒泡排序的关键字比较次数与数据元素的初始状态无关。第一趟的比较次数为n-1，第i趟的比较次数为n-i，第n-1趟（最后一趟）的比较次数为1，因此冒泡排序总的比较次数为$n(n-1)/2$ 冒泡排序的数据元素移动次数与序列的初始状态有关。在最好的情况下，移动次数为0次；在最坏的情况下，移动次数为$n(n-1)/2$ 冒泡排序的时间复杂度为$O(n^2)$。冒泡排序不需要辅助存储单元，其空间复杂度为$O(1)$。如果关键字相等，则冒泡排序不交换数据元素，他是一种稳定的排序方法。 Python支持对两个数字同时进行交换。a,b = b,a就可以交换a和b的值了。 123456789101112def bubble_sort(lists): count = len(lists) for i in range(0, count): for j in range(i + 1, count): if lists[i] &gt; lists[j]: lists[i], lists[j] = lists[j], lists[i] return listsif __name__ == '__main__': a_list=[20, 40, 100, 90, 50, 80, 70, 60, 110, 100] bubble_sort(a_list) print(a_list) 3.2 选择排序 每个回合都选择出剩下的元素中最大的那个，选择的方法是首先默认第一元素是最大的，如果后面的元素比它大的话，那就更新剩下的最大的元素值，找到剩下元素中最大的之后将它放入到合适的位置就行了。和冒泡排序类似，只是找剩下的元素中最大的方式不同而已。 对具有$n$个数据元素的序列进行排序时，选择排序需要进行$n-1$趟选择。进行第$i$趟选择时，后面已经有$i-1$个数据元素排好序，第$i$趟从剩下的$n-i+1$个数据元素中选择一个关键字最大的数据元素，并将它与第$i$个数据元素交换，这样即可使后面的$i$个数据元素排好序。 选择排序的关键字比较次数与序列的初始状态无关。对n个数据元素进行排序时，第一趟的比较次数为$n-1$，第$i$趟的比较次数是$n-1$次，第$n-1$趟（最后一趟）的比较次数是1次。因此，总的比较次数为$n(n-1)/2$ 选择排序每一趟都可能移动一次数据元素，其总的移动次数与序列的初始状态有关。当序列已经排好序时，元素的移动次数为0。当每一趟都需要移动数据元素时，总的移动次数为$n-1$ 选择排序的时间复杂度为$O(n^2)$。选择排序不需要辅助的存储单元，其空间复杂度为$O(1)$。选择排序在排序过程中需要在不相邻的数据元素之间进行交换，它是一种不稳定的排序方法。 123456789101112def select_sort(list): for i in range(0,len(list)): min=i for j in range(i+1,len(list)): if list[min]&gt;list[j]: min=j list[min],list[i]=list[i],list[min] return listif __name__ == '__main__': a_list = [20, 40, 100, 90, 50, 80, 70, 60, 110, 100] select_sort(a_list) print(a_list) 3.3 直接插入排序 对具有$n$个数据元素的序列进行排序时，直接插入排序需要进行$n-1$趟插入。进行第$j（1≤j≤n-1）$趟插入时，前面已经有$j$个元素排好序了，第$j$趟将$a_{j+1}$插入到已经排好序的序列中，这样即可使前面的$j+1$个数据排好序。 直接插入排序关键字比较次数和数据元素移动次数与数据元素的初始状态有关。在最好的情况下，待排序的序列是已经排好序的，每一趟插入，只需要比较一次就可以确定待插入的数据元素的位置，需要移动两次数据元素。因此总的关键字比较次数为$n-1$,总的数据元素移动次数为$2(n-1)$ 在最坏的情况下，待排序的序列是反序的，每一趟中，待插入的数据元素需要与前面已排序序列的每一个数据元素进行比较，移动次数等于比较次数。因此，总的比较次数和移动次数都是$n(n-1)/2$ 直接插入排序的时间复杂度为$O(n^2)$。直接插入排序需要一个单元的辅助存储单元，空间复杂度为$O(1)$。直接插入排序只在相邻的数据元素之间进行交换，它是一种稳定的排序方法。 1234567891011121314151617########直接插入排序########def insert_sort(lists): count = len(lists) for i in range(1, count): key = lists[i] j = i - 1 while j &gt;= 0: if lists[j] &gt; key: lists[j + 1] = lists[j] lists[j] = key j -= 1 return listsif __name__ == '__main__': a_list = [20, 40, 100, 90, 50, 80, 70, 60, 110, 100] insert_sort(a_list) print(a_list) 3.4 归并排序 典型的是二路合并排序，将原始数据集分成两部分(不一定能够均分)，分别对它们进行排序，然后将排序后的子数据集进行合并，这是典型的分治法策略。 在归并排序中，进行一趟归并需要的关键字比较次数和数据元素移动次数最多为$n$，需要归并的趟数$log_{2}n$，故归并排序的时间复杂度为$O(nlog_{2}n)$。归并排序小长度等于序列长度为$n$的辅助存储单元，故归并排序的空间复杂度为$O(n)$。归并排序是稳定的排序算法。 12345678910111213141516171819202122232425262728293031def merge_sort(a_list): print("Splitting ", a_list) if len(a_list) &gt; 1: mid = len(a_list) // 2 left_half = a_list[:mid] right_half = a_list[mid:] merge_sort(left_half) merge_sort(right_half) i=0;j=0;k=0; while i &lt; len(left_half) and j &lt; len(right_half): if left_half[i] &lt; right_half[j]: a_list[k] = left_half[i] i=i+1 else: a_list[k] = right_half[j] j=j+1 k=k+1 while i &lt; len(left_half): a_list[k] = left_half[i] i=i+1 k=k+1 while j &lt; len(right_half): a_list[k] = right_half[j] j=j+1 k=k+1 print("Merging ", a_list)if __name__ == '__main__': a_list = [54, 26, 93, 17, 77, 31, 44, 55, 20] merge_sort(a_list) print(a_list) 3.5 快速排序1234567891011121314151617def quick_sort(lists, left, right): if left &gt;= right: return lists key = lists[left] low = left high = right while left &lt; right: while left &lt; right and lists[right] &gt;= key: right -= 1 lists[left] = lists[right] while left &lt; right and lists[left] &lt;= key: left += 1 lists[right] = lists[left] lists[right] = key quick_sort(lists, low, left - 1) quick_sort(lists, left + 1, high) return lists 3.6 各种排序方法的时空复杂度 四、斐波那契数列1234567891011121314151617181920212223242526272829303132333435########递归实现效率低下##########def fib1(n): if n == 0: return 0 elif n == 1: return 1 else: return fib1(n - 1) + fib1(n - 2)print fib1(40)########通过将计算的值保存到一个dict中，后面计算时直接拿来使用，这种方式成为备忘(memo)，则会发现效率大大提高。########known = &#123;0: 0, 1: 1&#125;def fib2(n): if n in known: return known[n] res = fib2(n - 1) + fib2(n - 2) known[n] = res return known[n]print fib2(40)#########前面的Fibonacci函数都是树形递归的实现，哪怕是学一点算法就应该知道这种递归的低效了。在这里从树形递归改为对应的迭代可以把效率提升不少。Python的元组赋值特性是我很喜欢的一个东东，这玩意可以把代码简化不少。举个例子，以前的tmp=a;a=b;b=tmp;可以直接用一句a,b=b,a实现，既简洁又明了。########def fib(n): x,y=0,1 while(n): x,y,n=y,x+y,n-1 return xprint fib(40)########基本的逻辑和上面的例子一样，都是尾递归写法。主要的区别就是利用了Python提供的默认参数和三元操作符，从而把代码简化至一行。#######fib=lambda n,x=0,y=1:x if not n else fib(n-1,y,x+y)print fib(40) 五、哈希树 如果在数据元素与其存储位置之间建立对应关系，则可以根据数据元素直接找到其存储位置，避免进行关键字比较，答复提高查找效率，这就是哈希查找的思想。 哈希查找方法的基本过程如下：选取一个函数，用数据元素的关键字作为参数计算该元素的存储位置，该存储位置称为哈希地址，并将数据元素存储在该位置。查找时，用待查找数据元素的关键字作为参数计算一个存储位置，然后查找该位置的数据元素以确定查找是否成功。 哈希查找方法又称为杂凑法、散列法，其中选取的函数称为哈希函数，按照此方法建立的数据元素存储表称为哈希表，哈希表存储单元的数量称为哈希表长。 理想的情况是，对于具有n个数据元素的集合，选取一个关键字与存储位置一一对应的函数每个数据元素都对应一个存储位置，且每一个存储位置只对应一个数据元素。但是，当关键字不是连续分布且跨度很大时会浪费很多存储空间。有时，不同的关键字会映射到同一个存储地址，这种现象称为冲突。映射到同一个存储位置的关键字称为同义词。 哈希方法需解决以下问题： 构造哈希函数，哈希函数（线性函数直接定址、模哈希函数、平方取中法、折叠法）应尽可能简单，以提高计算速度；根据哈希函数计算的存储地址应该尽可能均匀分布，减少冲突现象。； 设计解决冲突的方案（开放定址法（线性探测法、二次探测法））。 六、Mapreduce6.1 MapReduce工作原理(1) 流程分析： 1.在客户端启动一个作业。2.向JobTracker请求一个Job ID。3.将运行作业所需要的资源文件复制到HDFS上，包括MapReduce程序打包的JAR文件、配置文件和客户端计算所得的输入划分信息。这些文件都存放在JobTracker专门为该作业创建的文件夹中。文件夹名为该作业的Job ID。JAR文件默认会有10个副本（mapred.submit.replication属性控制）；输入划分信息告诉了JobTracker应该为这个作业启动多少个map任务等信息。4.JobTracker接收到作业后，将其放在一个作业队列里，等待作业调度器对其进行调度（这里是不是很像微机中的进程调度呢，呵呵），当作业调度器根据自己的调度算法调度到该作业时，会根据输入划分信息为每个划分创建一个map任务，并将map任务分配给TaskTracker执行。对于map和reduce任务，TaskTracker根据主机核的数量和内存的大小有固定数量的map槽和reduce槽。这里需要强调的是：map任务不是随随便便地分配给某个TaskTracker的，这里有个概念叫：数据本地化（Data-Local）。意思是：将map任务分配给含有该map处理的数据块的TaskTracker上，同时将程序JAR包复制到该TaskTracker上来运行，这叫“运算移动，数据不移动”。而分配reduce任务时并不考虑数据本地化。5. taskTracker每隔一段时间会给JobTracker发送一个心跳，告诉JobTracker它依然在运行，同时心跳中还携带着很多的信息，比如当前map任务完成的进度等信息。当JobTracker收到作业的最后一个任务完成信息时，便把该作业设置成“成功”。当JobClient查询状态时，它将得知任务已完成，便显示一条消息给用户。 以上是在客户端、JobTracker、TaskTracker的层次来分析MapReduce的工作原理的，下面我们再细致一点，从map任务和reduce任务的层次来分析分析吧。 6.2 MapReduce工作原理(2) 流程分析 Map端： 1．每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片，当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。 2．在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序，如果此时设置了Combiner，将排序后的结果进行Combia操作，这样做的目的是让尽可能少的数据写入到磁盘。 3．当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个：1.尽量减少每次写入磁盘的数据量；2.尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的数据量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了。 4．将分区中的数据拷贝给相对应的reduce任务。有人可能会问：分区中的数据怎么知道它对应的reduce是哪个呢？其实map任务一直和其父TaskTracker保持联系，而TaskTracker又一直和JobTracker保持心跳。所以JobTracker中保存了整个集群中的宏观信息。只要reduce任务向JobTracker获取对应的map输出位置就ok了哦。 到这里，map端就分析完了。那到底什么是Shuffle呢？Shuffle的中文意思是“洗牌”，如果我们这样看：一个map产生的数据，结果通过hash过程分区却分配给了不同的reduce任务，是不是一个对数据洗牌的过程呢？呵呵。 Reduce端 1．Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce端接受的数据量相当小，则直接存储在内存中（缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由mapred.job.shuffle.merge.percent决定），则对数据合并后溢写到磁盘中。 2．随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，MapReduce都是反复地执行排序，合并操作，现在终于明白了有些人为什么会说：排序是hadoop的灵魂。 3．合并的过程中会产生许多的中间文件（写入磁盘了），但MapReduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。 到这里，MapReduce工作原理终于分析完了，不过我还会继续深入研究，请关注我的后续hadoop相关的博客。 6.3 mapreduce实现wordcount123456789101112131415161718192021222324252627282930313233343536373839404142############mapper#########! /usr/bin/pythonimport sys for line in sys.stdin: # input comes from STDIN (standard input) line = line.strip() # remove leading and trailing whitespace words = line.split() # split the line into words for word in words: # increase counters print '%s\t%s' % (word, 1) # write the results to STDOUT (standard output); # what we output here will be the input for the # Reduce step, i.e. the input for reducer.py # # tab-delimited; the trivial word count is 1 ############reducer##########! /usr/bin/pythonfrom operator import itemgetterimport sysword2count = &#123;&#125;# maps words to their counts for line in sys.stdin:# input comes from STDIN line = line.strip() # remove leading and trailing whitespace word, count = line.split() # parse the input we got from mapper.py try: count = int(count) # convert count (currently a string) to int word2count[word] = word2count.get(word, 0) + count except ValueError: pass # count was not a number, so silently # ignore/discard this linesorted_word2count = sorted(word2count.items(), key=itemgetter(0)) # sort the words lexigraphically;## this step is NOT required, we just do it so that our# final output will look more like the official Hadoop# word count examplesfor word, count in sorted_word2count: print '%s\t%s'% (word, count)# write the results to STDOUT (standard output) 七、海量数据寻找中位数 具体思路：用一个最大堆存放比中位数小（或等于）的元素，用一个最小堆存放比中位数大（或等于）的元素。这里关键的方法是insert()，每当要插入一个元素时，根据判断条件将它插入最大堆或是最小堆，并更新最大堆和最小堆，使得最大堆和最小堆中元素的个数之差不超过1，这样中位数就是最大堆或最小堆的堆顶元素。当最大堆和最小堆中元素个数不同（个数相差为1）时，元素个数多的那个堆的堆顶元素即为中位数；如果两者元素个数相同，那么中位数可以是最大堆和最小堆的堆顶元素的值取平均。下面的程序代码中，当两者元素个数相同时，将最大堆的堆顶元素看做中位数。 插入（insert） （1）如果最大堆为空，将元素插入最大堆； （2）如果最小堆为空，将元素插入最小堆； （3）如果元素比最大堆的堆顶元素小且最大堆中元素个数不大于最小堆中元素个数，将元素插入最大堆；如果如果元素比最大堆的堆顶元素小但最大堆中元素个数大于最小堆中元素个数，那么先把最大堆的堆顶元素插入最小堆，然后删除最大堆的堆顶元素，最后把元素插入最大堆； （4）如果元素比最小堆的堆顶元素大且最小堆中元素个数不大于最大堆中元素个数，将元素插入最小堆；如果如果元素比最小堆的堆顶元素大但最小堆中元素个数大于最大堆中元素个数，那么先把最小堆的堆顶元素插入最大堆，然后删除最小堆的堆顶元素，最后把元素插入最小堆； （5）如果最大堆中元素个数小于最小堆中元素个数，将元素插入最大堆；否则将元素插入最大堆。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
        <tag>实习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（14）：关联分析]]></title>
    <url>%2F2017%2F03%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、关联分析1.1 引言在数据挖掘与机器学习中，关联规则（Association Rules）是一种较为常用的无监督学习算法，与分类、聚类等算法不同的是，这一类算法的主要目的在于发掘数据内在结构特征之间的关联性。 简单一点来说，就是在大规模的数据集中寻找一些有意义有价值的关系。有了这些关系，一方面，可以帮助我们拓宽对数据及其特征的理解；另一方面，则可以实现推荐系统的构建与应用（例如购物篮分析等）。 在对关联规则有了基本的认识后，我们对其进行进一步的细分，以日常生活中的关联性举例，在逛超市的顾客中，购买面包的人很大程度上会购买牛奶，这一类的关联性被称为简单关联规则；再例如，购买汽车遮阳板的很多顾客会在近期内购买零度玻璃水，这样的事例不仅反映了事物间的关联关系，而且还具有时间上的先后顺序，因此这一类的关联性被称为序列关联规则。 广义上的关联规则包含了简单关联和序列关联，接下来我们分别对这两块知识进行深入学习。 1.2 简单关联规则初探首先我们需要明确关联分析中的一些基本概念： 事务：指关联分析中的分析对象，我们可以把它理解成为一种宽泛行为（例如顾客的一次超市购买行为，电脑的使用者的一次网页浏览行为等都可以称之为事务），由事务标识（TID）与项目集合组成。 项集：即事务中的一组项目的集合，单个的项目可以是一种商品、一个网页链接等。假设$X$为项集，$I$为项目全体且$I=\{i_1,i_2,···,i_n\}$，那么项集$X\subseteq I$。进一步的，如果$X$中包含$p$个项目，则称该项集为$p-$项集。以上图为例，这里包含了4个事务，$I$包含了5个项目。对于第一个事务而言，由于$X$包含了三个项目，所以该$X$是一个$3-$项集。明确了基本概念后，接下来学习关联规则的一般表现形式$$X\rightarrow Y\left(S=s\%,C=c\%\right)$$其中： $X$和$Y$分别为规则的前项和后项，前项为项目或项集，后项表示某种结论或事实。 $S=s\%$表示规则支持度为$s\%$，$C=c\%$表示规则置信度为$c\%$ 到这里大家可能会疑惑，直接得到关联规则不就可以了吗？为什么要在结论中加入支持度和置信度呢？这就涉及到关联分析中非常重要的一块内容——有效性的判别 1.3 简单关联规则的有效性实际上，在数据中使用关联分析进行探索时，我们可以找出很多关联规则，但并非所有的关联规则都是有效的，有的可能令人信服的程度并不高，也有的可能适用范围很有限，带有这些特征的所谓“关联规则”，我们则称之为不具有“有效性”。判断一条关联规则是否有效，需要用到以下两大测度指标，即规则置信度与规则支持度。 1.规则置信度（Confidence）置信度是对简单关联规则准确度的测量，定义为包含项目$A$的事务中同时也包含项目$B$的概率，数学表述为：$$Confidence\left(A\rightarrow B\right)=P\left(B|A\right)=\frac{P\left(AB\right)}{P\left(A\right)}$$ 置信度的本质就是我们所学过的条件概率，置信度越高，则说明$A$出现则$B$出现的可能性也就越高。假设在电脑$\rightarrow$杀毒软件的关联规则中，置信度$C=60\%$，表示购买电脑的顾客中有$60\%$的顾客也购买了杀毒软件。 2.规则支持度（Support） 支持度测量了简单关联规则应用的普适性，定义为项目$A$与项目$B$同时出现的概率，数学表述为：$$Support\left(A\rightarrow B\right)=P\left(B\cap A\right)=P\left(AB\right)$$ 假设某天共有100个顾客到商场购买物品，其中有10个顾客同时购买了电脑和杀毒软件，那么上述关联规则的支持度就为10%，同样，支持度越高，表明某一关联规则的适用性就越大。 一个有效的简单关联规则，势必同时具有较高的置信度与支持度。因为，如果支持度较高而置信度较低，则证明规则的可信度差；而相反，如果支持度较低而置信度较高，则说明规则的应用范围较小。 举例来说，假设在1000个顾客购买行为的事务中，只有一个顾客购买了烧烤炉，同时也只有他购买了碳，虽然规则“烧烤炉$\rightarrow$碳”的置信度很高，为100%，但支持度仅有0.1%，说明这条规则缺乏普遍性，应用价值不高。 所以一个有效的关联规则，必须具有较高的置信度与支持度，那么在实际应用中，我们就需要给定最小的置信度$C_{min}$与支持度$S_{min}$，只要同时大于$C_{min}$和$S_{min}$的规则，我们才可以将其定义为是“有效”的。 1.4 简单关联规则的实用性在对关联规则的有效性有一个基本的掌握后，我们在此基础上进行进一步的探讨——关联规则的实用性。 关联规则的实用性主要体现在以下两个方面： 1）是否具有实际意义。例如“怀孕$\rightarrow $女性”的关联规则就没有实用价值。 2）是否具有指导意义，即帮助我们在现有的基础上做出有价值的优化。 对第二点进一步展开说明，假设“牛奶$\rightarrow $男性顾客（$S=40\%，C=40\%$）”在$C_{min}$和$S_{min}$均为20%时是一条有效规则时，如果进一步计算发现顾客中男性的比例也为40%，也就是说购买牛奶的男性顾客等于所有顾客中的男性比例，那么这条规则就是一条前后项无关的随机性关联，因此它就没有有意义的指导信息，不具有实用性。 如何衡量关联规则具有实用性呢？这里我们就需要借助规则的提升度了。 规则提升度（Lift）：置信度与后项支持度之比，数学表述为：$$Lift\left(A\rightarrow B\right)=\frac{Confidence\left(A\rightarrow B\right)}{P\left(B\right)}=\frac{P\left(AB\right)}{P\left(A\right)P\left(B\right)}$$ 提升度反映了项目$A$的出现对项目$B$出现的影响程度。从统计角度来看，如果$A$的出现对项$B$的出现没有影响，即$A$与$B$相互独立的化，$P(AB)=P(A)P(B)$，此时规则提升度为1。所以，具有实用性的关联规则应该是提升度大于1的规则，即$A$的出现对$B$的出现有促进作用。同样，提升度越大，证明规则实用性越强。 这样我们就阐述清楚了关联规则的一些基本假定与判别标准，当数据集较小时，关联规则的使用较为简单，但是如果数据集很大的话，如何在这海量的数据中快速找出关联规则呢？这就引出了进一步要叙述的内容——简单关联规则下的$Apriori$算法。 二、Apriori算法2.1 简介在数据量庞大的前提下，由于简单搜索可能产生大量无效的关联规则，并导致计算效率底下。出于克服这些弊端的目的，Apriori算法应运而生，该算法自1996年提出后，经过不断地完善和发展，已成为简单关联分析中的核心算法。 2.2 频繁项集的相关定义频繁项集很好理解，他是指大于等于最小支持度$S_{min}$的项集。其中，若频繁项集中包含一个项目，则成为频繁$1-$项集，记为$L_1$；若包含$k$个项目，则成为频繁$k-$项集，记为$L_k$。频繁项集具有以下两个性质，这俩条性质将应用于我们后面频繁项集及其关联规则的寻找中： 1）频繁项集的子集必为频繁项集（假设项集$\{A,C\}$是频繁项集，那么$\{A\}$和$\{C\}$也为频繁项集） 2）非频繁集的超集一定也是非频繁的（假设项集$\{D\}$不是频繁项集，那么$\{A,D\}$和$\{C,D\}$也不是频繁项集） 进一步，当某一个$L_k$的所有超集都是频繁项集时，我们就可以称此$L_k$为最大频繁$k-$项集，确定它的目的就在于使之后的到的关联规则具有较高的普适性。 2.3 寻找频繁项集对频繁项集的寻找，是Apriori算法提高寻找规则效率的关键。它采用迭代的方式逐层寻找下层的超集，并在超集中发现频繁项集。经过层层迭代，直到最顶层得到最大频繁项集为止。在每一轮的迭代中都包含以下两个步骤： 1）产生候选集$C_k$，它是有可能成为频繁项集的项目集合； 2）修剪候选集$C_k$，即基于$C_k$计算相应的支持度，并依据最小支持度$S_{min}$对候选集$C_k$进行删减，得到新的候选集$C_{k+1}$，如此循环迭代，直到无法产生候选项集为止，这样最后一轮所得到的频繁项集就是Apriori所要求的最大频繁项集。 接下来我们以一个下例子帮助理解：假设我们指定的最小支持阀度为0.5（计数≥2） 在第一轮迭代过程中，由于$D$的支持度小于0.5（只有0.25），所以没有进入频繁项集，其余均进入频繁项集，定义为$L_1$。 在第二轮迭代中，候选集$C_2$是$L_1$中所有项目的组合，计算各项目支持度，淘汰$\{A,B\}$和$\{A,E\}$，其余进入频繁项集，定义为$L_2$。 在第三轮迭代中，只有$\{B,C,E\}$进入候选集$C_3$，而其余都没有进入，之所以会这样，是因为这里使用到了前面所提到的频繁项集的第二个性质：非频繁项集的超集一定也是非频繁的。所以，包含$\{A,B\}$与$\{A,E\}$的超集是不可能成为频繁项集的。 由于$L_3$不能继续构成候选集$C_4$，所以迭代结束，得到的最大频繁项集为$L_3\{B,C,E\}$。 2.4 在最大频繁项集的基础上产生简单关联规则得到最大频繁项集并不是最终的目的。之前在判断关联规则的有效性时，我们学习了置信度与支持度两个指标。其中，支持度已经在寻找最大频繁项集的过程中发挥了作用，那么，在接下来关联规则的产生上，就轮到置信度大显身手了。 首先，每个频繁项集都需要计算所有非空子集$L^*$的置信度，公式为$$C_{L’\rightarrow\left\{L-L’\right\}}=\frac{P\left(L\right)}{P\left(L’\right)}$$如果所求得的$C_{L’\rightarrow\left\{L-L’\right\}}$大于我们自行指定的$C_{min}$，则生成相应的关联规则${L’\rightarrow\left\{L-L’\right\}}$ 在上面的例子中，$L_3{\{B,C,E\}}$的非空子集就包括$\{B\}$，$\{C\}$，$\{E\}$，$\{B,C\}$，$\{B,E\}$，$\{C,E\}$，举例来说，根据公式可计算得到$$C_{C\rightarrow\left\{B,E\right\}}=\frac{P\left(B,C,E\right)}{P\left(C\right)}=\frac{2}{3}=66.7\%$$其余置信度依次为：$C_{B\rightarrow\left\{C,E\right\}}=66.7\%$，$C_{E\rightarrow\left\{B,C\right\}}=66.7\%$，$C_{\left\{B,C\right\}\rightarrow E}=100\%$，$C_{\left\{B,E\right\}\rightarrow C}=66.7\%$，$C_{\left\{C,E\right\}\rightarrow B}=100\%$ 如果我么设定$C_{min}=80\%$的话，只有$C_{\left\{C,E\right\}\rightarrow B}$和$C_{\left\{B,C\right\}\rightarrow E}$可以入围，如果设定为$50\%$，那么六条规则就都是有效规则了。置信度的选取和支持度一样，只有结合具体应用情况，算法才能给到我们切合实际的结论。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>关联分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统]]></title>
    <url>%2F2017%2F03%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[一、定义1.1 从搜索引擎说起人们在寻找信息时，常常需要借助搜索引擎，主动地提供准确的关键词来进行相应的搜索，但是当用户无法找到准确描述自己需求的关键字时，搜索引擎就无能为力了。和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。但是和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣与需求的信息。 1.2 推荐系统的具体应用近十年来，推荐引擎对互联网用户来说随处可见。以亚马逊为例，简单说说实际的应用。Amazon有个性化商品推荐列表和相关商品的推荐列表，以下为一种基于物品的的推荐算法（item-based method），它会给我推荐那些和之前我喜欢的物品相似的物品，因为我之前查找过关于算法数据结构相关的书籍，所以就给我推荐了以下这些计算机领域的经典图书除了个性化推荐列表，亚马逊另一个重要的推荐应用就是相关推荐列表。一种是包含购买此商品的顾客也同时购买，另一种是包含浏览过这个商品的顾客购买的其他商品 此外，还有一个应用就是打包销售，同时购买这些商品往往会给你一定的折扣。 除了亚马逊等电子商务领域的推荐系统应用，此外还有像Netflix会向用户推荐电影，豆瓣电台给用户推荐喜欢的歌曲，Facebook给用户推荐个性化物品和好友，Flipboard给特定的用户推荐特定领域的阅读资讯，雅虎的个性化广告投放等等。 二、推荐系统评测一个好的推荐系统不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。同时还要能帮助商家将那些被埋没在长尾的好商品介绍给可能对他们感兴趣的用户。 2.1 推荐系统实验方法首先，人们需要通过实验的办法来计算和获取一些指标，从而全面地测评推荐系统的好坏。主要的实验方法有离线实验、用户调查、在线AB测试。 2.1.1 离线实验一般离线试验的步骤如下： 1）通过日志系统获取用户行为数据，并按照一定格式生成一个标准的数据集； 2）将数据集按照一定的规则分成训练集和测试集； 3）在训练集上训练用户兴趣模型，在测试集上进行预测； 4）通过事先定义的离线指标评测算法在测试集上的预测结果。离线试验不需要有对实际系统的控制权、不需要用户参与实验、速度快、可以测试大量算法。但是它无法计算商业上关心的指标（点击率、转化率）、离线试验的指标和商业指标存在差距。 2.1.2 用户调查因为离线实验的指标与实际的商业指标存在差距，所以需要将算法直接上线测试，但在这之前必须进行用户调查，否则直接进行在线实验会有较高的风险，因为对算法会不会降低用户满意度谁都没有把握。用户调查可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。但是招募测试用户代价较大，很难组织大规模的测试一款能过户，因此会使得测试结果的统计意义不足。 2.1.3 在线实验AB测试时最常用的在线测评算法的实验方法。它通过一定的规则将用户随机分成机组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。AB测试可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。但是周期较长，必须进行长期的实验才能得到可靠的结果，因此常常只会用它测试那些在离线实验和用户调查中表现很好的算法。以上就是一个新的推荐算法最终上线所需要做的事。总结一下： 1）首先，需要通过离线试验证明它在很多离线指标上优于现有算法； 2）然后，需要通过用户调查确定它的用户满意度不低于现有算法。 3）最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。 2.2 离线评测指标令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。 准确率（Precison）:$$Precision=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|R\left(u\right)|}}$$ 召回率（Recall）：$$\textrm{Re}call=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|T\left(u\right)|}}$$ 覆盖率(Coverage)，描述了一个推荐系统对物品长尾的发掘能力， 最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例:$$Coverage=\frac{|\cup_{u\in U}R\left(u\right)|}{|I|}$$ 信息熵 ：$$H=-\sum_{i=1}^n{p\left(i\right)\log p\left(i\right)}$$ 基尼系数：$$Gini=\sum_{i=1}^n{p_i\left(1-p_i\right)}$$ 马太效应：即强者更强，弱者更弱。若一个系统会增大热门物品和非热门物品的流行度差异，让热门的物品更加热门，不认的物品更加不热门，那么这个系统就有马太效应。推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给它们感兴趣的某一类人群。但是现在主流的推荐算法都具有马太效应。可以使用基尼系数来评测推荐系统是否具有马太效应。如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2&gt;G1，就说明推荐系统具有马太效应。 多样性：如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即多样性。 多样性描述了推荐列表物品两两之间的不相似性。假设$s(i,j)\in[0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下：$$Diversity\left(R\left(u\right)\right)=1-\frac{\sum_{i,j\in R\left(u\right),i\ne j}{s\left(i,j\right)}}{\frac{1}{2}|R\left(u\right)|\left(|R\left(u\right)-1|\right)}$$ 而推荐系统整体的多样性可以定义为所有哟京沪推荐列表多样性的平均值：$$Diversity=\frac{1}{|U|}\sum_{u\in U}{Diversity\left(R\left(u\right)\right)}$$ 新颖性：给用户推荐那些它们以前没有听说过的物品。最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。但是这个指标比较粗略，因为不同用户不知道的东西是不同的。需要用户调查来准确统计新颖性。 惊喜度：惊喜度和新颖性两者之间是有区别的，如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。 三、基于邻域的算法基于邻域的算法是推荐系统中最基本的算法，在学术界和业界都有广泛研究与应用。它分为两大类，一类是基于用户的协同过滤算法，另一类是基于物品的协同过滤算法。 3.1 基于用户的协同过滤算法（user-based collaborative filtering）UserCF是推荐系统的元老级算法，也是最为著名的算法，它标志了推荐系统的诞生。这里首先介绍最基础的算法，然后在此基础上提出不同的改进方法。 3.1.1 基础算法在一个在线个性推荐系统中，当一个用户A需要个性化推荐时，可以先找到和他有相似兴趣的其他用户，然后将那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法就是基于用户的协同过滤算法UserCF主要包括两个步骤： 1）寻找和目标用户兴趣相似的用户集合这里的关键就是计算两个用户的兴趣相似度，这里，协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令$N(u)$表示用户$u$曾经有过正反馈（用户的行为倾向于指用户喜欢该物品，反之，负反馈指用户的行为倾向于指用户不喜欢该物品）的物品集合，令$N(v)$为用户$v$曾经有过正反馈的物品集合。我们可以使用Jaccard或者余弦相似度来计算它们之间的兴趣相似度。 Jaccard公式：$$w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{|N\left(u\right)\cup N\left(v\right)|}$$ 余弦相似度：$$w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{\sqrt{|N\left(u\right)||N\left(v\right)|}}$$ 2）找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。 3.1.2 离线算法评测书中通过MovieLens数据集上的离线试验来测评算法的性能。UserCF只有一个重要参数K，即为每个用户选出K个和他兴趣最相似的用户，然后推荐那K个用户感兴趣的物品。下图为选择不同的K值时算法的性能。 逐一分析各个指标： 准确率和召回率：准确率、召回率与K不成线性关系，在此数据集中，K=80左右会获得比较高的准确率和召回率。选择合适的K对获得高的推荐系统精要比较重要，但对K值不是很敏感，保持在一定区域内即可。 流行度：K越大则UserCF推荐结果就越热门，流行度就越高。因为K决定了UserCFA给你做推荐时参考多少和你兴趣相似的其他用户的兴趣，如果K越大，参考的人越多，结果就越来越趋近于全局热门的物品。 覆盖率：覆盖率随着K的增大而减小。因为随着K的增大，UserCF越来越倾向于推荐热门的物品，从而对长尾物品的推荐越来越少，覆盖率就越来越小了 此外对于Random算法（每次随机挑选10个用户没有产生过行为的物品推荐给当前用户）和MostPopular算法（按照物品的流行度给用户推荐他没有产生过行为的物品中最热门的10个物品）这两种基础算法（选取K=80），MostPopular算法准确率和召回率很高，但覆盖率非常低。与这两个极端相比，UserCF的准确率和召回率高得多，覆盖率也很高。 3.1.3 用户相似度计算的改进用余弦相似度来度量用户之间兴趣相似度过于粗糙，因为两个用户对热门物品采取过相同的行为并不能说明他们兴趣相似，但对于冷门物品才去过相同的行为更能说明他们兴趣的相似度。John S.Breese提出了以下公式：$$w_{uv}=\frac{\sum_{i\in N\left(u\right)\cap N\left(v\right)}{\frac{1}{\log\left(1+|N\left(i\right)|\right)}}}{\sqrt{|N\left(u\right)||N\left(v\right)|}}$$该公式通过$1/\log\left(1+|N\left(i\right)|\right)$惩罚了用户$u$和用户$v$共同兴趣列表中热门物品对他们相似度的影响。将基于上述用户相似度公式的UserCF算法记为User-IIF算法。 3.1.4 实际应用相比基于物品的协同过滤算法ItemCF，UserCFA在目前的实际应用中使用并不多。其中最著名的使用者是$Digg$，它的推荐思路为：用户在$Digg$中主要通过”顶”和”踩”两种行为表达自己对文章的看法。当用户顶了一篇文章，$Digg$就认为该用户对这篇文章有兴趣，而且愿意把这篇文章推荐给其他用户。然后$Digg$找到所有在该用户顶文章之前也顶了这一篇文章的其他用户，然后给他推荐那些人最近顶的其他文章。 3.2 基于物品的协同过滤算法（item-based collaborative filtering）基于物品的协同过滤算法是目前业界应用最多的算法。亚马逊、Netflix、Hulu、Youtube的推荐算法的基础都是ItemCF。 3.2.1 基础算法UserCF存在一些缺点： 1）随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系 2）很难对推荐结果作出解释 所以，亚马逊提出了基于物品的协同过滤算法。它给用户推荐那些和他们之前喜欢的物品相似的物品，主要通过分析用户的行为记录计算物品之间的相似度，它认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B，它也可以利用用户的历史行为给推荐结果提供解释。 ItemCF主要包括两个步骤： 1）计算物品之间的相似度 可以使用下面的公式定义物品的相似度：$$w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{|N\left(i\right)|}$$分母$|N(i)|$是喜欢物品$i$的用户数，而分子$|N\left(i\right)\cap N\left(j\right)|$是同时喜欢物品$i$和物品$j$的用户数。可以理解为喜欢物品$i$的用户中有多少比例的用户也喜欢物品$j$。但是如果$j$很热门，很多人喜欢，那么$w_{ij}$就会很大，接近1，也就是会造成任何物品都会和热门的物品有很大的相似度。为了避免推荐出热门的物品，可以使用下面的公式：$$w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{\sqrt{|N\left(i\right)||N\left(j\right)|}}$$它惩罚了物品$j$的权重，因此减轻了热门物品会和很多物品相似的可能性。从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是每个用户都可以通过它们的历史兴趣列表给物品“贡献”相似度。 2）根据物品的相似度和用户的历史行为给用户生成推荐列表 3.3 UserCF和itemCF的综合比较四、隐语义模型（矩阵分解模型）五、基于图的模型六、冷启动问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM]]></title>
    <url>%2F2017%2F02%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%2F</url>
    <content type="text"><![CDATA[线性可分支持向量机：硬间隔支持向量机 线性支持向量机：软间隔支持向量机学习一个线性分类器 非线性支持向量机：当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。 一、线性可分支持向量机与硬间隔最大化1.1 线性可分支持向量机假设给定一个特征空间上的训练数据集$$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}$$其中$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。 给定线性可分训练数据集，通过间隔最大化得到的分离超平面为$$w^T·x+b=0$$以及相应的分类决策函数$$f\left( x \right) =sign\left( w^T·x+b \right)$$ 该决策函数称为线性可分支持向量机 1.2 函数间隔与几何间隔一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面确定的情况下$|w^T·x+b|$能够相对地表示点$x$距离超平面的远近。而$w^T·x+b$的符号与类标记的符号是否一致能够表示分类是否正确，所以可用$y(w^T·x+b)$来表示分类的正确性与确信度，这就是函数间隔functional margin的概念 但是，函数间隔有一个不足之处，就是在选择分离超平面时，只要成比例地改变$w$和$b$，超平面并没有变化，而函数间隔却以同样比例变化了。因此，我们可以对分离超平面的法向量$w$加上某些约束，使得间隔确定，此时函数间隔成为几何间隔geometric margin。 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$w,b$关于样本点$(x_i,y_i)$的几何间隔为$$\gamma _i=y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right)$$ 定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即$$\gamma =\underset{i=1,···,N}{\min}\gamma _i$$ 超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。 函数间隔与几何间隔的关系为$$\gamma =\frac{\hat{\gamma}}{||w||}$$若$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 1.3 间隔最大化支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大的分离超平面是唯一的。 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据记性分类。即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 最大间隔分离超平面 接下来求一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，可以表示为下面的约束最优化问题：$$\underset{w,b}{\max}\,\,\gamma$$$$s.t\,\,\,\,y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right) \geqslant \gamma \,\,,\,\,i=1,2,···,N$$即最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma $，约束条件表示的是超平面$w,b$关于每个训练样本点的几何间隔至少是$\gamma $ 根据几何间隔和函数间隔的关系，可以将此问题改写为$$\underset{w,b}{\max} \frac{\hat{\gamma}}{||w||}$$$$s.t y_i\left( w·x_i+b \right) \geqslant \hat{\gamma} , i=1,2,···,N$$函数间隔$\hat{\gamma}$的取值不影响最优化的解。函数间隔因为$w$，$b$按比例改变为$\lambda w\mathrm{，}\lambda b$而成为$\lambda \hat{\gamma}$，但是对最优化问题中的不等式约束没有影响，对目标函数的优化也没有影响，即两者等价。这样，我们可以取$ \hat{\gamma}=1$，代入后注意到最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的。于是就得到下面的线性可分支持向量机学习的最优化问题。 线性可分支持向量机学习算法——最大间隔法 输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}$,其中,$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$ 输出：最大间隔分离超平面和分类决策函数 步骤如下： 1）构造并求解约束最优化问题：$$\underset{w,b}{\min}\frac{1}{2}||w||^2$$$$ s.t y_i\left( w·x_i+b \right) -1\geqslant 0, i=1,2,···,N$$求得最优解$w^$,$b^$ 2）由此得到分割超平面：$$w·x+b=0$$分类决策函数$$f\left( x \right) =sign\left( w·x+b \right)$$ 这其实是一个凸二次规划convex quadratic programming 问题，凸优化问题是指约束最优化问题$$\underset{w}{\min} f\left( w \right)$$$$ s.t s_i\left( w \right) \le 0 , i=1,2,···,k$$$$ h_i\left( w \right) =0 , i=1,2,···,l $$其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，上述凸优化问题成为凸二次规划问题。 支持向量和间隔边界 再线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例成为支持向量support vector。支持向量是使约束条件式等号成立的点，即$$y_i\left( w·x_i+b \right) -1=0$$对$y_i=+1$的实例点，支持向量在超平面$$H_1\mathrm{：}w·x_i+b=1$$对$y_i=-1$的负例点，支持向量在超平面$$H_2\mathrm{：}w·x_i+b=-1$$ 注意到$H_1$和$H_2$平行，并且没有实例点落在他们中间。在$H_1$和$H_2$之间形成一条长带，分离超平面与他们平行且位于他们中间。长带的宽度，即$H_1$与$H_2$之间的距离成为间隔margin，间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$。$H_1$和$H_2$称为间隔边界。 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在将俄边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量机的个数一般都很少，所以支持向量机由很少的“重要的”训练样本确定。 1.4 学习的对偶算法为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法dual algorithm。 这样做的优点是，一是对偶问题往往更容易求解；二是引入核函数，进而推广到非线性分类问题。 首先构建拉格朗日函数，为此，对每一个不等式约束引进拉格朗日乘子$a_i≥0，i=1,2,···,N$定义拉格朗日函数：$$L\left( w,b,a \right) =\frac{1}{2}||w||^2-\sum_{i=1}^N{a_iy_i\left( w·x_i+b \right) +\sum_{i=1}^N{a_i}}$$其中，$a=\left( a_1,a_2,···,a_N \right) ^T$原问题是极小极大问题：$$\underset{w,b}{\min}\underset{a}{\max}L\left( w,b,a \right)$$根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：$$\underset{a}{\max}\underset{w,b}{\min}L\left( w,b,a \right)$$ 为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$a$的极大。 1) 求$\underset{w,b}{\min}L\left( w,b,a \right)$ 将拉格朗日函数$L(w,b,a)$分别对$w,b$求偏导数并令其为$0$。$$\nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i}=0$$$$\nabla _bL\left( w,b,a \right) =\sum_{i=1}^N{a_iy_i}=0$$得到$$w=\sum_{i=1}^N{a_iy_ix_i}$$$$\sum_{i=1}^N{a_iy_i=0}$$将其代入拉格朗日函数，得到$$L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_iy_i\left( \left( \sum_{j=1}^N{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^N{a_i}}}}$$$$-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}$$ 2)求$\underset{w,b}{\min}L\left( w,b,a \right)$对$a$的极大，即是对偶问题$$\underset{a}{\max} \sum_{i=1}^N{a_i}-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}$$$$s.t. \sum_{i=1}^N{a_iy_i=0}$$$$a_i\geqslant 0, i=1,2,···,N$$ 将目标函数由求极大转换为极小，就得到下面与之等价的对偶最优化问题。$$\underset{a}{\min} \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}-\sum_{i=1}^N{a_i}$$$$s.t. \sum_{i=1}^N{a_iy_i=0}$$$$a_i\geqslant 0, i=1,2,···,N$$考虑原始最优化问题和对偶最优化问题，原始问题满足定理C.2的条件，所以存在$w,α,β$,使$w$是原始问题的解，$α,β$是对偶问题的解。这意味着求解原始问题可以转换为求解对偶问题。 对线性可分训练数据集，假设对偶最优化问题对$a$的解为$a=(a_1,a_2,···,a_N)$，可以由$a$求得原始最优化问题对$(w,b)$的解$w,b$。得到下面的定理。 设$a=(a_1,a_2,···,a_l)^T$是对偶最优化问题的解，则存在下标$j$，使得$a_j&gt;0$，并可按下式求得原始最优化问题的解$w,b$:$$w=\sum_{i=1}^N{a_{i} y_ix_i}$$$$b=y_i-\sum_i^N{a_{i} y_i\left( x_i·x_j \right)}$$ 证明 根据定理C.3,KKT条件成立，即得$$\nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i=0}$$$$\nabla _bL\left( w,b,a \right) =-\sum_{i=1}^N{a_{i} y_i=0}$$$$a_{i} \left( y_i\left( w·x_i+b \right) -1 \right) =0 , i=1,2,···,N$$$$y_i\left( w·x_i+b \right) -1\geqslant 0 , 1,2,···,N$$$$a_{i} \geqslant 0 , i=1,2,···,N$$由此得$$w=\sum_i^{}{a_{i} y_ix_i}$$其中至少有一个$a_j&gt;0$(反证法，假设$a^*=0$，由上可知$w=0$，而$w=0$不是原始最优化问题的解，产生矛盾)，对此$j$有$$y_j\left( w·x_j+b \right) -1=0$$$$a_{j} y_jx_j·x_i+b=1/y_j=y_j$$$$b=y_j-\sum_{i=1}^N{a_{i} y_i\left( x_i·x_j \right)}$$ 综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题的解$a$;再利用求得原始问题的解$w^,b^$,从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。 线性可分支持向量机学习算法 输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}$,其中,$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$ 输出：最大间隔分离超平面和分类决策函数 步骤如下 1）构造并求解约束最优化问题 $$\underset{a}{\min}\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_i}}}$$$$s.t. \sum_{i=1}^N{a_iy_j=0}$$$$a_i\geqslant 0, i=1,2,···,N$$ 求得最优解$a=(a_1,a_2,···,a_N)$ 2）计算$$w =\sum_i^{}{a_{i} y_ix_i}$$并选择$a$的一个正分量$a_j&gt;0$，计算$$b=y_j-\sum_{i=1}^N{a_{i} y_i\left( x_i·x_j \right)}$$ 3） 求得分离超平面$$w·x+b=0$$分类决策函数：$$f(x)=sign(w·x+b)$$ 在线性可分支持向量机中，$w和b$只依赖于训练数据中对应于$a_i&gt;0$的样本点$x_i,y_i$,而其他样本点对$w和b$没有影响。我们将训练数据中对应于$a_i&gt;0$的实例点$x_i\in R^n$称为支持向量。 对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。 二、线性支持向量机与软间隔最大化2.1 线性支持向量机通常情况是，训练数据中有一些特异点outlier，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\xi \geqslant 0$，使函数间隔加上松弛变量大于等于1.这样，约束条件变成$$y_i\left( w·x_i+b \right) \geqslant 1-\xi _i$$同时，对每个松弛变量$\xi \geqslant 0$，支付一个代价$\xi \geqslant 0$。当然，如果我们允许$\xi \geqslant 0$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0$的总和也要最小：目标函数由原来的$\frac{1}{2}||w||^2$变成$$\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}$$这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，控制目标函数中两项（“寻找 $margin$ 最大的超平面”和“保证数据点偏差量最小”）之间的权重，$C$越大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。 线性支持向量机对于给定的线性不可分的训练数据集，通过求解凸二次规划问题：$$\underset{w,b,\xi}{\min} \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}$$$$s.t. y_i\left( w·x_i+b \right) \geqslant 1-\xi _i , i=1,2,···,N$$$$\xi _i\geqslant 0, i=1,2,···\mathrm{，}N$$可证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间。 用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数：$$L\left( w,b,\xi ,a,u \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i-\sum_{i=1}^N{a_i\left( y_i\left( w·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^N{u_i\xi _i}}}$$ 首先求拉格朗日函数针对$w,b,\xi$的极小。$$\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^N{a_iy_ix_i}$$$$\frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^N{a_iy_i=0}$$$$\frac{\partial L}{\partial \xi _i}=0\Rightarrow C-a_i-u_i=0，i=1,2,3···,N$$将它们代入拉格朗日函数，得到和原来一样的目标函数。$$\underset{a}{\max} -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}$$$$s.t. \sum_{i=1}^N{a_iy_i=0}$$$$C-a_i-u_i=0$$$$a_i\geqslant 0$$$$u_i\geqslant 0$$ 不过，由于我们得到$C-a_i-u_i=0$，而又有$u_i&gt;0$（作为拉格朗日乘子的条件）,因此有$a_i≤C$,所以整个dual问题现在写作：$$\underset{a}{\max} -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}}$$$$s.t. \sum_{i=1}^N{a_iy_i=0}$$$$0\le a_i\le C , i=1,2,···,N$$和之前的结果对比一下，可以看到唯一的区别就是现在拉格朗日乘子$a$多了一个上限$C$。 构造并求解上述二次规划问题后求得最优解$$a=\left( a_{1},a_{2},···,a_{N} \right) ^T$$然后计算$$w=\sum_{i=1}^N{a_{i}y_ix_i}$$选择$a$的一个分量$a_i$适合约束条件$0&lt;a_i&lt;C$,计算$$b=y_j-\sum_{i=1}^N{y_ia_{i} \left( x_i·x_j \right)}$$对任一适合条件都可求得一个$b$，但是由于原始问题对$b$的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。 2.2 支持向量再现性不可分的情况下，将对偶问题的解中对应于$a_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。如图所示，这时的支持向量要比线性可分时的情况复杂一些。 图中，分离超平面由实线表示，间隔边界由虚线表示。正例点由$。$表示，负例点由$×$表示。图中还标出了实例$x_i$到间隔边界的距离$\frac{\xi _i}{||w||}$。 软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。 若$a_i^*&lt;C$，则$\xi _i=0$，支持向量恰好落在间隔边界上； 若$a_i^*=C,0&lt;\xi _i&lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间； 若$a_i^*=C，\xi _i=1$则$x_i$在分隔超平面上； 若$a_i^*=C,\xi _i&gt;1$，则$x_i$位于分离超平面误分一侧。 三、非线性支持向量机与核函数3.1 核技巧前面我们介绍了线性情况下的支持向量机，他通过寻找一个现行的超平面来达到对数据线性分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。 例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要他是线性的，就没有办法处理，SVM也不行。因为这样的数据本身就是线性不可分的。 此数据集为两个半径不同的圆圈加上了少量的噪音得到，所以一个理想的分界应该是一个圆圈而不是一条直线。如果用$X_1和X_2$来表示这个二维平面的两个坐标的话，则此方程可以写作$$a_1X_1+a_2X_{1}^{2}+a_3X_2+a_4X_{2}^{2}+a_5X_1X_2+a_6=0$$ 注意上面的形式，如果我们构造另外一个无谓的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1·X_2$ 那么显然，上面的方程在新的坐标系下可以写作：$$\sum_{i=1}^5{a_iZ_i+}a_6=0$$ 如果我们做一个映射$\phi :R^2\rightarrow R^5$，将$X$按照上面的规则映射为$Z$那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推倒的线性分类算法就可以进行处理了。这正是核方法处理非线性问题的基本思想。 总结一下，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。此即核技巧 核技巧应用到支持向量机，其基本思想就是通过一个非线性变换将输入空间（欧式空间$R^n或离散集合$）对应于一个特征空间（希尔伯特空间），使得在输入空间$R^n$中的超曲面模型对应于特征空间中的超平面模型（支持向量机），这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。 现在回到SVM的情形，假设原始的数据是非线性的，我们通过一个映射$\phi \left( · \right)$将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量$w$，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的高斯核函数），要表示一个无穷维的向量描述起来就比较麻烦。 我们似乎可以这样做，拿到非线性数据，就找一个映射$\phi（·）$，然后一股脑把原来的数据映射到新空间，再做线性SVM即可。但是在之前对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；但如果原始空间是三维，我们就会得到19维的新空间，这个数目是呈爆炸性增长的，这给映射的计算带来了很大困难，而且如果遇到无穷维的情况，就根本无从计算了，所以就需要核函数出马了。 核技巧的想法是，再学习与预测中只定义核函数$K\left( x,z \right)$，而不显式地定义映射函数$\phi（·）$。不像之前是映射到高维空间中，然后再根据内积公式进行计算，现在我们直接在原来的低维空间中进行计算，而不需要显式的写出映射后的结果。通常，直接计算$K(x,z)$比较容易，而通过$\phi \left( x \right) \mathrm{和}\phi \left( z \right)$计算$K(x,z)$并不容易。 最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的$\phi（·）$，然后通过这个$\phi（·）$得到对应的$K(·，·)$进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到的，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常是“胡乱”选择一个核函数即可——我们直到她对应了某个映射，虽然我们不知道这个映射具体是什么，由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。 我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i·x_j)=\phi \left( x_i \right) ·\phi \left( x_j \right) $来代替，此时对偶问题的目标函数成为：$$\underset{a}{\max} \sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}}$$ 同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为$$sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)$$ 在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐性地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的奇数。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。 3.2 常用核函数通常人们会从一些常用的核函数中选择，根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数。 多项式核函数polynomial kernel function$$K\left( x,z \right) =\left( x·z+1 \right) ^p$$对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为$$f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\left( x_i·x+1 \right) ^p+b} \right)$$ 高斯核函数gaussian kernel function$$K\left( x,z \right) =\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right)$$这个核就是会将原始空间映射为无穷维空间的那个家伙。不过如果$\sigma$选得很大的话，高次特征上的权重实际上衰减的非常块=快，所以实际上相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分，当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最为广泛的核函数之一。它对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数称为 $$f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right) +b} \right)$$ 字符串核函数 核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上，比如，字符串核实定义在字符串集合上的核函数，字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。 非线性支持向量机学习算法 输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}$,其中,$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$ 输出：分类决策函数 步骤如下： 1）选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题$$\underset{a}{\max}\,\,\,\,\,\,\,\,\sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}}$$$$s.t. \sum_{i=1}^N{a_iy_i=0}$$$$0\le a_i\le C , i=1,2,···,N$$求得最优解$$a^=\left( a_{1}^{},a_{2}^{},···,a_{N}^{} \right) ^T$$ 2）选择$a^$的一个正分量$a_i^$适合约束条件$0&lt;a_i&lt;C$,计算$$b^=y_j-\sum_{i=1}^N{y_ia_{i}^{}K\left( x_i·x_j \right)}$$ 3）构造决策函数：$$f(X)=sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)$$当$K(x,z)$是正定核函数时，该问题为凸二次规划问题，解是存在的。 四、序列最小最优化算法（SMO）通常对于优化问题，我们没有办法的时候就会想到最笨的办法，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以$Gradient Descent$和$Gradient Ascend$并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的$Gradient Descent$并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于二次规划问题，极值只有一个，所以是没有局部极值的问题。 另外还有一种叫做$Coordinate Descend$的变种，它每次只选择一个维度，例如$a=(a_1,···,a_n)$，它每次选取$a_i$为变量，而将其他都看成是常数，从而原始的问题在这一步编程一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。$Coordinate Descend$的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当$a$只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。 然后这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的二次规划问题，也是很好求解的。但是有了约束条件，结果让$Coordinate Descend$变得很尴尬了，直接根据第二个约束条件$\sum_{i=1}^N{a_iy_i=0}$,$a_1$的值立即就可以定下来，事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切停留在初始值。 所以SMO一次选取了两个坐标来进行优化。例如，我们假设现在选取$a_1$和$a_2$为变量，其余为常量，则根据约束条件我们有：$$\sum_{i=1}^N{a_iy_i=0}\Longrightarrow a_2=\frac{1}{y_2}\left( -\sum_{i=3}^N{a_iy_i-a_1y_1} \right) \Longleftrightarrow y_2\left( K-a_1y_1 \right)$$其中那个从3到n的作和都是常量，我们统一记作K。将这个式子代入原来的目标函数中，可以消去$a_2$，从而变成一个一元二次函数。总之现在变成了一个带区间约束的一元二次函数极值问题。唯一要注意的就是这里的约束条件，一个就是$a_1$本身需要满足$0≤a_i≤C$,然后由于$a_2$也要满足同样的约束，即：$0≤y_2(K-a_1y_1)≤C$，可以得带$a_1$的一个可行区间，同$[0,C]$交集即可得到最终的可行区间。投影到$a_1$轴上所对应的区间即是$a_1$的取值范围，在这个区间内求二次函数的最大值即可完成SMO的一步迭代。 同$Coordinate Descent$一样，SMO也会选取不同的两个$coordinate$维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO也并不是一次或随机地选取两个坐标函数极值问题，而是有一些启发式的策略来选取最优的两个坐标维度。 五、参考资料pluskid支持向量机系列]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类]]></title>
    <url>%2F2017%2F02%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都较前一次好。 聚类算法主要包括以下五类： 基于分层的聚类（hierarchical methods） 这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：BIRCH算法（1996）、CURE算法、CHAMELEON算法等。 基于划分的聚类（partitioning methods） 给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：K-means算法、K-medoids算法、CLARANS算法 基于密度的聚类（density-based methods） 基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有：DBSCAN（Density-Based Spatial Clustering of Applic with Noise）算法（1996）、OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）、DENCLUE算法（1998）、WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据） 基于网格的聚类（grid-based methods） 这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：STING（Statistical Information Grid）、CLIQUE（Clustering In Quest）算法（1998）、WaveCluster算法。其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。 基于模型的聚类（model-based methods） 基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。 二、相似度、距离计算方法 给定$n$维空间$R^n$中的两个向量$X=(x_1,x_2,···,x_n)^T$和$y=(y_1,y_2,···,y_n)^T$，$x,y$之间的距离可以反映两者的相似程度，一般采用$L_p$距离$$dist\left( X,Y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^p} \right) ^{\frac{1}{p}}$$，其中$p≥1$，也称为闵可夫斯基距离（Minkowski）距离。常用的$p$为$1,2,+\infty$，此时相应的距离公式分别为 1.当$p=1$时，称为曼哈顿距离（Manhattan distance），改名字的由来起源于在纽约市去测量街道之间的距离就是由人不行的步数来确定的。$$d\left(x,y\right)=\sum_{i=1}^n{|x_i-y_i|}$$ 当$p=2$时，称为欧几里得距离（Euclidean distance）$$d\left(x,y\right)=\left(\sum_{i=1}^n{\left(x_i-y_i\right)^2}\right)^{\frac{1}{2}}$$ 当$p=+\infty$时，称为最大值距离（Maximum distance）$$d\left(x,y\right)=\underset{1\le i\le n}{\max}|x_i-y_i|$$ 杰卡德相似系数（Jaccard）$$J\left( A,B \right) =\frac{|A\cap B|}{|A\cup B|}$$ 余弦相似度（Cosine Similarity）$$\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}$$ pearson相似系数$$\rho _{XY}=\frac{cov\left( X,Y \right)}{\sigma _x\sigma _y}=\frac{E\left[ \left( x-u_x \right) \left( y-u_y \right) \right]}{\sigma _x\sigma _y}$$ 相对熵（K-L）距离$$D\left(p||q\right)=\sum_x{p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}}=E_{p\left(x\right)}\log\frac{p\left(x\right)}{q\left(x\right)}$$ Hellinger距离$$D_a\left(p||q\right)=\frac{2}{1-a^2}\left(1-\int{p\left(x\right)^{\frac{1+a}{2}}q\left(x\right)^{\frac{1-a}{2}}}dx\right)$$ 余弦相似度与pearson相似系数的比较 $n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为：$$\cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}=\frac{\sum_{i=1}^n{x_iy_i}}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}}·\sqrt{\sum_{i=1}^n{y_{i}^{2}}}}$$ 这两个向量的相关系数是：$$\rho_{XY}=\frac{cov\left(X,Y\right)}{\sigma_x\sigma_y}=\frac{E\left[\left(x-u_x\right)\left(y-u_y\right)\right]}{\sigma_x\sigma_y}$$$$=\frac{\sum_{i=1}^n{\left(x_i-\mu_x\right)\left(y_i-\mu_y\right)}}{\sqrt{\sum_{i=1}^n{\left(x_i-\mu_x\right)^2}}\sqrt{\sum_{i=1}^n{\left(y_i-\mu_y\right)^2}}}$$相关系数即将$x,y$坐标向量各自平移到原点后的夹角余弦。这即揭示了为何文档间求距离使用夹角余弦，因为这个物理量表征了文档去均值化后的随机向量间的相关系数。 三、K-Means算法3.1 原理K-Means算法属于基于划分的聚类算法，对N 维欧氏空间中的点进行聚类，是一种最简单的无监督学习方法。它通过迭代来实现，其基本思想是：每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。 3.2 基本假设K-Means聚类需要对数据进行一个基本假设：对于每一个 cluster ，我们可以选出一个中心点 (center) ，使得该 cluster 中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点 2.5 来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是 2 ，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。 3.3 算法步骤假定输入样本为$S=x_1,x_2,···,x_n$，则算法步骤为： 1、选择初始的K个类别中心$\mu_1,\mu_2,···,\mu_k$。这个过程通常是针对具体地问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑一个K-Means，并取其中最好的一次结果。 2、对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即：$$label_i=arg\underset{1\le j\le k}{\min}||x_i-\mu_j||$$ 3、将每个类别中心更新为隶属于该类别的所有样本的均值$$\mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}{x_i}$$ 4、重复前两步，直到类别中心的变化小于某阈值或者达到最大迭代次数 3.4 理论分析基于上述的假设，我们导出K-Means所要优化的目标函数：设我们一共有N个数据点需要分为K个Cluster，K-Means需要最小化的损失函数为：$$J=\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^K{r_{ij}||x_i-\mu_j||^2}}$$这个函数，其中$r_{ij}$在数据点$n$被归类到$Cluster(j) $的时候为1，否则为0.直接寻找$r_{ij}$和$\mu_j$来最小化$J$并不容易，不过我们可以通过反复迭代以下两步的方法来进行： 1、先固定$\mu_j$，选择最优的$r_{ij}$，很容易看出，只要将数据点归类到离它最近的那个中心就能保证$J$最小，通俗来讲，因为每个样本点都有一个$r_{ij}$，不是0就是1，那么我们要想让$J$最小，就要保证当一个样本的$r_{ij=1}$时，与类别中心距离的平方和达到最小。这一步即 2、然后固定$r_{ij}$，再求最优的$\mu_j$。将$J$对$\mu_k$求导并令导数等于零，即令 $$\frac{\partial J}{\partial\mu_j}=\sum_{i=1}^{N_j}{r_{ij}\left(x_i-\mu_j\right)}=0$$很容易得到$J$最小的时候$\mu_j$应该满足$$\mu_j=\frac{\sum_i{r_{ij}x_i}}{\sum_i{r_{ij}}}$$ $\mu_j$的值是所有$Cluster(j)$中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$智慧不断地减小或者保持不变，而不会增加，这保证了K-Means最终或到达一个极小值。虽然K-Means并不能保证总是得到全局最优解，但是对于这样的问题，像K-Means这样复杂度的算法，这样的结果已经是很不错了。 3.5 算法演练下面看一个来自WIKI的实例 1、随机生成三个初始的中心点（这个中心点不一定是样本点），即图中红、绿、蓝三个小圈； 2、计算每个样本点与这三个中心店的距离，并将它们归属到离得最近的中心点对应的Cluster。此时图中分成了三个簇，分别是红色、绿色、蓝色部分； 3、重新分别计算三个簇中所有样本点的类别中心，指定为新的类别中心。此时红色、绿色、蓝色类的中点都发生了迁移。 4、反复迭代第2步和第3步，直至收敛。 3.6 总结 优点： 是解决聚类问题的一种经典算法，简单、快速 对处理大数据集，该算法保持可伸缩性和高效率 当簇近似为高斯分布时，它的效果较好 缺点 在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用 必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同 只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇 对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。比如数组1，2，3，4，100的均值为22，显然距离“大多数”数据1、2、3、4比较远，如果改成数组的中位数3，在该实例中更为稳妥，这种聚类也叫作K-mediods聚类 3.7 Canopy算法四、K-Mediods算法五、层次聚类（Hierarchical Clustering） 六、DBSCAN算法6.1 密度聚类方法密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为DBSCAN算法和密度最大值算法。 6.2 DBSCAN算法原理DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。 6.3 若干概念 对象的$\varepsilon -$领域：给定对象在半径$\varepsilon$内的区域 核心对象：对于给定的数目$m$，如果一个对象的$\varepsilon -$领域至少包含$m$个对象，则称该对象为核心对象。 直接密度可达：给定一个对象集合$D$，如果p是在q的$\varepsilon -$领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。如图$\varepsilon =1,m=5$，q是一个核心对象，从对象q出发到对象p是直接密度可达的。 密度可达：如果存在一个对象链$p_1p_2···p_n$，$p_1=q,p_n=p$，对$p_i\in D,(1≤i≤n)$,$p_{i+1}$是从$p_i$关于$\varepsilon$和$m$直接密度可达的，则对象$p$是从对象$q$和$m$密度可达的。 密度相连：如果对象集合$D$中存在一个对象$O$，使得对$p$和$q$是从$O$关于$\varepsilon $和$m$密度可达的，那么对象$p$和$q$是关于$\varepsilon $和$m$密度相连的。 簇：一个基于密度的簇是最大的密度相连对象的集合。 噪声：不包含在任何簇中的对象称为噪声。 6.4 算法步骤下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤： 1、首先随机选择A点为算法实施的切入点，我们将$\varepsilon $设置为图中圆的半径，对象个数$m（minPts）$设定为4。这里我们看到，A点的$\varepsilon - $领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。 2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点 3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。 从上述算法可知： 每个簇至少包含一个核心对象； 非核心对象可以是簇的一部分，构成了簇的边缘（edge）； 包含过少对象的簇被认为是噪声； 6.5 总结 优点 无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means. 可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced. 对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers. 只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.) 加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree. 参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood. 缺点 边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components. 维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance. 不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters. 参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult. 七、OPTICS算法八、密度最大值聚类8.1 引言2014年6月，Alex Rodriguez和Alessandro Laio在$Science$上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。 8.2 若干概念 局部密度$\rho_i$的定义为：$$\rho_i=\sum_j{\chi\left(d_{ij}-d_c\right)}$$，其中，$$\chi\left(x\right)=\left\{\begin{array}{l} 1 if x&lt;0\\ 0 if otherwise\\\end{array}\right.$$其中$d_c$是一个截断距离，$\rho_i$即到对象$i$的距离小于$d_c$的对象的个数。由于该算法只对$\rho_i$的相对值敏感，所以对$d_c$的选择是比较稳健的。 高局部密度点距离$\delta_i$，其定义为：$$\delta_i=\underset{j:\rho_j&gt;\rho_i}{\min}\left(d_{ij}\right)$$即在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离。而极端地，对于密度最大的那个对象，我们设置$\delta=max(d_{ij})$；只有那些密度是局部或者全局最大的点才会有远大于正常值的高局部密度点距离。 8.3 聚类过程这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下： 1、首先计算每一个点的局部密度$\rho_i$，如图中，$\rho_1=7,\rho_8=5,\rho_{10}=4$ 2、然后对于每一个点$i$计算在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离$\delta$ 3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图 4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。 5、将其他的点分配给距离其最近的有着更高的局部密度的簇。（Assign each point to the same cluster of its nearest neighbor of higher density）左图是所有点在二维空间的分布，右图是以$\rho$为横坐标，以$\delta$为纵坐标绘制的决策图。容易发现，1和10两个点的$\rho_i$和$\delta_i$都比较大，作为簇的中心点。26、27、28三个点的$\delta$也比较大，但是$\rho比较小$，所以是异常点。 8.4 一些关键点 簇中心的识别 那些有着比较大的局部密度$\rho_i$和很大的高局部密度$\delta_i$的点被认为是簇的中心；而高局部密度距离$\delta_i$较大但局部密度$\rho_i$较小的点是异常点；确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。在上图中可以分明地用肉眼判断聚类中心，但是有些情况下无法用肉眼来判断。不过，对于那些在决策图中无法用肉眼判断出聚类中心的情形，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将$\rho$值和$\delta$值综合考虑的量$$\gamma_i=\rho_i\delta_i$$，显然$\gamma$值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。我们把排序后的$\gamma$在坐标平面（下标为横轴，$\gamma$值为纵轴）画出来，由图可见，非聚类中心的$gamma$值比较平滑，而从非聚类中心过渡到聚类中心时$\gamma$有一个明显的跳跃，这个跳跃用肉眼或数值检测应该可以判断出来。作者在文末还提到，对于人工随机生成的数据集，$\gamma$的分布还满足幂次定律，即$log\gamma$，且斜率依赖于数据维度。 截断距离$d_c$的选择 一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。参数$d_c$的选取，从某种意义上决定这聚类算法的成败，取得太大或者太小都不行：如果取得太大，将使得每个数据点的$\rho$值都很大以致区分度不高，极端情况是取$d_c&gt;d_{max}$，则所有的数据点都归属于一个Cluster了；如果$d_c$取得太小，同一个Cluster中就可能被拆分成多个，极端情况是$d_c&lt;d_{min}$，则每个数据点都单独称为一个Cluster。作者将比例锁定在数据量的1%~2%，也是基于肉感数据集的经验值。 选定簇中心之后 在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$d_c$的点(这个区域由这样的数据点构成：它们本身属于该Cluster，但在与其距离不超过$d_c$的范围内，存在属于其他Cluster的数据点). 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$\rho_h$. 该类簇中所有局部密度大于$\rho_h$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下A图为生成数据的概率分布，B、C二图为分别从该分布中生成了4000，1000个点。D,E分别是B,C两组数据的决策图（decision tree），可以看到两组数据都只有五个点有比较大的$\rho_i$和很大的$\delta_i$，这些点作为类簇的中心，在确定了类簇的中心之后，每个点被划分到各个类簇（彩色点），或者划分到类簇光晕（黑色点），F图展示的是随着抽样点数量的增多，聚类的错误率在逐渐下降，说明该算法是鲁棒的。 九、谱聚类（Spectral Clustering）十、降维（Dimensionality Reduction）十一、聚类数目的选择（Deciding the Number of Clusterings）十二、高斯混合模型、EM十三、矢量量化（Vector Quantization）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（1）：pandas入门]]></title>
    <url>%2F2017%2F02%2F20%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9Apandas%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这篇文章是对pandas的一个简单的介绍，详细的介绍请参考官方Cookbook。 通常我们会按照下面的格式引入所需要的包： 123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 1.创建对象可以通过Data Structure Intro Setion章节来查看有关该节内容的详细信息。 1.1 可以通过传递一个list对象来创建一个Series，pandas会默认创建整型索引：123456789S = pd.Series([1,3,5,np.nan,6,8])S0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 余下内容详见 kaggle系列（1）：pandas入门]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（10）：朴素贝叶斯]]></title>
    <url>%2F2017%2F02%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件假设的分类方法。 对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用的方法。 一、朴素贝叶斯的学习与分类1.1贝叶斯定理先看什么是条件概率 $P(A|B$表示事件$B已经发生的前提下，事件$A发生的概率，叫做事件$B$发生下事件$A$的条件概率。其基本求解公式为$$P\left(A|B\right)=\frac{P\left(AB\right)}{P\left(B\right)}$$ 贝叶斯定理便是基于条件概率，通过$P(A|B)$来求$P(B|A)$： $$P\left(B|A\right)=\frac{P\left(A|B\right)·P\left(B\right)}{P\left(A\right)}$$ 顺便提一下，上式中的分母，可以根据全概率公式分解为：$$P\left(A\right)=\sum_{i=1}^n{P\left(B_i\right)P\left(A|B_i\right)}$$ 1.2 特征条件独立假设这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。 给定训练数据集$(X,Y)$，其中每个样本$X$都包括$n$维特征，即$x=(x_1,x_2,···,x_n)$，类标记集合含有$K$种类别，即$y=(y_1,y_2,···,y_k)$ 如果现在来了一个新样本$x$我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解$P(y_1|x),P(y_2|x),P(y_k|x)$中最大的那个，即求后验概率最大的输出：$arg\underset{y_k}{\max}P\left(y_k|x\right)$ 那$P(y_k|x)$怎么求解？答案就是贝叶斯定理： $$P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{P\left(x\right)}$$ 根据全概率公式，可以进一步分解上式中的分母：$$P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{\sum_{i=1}^n{P\left(x|y_k\right)P\left(y_k\right)}}（公式1）$$ 先不管分母，分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来，而条件概率$P(x|y_k)=P(x_1,x_2,···,x_n|y_k)$，它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为$k$个，那么参数个数为$k\prod_{j=1}^n{S_j}$ 这显然是不可行的。针对这个问题，朴素贝叶斯算法对条件概率分布做了独立性的假设，通俗地讲就是说假设各个维度的特征$x_1,x_2,···,x_n$互相独立，由于这是一个较强的假设，朴素贝叶斯算法也因此得名。在这个假设的前提上，条件概率可以转化为：$$P\left(x|y_i\right)=P\left(x_1,x_2,···,x_n|y_i\right)=\prod_{i=1}^n{P\left(x_i|y_i\right)} （公式2）$$ 这样参数规模就降到了$\sum_{i=1}^n{S_ik}$ 以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率$P(y_k)$和条件概率$P(x|y_k)$的求解问题就都解决了，那么我们是不是可以求解我们所需要的后验概率$P(y_k|x)$了 答案是肯定的。我们继续上面关于$P(y_k|x)$的推导，将公式2代入公式1中得到： $$P\left(y_k|x\right)=\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}$$ 于是朴素贝叶斯分类器可表示为： $$f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k|x\right)=arg\underset{y_k}{\max}\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}$$ 因为对于所有的$y_k$，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分裂期最终表示为：$$f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}$$ 二、朴素贝叶斯法的参数估计2.1 极大似然估计根据上述，可知朴素贝叶斯要学习的东西就是$P(Y=c_k)$和$P(X^{j}=a_{jl}|Y=c_k)$，可以应用极大似然估计法估计相应的概率（简单讲，就是用样本来推断模型的参数，或者说是使得似然函数最大的参数）。 先验概率$P(Y=c_k)$的极大似然估计是$$P\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}}{N},\,\,k=1,2,···,K$$ 也就是用样本中$c_k$的出现次数除以样本容量。 推导如下： 设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},···,a_{jl}}$，条件概率$P(X^{j}=a_{jl}|Y=c_k)$的极大似然估计是：$$P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}}{\sum_{i=1}^N{I\left(y_i=c_k\right)}}$$ 式中，$x_i^{j}$是第$i$个样本的第$j$个特征。 例题如下： 2.2 贝叶斯估计极大似然估计有一个隐患，假设训练数据中没有出现某种参数与类别的组合怎么办？比如上例中当$Y=1$对应的$X^{(1)}$的取值只有$1$和$2$。这样可能会出现所要估计的概率值为0的情况，但是这不代表真实数据中就没有这样的组合。这时会影响到后验概率的计算结果，使分类产生偏差。解决办法是贝叶斯估计。 条件概率的贝叶斯估计：$$P_{\lambda}\left(X^{\left(j\right)}=a_{jl}\parallel Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}+\lambda}{\sum_{i=1}^N{I\left(y_i=c_k\right)}+S_j\lambda}$$ 其中$\lambda≥0$，$S_j$表示$x_j$可能取值的中数。分子和分母分别比极大似然估计多了一点东西，其意义为在随机变量各个取值的频数上赋予一个正数$\lambda≥0$。当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。 先验概率的贝叶斯估计： $$P_{\lambda}\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}+\lambda}{N+K\lambda}$$ 例题如下： 三、python代码实现3.1 朴素贝叶斯文档分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-"""Created on 下午5:28 22 03 2017bayes algorithm: classify a words as good or bad [text classify]@author: plushunter"""from numpy import *class Naive_Bayes: def __init__(self): self._creteria = "NB" #创建不重复词集 def _creatVocabList(self,dataSet): vocabSet = set([]) # 创建一个空的SET for document in dataSet: vocabSet = vocabSet | set(document) # 并集 return list(vocabSet) # 返回不重复词表（SET的特性） #文档词集向量模型 def _setOfWordToVec(self,vocabList, inputSet): """ 功能:给定一行词向量inputSet，将其映射至词库向量vocabList，出现则标记为1，否则标记为0. """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 return returnVec #文档词袋模型 def _bagOfsetOfWordToVec(self,vocabList, inputSet): """ 功能：对每行词使用第二种统计策略，统计单个词的个数，然后映射到此库中 输出：一个n维向量，n为词库的长度，每个取值为单词出现的次数 """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 #更新此处代码 return returnVec def _trainNB0(self,trainMatrix, trainCategory): """ 输入：训练词矩阵trainMatrix与类别标签trainCategory,格式为Numpy矩阵格式 功能：计算条件概率p0Vect、p1Vect和类标签概率pAbusive """ numTrainDocs = len(trainMatrix)#样本个数 numWords = len(trainMatrix[0])#特征个数，此处为词库长度 pAbusive = sum(trainCategory) / float(numTrainDocs)#计算负样本出现概率（先验概率） p0Num = ones(numWords)#初始词的出现次数为1，以防条件概率为0，影响结果 p1Num = ones(numWords)#同上 p0Denom = 2.0#类标记为2，使用拉普拉斯平滑法, p1Denom = 2.0 #按类标记进行聚合各个词向量 for i in range(numTrainDocs): if trainCategory[i] == 0: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) else: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom)#计算给定类标记下，词库中出现某个单词的概率 p0Vect = log(p0Num / p0Denom)#取log对数，防止条件概率乘积过小而发生下溢 return p0Vect, p1Vect, pAbusive def _classifyNB(self,vec2Classify, p0Vec, p1Vec, pClass1): """ 该算法包含四个输入: vec2Classify表示待分类的样本在词库中的映射集合， p0Vec表示条件概率P(wi|c=0)P(wi|c=0)， p1Vec表示条件概率P(wi|c=1)P(wi|c=1)， pClass1表示类标签为1时的概率P(c=1)P(c=1)。 p1=ln[p(w1|c=1)p(w2|c=1)…p(wn|c=1)p(c=1)] p0=ln[p(w1|c=0)p(w2|c=0)…p(wn|c=0)p(c=0)] log取对数为防止向下溢出 功能:使用朴素贝叶斯进行分类,返回结果为0/1 """ p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1 - pClass1) if p1 &gt; p0: return 1 else: return 0 #test def testingNB(self,testSample): "step1：加载数据集与类标号" listOPosts, listClasses = loadDataSet() "step2：创建词库" vocabList = self._creatVocabList(listOPosts) "step3：计算每个样本在词库中出现的情况" trainMat = [] for postinDoc in listOPosts: trainMat.append(self._bagOfsetOfWordToVec(vocabList, postinDoc)) p0V, p1V, pAb = self._trainNB0(trainMat, listClasses) "step4：测试" thisDoc = array(self._bagOfsetOfWordToVec(vocabList, testSample)) result=self._classifyNB(thisDoc, p0V, p1V, pAb) print testSample, 'classified as:', result # return result#### 加载数据集def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return postingList, classVec#测试if __name__=="__main__": clf = Naive_Bayes() testEntry = [['love', 'my', 'girl', 'friend'], ['stupid', 'garbage'], ['Haha', 'I', 'really', "Love", "You"], ['This', 'is', "my", "dog"], ['maybe','stupid','worthless']] for item in testEntry: clf.testingNB(item) 3.2 使用朴素贝叶斯过滤垃圾邮件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-"""Created on 下午8:47 22 03 2017Email_Classify @author: plushunter """import reimport Bayesfrom numpy import *# mysent='This book is the best book on Python or M.L I have ever laid eyes upon.'# regEx = re.compile('\\W*')# listOfTokens=regEx.split(mysent)# tok=[tok.upper() for tok in listOfTokens if len(tok)&gt;0]# print tok## emailText=open('email/ham/6.txt').read()# listOfTokens=regEx.split(emailText)# print listOfTokensdef textParse(bigString): import re listOfTokens=re.split(r'\w*',bigString) return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]def spamTest(): clf = Bayes.Naive_Bayes() docList=[] classList=[] fullText=[] for i in range(1,26): wordList=textParse(open('email/spam/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(open('email/ham/%i.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=clf._creatVocabList(docList) trainingSet=range(50);testSet=[] for i in range(10): randIndex=int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMatix=[];trainClasses=[] for docIndex in trainingSet: trainMatix.append(clf._bagOfsetOfWordToVec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=clf._trainNB0(array(trainMatix),array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = clf._bagOfsetOfWordToVec(vocabList,docList[docIndex]) if clf._classifyNB(array(wordVector), p0V, p1V, pSpam)!=classList[docIndex]: errorCount+=1 print 'the error rate is :',float(errorCount)/len(testSet) 四、参考资料判别模型·生成模型·朴素贝叶斯方法维基百科：Naive Bayes classifier数学之美番外篇：平凡而又神奇的贝叶斯方法朴素贝叶斯理论推导与三种常见模型机器学习实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（9）：感知机]]></title>
    <url>%2F2017%2F02%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Introduction感知机（perceptron）是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值。 感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，导入基于误分类的损失函数，利用梯度下降对损失函数进行极小化，求得感知机模型，属于判别模型 感知机学习算法简单易于实现，分为原始形式和对偶形式。1957年由Rosenblatt提出，是神经网络和支持向量机的基础 本章框架如下： 感知机模型 感知机的学习策略（损失函数） 感知机学习算法（原始形式与对偶形式），并证明算法的收敛性 一、 感知机模型1.1 感知机模型感知机是一种线性分类器，属于判别模型。 假设我们的输入空间（特征空间）是$\chi \subseteq R^{\boldsymbol{n}}$，输出空间是$\boldsymbol{y}=\left\{ +1,-1 \right\}$。输入$\boldsymbol{x}\in \boldsymbol{\chi }$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \boldsymbol{y}$表示实例的类别。由输入空间到输出空间的函数$$f\left( x \right) =\mathrm{sign}\left( \boldsymbol{w}·x+\boldsymbol{b}\right)$$其中，$\boldsymbol{w}\in \boldsymbol{R}^{\boldsymbol{n}}$为权值或权值向量，$\boldsymbol{b}\in \boldsymbol{R}^{\boldsymbol{n}}$叫做偏置，$\mathrm{sign}$是符号函数，即$$\mathrm{sign}\left( \mathrm{x} \right) =\left\{ \begin{array}{l} +1\mathrm{， x}\geqslant 0\\ -1\mathrm{， x}&lt;0\\\end{array} \right.$$它的假设空间为：函数集合$$\left\{ f|f\left( x \right) =w·x+b \right\}$$ 感知机学习就是由训练数据集（实例的特征向量及类别）求得感知机模型，即求得模型参数$w,b$。 感知机预测即为通过学习得到的感知机模型，对新的输入实例给出其对应的输出类别。 1.2 感知机的几何解释线性方程$w·x+b=0$对应于特征空间$R^{\boldsymbol{n}}$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距，超平面将特征空间划分为两个部分。两部分的特征向量被分为正、负两类，超平面$S$也称为分离超平面。 二、感知机学习策略2.1 数据集的线性可分性给定一个数据集$$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $$若存在某个超平面$S$$$w·x+b=0$$能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对所有的$y_i=+1$的实例，有$w·x+b&gt;0$；对所有的$y_i=-1$的实例，有$w·x+b&lt;0$。则数据集$T$为线性可分数据集；否则称数据集线性不可分。 2.2 感知机学习策略我们选择将误分类点到超平面$S$的总距离作为感知机模型的损失函数 由几何解释可以清楚地看到，任一点到超平面$S$的距离为:$$\frac{1}{||w||}|w·x_0+b|$$而我们对于误分类点的定义为：$$-y_i\left( w·x_0+b \right) &gt;0$$误分类点到超平面的距离：$$-\frac{1}{||w||}y_i\left( w·x+b \right)$$则误分类点到超平面的总距离：$$-\frac{1}{||w||}\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}$$据上述我们定义损失函数为:$$L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}$$其中$M$为误分类点的集合，此即为感知机学习的经验风险函数。一个特定样本点的损失函数，在误分类时是参数$w,b$的线性函数，在正确分类时是0.因此，给定训练数据集$T$，损失函数$L(w,b)$是$w,b$的连续可导函数。感知机学习的策略就是在假设空间中选取使损失函数最小的模型参数，即感知机模型。 三、感知机学习算法这样我们就把感知机的学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。 3.1 感知机学习算法首先我们确定要求解的最优化问题是：$$\min_{w,b}L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}$$通过随机梯度下降法来求解最优化问题。首先，任意选择一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降，而不是一次使$M$中所有误分类点的梯度下降。 计算得到梯度为：$$\nabla _wL\left( w,b \right) =-\sum_{x_i\in M}{y_ix_i}$$$$\nabla _bL\left( w,b \right) =-\sum_{x_i\in M}{y_i}$$对权值进行更新：$$w\gets w+\eta y_ix_i$$$$b\gets b+\eta y_i$$其中$\eta$称为学习率，通过迭代可以期待损失函数不断减小，直到为0. 对于上述算法过程，我们可以有一个直观的解释：当一个实例点被误分类，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以较少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。当然感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。 3.2 对偶形式对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$，我们假设初始值$w_0$和$b_0$均为0。对误分类点($x_i$,$y_i$)通过$$w\gets w+\eta y_ix_i$$$$b\gets b+\eta y_i$$逐步修改$w,b$,设修改$n$次，则最后学习到的$w,b$可以分别表示为$$w=\sum_{i=1}^N{n_i\eta y_ix_i}=\sum_{i=1}^N{a_iy_ix_i}$$$$b=\sum_{i=1}^N{a_iy_i}$$当$\eta =1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类、换句话说，这样的实例对学习结果影响最大。 因为对偶形式的训练实例仅以内积的形式出现。为了方便，可预先将训练实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。$$G=\left[ x_i·x_j \right] _{N\times N}$$与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。 总结感知机学习算法的对偶形式如下： 输入：线性可分的数据集训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $，其中$x_i\in \chi =\boldsymbol{R}^{\boldsymbol{n}}$$y_i\in \boldsymbol{y}=\left\{ -1,+1 \right\} ,i=1,2,···,N $，学习率$\eta \left( 0&lt;\eta \le 1 \right) $； 输出：$a,b$；感知机模型$$f\left( x \right) =sign\left( \sum_{j=1}^N{a_jy_jx_j}·x+b \right)$$其中$a=\left( a_1,a_2,···,a_N \right) ^T$ 1）$a\gets 0,b\gets 0$ 2）在训练集中选取数据$\left( x_i,y_i \right)$ 3）如果$y_i\left( \sum_{j=1}^N{a_jy_jx_j·x_i+b} \right) \le 0$$$a_i\gets a_i+\eta$$$$b\gets b+\eta y_i$$ 4）转至(2)直到没有误分类数据 四、参考资料李航《统计学习方法》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（8）：XgBoost]]></title>
    <url>%2F2017%2F01%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9AXgBoost%2F</url>
    <content type="text"><![CDATA[一、XGBoost简介在数据建模中，经常采用Boosting方法通过将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。 针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。 在Kaggle的希格斯子信号识别竞赛，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。 二、监督学习的三要素因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。 2.1 模型模型指的是给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测$$\hat{y}_i=\sum_j{w_jx_{ij}}$$这里的预测值$y$可以由不同的解释，比如我们可以把它作为回归目标的输出，或者进行$sigmoid$变换得到概率（即用$\frac{1}{1+e^{-\hat{y}_i}}$来预测正例的概率），或者作为排序的指标等。而一个线性模型根据$y$的解释不通（以及设计对应的目标函数）用到回归、分类或者排序等场景。 2.2 参数参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数：$$\varTheta =\left\{w_j|j=1,2,···,d\right\}$$ 2.3 目标函数：误差函数+正则化项模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。 1）$L(\varTheta)$：损失函数$L=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}$，常见的损失函数有： 平方损失：$l\left(y_i,\hat{y}_i\right)=\left(y_i-\hat{y}_i\right)^2$ Logistic损失：$l\left(y_i,\hat{y}_i\right)=y_i\ln\left(1+e^{-y_i}\right)+\left(1-y_i\right)\ln\left(1+e^{y_i}\right)$ 2）$\varOmega\left(\varTheta\right)$：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有$L1$正则与$L2$正则 $L1$正则（lasso）：$\varOmega\left(w\right)=\lambda ||w||_1$ $L2$正则：$\varOmega\left(w\right)=\lambda ||w||^2$ 这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，$Bias$可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而$Variance$是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的$Bias$。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 2.4 优化算法上面三个部分包含了机器学习的主要成分，也是机器学习工具划分模型比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计问题，比如常见的例子如决策树的学习算法的每一步去优化基尼系数，然后剪枝，但是没有考虑到后面的目标是什么。而这些启发式优化方法背后往往隐含了一个目标函数，理解了目标函数本身也有利于我们设计相应的学习算法。 三、回归树与树集成3.1 回归树在介绍$XGBoost$之前，首先得了解一下回归树和树集成的概念，其实在$AdaBoost$算法中已经详细讲述过这一部分了。Boosting Tree最基本的组成部分叫做回归树（regression tree），下面就是一个回归树的例子。它把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会有一个实数分数。具体地，下图给出了一个判断用户是否会喜欢电脑游戏的回归树模型，每个树叶的得分对应了该用户有多可能喜欢电脑游戏（分值越大可能性越大）。 3.2 树集成上图中的回归树只用到了用户年龄和性别两个信息，过于简单，预测的准确性自然有限。一个回归树往往过于简单无法有效地预测，因此一个更加强有力的模型叫做tree ensemble。在上图中使用两个回归树对用户是否喜欢电脑游戏进行了预测，并将两个回归树的预测结果加和得到单个用户的预测结果。在实际的预测模型建立过程中，我们通过不断地增加新的回归树，并给每个回归树赋予合适的权重，在此基础上综合不同的回归树得分获得更为准确的预测结果，这也就是树集成的基本思路。在预测算法中，随机森林和提升树都采用了树集成的方法，但是在具体地模型构造和参数调整的方法有所差别。 在这个树集成模型中，我们可以认为参数对应了树的结构，以及每个叶子节点上面的预测分数。 那么我们如何来学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。决策树学习往往充满了启发式算法，如先优化基尼系数，然后再剪枝，限制最大深度等等。其实这些启发式算法背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法。 四、XGBoost的推导过程4.1 XGBoost的目标函数与泰勒展开对于tree ensemble，我们可以把某一个迭代后集成的模型写成为：$$\hat{y}_i=\sum_{k=1}^K{f_k\left(x_i\right)}, f_k\in\mathscr{F}$$其中每个$f$是一个在函数空间($\mathscr{F}$)里面的函数，而$\mathscr{F}$对应了所有regression tree的集合。我们设计的目标函数也需要遵循前面的主要原则，包含两部分$$Obj\left(\varTheta\right)=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}+\sum_{k=1}^K{\varOmega\left(f_k\right)}$$其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数$f$到模型里面： 其中$\hat{y}_i\left(t-1\right)$就是前$t-1$轮的模型预测，$f_t{(x_i)}$为新$t$轮加入的预测函数。这里自然就涉及一个问题：如何选择在每一轮中加入的$f(x_i)$呢？答案很直接，选取的$f(x_i)$必须使得我们的目标函数尽量最大地降低（这里应用到了Boosting的基本思想，即当前的基学习器重点关注以前所有学习器犯错误的那些数据样本，以此来达到提升的效果）。先对目标函数进行改写，表示如下：如果我们考虑平方误差作为损失函数，公式可改写为：更加一般的，对于不是平方误差的情况，我们可以采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行下一步的计算。泰勒展开一般表达式为：用泰勒展开来近似我们原来的目标：首先定义得到如果移除掉常数项，我们会发现这个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。可能有人会问，这个方式似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？这是因为，这样做首先有理论上的好处，它会使我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。然后这一个抽象的形式对于工程商实现机器学习工具也是非常有帮助的。因为它包含所有可以求到的目标函数，也就是说有了这个形式，我们写出来的代码可以用来求解包括回归、分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般化。### 4.2 决策树的复杂度到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。其中结构部分$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$范数平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图给出了复杂度计算的一个例子。### 4.3 目标函数的最小化接下来是最关键的一步，在这种新的顶一下，我们可以把目标函数进行如下改写，其中$I$被定义为每个叶子上面样本集合$I_j=\{i| q(x_i)=j\}$这一目标包含了$T$个互相独立的单变量二次函数我们可以定义那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益可以观察到上式是由$T$个相互独立的单变量二次函数再加上$L1$范数构成。这样的特性意味着单个树叶的权重计算与其他树叶的权重无关，所以我们可以非常方便计算第$j$个树叶的权重，以及目标函数。由此，我们将目标函数转换为一个一元二次方程求最小值的问题（在此式中，变量为$w_j$，函数本质上是关于$w_j$的二次函数），略去求解步骤，最终结果如下所示：乍一看目标函数的计算与回归树的结构$q$函数没有什么关系，但是如果我们仔细回看目标函数的构成，就会发现其中$G_j$和$H_j$的取值是由第$j$个树叶上数据样本所决定的。而第$j$个树上所具有的数据样本则是由树结构$q$函数决定的。也就是说，一旦回归树的结构$q$确定，那么相应的目标函数就能够根据上式计算出来。那么回归树的生成问题也就转换为找到一个最优的树结构$q$，使得它具有最小的目标函数。计算求得的$Obj$代表了当指定一个树的结构的时候，目标函数上面最多减少多少。我们可以把它叫做结构分数（structure score）。可以把它认为是类似于基尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子，它根据决策树的预测结果得到各样本的梯度数据，然后计算出实际的结构分数。这个分数越小，代表这个树的结构越好： 4.4 枚举树的结果——贪心法在前面分析的基础上，当寻找到最优的树结构时，我们可以不断地枚举不同树的结构，利用这个打分函数来寻找一个最优结构的树，加入到我们的模型中，然后再重复这样的操作。不过枚举所有树结构这个操作不太可行，在这里XGBoost采用了常用的贪心法，即每一次尝试区队已有的叶子加入一个分割。对于一个剧透的分割方案，我们可以获得的增益可以由如下公式计算得到： 这个公式形式上跟ID3算法（采用信息熵计算增益）或者CART算法（采用基尼指数计算增益） 是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的$\gamma$即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数$\lambda$，是正则项里leaf score的$L2$模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。 对于每次扩展，我们还是要枚举所有可能的分割方案，那么如何高效地枚举所有的分割呢？假设需要枚举所有$x&lt;a$这样的条件，那么对于某个特定的分割$a$我们要计算$a$左边和右边的导数和，在实际应用中如下图所示： 我们可以发现对于所有的$a$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度与$G_L$和$G_R$。然后用上面的公式计算每个分割方案的分数就可以了。 但需要注意是：引入的分割不一定会使得情况变好，因为在引入分割的同时也引入新叶子的惩罚项。所以通常需要设定一个阈值，如果引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。此外在XGBoost的具体实践中，通常会设置树的深度来控制树的复杂度，避免单个树过于复杂带来的过拟合问题。 以上介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习的整个过程。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。 五、QA5.1 机器学习算法中GBDT和XGBOOST的区别有哪些？ 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 方差-方差权衡：XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数$T$、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率（xgboost中的$\epsilon$）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）：XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。 缺失值处理：XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。 XGBoost工具支持并行：Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第$t$次迭代的损失函数里包含了前面$t-1$次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 线程缓冲区存储：按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。 可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>XgBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以赛亚·伯林 谈话录 摘]]></title>
    <url>%2F2017%2F01%2F23%2F%E4%BB%A5%E8%B5%9B%E4%BA%9A%C2%B7%E4%BC%AF%E6%9E%97%20%E8%B0%88%E8%AF%9D%E5%BD%95%20%E6%91%98%2F</url>
    <content type="text"><![CDATA[其一他反对那种以为可以依据科学的、政治的、甚至美学的价值在人世间创造一个乌托邦的主张。鉴于人类历史实际上是各种经常相互碰撞的价值和思想的产生地及其变化发展的实验场所这一事实，伯林追溯多元论在伦理学、政治学和美学等领域的出现。 但是，如果我说的不错，不光最终解决这个概念本身是不切实际的，而且，各种价值之间也不可避免地是相互碰撞的、不可协调的。最终解决的可能性（即使我们忘记了这个词组带有希特勒时期的恐怖感）会制造出一种幻觉，一种非常危险的幻觉。因为，如果人们真的相信这种解决是可能的，那么，为了达到这个目标付出多少都绝不为过：为了使人类永远公正、幸福、富于创造性以及和谐协调，有什么不可以为此付出的呢？为了做成这样的蛋卷，我们可以打破无限数量的鸡蛋，这就是列宁、托洛茨基以及我们所了解的波尔布特的信念。既然我知道通往社会问题最终解决的唯一正确道路，我也就知道人类车队必须沿着什么路线走；因为你没有我这种知识，你就不能有选择自由，哪怕是最低限度的选择自由，否则你就达不到目的地。你声明采取某种方式将使你更幸福、更自由，或将使你有自由呼吸的空间，而我知道你这样想是错误的。我知道你需要什么，人民大众需要什么。如果出现由于无知或恶意而酿成的反抗，那就必须振亚下去，为了大多数人永远幸福，消灭成千上万人也许是必要的。除了心甘情愿地将他们全都牺牲掉，我们，明白此中道理的我们，又有什么选择？“（扭曲的人性之材）《往事与随想》赫尔岑 选择概念（关键地位）在思想史的工作中，伯林研究了那些勇敢地、公开地跟占统治地位的理性体系作斗争的思想，赞赏他们的观点和立场。他特别重视这些思想家的自由思想。正式对这种普遍存在的自由思想的肯定和褒赏，显示了他的思想史研究具有重要意义。通过他的研究工作，伯林向我们宣示，在人类历史上没有绝对的价值，而且，人类历史与众多悲剧性后果相伴，充满着那些企图通过坚信最终绝对真理而避免做出悲剧性选择的人们的困苦。普列汉诺夫 伯林：”普列汉诺夫的确是一位富有才华的马克思主义著作家。我完全被他的书迷住了，因为他学识渊博，说理精辟，行文机智，情趣横生，极富吸引力。他是真正的马克思主义之父“赫尔岑 伯林：”赫尔岑成了我的人生楷模。他是一个非常杰出的作家，一个敏锐的真正的政治思想家，非常有独创性。他的自传大概是我一生中度过的最精彩的自传，比卢梭的自传还要好。正是赫尔岑使我爱上了社会思想史和政治思想史，这就是我研究思想史的真正的开端。“《日瓦戈医生》 帕斯捷尔纳克 伯林：”他当然是一位伟大的诗人。这么说吧，诗人可以有两种类型。第一种诗人，写诗时是诗人，而写散文诗是作家，像普希金。第二种诗人，写诗时是诗人，而写散文时也是诗人。帕斯捷尔纳克就属于第二种类型的诗人。他的散文总是诗化的散文，我看他本质上不是一个散文作家。他是晚近俄罗斯伟大的诗人之一，他的小说是伟大的诗化小说。尽管他置身于那虚浮造作的时代中心，却能诚实地描写爱——男主人公对女主人公的爱，极少有作家能够做到这点。正是他的诗使他赢得俄国人和阅读俄文作品的外国人的广泛钦佩，实际上，只有约瑟夫·布罗茨基的成就可与他相媲美。阿赫玛托娃和曼德尔施塔姆都差得远。依我看，帕斯捷尔纳克在各方面都堪称是活着的最优秀的俄国诗人。但是并非所有天才都表里如一，帕斯捷尔纳克也同样如此。他谈起话来稀奇古怪，经常让听者捉摸不透，但总让你感到才气逼人。再也没有比听他谈话更迷人的事了。据我的体会，只有弗吉尼亚·伍尔夫谈到某些东西时像他那样迷人。当然弗吉尼亚·伍尔夫有点狂妄。“马雅可夫斯基 伯林：”他是一个大胆的革新者，一个惊人的雄辩家，一个真正的革命者，但是，他的诗作我看比不上帕斯捷尔纳克、曼德尔施塔姆和阿赫玛托娃。“如果你对某些思想有兴趣，并引起你思考一些问题，那么你就不能不考虑这些思想的历史。因为思想不是单子，他们不是在真空中产生的，而跟人们的信念、生活方式、人生观和世界观紧密相连。思想之间相互碰撞和影响，并不断呈现，成为所谓”智性气候“的组成部分，他和物质因素一样，形成人们的行为和感情，并且历史地变迁着。哲学不是一种积累性的学科。古代那些基本的哲学思想、观点、理论和见解现在仍然是哲学的中心内容。他们有其特定的横贯历史的生命。哲学，如果教得好的话，其用处之一就是让人透过政治上冠冕堂皇的辞藻，识别各种谬论、欺骗、恶作剧、赘疣、感情上的讹诈，以及各种各样的诡辩和伪装，他能大大增强人们的批判能力。哲学不外是要在看不到办法的地方力图去寻找问题的答案。自我理解是哲学的主要目的之一，哲学的目标就是要理解人、事物、词语三者之间的相互关系。在叔本华那里我没有发现包罗万象的形而上学大厦那样的东西。人们可能对叔本华的体系一无所知，但照样能从他许多尖锐的有时是深刻的见解中获得教益。黑格尔的体系在我看来，似乎是希腊神话中的独眼巨人波吕斐摩斯的阴森森的黑洞，一进去就出不来，每一个脚印都指示一条道路，正如拉丁诗人所说的那样。伯林：”他（指施特劳斯）和他的门生都相信，善于恶、对与错都直接得自某种先天的启示，某种”形而上学之眼“，也就是靠使用柏拉图式的那种无缘分享的理性官能。柏拉图、亚里士多德、《圣经》、犹太教法典、迈蒙尼德，也许还有阿奎那和中世纪的其他经院哲学家，都通晓什么样的人生才是最美好的，他们的门生现在也执着于此，而我却没有这种荣幸。“### 其二世界上存在的一切不外乎就是人、物和人脑中的观念————目标、情感、希望、畏惧、选择、想象的情景和所有其他形式的人类经验。这就是我所认识的全部东西。我无法做到无所不知。也许有一个永恒真理和永恒价值的世界，有一种只有真正的思想家才能具有的魔眼，而这只属于恐怕我永远无法进入的极少数精英的领地。 在关键时期，在历史转折关头，当各种因素大体上平衡地出现的时候，个人以及他们的抉择的行动，本身不一定可被预见（确实很少被预见），但却能决定历史的进程。我不相信历史是一部戏剧（这是赫尔岑使用的概念，他认为历史不是一部多幕剧，一部由上帝或大自然赋予主题的演出，不是有图像可辨的地毯）。而马克思和黑格尔都认为历史是一部有结局的多幕剧，它在达到高潮之后（在马克思看来，要经过可怕的冲突、苦难、灾祸）天堂之门将会启开，那就是戏剧的收场，历史（马克思称为史前时期）便会从此结束，一切事物都永恒协调，人们将合乎理性地合作共事。 维科和赫尔德相信历史进程有一定的形式，但不相信历史是一部有结局的戏剧。 我感兴趣的是维科和赫尔德的文化多元性的信念。实际上，每种文化都有自己的重心，各种文化有着各不相同的、新颖的、不可预见的思想及其互相冲突的倾向。维科最先理解到，文化就是世界相对于社会的意义，就是男男女女对于他们自身与别人和环境发生关联的集体意识；文化影响思想、情感、行为、举动的特定形式；文化是多种多样的。维科划分不同时期的文化，赫尔德则对不同时代的不同民族的文明做了区分。 我认为历史不是一个呆板的单线条的进步过程。伏尔泰说，历史是理性、知识和艺术品创造不断进步的过程，有时被可怕的干扰所打破，突然陷入野蛮状态。 预言是一种普遍却难以信赖的活动。我在维科和赫尔德的著作中经所读到的，是人类历史固有的文化多样性的观点。历史并不直线行走，不同文化之间相互作用，有时就是因果性质的作用。通向未来或过去的道路不存在唯一的钥匙，他不好跟自然科学作类比。后者的定律对重复出现的因果链是开放的，这样的因果链能总结为一般的规律。 我们能够理解不同民族和地区人们的生活方式（即使他们的生活方式跟我们的差异很大，即使他们憎恨我们或有时候被我们所谴责），这样的事实表明，我们大家能够穿越时空进行沟通。当我们认了理解了那些与我们在文化上有很大差别的群体的时候，即意味着某种强大的富于同情心的理解、洞察和共感的存在。即便其他文化排斥我们，依靠移情的想象力，我们也可以设想，为什么他们会产生这样的思想和感情，并采取相应的行动达到预定的目标。 一般性的价值观是有的，这是关于人类的经验事实，莱布尼茨称为事实的真理而不是原理性的真理。不同时空的芸芸众生，绝大多数人都共同拥有某些价值观，不论这些价值观是否自觉明晰，也不论他们在态度、举止和行动上的表现如何。另一方面，人与人之间、社会与社会之间，又确实存在着很大的差异。如果你确实了解了个人之间、团体之间、民族之间、各个完整的文明之间所存在的差异，运用想象进入他们的思想、情感世界、设想你自己置身于他们的生存环境中会怎样认知世界并审视自己与他们的关系，那么，即使你对所观察到的东西很反感（全部了解当然并不等于全部谅解），也肯定会减少盲目的偏执和狂热。想象会产生狂热，但通过想象洞察了不同于自己的境况，结果必定能减少狂热。 我认为，人们在把一个有思考里的人称为疯子或神经错乱者时务必小心谨慎。迫害不是来自神经错乱，而产生于把骇人听闻的谬误深信为真理，进而导致罄竹难书的恶果。 了解自己及他人，懂得理性的方法，掌握作为知识和全部科学基础的证据，以及力图验证直观确定性，这些对我们来说都有着根本的重要性。人权这个观念建立在一个正确的信念之上，那就是普遍存在着某种特定的品性。自由、正义、对幸福的追求、真诚、爱。这符合整个人类的利益，而不只是符合作为这个或那个民族、宗教、职业、身份的成员的利益。满足这些要求，保护人们这些要求不被忽视或否认，都是正当的。 你必须懂得什么是正义，什么是自由，什么是社会契约，并对不同类别的自由、权威、义务作出区分等等。政治理论的分野往往围绕“为什么有些人要服从理你些人”这个中心问题来展开。多数政治理论都是对这个问题的回答。实质上不是为什么服从，而是为什么应该服从和服从到什么程度。 关于消极自由的问题是：拦在我面前有什么障碍要排除？其他人怎样妨碍着我？其他人这样做是有意的还是无意的？是间接的还是有制度依据的？关于积极自由的问题是：谁管我？别人管还是自己管？如果是别人，他凭借什么权利？什么权威？如果我有权自主，自己管自己，那么，我会不会失去这个权利？能不能丢掉这个权利？放弃这个权利再恢复这个权利？具体怎么做？还有，谁制定法律？或谁执行法律？征求过我的意见吗？是多数人在统治吗？为什么？是因为上帝、牧师、还是党？是出于公共舆论的压力？传统的压力？还是摄于什么权威？积极自由在正常生活中虽然更重要，但与消极自由相比更频繁地被歪曲和滥用。历史上虚伪的积极自由所造成的危害比现代虚伪的消极自由所造成的危害更大。真诚地相信错误的东西是很危险的，是没有道德价值或精神价值的，至少是令人遗憾的。自由社会的好处在于容许各种各样相互冲突的意见存在而不被压制。多元论确认：既然对于道德和政治问题以至任何价值问题不可能有一个最终的解答，并且，人们给出的或有权给出的某些解答是相互矛盾的，那么，在实际生活的某些领域，有些价值便可能变得互不相容，这样，如果要避免破坏性的冲突的话，就应该妥协，而最低限度的宽容，不管你情不情愿，都是必不可少的。人们可以选择一种生活或另一张生活，而不能同时过两种生活；没有更为根本的标准用来决定正确的选择；既然选择这种也行，选择那种也行，在客观上就不能说一种生活优于另一种生活。它是人们想做什么和成为什么的问题。浪漫主义认为，价值不是发现而是创造出来的，生活的目的就是生活本身。生活就是生活，没有目的。政治哲学的任务是审视生活。要做的事就是审查为实现各种社会目标而提出的种种主张的合理性，检查为确定和实现这些目标而采取的种种方法的正当性。政治哲学要力图澄清构成有关观点的词和概念，使人们能理解自己相信的是什么，自己的行动表示什么。政治哲学还对那些维护或者反对人们所追求的各种目标的辩论作出评价，并防止麦克米兰所引述的胡说八道。对人类的问题，追求一种唯一的、最后的、普遍的解决，无异于是追求海市蜃楼。对于人类生活破坏严重的，莫过于那种迷信了：凡美好生活都是跟政治或者军事力量相联结的。多数英国哲学家似乎都太单薄、太技术化；跟英国哲学家相比，多数法国哲学家似乎都太含糊、太夸饰。我认为马基雅维利是指出现实的各种价值是相互冲突的第一人。依照马基雅维利的观点，你可以选择做一个罗马人或一个基督教徒和殉道者，或者起码可以做一个当权者统治下的受害者。任何真正的问题在某种意义上都是当代的问题。 我认为维科是理解了并告诉我们什么是人类文化的第一人。他不自觉地确立了文化的观念。就我所知，在他之前没有谁有过这样的想法，要努力去重构人们是如何看待生活在周围环境中的自己，如何看待（或感受）与自己发生关联的自然界和其他人————作为在时间中持续存在下来的一类生灵。他反思思想、情感、世界观等各类行为以及肉体的、情绪的、理智的、精神的等多种反应的本质，而正是这些行为和反应构成了文化。如果你想了解人们怎样生活，你必须了解他们的崇拜仪式，文字的内涵，他们通常运用什么类型的想象、明喻、隐喻，他们如何吃、喝、抚养小孩，如何看待自己，如何过私人的、社会的、经济的和政治的生活等等。作为一种模式的文化不是一个孤立的有机体，而是一种存在方式，树立这种理念正是维科对思想史的主要贡献。维科的值得重视的观点是：各不相同的人类思想、行为、感情和行动是互相联结和互相启发的。 米什莱按照维科的思路，认为历史就是社会跟自然力量作斗争并力图运用自然力量去创造让人们能生存和发展的生活方式的历史。人的历史是跟自然界，跟各种力量，跟一切人为的和非人为的障碍进行斗争的历史，这就是米什莱关于人类从各种羁绊中朝向自我解放不断进步的观点。 我们谈论自然界，但我们所知道的自然界仅是我们在外部世界所发现的东西。我们也看见和感触我们的身体，但我们还能说出他作为人之具体化有什么样的感受，这是一种“内在的审视”；人既是观察者，又是行动者。这就是“新科学”的大概意思。理解对于意图、感情、希望、恐惧、努力、意识和无意识的认识，而科学是对处在空间中的物体的认识。换句话说，我们可以看见桌子是什么样的，但我们看不见桌子为什么是这样的。理解过去的文化就是去理解前任所追求的东西；他们怎样看待与他人发生关联的自己，怎样看待世界以及生活在这个世界中的自己。 其三在赫尔德看来，“归属于”的意思是，你说什么，不必多做解释，大家就能了解；你的姿态、语言、所有参与交流的因素，不需敬你熟悉的人作介绍，大家都能把握。是语言、习惯、姿态或本能的反应创造了联合和团结，即创造了具有自己特色的观点、文化和社会共同体。 我一开始读维科的著作简直就被他迷住了。我总是从接受邀请做讲演或写文章开始研究的。 唯一真实的东西是精神，是人与上帝的关系，人与人的关系，别无其他了。内在的精神，个人的灵魂的底蕴，内心世界，这是唯一实在的东西，至于礼节、学问和教阶制度，统统不在话下。 赫尔德乐观地相信，人类大花园中的所有花卉都能和谐地生长，各种文化都能相互激励，为创造这种和谐的境界作出自己的贡献。绝不主张政治上的民族主义，政治上的民族主义必然大致侵略和培植民族自豪感。一个民族不是一个国家，而是一个文华实体，同一民族的人说共同的语言，生活在共同的地域，有着共同的习惯、共同的历史额共同的传统。 依我的看法，强烈的民族主义不过是耻辱心理的表现。高度发达的民族不会产生民族主义。民族主义是对伤害的反应。民资注意对一切事物均构成威胁。民族主义就等于我们对自己说，因为我们是德国人或法国人，所以我们是最优秀的人，我们完全有权做我们要做的事，一旦你把一切行为的根据放在民族这个超越个人的权威上，那就会扩展到政党，到阶级，到教会，通往压迫的道路便从此打开了。 你不能阻止科学的进步，造成灾难的不是武器，而在于使用武器的人。智能的进步是不能阻止的，人们所能做的是防止科学的滥用。廓清腐败的社会，荡涤一切污泥浊水，然后再向前进。 熊彼特正确的说过，那些相信观念必定绝对不变的人是偶像崇拜者。文明意味着必须允许变化的可能，意味着永不停息地去追求自己信奉的理想，为之献身也在所不惜。 不同的个人、集团。文化之间可以沟通，因为人的价值并非无限地多；他们共属于一条水平线，即客观的常常又相互矛盾的人类价值，在他们之间必须进行（常常是痛苦的）选择。 我自己感觉不到有这种既在现实生活之内又超越现实生活的实在。我不是宗教徒。但我对信教者的宗教体验评价颇高。我深深地被犹太教堂也包括基督教堂和伊斯兰教寺院中的宗教仪式所打动。我想，不理解信教是怎么一回事的人恐怕也不理解人为什么而活着。因此干巴巴的无神论者都是瞎子聋子，不了解人生的深刻体验，或者说不了解人生的内在底蕴，就像瞎子不能欣赏美景一样。光有感觉能力的人不能充分理解他人，包括信教者、不信教者、神秘主义者、儿童、诗人、艺术家等等。 我有一种深信不疑的看法，有些道德的、社会的和政治的价值是相互抵触的，任何一个社会总有一些价值是不能彼此调和的。换句话说，人们爱以生存的某种最终的价值，不光在实践上而且在原则上、在概念上都是不可兼得的，或者说不可彼此结合的。没有哪一个精于心机的人，同时又是无所计较，一切都听其自然的人。你不能把充分的自由跟充分的平等结合起来。给狼充分自由就不能同时也给羊有充分自由。正义和慈悲，知识和幸福，如此等等，都可能相互冲突，不可兼得。既然是这样，人类的问题（归根到底是如何生活的问题）就不可能全都求得完满的解决。这不是因为实际上有困难，找不到妥善的解决方法，而是因为这些价值本身在概念性质上都是有缺陷的。乌托邦式的解决在原则上没有缺陷，可以成立，但这样的解决是企图把不可结合的东西结合起来。某些人类的价值之所以不能相互结合，就因为他们本身是不能并存的。因此只能在彼此之间进行选择。选择可能很痛苦，如果你选择A，你就得忍痛失去B。在最终的各种人类价值之间不可避免要作出这样的选择。在任何可以想象的社会，选择都可能是痛苦而且是不可避免的。互不相容的价值本身始终是不能相容的。我们所能做的是防止选择太痛苦，这就意味着，我们需要有一种机制，使得人们对各种价值的追求尽可能不违背自己深刻的道德信念。在多元化的自由社会里，不可避免要作出各种妥协和折中，经过权衡利弊而避免最坏的情况。再三斟酌，取其一方。平等多重要？自有多重要？正义多重要？慈悲多重要？善良对重要？真理多重要？掂量掂量就知道了。知识和幸福也不总是牢牢结合的。一个知道自己患了癌症的人不会因为有了这种知识而感到幸福；无知会使他少一些自由，但同时却使他觉得多了一些幸福。这就是说，人生问题的某种最终解决，没有普遍适用的始终不变的可行保准。那些相信可能有完美无缺的社会的人必定以为，为了实现这种美好的社会，作出多大牺牲都是必要的，为了达到这种理想的目标，付出多大的代价都是值得的。他们想，如果必须要流血才能创造这种美好的社会，那么就流血吧，不管流谁的血，也不管流多少血。不打破鸡蛋怎么能做出上等的蛋卷，课时人们一旦养成打破鸡蛋的习惯，他们久不久罢手，鸡蛋打破了，蛋卷却没有做成。凡是以为对人生问题可以求得最终解决的这种狂热的信念，不能把导致灾难、痛苦、流血和可怕的压迫。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（7）：GBDT]]></title>
    <url>%2F2017%2F01%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT%2F</url>
    <content type="text"><![CDATA[引言GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。自算法的诞生之初，它就和SVM一起被认为是泛化能力（generalization）较强的算法。近些年来更因为被用于构建搜索排序的机器学习模型而引起广泛的关注。它最早见于yahoo，后被广泛应用在搜索排序、点击率预估上。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。 除此之外，GBDT还是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，而且相比较于其他算法还有着出众的准确率，如此优异的性能也让GBDT收获了机器学习领域的“屠龙刀”这一赞誉。 本文首先介绍GBDT中的DT，即回归树，这是它的基础算法；然后叙述提升树，它是以决策树为基函数的提升方法；接着介绍GBDT中的GB，即梯度提升；最后导出GBDT算法的整个流程。 二、Regression Desicion Tree：回归树2.1 回归树简介树模型也分为决策树和回归树，决策树常用来分类问题，回归树常用来预测问题。决策树常用于分类标签值，比如用户性别、网页是否是垃圾页面、用户是不是作弊；而回归树常用于预测真实数值，比如用户的年龄、用户点击的概率、网页相关程度等等。 回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得到一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值寻找最优切分变量和最优切分点，但衡量的准则不再是分类树中的基尼系数，而是平方误差最小化。也就是被预测错误的人数越多，平方误差就越大，通过最小化平方误差找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。 2.2 回归树的生成首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$$D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$$我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]$$遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值：$$R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\} , R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}&gt;s\right\}$$$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i} , x\in R_m , m=1,2$$ 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树：$$f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)}$$ 三、Boosting Decision Tree：提升树3.1 提升树模型提升方法采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（Boosting tree）。对分类问题构建的决策树是二叉分类树，对回归问题构建决策树是二叉回归树。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。提升树模型可以表示为决策树的加法模型：$$f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}$$其中$T\left(x;\varTheta_m\right)$表示决策树；$\varTheta_m$为决策树的参数；$M$为树的个数。 3.2 提升树算法对回归问题的提升树算法来说，给定当前模型 $f_{m-1}{(x)}$只需要简单地拟合当前模型的残差。现将回归问题的提升树算法叙述如下： 1）初始化$f_0{(x)}=0$ 2）对$m=1,2,···,M$ a）计算残差$$r_{mi}=y_i-f_{m-1}\left(x_i\right) , i=1,2,···,N$$ b）拟合残差$r_{mi}$学习一个回归树，得到 $T\left(x;\varTheta_m\right)$ c）更新$f_m{(x)}=f_{m-1}{(x)}+T(x;\varTheta_m )$ 3）得到回归问题提升树$$f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}$$ 接下来通过训练一个用于预测年龄的模型来展现算法的运行流程 1）首先，训练集有4个人$A,B,C,D$，它们的年龄分别是$14,16,24,26$，其中$A,B$分别是高一和高三学生；$C,D$分别是应届毕业生和工作两年的员工，可用于分枝的特征包括上网时长、购物金额、上网时段和对百度知道的使用方式。如果是一棵传统的回归决策树来训练，会得到下图所示结果： 2）但是如果用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只限定两棵树。我们会得到如下所示结果：第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第二棵树只有两个值1和-1，直接可分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 3）将两棵回归树预测结果进行汇总，解释如下： A：14岁高一学生；购物较少；经常问学长问题；预测年龄A = 15 – 1 = 14 B：16岁高三学生；购物较少；经常被学弟问问题；预测年龄B = 15 + 1 = 16 C：24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 D：26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26 对比初始的回归树与GBDT所生成的回归树，可以发现，最终的结果是相同的，那我们为什么还要使用GBDT呢？ 答案就是对模型过拟合的考虑。过拟合是指为了让训练集精度更高，学到了很多“仅在训练集上成立的规律”，导致换一个数据集后，当前规律的预测精度就不足以使人满意了。毕竟，在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 在上面这个例子中，初始的回归树为达到100%精度使用了3个特征（上网时长、时段、网购金额），但观察发现，分枝“上网时长&gt;1.1h”很显然过拟合了，不排除恰好A上网1.5h, B上网1小时，所以用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的。 而在GBDT中，两棵回归树仅使用了两个特征（购物金额与对百度知道的使用方式）就实现了100%的预测精度，其分枝依据更合乎逻辑（当然这里是相比较于上网时长特征而言），算法在运行中也体现了“如无必要，勿增实体”的奥卡姆剃刀原理。 3.3 提升树实例下表为训练数据，$x$的取值范围为区间$[0.5,10.5]$，$y$的取值范围为区间$[5.0,10.0]$，学习这个回归问题的提升树模型，考虑只用二叉树作为基函数：（1）步骤一：求$f_1(x)$即回归树$T_1(x)$ 1）首先通过以下优化问题：$$\min_s\left[\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}\right]$$求解训练数据的切分点$s$：$$R_1=\left\{x|x\le s\right\},R_2=\left\{x|x&gt;s\right\}$$容易求得在$R_1$，$R_2$内部使平方误差达到最小值的$c_1,c_2$为$$c_1=\frac{1}{N_1}\sum_{x_i\in R_1}{y_i}\ ,\ c_2=\frac{1}{N_2}\sum_{x_i\in R_2}{y_i}$$这里$N_1,N_2$是$R_1,R_2$的样本点数。 2）具体地，求解训练数据的切分点。根据所给数据，考虑如下切分点：$$1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5$$对各切分点，不难求出相应的$R_1,R_2,c_1,c_2$及$$m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}$$例如，当$s=2.5$时，$$R_1=\{1,2\}，R_2=\{3,4,···,9,10\}，c_1=5.63,c_2=7.73$$$$m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}=12.07$$遍历所有的$s$，计算$m(s)$，结果列表如下：可知当$s=6.5$时$m(s)$达到最小值，此时$$R_1=\{1,2,···,6\},R_2=\{7,8,9,10\},c_1=6.24,c_2=8.91$$所以回归树$T_1(x)$为$$T_2\left(x\right)=\left\{\begin{matrix} 6.24&amp; x&lt;6.5\\ 8.91&amp; x\geqslant 6.5\\\end{matrix}\right.$$$$f_1\left(x\right)=T_1\left(x\right)$$用$f_1(x)$拟合训练数据的残差，表中$r_{2i}=y_i-f_1(x_i)$平方损失误差为：$$L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=1.93$$ （2）步骤二：求$T_2(x)$，方法与求$T_1(x)$一样，只是拟合的数据是上一步得到的残差，可以得到：$$T_2\left(x\right)=\left\{\begin{matrix} -0.52&amp; x&lt;3.5\\ 0.22&amp; x\geqslant 3.5\\\end{matrix}\right.$$$$f_2\left(x\right)=f_1\left(x\right)+T_2\left(x\right)=\left\{\begin{matrix} 5.72&amp; x&lt;3.5\\ 6.46&amp; 3.5\le x&lt;6.5\\ 9.13&amp; x\geqslant 6.5\\\end{matrix}\right.$$用$f_2(x)$拟合训练数据的平方损失误差是$$L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=0.79$$继续迭代$$T_3\left(x\right)=\left\{\begin{matrix} 0.15&amp; x&lt;6.5\\ -0.22&amp; x\geqslant 6.5\\\end{matrix}\right.——L\left(y,f_3\left(x\right)\right)=0.47$$$$T_4\left(x\right)=\left\{\begin{matrix} -0.16&amp; x&lt;4.5\\ 0.11&amp; x\geqslant 4.5\\\end{matrix}——L\left(y,f_4\left(x\right)\right)=0.30\right.$$$$T_5\left(x\right)=\left\{\begin{matrix} 0.07&amp; x&lt;6.5\\ -0.11&amp; x\geqslant 6.5\\\end{matrix}\right.——L\left(y,f_5\left(x\right)\right)=0.23$$$$T_6\left(x\right)=\left\{\begin{matrix} -0.15&amp; x&lt;2.5\\ 0.04&amp; x\geqslant 2.5\\\end{matrix}\right.$$$$f_6\left(x\right)=f_5\left(x\right)+T_6\left(x\right)=T_1\left(x\right)+···+T_5\left(x\right)+T_6\left(x\right)$$$$=\left\{\begin{matrix} 5.63&amp; x&lt;2.5\\ 5.82&amp; 2.5\le x&lt;3.5\\ 6.56&amp; 3.5\le x&lt;4.5\\ 6.83&amp; 4.5\le x&lt;6.5\\ 8.95&amp; x\geqslant 6.5\\\end{matrix}\right.$$用$f_6(x)$拟合训练数据的平方损失误差是$$L\left(y,f_1\left(x\right)\right)=\sum_{i=1}^{10}{\left(y_i-f_1\left(x_i\right)\right)^2}=0.17$$假设此时已满足误差要求，那么$f(x)=f_6(x)$即为所求提升树。 四、Gradient Boosting Decision Tree：梯度提升决策树4.1 GBDT简介提升树利用加法模型与向前分布算法实现学习的优化过程，即是通过迭代得到一系列的弱分类器，进而通过不同的组合策略得到相应的强学习器。在GBDT的迭代中，假设前一轮得到的抢学习器为$f_{t-1}{(x)}$，对应的损失函数则为$L(y,f_{t-1}{(x)})$。因此新一轮迭代的目的就是找到一个弱分类器$h_t{(x)}$，使得损失函数$L(y,f_{t-1}{(x)}+h_t{(x)})$达到最小。 因此问题的关键就在于对损失函数的度量，这也正是难点所在。当损失函数是平方损失和指数损失时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化没那么容易，如绝对值损失函数和Huber损失函数。常见的损失函数及其梯度如下表所示：那我们怎么样才能找到一种通用的拟合方法呢？ 针对这一问题，Freidman提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值$$-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$作为回归问题中提升树算法的残差的近似值（与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例，拟合一个回归树），这就是梯度提升决策树。 4.2 GBDT算法步骤算法步骤如下：接下来对上图中的算法步骤进行详细解释： 1）初始化弱分类器，估计使损失函数极小化的一个常数值，此时树仅有一个根结点$$f_0\left(x\right)=arg\min_c\sum_{i=1}^N{L\left(y_i,c\right)}$$ 2）对迭代轮数$1,2,···,M$ a）对$i=1,2,···,N$，计算损失函数的负梯度值在当前模型的值，将它作为残差的估计。即$$r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。 b）对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj}，j=1,2,···,J$ c）对$j=1,2,···,J$计算$$c_{mj}=arg\min_c\sum_{x_i\in R_{mj}}{L\left(y_i,f_{m-1}\left(x_i\right)+c\right)}$$即利用线性搜索估计叶结点区域的值，使损失函数极小化 d）更新回归树$$f_m\left(x\right)=f_{m-1}\left(x\right)+\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}$$ 3）得到输出的最终模型$$\hat{f}\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}}$$ 五、关于GBDT的一些问题5.1 GBDT与AdaBoost的区别六、参考资料GBDT：梯度提升决策树]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（6）：AdaBoost]]></title>
    <url>%2F2017%2F01%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost%2F</url>
    <content type="text"><![CDATA[一、集成学习1.1 定义所谓集成学习（ensemble learning），是指通过构建多个弱学习器，然后结合为一个强学习器来完成分类任务。并相较于弱分类器而言，进一步提升结果的准确率。严格来说，集成学习并不算是一种分类器，而是一种学习器结合的方法。 下图显示了集成学习的整个流程：首次按产生一组“个体学习器”，这些个体学习器可以是同质的（homogeneous）（例如全部是决策树），这一类学习器被称为基学习器（base learner），相应的学习算法称为“基学习算法”；集成也可包含不同类型的个体学习器（例如同时包含决策树和神经网络），这一类学习器被称为“组件学习器”（component learner）。 集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，它基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，直观一点理解，就是我们平时所说的“三个臭皮匠，顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。 1.2 集成学习的条件当然，这种通过集成学习来提高学习器（这里特指分类器）的整体泛化能力也是有条件的： 首先，分类器之间应该具有差异性，即要有“多样性”。很容易理解，如果使用的是同一个分类器，那么集成起来的分类结果是不会有变化的。‘ 其次，每个个体分类器的分类精度必须大于0.5，如果$p&lt;0.5$那么随着集成规模的增加，分类精度会下降；但如果是大于0.5的话，那么最后最终分类精度是可以趋于1的。 因此，要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。 1.3 集成学习的分类当前，我们可以立足于通过处理数据集生成差异性分类器，即在原有数据集上采用抽样技术获得多个训练数据集来生成多个差异性分类器。根据个体学习器的生成方式，目前集成学习方法大致可分为两大类：第一类是个体学习器之间存在强依赖关系、必须串行生成的序列化方法，这种方法的代表是“Boosting”；第二类是个体学习器间不存在强依赖关系、可同时生成的并行化方法，它的代表是“Bagging”和“Random Forest” Bagging：通过对原数据进行有放回的抽取，构建出多个样本数据集，然后用这些新的数据集训练多个分类器。因为是有放回的采用，所以一些样本可能会出现多次，而其他样本会被忽略。该方法是通过降低基分类器方法来改善泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会降低。 Boosting：提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。 Bagging与Boosting的区别： 二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。 bagging是减少variance，而boosting是减少bias。Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行。 二、AdaBoost算法2.1 AdaBoost算法思想对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确地分类规则（强分类器）容易得多。提升算法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 这样，对提升方法来说，有两个问题需要回答：一是在每一轮如果改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。对于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题就被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。 AdaboostBoost的算法的框架如下图所示 具体来说，整个AdaBoost算法包括以下三个步骤： 1）初始化训练样本的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：$1/N$。 2）训练弱分类器。具体训练过程中，如果某个样本已经被准确地分类，那么在构造下一个训练集中，它的权值就会被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本被用于训练下一个分类器，整个训练过程如果迭代地进行下去，使得分类器在迭代过程中逐步改进。 3）将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中权重较大，否则较小。得到最终分类器。 2.2 AdaBoost算法流程现在叙述AdaBoost算法。假定给定一个二类分类的训练数据集$$T=\{(x_1,y_1),(x_2,y_2),···,(x_n,y_n)\}$$其中$y_i$属于二分类的标记组合，即$y_i\in\{+1,-1\}$，AdaBoost算法利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成一个强分类器。 步骤一：首先，初始化训练数据的权值分布。假设每一个训练样本最开始时都被赋予相同的权值：$1/N$，即每个训练样本在基本分类器的学习中作用相同，这一假设保证步骤一能够在原始数据上学习基本分类器$G_1{(x)}$，数学化的语言表示为：$$D_1=\left(w_{11},w_{12},···,w_{1i},···,w_{1N}\right) , w_{1i}=\frac{1}{N} ,i=1,2,···,N$$步骤二：AdaBoost反复学习基本分类器，在每一轮$m=1,2,···,M$顺次执行下列操作： 1）使用当前权值分布为$D_m$的训练数据集，学习得到基分类器$$G_m\left(x\right):\chi\rightarrow\left\{-1,+1\right\}$$ 2）计算上一步得到的基分类器$G_m{(x)}$在训练数据集上的分类误差率$e_m$为$$e_m=P\left(G_m\left(x\right)\ne y_i\right)=\frac{\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}}{\sum_{i=1}^N{w_{mi}}}=\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}$$这里$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum_{i=1}^N{w_{mi}=1}$。这表明，$G_m{(x)}$在加权的训练数据集上的分类误差率是被$G_m{(x)}$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m{(x)}$的分类误差率的关系。 3）计算$G_m$前面的权重系数$a_m$，该系数表示$G_m$在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类器中所占的权值，系数计算公式如下：$$a_m=\frac{1}{2}\log\frac{1-e_m}{e_m}$$这里的对数是自然对数，由表达式可知，当$e_m≤\frac{1}{2}$时，$a_m≥0$，并且$a_m$随着$e_m$的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而$e_m≥\frac{1}{2}$则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。 4）更新训练数据集的权值分布为下一轮作准备$$D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,$$其中$$w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N$$我们也可以写成：$$w_{m+1,i}=\left\{\begin{matrix} \frac{w_{mi}}{Z_m}e^{-a_m} ,&amp; G_m\left(x_i\right)=y_i\\ \frac{w_{mi}}{Z_m}e^{a_m} ,&amp; G_m\left(xi\right)\ne y_i\\\end{matrix}\right. $$由此可知，被基本分类器$G_m{(x)}$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值$e^{2a_m}=\frac{e_m}{1-e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。这里我们还引入了一个规范化因子，它的作用在于使$D_{m+1}$成为一个概率分布。具体公式为$$Z_m=\sum_{i=1}^N{w_{mi}}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N$$重复步骤二中的1至4步骤，得到一系列的权重参数$a_m$和基分类器$G_m$。 步骤三：将上一步得到的基分类器根据权重参数线性组合$$f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}$$得到最终分类器$G_{(x)}$$$G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)$$线性组合$f(x)$实现了$M$个基本分类器的加权表决。系数$a_m$表示了基本分类器$G_m{(x)}$的重要性，这里，所有的$a_m$之和并不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。 2.3 AdaBoost算法的一个实例下图为给定的训练样本，假设$Y\in \{+1,-1\}$，且弱分类器由$xv$产生（$v$为阈值，目的在于使分类器在训练样本上的分类误差率最低），接下来我们就要使用AdaBoost算法得到一个强分类器。 首先，初始化训练数据的权值分布，得到： $$D_1=(w_{11},w_{12},w_{1,10}) , w_{1i}=\frac{1}{10},i=1,2,····,10$$在此基础上，开始M轮迭代。 根据X和Y的对应关系，要把这10个数据分为两类，一类是1，一类是-1，根据数据的特点发现：$（0，1，2，6，7，8）$对应的类是1，$(3,4,5,9)$对于的类是-1，抛开孤独的9不说，$(0,1,2),(3,4,5),(6,7,8)$这是3类不同的数据，分别对应的类是$(1,-1,1)$,直观上推测可知，可以找到对应的数据分界点，比如$2.5、5.5、8.5$，将这几类数据分成两类。 1.第一次迭代（m=1）: 1）在第一次迭代时，已知$w_{1i}=\frac{1}{10}$，经过计算可得：在权值分布为$D_1$的训练数据上，阈值$v$取2.5或8.5时分类误差率为0.3，取5.5时分类误差率为0.4，遵循分类误差率最低原则，从2.5或8.5 中任意选取一个阈值，这里选取2.5，故基本分类器为$$G_1\left(x\right)=\left\{\begin{matrix}1&amp;x2.5\\\end{matrix}\right.$$ 2）$G_1{(x)}$在训练集上的误差率：$$e_1=P(G_1{(x_i)≠y_i})=0.3$$ 3) 根据$e_1$计算得到$G_1{(x)}$的系数：$$a_1=\frac{1}{2}\log\frac{1-e_1}{e_1}=0.4236$$这个系数就代表$G_1{(x)}$在最终的分类函数中所占的权值。 4）更新训练数据的权值分布，用于下一轮迭代$$D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,$$其中$$w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N$$由此得到$D_2=(0.0715,0.0715,0.715,0.0715,0.0715,0.715,0.166,0.166,0.166,0.0715)$根据$D_2$可知，分对的样本权重由0.1下降到了0.0715，分错的样本$(6,7,8)$的权值由0.1上升至0.166。此时分类函数为$f_1{(x)}=0.4236G_1{(x)}$，第一个分类器$sign[f_1{(x)}]$在训练样本上有三个误分类点（第一轮的误分类点即第一个基分类器的误分类点）。 2.第二次迭代（m=2）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_2$，在此基础上，经过计算可得，阈值$v$是8.5时分类误差率最低，因此第二个基本分类器为$$G_2\left(x\right)=\left\{\begin{matrix}1&amp;x8.5\\\end{matrix}\right. $$ 2）误差率：$$e_2=P(G_2{(x_i)≠y_i})=0.0715×3=0.2143$$ 3）$G_2{(x)}$的系数为:$$a_2=\frac{1}{2}\log\frac{1-e_2}{e_2}=0.6496$$ 4）更新训练样本的权值分布，得到$D_3=(0.0455,0.0455,0.0455,0.1667，0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，相较于$D_2$，被分错的样本3，4，5的权值变大，其他被分对的样本的权值变小。经过第二轮迭代后，分类函数为$f_2{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}$，第二个分类器为$sign[f_2{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}]$。将10个样本点依次带入到第二个分类器中，可得到此时依然有着3个误分类点$(3,4,5)$，为此需要进行下一轮迭代。 3.第三次迭代（m=3）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_3$，在此基础上，经过计算可得，阈值$v$是5.5时分类误差率最低，因此第三个基本分类器为$$G_3\left(x\right)=\left\{\begin{matrix}{} 1&amp; x&gt;5.5\\ -1&amp; x&lt;5.5\\\end{matrix}\right.$$ 2）误差率:$$e_3=P(G_3{(x_i)≠y_i})=0.0455×4=0.1820$$ 3）$G_3{(x)}$的系数为$$a_3=\frac{1}{2}\log\frac{1-e_3}{e_3}=0.7514$$ 4）更新训练样本的权值分布，得到$D_4=（0.125，0.125，0.125，0.102，0.102，0.102，0.065，0.065，0.065，0.125）$此时分类函数为$f_3{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}+0.7514G_3{(x)}$，第三个分类器为$sign[f_3{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}+0.7514G_3{(x)}]$，同样将10个样本点依次代入第三个分类器中，可发现没有误分类点，全部样本点已正确分类，因此停止迭代。算法运行完毕。 从上述过程可以发现，如果某些样本被分错，那么它们在下一轮迭代中的权重将会被增大，同时，其它被分错的样本在下一轮迭代中的权值将会被减小。就这样，分错样本权值增大，分对样本权值变小。而每一轮的迭代中，总是选取让误差率最低的阈值来设计基本分类器，因此误差率$e$(所有被$G_m{(x)}$误分类样本的权值之和)在迭代中将不断降低。 三、AdaBoost算法的训练误差分析AdaBoost最基本的性质是它能在学习过程中不断减小训练误差，即在训练数据集上的分类误差率。关于这个问题有下面的定理 3.1 AdaBoost的训练误差界首先，给出AdaBoost的训练误差界的定理：$$\frac{1}{N}\sum_{i=1}^N{I\left(G\left(x_i\ne y_i\right)\right)}\le\frac{1}{N}\sum_i{\exp\left(-y_if\left(x_i\right)\right)=\prod_m{Z_m}}$$其中$$G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)$$$$Z_m=\sum_{i=1}^N{w_{mi}\exp\left(-a_my_iG_m\left(x_i\right)\right)}$$$$f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}$$证明如下 1）当$G(x_i)≠y_i$时，$y_if(x_i)&lt;0$，因而$exp(-y_if(x_i))≥1$。由此直接推导出前半部分。 2）后半部分的推导需要用到$$w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N$$$$w_{mi}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right)=Z_mw_{m+1,i}$$推导如下：$$\frac{1}{N}\sum_i{\exp\left(-y_if\left(x_i\right)\right)}$$$$=\frac{1}{N}\sum_i{\exp\left(-\sum_{m=1}^M{a_my_iG_m\left(x_i\right)}\right)}$$$$=\sum_i{w_{1i}\prod_{m=1}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}$$$$=Z_1\sum_i{w_{2i}\prod_{m=2}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}$$$$=Z_1Z_2\sum_i{w_{3i}\prod_{m=3}^M{\exp\left(-a_my_iG_m\left(x_i\right)\right)}}$$$$=Z_1Z_2···Z_{M-1}\sum_i{w_{Mi}\exp\left(-a_My_iG_m\left(x_i\right)\right)}$$$$=\prod_{m=1}^M{Z_m}$$这一定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。 3.2 二类分类问题AdaBoost的训练误差界对于二分类问题，有如下结果：$$\prod_{m=1}^M{Z_m}=\prod_{m=1}^M{\left[2\sqrt{e_m\left(1-e_m\right)}\right]}=\prod_{m=1}^M{\sqrt{\left(1-4\gamma_{m}^{2}\right)}}\le\exp\left(-2\sum_{m=1}^M{\gamma_{m}^{2}}\right)$$这里$$\gamma_m=\frac{1}{2}-e_m$$证明： 1）首先，等式部分$$Z_m=\sum_{i=1}^N{w_{mi}\exp\left(-a_my_iG_m\left(x_i\right)\right)}$$$$=\sum_{y_i=G_m\left(x_i\right)}{w_{mi}\exp\left(-a_m\right)}+\sum_{y_i\ne G_m\left(x_i\right)}{w_{mi}\exp\left(a_m\right)}$$$$=\left(1-e_m\right)e^{-a_m}+e_me^{a_m}$$$$=2\sqrt{e_m\left(1-e_m\right)}$$$$=\sqrt{1-4\gamma_{m}^{2}}$$ 2）不等式部分，先由$e^x$和$\sqrt{(1-x)}$在$x=0$处的泰勒展开式$$e^x=1+x+\frac{1}{2}x^2+···+\frac{1}{n!}x^n+O\left(x^n\right)$$$$\left(1-x\right)^{\frac{1}{2}}=1-\frac{1}{2}x-\frac{1}{8}x^2+···+O\left(x^n\right)$$推出不等式$$\sqrt{\left(1-4\gamma_{m}^{2}\right)}\le\exp\left(-2\gamma_{m}^{2}\right)$$进而得到。 3.3 推论由上述两个定理推出，如果存在$\gamma &gt;0$，对所有$m$有$\gamma_m\geqslant\gamma$，则$$\frac{1}{N}\sum_{i=1}^N{I\left(G\left(x_i\right)\ne y_i\right)}\le\exp\left(-2M\gamma^2\right)$$这个结论表明在此条件下Adaboost的训练误差是以指数速率下降的。 四、AdaBoost算法的数学推导AdaBoost算法还有另一个解释，AdaBoost算法可以被认为模型是加法模型，损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。 4.1 前向分布算法在推导之前，先敲定几个概念： 加法模型（additive model）:$$f\left(x\right)=\sum_{m=1}^M{\beta_mb\left(x;\gamma_m\right)}$$。其中$b\left(x;\gamma_m\right)$为基函数，$\beta_m$为基函数的系数。 损失函数极小化：在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小化$$\underset{\beta_m,\gamma_m}{\min}\sum_{i=1}^N{L\left(y_i,\sum_{m=1}^M{\beta_mb\left(x_i;\gamma_m\right)}\right)}$$ 前向分布算法（forward stagewise algorithm）：该算法的基本思路为：由于学习的是加法模型，如果可以从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（即损失函数极小化表达式），那么就可以简化优化的复杂度。具体地，每一步只需要优化如下损失函数：$$\underset{\beta ,\gamma}{\min}\sum_{i=1}^N{L\left(y_i,\sum_{m=1}^M{\beta b\left(x;\gamma\right)}\right)}$$ 4.2 基于前向分布算法的AdaBoost推导前向分布算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器$f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}$，它由基本分类器$G_m{(x)}$及其系数$a_m$组成。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。下面证明前向分步算法的损失函数是指数函数$L\left(y,f\left(x\right)\right)=\exp\left[-yf\left(x\right)\right]$时，其学习的具体操作等价于AdaBoost算法学习的具体操作。 假设经过$m-1$轮迭代前向分步算法已经得到$f_{m-1}{(x)}$：$$f_{m-1}\left(x\right)=f_{m-2}\left(x\right)+a_{m-1}G_{m-1}\left(x\right)$$$$=a_1G_1\left(x\right)+a_2G_2\left(x\right)+···+a_{m-1}G_{m-1}\left(x\right)$$在$m$轮迭代得到$a_m，G_m{(x)}和f_m{(x)}$，表示为$$f_m\left(x\right)=f_{m-1}\left(x\right)+a_mG_m\left(x\right)$$此时参数$a_m$和$G_m{(x)}$均未知。因此，我们的目标是要得到最小化损失函数，通过最小化损失函数来得到模型中所需要的参数。而在AdaBoost算法中，每一轮都需要更新样本的权值参数，故而在每一轮的迭代中需要加工损失函数极小化，然后据此得到每个样例的权重更新参数。这样在每轮的迭代过程中只需要将当前基函数在训练集上的损失函数最小，最终使得$f_m{(x)}$在训练样本上的指数损失最小。极小化损失函数为：$$\left(a_m,G_m\left(x\right)\right)=arg\min_{a,G}\sum_{i=1}^N{\exp\left[-y_i\left(f_{m-1}\left(x_i\right)+aG\left(x_i\right)\right)\right]}$$我们先假定$G_1{(x)},G_2{(x)},···,G_{m-1}{(x)}$和$a_1,a_2,···,a_{m-1}$已知，求解$G_m{(x)}$和$a_m$。可以将上式表示为$$\left(a_m,G_m\left(x\right)\right)=arg\min_{a,G}\sum_{i=1}^N{\bar{w}_{mi}\exp\left[-y_iaG\left(x_i\right)\right]}$$其中$$\bar{w}_{mi}=\exp\left[-y_if_{m-1}\left(x_i\right)\right]$$因为$\bar{w}_{mi}$既不依赖$a$也不依赖于$G$，所以与最小化无关。但它依赖于$f_{m-1}{(x)}$，随着每一次迭代而发生改变。现证使上式达到最小的$a_m$和$G_m(x)$就是AdaBoost算法所得到的$a_m$和$G_m(x)$。求解可分为两步 1）首先，求$G_m(x)$。对任意的$a&gt;0$，使指数损失函数最小的$G(x)$由下式得到：$$G_{m}\left(x\right)=arg\min_G\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}$$此分类器$G_m^*(x)$即为AdaBoost算法的基本分类器$G_m{(x)}$，因为它是使第$m$轮加权训练数据分类误差率最小的基本分类器。 2）然后，求$a_m$$$\sum_{i=1}^N{\bar{w}_{mi}\exp\left[-y_iaG\left(x_i\right)\right]}=\sum_{y_i=G_m\left(x_i\right)}{\bar{w}_{mi}e^{-a}}+\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}e^a}$$$$=e^a\sum_{y_i\ne G_m\left(x_i\right)}^{}{\bar{w}_{mi}}-e^{-a}\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}}+e^{-a}\sum_{y_i\ne G_m\left(x_i\right)}{\bar{w}_{mi}}+e^{-a}\sum_{y_i=G_m\left(x_i\right)}{\bar{w}_{mi}}$$$$=\left(e^a-e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}+e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}$$将已求得的$G_m{(x)}$代入上式，对$a$求导并使导数为0，即得到使其损失函数最小的$a$。设$$g\left(a\right)=\left(e^a-e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}+e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}$$求导，并令其为0$$\frac{\partial g\left(a\right)}{\partial a}=\left(e^a+e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}-e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}=0$$得到$$\left(e^a+e^{-a}\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}=e^{-a}\sum_{i=1}^N{\bar{w}_{mi}}$$$$\left(e^{2a}+1\right)\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}=\sum_{i=1}^N{\bar{w}_{mi}}$$$$\left(e^{2a}+1\right)\frac{\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}}{\sum_{i=1}^N{\bar{w}_{mi}}}=1$$令$$e_m=\frac{\sum_{i=1}^N{\bar{w}_{mi}I\left(y_i\ne G\left(x_i\right)\right)}}{\sum_{i=1}^N{\bar{w}_{mi}}}$$$e_m$是分类误差率，得到$$e_m\left(e^{2a}+1\right)=1$$最终得到使损失函数最小的$a$$$a_m^*=\frac{1}{2}\log\frac{1-e_m}{e_m}$$这就是之前我们的权重系数$a_m$的来源。最后来看一下每一轮样本权值的更新。由$$f_m\left(x\right)=f_{m-1}\left(x\right)+a_mG_m\left(x\right)$$以及$$\bar{w}_{mi}=\exp\left[-y_if_{m-1}\left(x\right)\right]$$可得$$\bar{w}_{m+1,i}=\exp\left[-y_if_m\left(x\right)\right]=\exp\left[-y_i\left(f_{m-1}\left(x\right)+a_mG_m\left(x\right)\right)\right]$$$$=\bar{w}_{mi}\exp\left[-y_ia_mG_m\left(x\right)\right]$$从这一步中我们可以看到，这与开篇中所提到的AdaBoost的算法流程中的权重系数$(w_{m+1,i})$仅相差一个规范化因子$Z_m$，因而是等价的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（5）：随机森林]]></title>
    <url>%2F2017%2F01%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[一、基本原理顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 我们可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个特征中选择m个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。 下图为随机森林算法的示意图： 随机森林算法有很多优点： 在数据集上表现良好 在当前的很多数据集上，相对其他算法有着很大的优势 它能够处理很高维度（feature很多）的数据，并且不用做特征选择 在训练完后，它能够给出哪些feature比较重要 在创建随机森林的时候，对generlization error使用的是无偏估计 训练速度快 在训练过程中，能够检测到feature间的互相影响 容易做成并行化方法 实现比较简单 二、随机森林的生成2.1 生成步骤步骤如下： 1）如果训练集大小为$N$，对于每棵树而言，随机且有放回地从训练集中抽取$N$个训练样本（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本 2）如果每个样本的特征维度为$M$，指定一个常数$m$，且$m$&lt;$M$，随机地从$M$个特征中选取$m$个特征子集，每次树进行分裂时，从这$m$个特征中选择最优的； 3）每棵树都尽可能最大程度地生长，并且没有剪枝过程。 2.2 影响分类效果的参数随机森林的分类效果（即错误率）与以下两个因素有关： 1）森林中任意两棵树的相关性：相关性越大，错误率越大 2）森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低 减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。 2.3 袋外误差率如何选择最优的特征个数m，要解决这个问题，我们主要依据计算得到的袋外错误率oob error（out-of-bag error）。 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言，部分训练实例没有参与这棵树的生成，它们称为第k棵树的oob样本。 袋外错误率（oob error）计算方式如下： 1）对每个样本计算它作为oob样本的树对它的分类情况 2）以简单多数投票作为该样本的分类结果 3）最后用误分个数占样本总数的比率作为随机森林的oob误分率 三、随机采样与完全分裂 在建立每一棵决策树的过程中，有两点需要注意，分别是采样与完全分裂。 3.1 随机采样首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。 3.1.1 有放回抽样的解释如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。 3.1.2 对Bagging的改进随机森林对Bagging的改进就在于随机采用的不同，即以下两点： 1）Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本； 2）bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Random forest效果比bagging效果好！ 3.2 完全分裂之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。 四、随机森林的变体也可以使用SVM、Logistic回归等其他分 类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 比如回归问题，图中离散点为臭氧(横轴)和温度(纵轴)的关系，试拟合变化曲线，记原始数据为D，长度为N(即图中有N个离散点) 算法过程为： 1）做100次bootstrap，每次得到的数据Di，Di的长度为N 2）对于每一个Di，使用局部回归(LOESS)拟合一条曲线(图 中灰色线是其中的10条曲线) 3）将这些曲线取平均，即得到红色的最终拟合曲线 4）显然，红色的曲线更加稳定，并且没有过拟合明显减弱]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（4）：决策树]]></title>
    <url>%2F2017%2F01%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文结合李航博士的《统计学习方法》与周志华老师的《机器学习》决策树部分，《统计学习方法》重理论的证明推导，《机器学习》注重讲解算法的特点与扩展。 INTRODUCTION决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是$if-then$规则的集合，也可认为是定义在特征空间与类空间上的条件概率分布。下图是一个简单的决策树示例： 决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。 一个性能良好的决策树，是一个与训练数据矛盾较小的决策树，同时又具有很好地泛化能力。言外之意就是说，好的决策树不仅对训练样本有很好的分类效果，对于测试集也有较低的误差率。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。 一、决策树模型的两种解释1.1 决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部结点表示一个特征或属性，叶节点表示一个类。 1.1.1 决策树与if-then规则可以将决策树看成一个if-then规则的集合。即由决策树的根结点到叶节点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。 决策树的路径或其对应的if-then规则集合的重要性质：互斥且完备（每一个实例都被一条路径或一条规则所覆盖，且只被一条路径或一条规则所覆盖，这里的覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件） 1.1.1 决策树与条件概率分布决策树还表示给定特征条件下类的条件概率分布，它定义在特征空间的一个划分。将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个单元。 假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(X|Y)$,各叶结点上的条件概率往往偏向于某一个类，即属于某一类的概率越大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。 二、特征选择2.1 特征选择问题若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。 2.2 信息增益2.2.1 熵在信息论与概率统计中，熵表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为$$P\left( X=x_i \right) =p_i,i=1,2,···,n$$则随机变量$X$的熵定义为$$H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}$$若$p_i$等于0，定义$0log0=0$，熵的单位为比特或者纳特。 2.2.2 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望$$H\left( Y|X \right) =\sum_{i=1}^n{p_iH\left( Y|X=x_i \right)}$$经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和条件经验熵。 2.2.3 信息增益信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即$$g\left( D,A \right) =H\left( D \right) -H\left( D|A \right)$$一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 于是我们可以应用信息增益准则来选择特征，信息增益表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。对数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。 2.2.4 信息增益算法根据信息增益准则的特征选择方法为对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 在描述算法前，先对符号进行说明：设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$,$k=1,2,···,K$,$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,···,a_n}$,根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,···,D_n$,$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$,即$D_{ik}=D_i\cap{C_k}$,$D_{ik}$为$D_{ik}$的样本个数。 具体算法步骤如下： 1）计算数据集$D$的经验熵$H(D)$$$H\left( D \right) =-\sum_{k=1}^K{\frac{|C_k|}{|D|}\log _2\frac{|C_k|}{|D|}}$$ 2）计算特征$A$对数据集$D$的经验条件熵$H(D|A)$$$H\left( D|A \right) =\sum_{i=1}^n{\frac{|D_i|}{|D|}H\left( D_i \right)}=-\sum_{i=1}^n{\frac{|D_i|}{|D|}\sum_{k=1}^K{\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}}}$$ 3）计算信息增益$$g\left( D,A \right) =H\left( D \right) -H\left( D|A \right) $$ 2.3 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比表示特征$A$对训练数据集$D$的信息增益比。$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即$$g_R\left( D,A \right) =\frac{g\left( D,A \right)}{H_A\left( D \right)}$$ 2.4 基尼系数分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为$$Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}$$若样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即$$D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1$$则在特征A的条件下，集合$D$的基尼指数定义为$$Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)$$ 基尼系数Gini(D)表示集合$D$的不确定性，表示经A=a分割后集合D的不确定性。基尼系数越大，样本集合的不确定性越大，与熵类似。 从下图可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。 三、决策树的生成3.1 ID3算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地建构决策树。 其具体方法为：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。但是ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。 其算法步骤如下： 1） 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$; 2）若$A=\varnothing $,则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 3） 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$; 4） 如果$A_g$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ 5） 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成树$T$,返回$T$ 6） 对第i个子结点，以$D_i$为训练集,以$A-{A_g}$为特征集，递归地调用（1）~（5），得到子树$T_i$，返回$T$。 3.2 C4.5与ID3算法相似，C4.5算法对ID3算法进行了改进，C4.5在生成的过程中，用信息增益比来选择特征 3.3 CART分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART是在给定输入随机变量X条件下输出变量Y的条件概率分布的学习方法。它假定决策树是二叉树，内部取值为“是”（左分支）和“否”（右分支）。它的基本步骤为 1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这是用损失函数最小作为剪枝的标准。 3.3.1 分类树对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。 具体算法步骤如下： 1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值$a$，根据样本点对$A=a$的测试为”是”或者“否”将D分割为$D_1$和$D_2$两部分，计算其基尼系数。 2）在所有可能的特征A以及他们所有可能的切分点$a$中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。 4）生成CART决策树 3.3.2 回归树首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$$D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$$我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]$$遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值：$$R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\} , R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}&gt;s\right\}$$$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i} , x\in R_m , m=1,2$$ 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树：$$f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)}$$ 四、决策树的剪枝4.1 剪枝决策树的过拟合指的是学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化，即剪枝（从已生成的树上裁剪调一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型）。 设树$T$的叶结点个数为$|T|$,$t$是树$T$的叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,···,K$，$H_t(T)$为叶结点$t$上的经验熵，$a≥0$为参数，则决策树学习的损失函数可以定义为$${ C }_{ a }(T)=\sum _{ t=1 }^{ |T| }{ { N }_{ t }{ H }_{ t }(T)+a } |T|$$其中经验熵为$${ H }_{ t }(T)=-\sum _{ k }{ \frac { { N }_{ tk } }{ N_{ t } } log\frac { { N }_{ tk } }{ { N }_{ t } } } $$在损失函数中，将右端第一项记作$$C\left( T \right) =\sum_{t=1}^{|T|}{N_tH_t\left( T \right) =-\sum_{t=1}^{|T|}{\sum_{k=1}^K{N_{tk}\log \frac{N_{tk}}{N_t}}}}$$这时有$$C_a\left( T \right) =C\left( T \right) +a|T|$$其中，$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$a≥0$控制两者之间的影响。较大的$a$促使选择较简单的模型，较小的$a$促使选择较复杂的模型。$a=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 决策树生成只考虑了通过信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。此损失函数的极小化等价于正则化的极大似然估计，即利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 4.2 CART剪枝CART剪枝算法从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测其具体步骤如下： 1）首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个字数序列$\left\{ T_0,T_1,T_{2,….,}T_n \right\}$; 在剪枝过程中，计算子树的损失函数：$$C_a\left( T \right) =C\left( T \right) +a|T|$$其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼系数），$|T|$为子树的叶结点个数，$a≥0$为参数，$C_a(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。 对固定的$a$，一定存在使损失函数$C_a(T)$最小的子树，将其表示为$T_a$。$T_a$在损失函数$C_a(T)$最小的意义下是最优的，且是唯一的。$a$大的时候，最优子树$T_a$偏小；当$a$小的时候，最优子树$T_a$偏大。极端情况，$a=0$时，整体树是最优的。当$a\rightarrow \infty$，根结点组成的单结点树是最优的。 Breiman等人证明：可以用递归地方法对树进行剪枝。将$a$从小增大，$0=a_0&lt;a_1&lt;…..a_n&lt;+\infty$产生一系列的区间$[a_i,a_{i+1})$,$i=0,1,…,n$;剪枝得到的子树序列对应着区间$a\in \left[ a_i,a_{i+1} \right)$，$i=0,1,2,…,n$的最优子树序列为$\left\{ T_0,T_1,T_2,…,T_n \right\}$，序列的子树是嵌套的。 具体地，从整体树$T_0$开始剪枝，对$T_0$的人以内部结点$t$，以$t$为单结点树的损失函数是$$C_a\left( t \right) =C\left( t \right) +\alpha$$以$t$为根结点的子树$T_t$的损失函数是$$C_a(T_t)=C(T_t)+a|T_t|$$当$a=0$及$a$充分小时，有不等式$$C_a\left( T_t \right)小于C_a\left( T \right) $$当$a$增大时，在某一$a$有$$C_a(T_t)等于C_a(t)$$当$a$再增大时，有不等式$$C_a(T_t)大于C_a(T)$$只要$\alpha =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$ ，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。 为此，对$T_0$中的每一个内部结点$t$，计算$$g\left( t \right) =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$$它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$a_1$，$T_1$为区间$[a_1,a_2)$的最优子树。 如此剪枝下去，直至得到根结点。在这一过程中，不断得增加$a$的值，产生新的区间。 2）在剪枝得到的子树序列$T_0,T_1,…,T_n$中通过交叉验证选取最优子树$T_a$ 具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,…,T_n$都对应一个参数$a_1,a_2,…,a_n$。所以当最优子树$T_k$确定时，对应的$a_k$也就确定了，即得到最由决策树$T_a$. 五、参考资料李航《统计学习方法》周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[胡乱记]]></title>
    <url>%2F2017%2F01%2F15%2F%E8%83%A1%E4%B9%B1%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[其一：人女人的风度在于表达她对自己的看法，以及界定别人对待她的分寸。她的风度从姿态、声音、见解、表情、服饰、品味和选定的场合上体现出来——实际上，她所作的一切，无一不为她的风度增色。……男性观察女性；女性注意自己被别人观察。这不仅决定了大多数的男女关系，还决定了女性自己的内在关系，女性自身的观察者是男性，而被观察者为女性。这样被动主动的交互构成了男人们最基本的窥淫欲和恋物癖的现实表现以及女人在这个环境下被预设为被男人品赏的艺术品的尴尬局面。 知识分子对人民的引导应该是一个缓慢感化逐渐深入的过程，一开始就让老百姓穿高端的品牌，吃喝上等的烟酒是不切实际的，他们穿着不合适，吃进去还反胃，最后还弄个“文化强行入侵”的罪名，如此悲剧来源于知识分子自以为是的旨趣，该杜绝。阳春白雪对下里巴人所持有的态度不应是嫌弃，该是怜悯。 我们都是时代的弄潮儿，这不错，但是必须要清楚地认识到，我们弄得潮有大也有小，也有压根儿就不知道怎么弄潮，你不该为之抱怨，这是历史的必然，总会有潜藏的智慧不被挖掘，总会有好马遇不到伯乐。但是，于自身来说，你不该愤世骇俗，你得用尽全力去弄点潮出来，加把劲，再加把劲，说不定就弄出大潮来了嘞。 其实我们真的太年轻，年轻到不懂得原来爱情到了考虑现实的时候，到了和金钱权利挂钩之后，就再也不是原来的摸样的了。所以，趁自己还没有沾染那些东西的时候，好好地珍惜这份来之不易的爱情。如果你要考虑现实，那就去考虑吧，跟着自己的心走。每个人确实需要更长远的眼光，要看到远方的事情，但是青春真的就那么就一眨眼就不在了，有多少人做着自己喜欢的事情，爱着真正爱的人，过着内心喜欢和向往的生活？有多少人在不停忙碌着，却不知是为了什么？有多少人在混混沌沌着，也不知是为了什么？有太多的牵绊，有太多的不舍，有太多的不甘心。待到我们放下这些所谓的不舍、牵绊与不甘心，也正是我们真正活着的时候。一个人永远守护另一个人是不可能的。希望你记住我，记住我曾经这样存在过。记忆这东西总有些不可思议。身临其境的时候，几乎未曾意识到那片风景，未曾觉得它有什么撩人情怀之处，但到了十几年之后我们可能还无法忘怀。我老是介于“不充分”和“完全不够”之间，我总是感到饥渴，真想拼着劲儿得到一次爱。容许我百分之百的任性。村上春树讲：“ 把人生当做饼干罐就可以了。饼干罐不是装了各种各样的饼干，喜欢的和不喜欢的都在里面吗？如果先一个劲儿的挑你喜欢吃的，那么剩下的就全是不大喜欢的。每次遇到麻烦我就这样想，先把这个应付过去，往下就好办了。 计划实在是一样满足虚荣心的好玩意儿，你欢腾于欣赏顺利达成的自己并且毫不自禁的开始畅想那种虚无的满足感，这样的力量实在是软弱的，毫无生机的，既然没实质的胜利，那你就是个十足的阿q，精神胜利的恶魔在肆虐你，让你分不清方向，你依旧在狂笑，不带一点点对自己的歉意。 所谓的有力不取决于肉体所能承载的负重和劳累，每天听到上千种声音，每天走6000步，每天看到1000种颜色，每天接触不一样滋味的空气，我的器官接受了周围的一切顺其自然的，你想看到的或者想避开的，都会一溜烟的在你神经的深处划过轨迹，记住的便是你的选择和潜意识里面想要的得到的，所以珍惜你记住的同时缅怀你没记住的。 其二：物电影的发展不是绝对的，而是在一盘散沙里面掏出金来，时不时又混点泥沙进去，往复循环，进步的地方我们看的很明显，但是退步的趋势也是在很大一部分的元素里面可以显而易见的。看电影是一个艺术门道，主观的感觉是用来解决温饱的，那么深刻地剖析就是让你开始挑剔一部好电影的“色香味”，才可以真的把整个电影的精髓挖掘出来。 谁都有你想不到的难题，困扰于心的在别人看来都是些鸡毛蒜皮的小事。在这里可以学到一个受用的经验：当缠绕着你的那些忧思无法被自己打败以及翻越的时候，把视线放远些，不要局限于一个角度，你看到的就不会是墙角的草，而是大草原的葱茏，那是何其美妙的世界。当然，这需要有一定的阅历（阅读，经历很重要），去扩展你的经历簿。 如果把马蜂窝移到自己的居室，帮里头的蜜蜂精心准备好繁茂生长的花朵，恰到好处的阳光，各种生命的迹象，以至于让他们察觉不到自己身处异境，即使有人类在他们周围逡巡，也不会折回或做出抵抗。让这样的环境持续尽量久一些，蜜蜂也会在自己毫无察觉的情况下发生细微的变化，那是依赖于环境的惯性适应。强行改变人类的行为亦是如此，扭曲变形，失去重心没有防线，离开安全区域的不安与彷徨，偏离轨道与甚者再也无可复原。 对于哲学问题除非你长期为之苦思冥想，否则你根本说不清到底是些什么问题。要对哲学史有很好的说明，你必须竭尽所能从其”内部“看清各个哲学问题，设身处地地进入你所讨论的哲学家们的内心世界。你必须弄清那些问题对为之乐此不疲的哲学家意味着什么，弄清哲学家们始终关注的焦点。不如此”艰辛“是不可能写出真正的思想史的。另外，除非你自己专注于相关领域并进行深入研究，否则你无法写出他人在该领域艰难跋涉的历史。意识形态的历史，严格来说，只有那些热衷于意识形态问题并懂得如何思考意识形态问题的人才能写。 哲学解决的是观念之间、词语之间或表述方法之间的冲突产生出来的各种疑难。不同于经验问题。关于生活的目的、善和恶、自由和必然、客观性和相对性等的一系列问题，既不能考查阅最高级的辞典来解决，也不能用经验方法或数学推理方法来解决。设身处地地进入思想家们的内心和世界观是必要的，移情也是不可或缺的，尽管这样做面临证据不足和不确定性，乃至困难重重。诸如在研究马克思时，应该力图使自己像马克思本人在柏林、在巴黎、在布鲁塞尔和在伦敦那样，思考它的各种概念、范畴及其德语词汇。他们的思想是怎样产生的？在什么特定的时间、地点、社会条件下产生的？他们的思想可能很多人都有同感，但毕竟那是属于他们自己的。你必须不断反问自己，是什么东西让他们烦恼？什么东西使他们对这些问题苦苦思索？他们的理论或著作是怎么样在他们头脑中成熟的？人们不能完全抽象地超历史地谈论各种思想；但是，人们也不能孤立地仅在具体的历史环境中来描述各种思想，好像这些思想在他们的框架之外没有任何意义似的。这是一种复杂的、含糊不清的、需要借助心理学视野以及丰富想象力的研究工作，他不可能获得什么必然性的结论、在多数情况下，只能达到高度的持之有故和言之有理，达到理智能力的首尾一贯和清楚明白，还有独创性和有效性。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
        <tag>人与物</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（3）：逻辑斯谛回归]]></title>
    <url>%2F2017%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数： $$F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}}$$$$f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2}$$式中，$\mu$为位置参数，$\gamma&gt;0 $为形状参数。逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足$$F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2}$$曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。 二、逻辑斯谛回归模型线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个： 1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值； 2）预测结果受样本噪声的影响比较大。 2.1 LR模型表达式LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即$$h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}$$其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$ 2.2 理解LR模型2.2.1 对数几率一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是：$$logit\left(p\right)=\log\frac{p}{1-p}$$对LR而言，根据模型表达式可以得到：$$\log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx$$即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型 2.2.2 函数映射除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。 2.2.3 概率解释LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。 这里从事件、变量以及结果的角度给予解释。 我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？ 一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。 那么对于第$i$个样本，概率公式表示如下：$$P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0 |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}$$将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率：$$P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)})^{y(i)}）$$上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为：$$P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}$$通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。 三、模型参数估计3.1 Sigmoid函数 上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 此外非常重要的，sigmoid函数求导后为： 以下的推导中会用到，带来了很大的便利。 3.2 参数估计推导上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。参数在整个样本空间的似然函数可表示为：$$L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right)$$$$=\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)}$$$$=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}$$为了方便参数求解，对这个公式取对数，可得对数似然函数：$$l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)}$$$$=\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}$$最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。先不考虑累加和，我们针对每一个参数$w_j$求偏导：$$\frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right)$$$$=\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx$$$$=\left(y-h_{\theta}\left(x\right)\right)x_j$$最后，通过扫描样本，迭代下述公式可求得参数：$$\theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}$$其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。 以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。 四、Softmax回归待补充 五、参考资料对线性回归，logistic回归和一般回归的认识]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑斯谛回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离别记]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%A6%BB%E5%88%AB%2F</url>
    <content type="text"><![CDATA[离别的仪式以其该有的苍白的方式展开了，一个拥抱，一声保重，一段你们来时走过的路，它竭力地想被赋予更多的力量与情感，那么看吧，看看在我们心灵藏匿最深的地方，那里骚动起了真诚而又岌岌可危的猛兽，在故意掩藏的角落里缠绵、撕咬、交姌。你可以尽情嗅到生命的律动，几近沸腾的血液在体内加快步伐，血泵察觉到了仪式的情绪，它已经完全失去控制，它发了疯似的让灵动的双眼淌着热泪，身体仿佛接受了呼啸的寒风开始战栗起来，此刻即将分别，每一个鲜活的生命就要前往这个国度的各个角落，那里的星辰大海，下一次见面的时候请一定告诉我。 生活总需要特定的仪式来配合鼓舞，四年前我们仪式着来，四年后仪式着走，这无关乎虚荣，也绝不是空穴矫情，站在这个时间的关口，回望过去的日子，它不温不火，但也一定不平凡。当年踏上这片土地å，驾着云轻吟功与名，如今洒脱地背着旗帜四处宣告这四年的狂热与藏匿，我们汲汲求索，生活让我们的心灵焦灼躁动，却反而褪出一种光泽来，它们每一样都隐秘而伟大，孕育着一股神奇的力量。我们曾如孩童般痴笑着欢送旧时光，如酒神般渴求下一秒思想的闪耀，过去哪怕是闯入一块新天地的边境，也要花光我们所有的力气，迟来的矍铄如同一杯烈酒，蓄满无畏与重生的快感，愿我们承受得住这些深渊，一直自命不凡地在里面挣扎，生命不止，它最好一刻也不要停止，也愿我们每一次触碰这个星球时都可以感受到有一种声音在呼喊，那声音在遥远的未来回响，也发轫于不朽的丰碑与偶像存留的尘土。 我们即将面临一次非同寻常、却令人困倦的旅程，我们今天就要乘上一列所谓的失控列车。没有人可以诚恳地向我们展示前方有什么，而那些落在后面的人也会因接续我们这一毫无秩序的开端而爱莫能助。但我们必须要承认，这段旅途有去无回，我们可以宽慰自身的也只能是这样的思想：无论遇到什么不开心的事情，无论是哪一个车站都会一闪而过，那只是影片的一个人微妙的段落，列车绝不会在一个车站停留地太久。今天我们一起驻守的这块地方，即将成为过去的一站，我们只能挥手道别，在它还保持着正常的模样之前，在它还没有成为一张照片之前，就让我们怀着我们所有的温情再看它一眼，那也是在打量我们的过去。 先生们，我只是吝啬地留了一个苦涩的脸庞在你们记忆里，而你们在我的视野里留下了一个个鲜活的梦。篮球梦、炸金花梦、LOL梦，亦或是妹子梦、富帅梦、伟光正梦，它们都即将循着自己的路子熠熠生辉，都端正着自己的仪式带着未完成的梦离开吧，在812留下的痕迹就让我们在年迈的时候再来品味，倘若你们还记得床底的乌烟瘴气与泛黄纯白色纸巾，就算是重走了一回青春。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（2）：线性回归]]></title>
    <url>%2F2017%2F01%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、线性回归模型线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。 我们可以有这样的模型表达：$$y=\theta_0+\theta_1x_1+\theta_2x_2+···+\theta_nx_n$$其中，$x_1,x_2,···,x_n$表示自变量（特征分量），$y$表示因变量，$\theta_i$表示对应自变量（特征）的权重，$\theta_0$是偏倚项（又称为截距）。 对于参数$\theta$，在物理上可以解释为：在自变量（特征）之间相互独立的前提下，$\theta_i$反映自变量$x_i$对因变量$y$的影响程度，$\theta_i$越大，说明$x_i$对结果$y$的影响越大。因此，我们可以通过每个自变量（特征）前面的参数，可以很直观的看出那些特征分量对结果的影响比较大。 如果令$x_0=1,y=h_\theta{(x)}$，可以将上述模型写成向量形式，即：$$h_\theta\left(x\right)=\sum_{i=0}^n{\theta_ix_i}=\theta^Tx$$其中$\theta=(\theta_0,\theta_1,···,\theta_n)，x=(1,x_1,x_2,···,x_n)$均为向量，$\theta^T$为$\theta$的转置。 在上述公式中，假设特征空间与输入空间$x$相同。准确地讲，模型表达式要建立的是特征空间与结果之间的关系。在一些应用场合中，需要将输入空间映射到特征空间中，然后建模，定义映射函数为$\varPhi\left(x\right)$，因此我们可以把公式写成更通用的表达公式：$$h_\theta\left(x\right)=\theta^T\varPhi\left(x\right)$$特征映射相关技术，包括特征哈希、特征学习、$Kernel$等。 二、目标函数2.1 目标函数上面的公式的参数向量$\theta$是$n+1$维的，每个参数的取值是实数集合，也就是说参数向量$\theta$在$n+1$维实数空间中取值结果有无穷种可能。 那么，如何利用一个规则或机制帮助我们评估求得的参数$\theta$，并且使得线性模型效果最佳呢？直观地认为，如果求得参数$\theta$线性求和后，得到的结果$h_\theta{(x)}$与真实值$y$之差越小越好。 这时我们需要映入一个函数来衡量$h_\theta{(x)}$表示真实值$y$好坏的程度，该函数称为损失函数（loss function，也称为错误函数）。数学表示如下：$$J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^n{\left(\left(h_\theta\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right)^2}$$$$\min_\theta J\left(\theta\right)$$这个损失函数用的是$x^{(i)}$的预测值$h_\theta{(x^{(i)})}$与真实值$y^{(i)}$之差的平方和。如果不考虑诸如过拟合等其他问题，这就是我们需要优化的目标函数。 2.2 目标函数的概率解释一般地，机器学习中不同的模型会有相应的目标函数。而回归模型（尤其是线性回归类）的目标函数通常用平方损失函数来作为优化的目标函数（即真实值与预测值之差的平方和）。为什么要选用误差平方和作为目标函数呢？答案可以从概率论中的中心极限定理、高斯分布等知识中找到。 2.2.1 中心极限定理目标函数的概率解释需要用到中心极限定理。中心极限定理本身就是研究独立随机变量和的极限分布为正态分布的问题。 中心极限定理的公式表示为：设$n$个随机变量$X_1,X_2,···,X_n$相互独立，均具有相同的数学期望与方差，即$E(X_i)=\mu ;D(X_i)=\sigma^2$，令$Y_n$为随机变量之和，有$$Y_n=X_1+X_2+···+X_n$$$$Z_n=\frac{Y_n-E\left(Y_n\right)}{\sqrt{D\left(Y_n\right)}}=\frac{Y_n-n\mu}{\sqrt{n}\sigma}\rightarrow N\left(0,1\right)$$称随机变量$Z_n$为$n$个随机变量$X_1,X_2,···,X_n$的规范和。 它的定义为：设从均值为$\mu$、方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本，当$n$充分大时，样本均值的抽样分布$\frac{Y_n}{n}$近似服从于均值为$\mu$、方差为$\sigma^2$的正态分布。 2.2.2 高斯分布假设给定一个输入样例$x^{(i)}$根据公式得到预测值$\theta^Tx^{(i)}$与真实值$y^{(i)}$之间存在误差，即为$\varepsilon^{\left(i\right)}$。那么，它们之间的关系表示如下：$$y^{\left(i\right)}=\theta^Tx^{\left(i\right)}+\varepsilon^{\left(i\right)}$$而这里假设误差$\varepsilon^{\left(i\right)}$服从标准高斯分布是合理的。 解释如下： 回归模型的最终目标是通过函数表达式建立自变量$x$与结果$y$之间的关系，希望通过$x$能较为准确地表示结果$y$。而在实际的应用场合中，很难甚至不可能把导致$y$的所有变量（特征）都找出来，并放到回归模型中。那么模型中存在的$x$通常认为是影响结果$y$最主要的变量集合（又称为因子，在ML中称为特征集）。根据中心极限定理，把那些对结果影响比较小的变量（假设独立同分布）之和认为服从正态分布是合理的。 可以用一个示例来说明误差服从高斯分布是合理的： $Andrew Ng$的课程中第一节线性回归的例子中，根据训练数据建立房屋的面积$x$与房屋的售价$y$之间的函数表达。它的数据集把房屋面积作为最为主要的变量。除此之外我们还知道房屋所在的地段（地铁、学区、城区、郊区），周边交通状况，当地房价、楼层、采光、绿化面积等等诸多因素会影响房价。 实际上，因数据收集问题可能拿不到所有影响房屋售价的变量，可以假设多个因素变量相互独立，根据中心极限定理，认为变量之和服从高斯分布。即：$$\epsilon^{\left(i\right)}=y^{\left(i\right)}-\theta^Tx^{\left(i\right)}$$那么$x$和$y$的条件概率可表示为：$$p\left(y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)$$ 2.2.3 极大似然估计与损失函数极小化等价根据上述公式估计得到一条样本的结果概率，模型的最终目标是希望在全部样本上预测最准，也就是概率积最大，这个概率积就是似然函数。优化的目标函数即为似然函数，表示如下：$$\max_\theta L\left(\theta\right)=\prod_{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)$$对$L(x)$取对数，可得对数似然函数：$$\max_\theta l\left(\theta\right)=-m\log\sqrt{2\pi}\sigma -\frac{1}{2\sigma^2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}$$由于$n,\sigma$都为常数，因此上式等价于$$\min_\theta\frac{1}{2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}$$我们可以发现，经过最大似然估计推导出来的待优化的目标函数与平方损失函数是等价的。因此可以得出结论： 线性回归误差平方损失极小化与极大似然估计等价。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是一个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。 那上面为什么是条件概率$p(y|x;\theta)$呢？因为我们希望预测值与真实值更接近，这就意味着希望求出来的参数$\theta$，在给定输入$x$的情况下，得到的预测值等于真实值得可能性越大越好。而$\theta$，$x$均为前提条件，因此用条件概率$p(y|x;\theta)$表示。即$p(y|x;\theta)$越大，越能说明估计的越准确。当然也不能一味地只有该条件函数，还要考虑拟合过度以及模型的泛化能力问题。 三、参数估计如何调整参数$\theta$使得$J(\theta)$取得最小值？方法有很多，这里介绍几种比较经典的方法，即最小二乘法、梯度下降法以及牛顿法。 3.1 最小二乘法3.1.1 目标函数的矩阵形式将$m$个$n$维样本组成矩阵$X$：$$\left(\begin{matrix} 1&amp; x_{1}^{\left(1\right)}&amp; x_{1}^{\left(2\right)}&amp; ···&amp; x_{1}^{\left(n\right)}\\ 1&amp; x_{2}^{\left(1\right)}&amp; x_{2}^{\left(2\right)}&amp; ···&amp; x_{2}^{\left(n\right)}\\ ···&amp; ···&amp; ···&amp; &amp; \\ 1&amp; x_{m}^{\left(1\right)}&amp; x_{m}^{\left(2\right)}&amp; ···&amp; x_{m}^{\left(n\right)}\\\end{matrix}\right)$$则目标函数的矩阵形式为$$J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^m{\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^2}=\frac{1}{2}\left(X\theta -y\right)^T\left(X\theta -y\right)$$ 3.1.2 最小二乘法求解对$\theta$求导，梯度（矩阵求导）：$$\nabla_\theta J\left(\theta\right)=\nabla_\theta\left(\frac{1}{2}\left(X\theta-y\right)^T\left(X\theta-y\right)\right)$$$$=\nabla_\theta\left(\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^Ty-y^Ty\right)\right)$$$$=\frac{1}{2}\left(2X^TX\theta-X^Ty-\left(y^TX\right)^T\right)$$$$=X^TX\theta-X^Ty$$令其为零，求得驻点：$$\theta=\left(X^TX\right)^{-1}X^Ty$$ 3.2 梯度下降法梯度下降法是按下面的流程进行的： 1）首先对$\theta$赋值，这个值可以是随机的，也可是让$\theta$是一个全零的向量； 2）改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。 为了更清楚，给出下面的图： 这是一个表示参数$\theta$与目标函数$J(\theta)$的关系图，红色的部分是表示$J(\theta)$有比较高的取值，我们需要的是，能够让$J(\theta)$的值尽量的低。也就是深蓝色的部分。$\theta_0$和$\theta_1$表示$\theta$向量的两个维度。 在上面提到梯度下降法的第一步是给$\theta$一个初值，假设随机给的初值是在图上的十字点。然后我们将$\theta$按照梯度下降的方向进行调整，就会使得$J(\theta)$往更低的方向进行变化，如图所示，算法的结束将是在$\theta$下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，比如下面这张图中描述的就是一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法会在很大程度上被初始点的选择影响而陷入局部最小点。 下面对于目标函数$J(\theta)$求偏导数：$$\frac{\partial}{\partial\theta_j}J\left(\theta\right)=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)^2$$$$=2·\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)\frac{\partial}{\partial\theta_j}\left(h_{\theta}\left(x\right)-y\right)$$$$=\left(h_{\theta}\left(x\right)-y\right)x_j$$ 下面是更新的过程，也就是$\theta_i$会向着梯度最小的方向进行减少。$\theta$表示更新之前的值，$a$表示步长，也就是每次按照梯度减少的方向变化多少，由于求得是极小值，因此梯度方向是偏导数的反方向，结果为$$\theta :=\theta_j+a\left(h_{\theta}\left(x\right)-y\right)x_j$$一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量$\theta$，每一维分量$\theta_i$都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管他是全局的还是局部的。 在对目标函数$J(\theta)$求偏导时，可以用更简单的数学语言（倒三角表示梯度）进行描述：$$\nabla_{\theta}J=\left[\begin{array}{c} \frac{\partial}{\partial\theta_0}J\\ ···\\ ···\\ \frac{\partial}{\partial\theta_n}J\\\end{array}\right]$$$$\theta :=\theta +a\nabla_{\theta}J$$将梯度下降法应用到线性回归有三种方式：批处理梯度下降法、随机梯度下降法。 3.2.1 批量梯度下降法（BGD）可以看出，参数$\theta$的值每更新一次都要遍历样本集中的所有的样本，得到新的$\theta_j$，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。注意到，虽然梯度下降法易受到极小值的影响，但是一般的线性规划问题只有一个极小值，所以梯度下降法一般可以收敛到全局的最小值。例如，$J$是二次凸函数，则梯度下降法的示意图为：图中，一圈上表示目标函数的函数值类似于地理上的等高线，从外圈开始逐渐迭代，最终收敛全局最小值。 3.2.2 随机梯度下降算法（SGD）在这个算法中，我们每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。 批量梯度下降法，每更新一次，需要用到样本集中的所有样本；随机梯度下降法，每更新一次，只用到训练集中的一个训练样本，所以一般来说，随机梯度下降法能更快地使目标函数达到最小值（新样本的加入，随机梯度下降法有可能会使目标函数突然变大，迭代过程中在变小。所以是在全局最小值附近徘徊，但对于实际应用俩说，误差完全能满足要求）。另外，对于批量梯度下降法，如果样本集增加了一些训练样本，就要重新开始迭代。由于以上原因，当训练样本集较大时，一般使用随机梯度下降法。 四、参考资料对线性回归，logistic回归和一般回归的认识]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）：K近邻]]></title>
    <url>%2F2017%2F01%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AK%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[一、K近邻算法K近邻算法简单、直观。首先给出一张图，根据这张图来理解最近邻分类器。 根据上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形或者红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 我们常说，物以类聚，人以群分，判别一个人是一个什么样的人，常常可以从他身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于那一类数据么，好说，从他的另据下手。但一次性看多少个邻居呢？从上图中，你还可以看到： 如果K=3,绿色圆点最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 如果K=5,绿色圆点的最近5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色这个待分类点属于蓝色的正方形一类。 于此，我们看到，KNN算法为给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。 K近邻法算法步骤如下： 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),···，（x_N,y_N）}$，其中，$x_i$是实例的特征向量，$y_i$是实例的类别；新实例的特征向量$x$ 输出：新实例$x$所属的类别$y$ 1)根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这k个点的$x$领域记作$N_k(x)$; 2)在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$: $$y=arg\underset{c_j}{\max}\sum_{x_i\in N_k\left( x \right)}^{}{I\left( y_i=c_i \right) , i=1,2,···,N;j=1,2,···,K}$$其中$I$为指示函数，即当$y_i=c_i$时为1，否则为0. 二、K近邻模型2.1 模型k近邻法中，当训练集、距离度量、K值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 2.2 距离度量特征空间中两个实例点的距离可以反映出两个实力点之间的相似性程度。K近邻模型的特征空间一般是N维实数向量空间，使用的距离可以是欧式距离，也可以是其他距离。 欧氏距离：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中 $$d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( x_i-y_i \right) ^2}}$$ 曼哈顿距离：我们可以定义曼哈顿距离的正式意义为$L1$距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投射的距离总和。 通俗来讲，想想你在曼哈顿要从一个十字路口开车到另一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,此即曼哈顿距离名称的来源，同时，曼哈顿距离也称为城市街区距离。 $$d\left( x,y \right) =\sum_{i=1}^n{|x_i-y_i|}$$ 切比雪夫距离：$$d\left( x,y \right) =\underset{k\rightarrow \infty}{\lim}\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}}$$ 闵可夫斯基距离：它不是一种距离，而是一组距离的定义。$$d\left( x,y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}}$$ 当p=1时，即曼哈顿距离 当p=2时，即欧式距离 当$p\rightarrow \infty$，即切比雪夫距离 标准化欧氏距离：对样本集先进行标准化$\hat{x}_i=\frac{x_i-\bar{x}}{s}$经过简单的推导就可以得到来标准化欧氏距离。$$d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( \frac{x_i-y_i}{s} \right) ^2}}$$ 夹角余弦：几何中夹角余弦可用来衡量两个向量方向的相似度，机器学习中借用这一概念来衡量向量之间的相似度。 $$\cos \left( \theta \right) =\frac{a·b}{|a|·|b|}$$ 2.3 K值的选择K值得选择会对K近邻法的结果产生重大影响。 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值得减小就意味着整体模型变得复杂，容易发生过拟合（容易受到训练数据的噪声而产生的过拟合的影响）。 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远的训练实例也会对预测器作用，使预测发生错误，且K值得增大就意味着整体的模型变得简单。 如果K=N。那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的（最近邻列表中可能包含远离其近邻的数据点），如下图所示。 在实际应用中，K值一般取一个比较小的数值。通常采用交叉验证法来选取最优的K值（经验规则：K一般低于训练样本数的平方根）。 2.4 分类决策规则K近邻法中的分类决策规则往往是多数表决，即由输入实例的K个邻近的训练实例中的多数类决定输入实例的类。 三、K近邻的优缺点3.1 优点 简单、易于理解、易于实现、无需估计参数、无需训练。 适合对稀有事件进行分类（如大概流式率很低时，比如0.5%，构造流失预测模型）； 特别适合多酚类问题，如根据基因特征来判断其功能分类，KNN比SVM的表现要好。 3.2 缺点 懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢。 可解释性较差，无法给出决策树那样的规则。 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 KNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找K个近邻），懒惰的后果，构造模型很简单，但在测试样本分类地系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量。 决策树和基于规则的分类器都是积极学习eager learner的例子，因为一旦训练数据可用，它们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为消极学习法lazy learner。最近邻分类器就是这样的一种方法。 四、python代码实现4.1 K-近邻算法简单示例KNN算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/python# coding=utf-8"""Created on Feb 22 2017KNN@author: plushunter"""# coding=utf-8#!/usr/bin/pythonfrom numpy import *import operatorclass KNNClassifier(): def __init__(self,k=3): self._k=k def _calDistance(self,inputX,trainX): dataSetSize=trainX.shape[0] # tile for array and repeat for matrix in Python diffMat=tile(inputX,(dataSetSize,1))-trainX sqDiffMat=diffMat**2 # take the sum of difference from all dimensions,axis=0是按列求和,axis=1 是按行求和 sqDistances=sqDiffMat.sum(axis=1) distances=sqDistances**0.5 # argsort returns the indices that would sort an array.argsort函数返回的是数组值从小到大的索引值 # http://www.cnblogs.com/100thMountain/p/4719503.html # find the k nearest neighbours sortedDistIndicies = distances.argsort() return sortedDistIndicies def _classify(self,sample,trainX,trainY): if isinstance(sample,ndarray) and isinstance(trainX,ndarray) and isinstance(trainY,ndarray): pass else: try: sample=array(sample) trainX=array(trainX) trainY=array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") sortedDistIndicies=self._calDistance(sample,trainX) classCount=&#123;&#125;#create the dictionary for i in range(self._k): label=trainY[sortedDistIndicies[i]] classCount[label]=classCount.get(label,0)+1 #get(label,0) : if dictionary 'classCount' exist key 'label', return classCount[label]; else return 0 sorteditem=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #operator.itemgetter(1) can be substituted by 'key = lambda x: x[1]' return sorteditem[0][0] def classify(self,inputX,trainX,trainY): if isinstance(inputX,ndarray) and isinstance(trainX,ndarray) \ and isinstance(trainY,ndarray): pass else: try: inputX = array(inputX) trainX = array(trainX) trainY = array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") d = len(shape(inputX)) results=[] if d == 1: result = self._classify(inputX,trainX,trainY) results.append(result) else: for i in range(len(inputX)): result = self._classify(inputX[i],trainX,trainY) results.append(result) return resultsif __name__=="__main__": trainX = [[1,1.1], [1,1], [0,0], [0,0.1]] trainY = ['A','A','B','B'] clf=KNNClassifier(k=3) inputX = [[0,0.1],[0,0]] result = clf.classify(inputX,trainX,trainY) print result#output which type these belongs to/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/KNN.py['B', 'B']Process finished with exit code 0 4.2 KNN实现手写识别系统1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/python# coding=utf-8"""Created on Mar 22 2017KNN: Hand Writing@author: plushunter"""from numpy import *import operatorfrom os import listdirimport KNNdef img2vector(filename): returnVect = zeros((1, 1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() # n = len(lineStr) # if not n: # continue for j in range(32): returnVect[0, 32 * i + j] = int(lineStr[j]) return returnVectdef loadDataSet(filedir): FileList = listdir(filedir) m = len(FileList) X = zeros((m,1024)) Y = [] for i in range(m): fileNameStr = FileList[i] classNumStr = int(fileNameStr.split('_')[0]) Y.append(classNumStr) X[i, :] = img2vector(filedir + "/" + fileNameStr) return X,Ydef handWritingClassTest(inputX,inputY,trainX,trainY): cls=KNN.KNNClassifier(k=3) error=0.0 result = cls.classify(inputX,trainX,trainY) for i in range(len(result)): if result[i] != inputY[i]: error+=1 precision_rate =1- error /len(inputY) print precision_rate # return errorRatedef main(): trainDir = "digits/trainingDigits" testDir = "digits/testDigits" trainX,trainY = loadDataSet(trainDir) inputX,inputY = loadDataSet(testDir) handWritingClassTest(inputX,inputY,trainX,trainY)if __name__=="__main__": main()#output precision_rate/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/HandWriting.py0.988372093023Process finished with exit code 0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>K近邻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二零一七第一天]]></title>
    <url>%2F2017%2F01%2F01%2F%E4%BA%8C%E9%9B%B6%E4%B8%80%E4%B8%83%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[其一二零一六就像是一出异乎寻常的戏码，在戏里，我扮演了一个再寻常不过的角色，狼狈得被汹涌的剧情推搡着向前，迷失在混乱的舞台，舞台中央的鲜花与热泪，在光环底下肆意飞扬，那个失落的孩子站在舞台边呆呆凝视，视力模糊，它们在晃动，狂妄的审视，或者只是在给生活与艺术授予赞词。 如何才可以抓住风？如何才可以让仅存的模样愈加沉醉？如何让这伟大的世界变得安静啊？ 我想要回感受力，我察觉到它们正离我而去，嗅觉不再敏锐，耳旁嘈杂不堪，视觉里容不下美，身体的每一个部位都在宣告，你不要再挣扎，你已结束兴盛的时代，那个王朝早已落幕，你只能徘徊在硝烟四起的边疆了罢。 我不甘啊，我还那么年轻。 其二二零一七呐，有好多期待，我要带着不甘重新上路了。 第一件，物。神奇的物，帮助我们和这个曼妙的世界沟通。它们总能贴合我们的感官，让那些险些失去的感觉聚焦，消沉从而变得细腻可触，欢喜也可以沉浸许久，它们与我们的身体交融。 私人物品就像一种符号，为其在和自己关系密切的现实世界与社交网络中的身份提供具象化的证据。它是生活故事的标签，这些故事依附于私人物品铺展开来，串联起一个人的过去、现在与未来。携带着与气质相符的物，让物呈现你的身体与灵魂吧，又何尝不可呢，不要说话，闭上眼睛，不要透露心声，关上襟怀。物不是其它，它就是你。第二件，人。怎么会有那么多可爱的人呢？我历数不过来了，我默默看着他们走过、停留、消逝，然后有一天，又重新出现，谁知道是哪一天，谁又知道会出现在哪个场景里，我唯一可以做的，便是等待着随便哪一种未来。 有的人，因为音乐而靠近有的人，因为日常运作而汇合有的人，因为城市的穿梭而擦肩而过有的人，你永远遇见不到，但它们都有繁盛的心灵花园，你要学会自己去探望。 第三件，事。 我做不好很多事情，它们有的躺在我的计划簿子里，有的早已糜烂，有的做了一半就没有兴致了，有的也因为心有余而力不足，变得苍白烂尾。能做的就那么多，唯一希望的便是可以做的更好一些。 忽然觉得计划这玩意儿真神奇，它跨越时间区间去规定通往未来之路，让未来似乎变得清晰可触，在计划里，每个人都是自恋的，他们看到了更加完好的自己，从这种虚假的完备换取了瞬时的慰藉，每次向计划簿里添加精致的清单之时，便会看到镜像中的自己，绚丽地燃烧。而慢慢地，热情燃烧殆尽，才发觉，自己偏移的太远。 其三二零一七要做更多有趣的事啊，生活雅致还得有。 计算机真是一个让人着迷的东西，还有很多的未知领域需要探索，让好奇心与求知欲都尽情地来驱动自己吧。 机器学习同样让人痴迷，16年下半年的时间支离破碎的，都没有系统的梳理下，作为今年的主线吧。 你好久不读书了，清单正在消沉，逐渐变得索然无味，当你再次拾起的时候，我相信它们会踊跃呼唤你的名。 美食、音乐、旅行，这些美妙生活的配方，没有它们，可能就没有那些人了，你也失去了让感受力生根发芽的机会。 我还是很瘦，每次在健身房的镜子前看着自己的样子，一脸嫌弃，迅速逃离。近来健身慢慢起了效果。不知道为什么每次去健身房都会先练胸，或许因为那是更接近心房的地方吧。 好像还有未完待续的事情留待这一年去做，用心帮助更多的学弟学妹，他们都是最可爱的人。 分享真是是一件让人振奋的事，一个人的日志，可以直接通往他的心灵花园，所以很多人选择把它藏起来，那是属于自己的东西。很多人只与自己说真话，虽然看起来他和很多人对话；也有很多人不与别人说话，它们还没有准备好，它们害怕话一说出口就会变得浅薄无趣，就索性让自己保有最温热的部分了。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[享书记]]></title>
    <url>%2F2016%2F12%2F24%2F%E4%BA%AB%E4%B9%A6%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[小序现在书就在我的手上，约摸有个十来本，我想说说我接下来和他们即将擦出的火花。 对，书还没放上书架，买来只是随意堆在床边，还没有带来和井然有序俱来的那种轻微无聊的单调滋味，我还没有准备好把淘来的书展示给朋友们看，就像是没有精心剪辑的影片，那种随意拼凑出来的画面固然有吸引力，但是我会赋予更多关于藏书的意义（每本书的每一册都有其命运）。 其一从我接手一本书那一时刻起，我便承担起了这本书的美妙的命运，或者说这本书将在我的手上重生，这会是一次有价值的邂逅，和自己产生共鸣的那一刻我便认定了这孩子，他让我同样获取了孩童心态，我就像是个孩子一样抚摸每一页，感受那种苍老或幽静的感觉，我为他取一个专属于我与他之间的名字，遇到一个好的主人成为你存在在这个世界上的荣耀，你可以安详的躺在我的书架上，享受散发你香味的漫长的一生，哪一天或许我心血来潮了，我会来看看你，或者和你深入的交往沟通，你需要理解我占有你时陶醉的心情，我也会因为复活你的生命沾沾自喜，于你而言，就像是复活了一个时代，那是我最深层次的动机。 我享受这样的快感——我将你们锁入我的圈子，永久成为我无法拒绝的暧昧对象。每一个关于你的回忆和念头，便是我的所有财富的基座，支架和锁钥，我打开每一个味觉嗅觉视觉触觉神经细胞，探索你的每一个细节，你的精髓我会暗自藏在心里，由此引发的灵感仿佛可以透过你们看到遥远的过去，出版那一天的情形，那位伟岸的作者在字里行间留下的最捉摸不透的暗号，装帧师傅设计封面时所被浸透的头脑风暴，你以前的那个主人如何把你捧在手心私密的对话。 其二该是遇到怎样的多舛而繁华的命运才可以到我手上，我感谢每一位呵护你照顾你帮助你与你彻夜畅谈的每一位先人们，现在我是“老者”，你会陪伴我走向我最寂寥最繁荣的日子。 我或许不会再回头去看你，就如曾经有个庸人赞美一番阿那托尔·法朗士的书斋，最后问了一个常见的 问题：“法朗士先生，这些书您都读过了吗？”回答是足以说明问题的： “还不到十分之一。不过我想您并不是每天都用您的塞弗尔瓷器吧？” 我想你安安静静的躺在我帮你精心设计的书架上，和周围的那些你的兄弟姐妹们和谐的交谈，就像是智者之间毫无遮拦的对话，那是你们伟大的灵魂有缘的碰撞和融合，又偶尔，我会来和你们打个招呼，或和你们中的某一位共处出在小屋子中，一盏台灯， 一杯清茶，度过一个思想火花四溅的午后。 我突然敞开怀笑了，哦， 藏书者的一大乐事，散逸人的一大福祉。我愿无声无息，无誉无毁躲在施比兹韦格的“书虫”面具后。没有人比这样的人更有富足感 了，因为附体的神灵或是精怪，会让爱着你们的人——与你们保持着最为亲密的拥有关系。就像是你们活在我的精神世界里面一样，我不会忍心舍弃你们而去。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[疯子]]></title>
    <url>%2F2016%2F12%2F20%2F%E7%96%AF%E5%AD%90%2F</url>
    <content type="text"><![CDATA[其一二十一世纪的第十三个年头，顺势来到的一天，圣索菲亚教堂前，游客不多，一些人交流，一些人快步，一些人，像我一样，对着教堂说话，像是对着自己。 丹尼斯大街与路易斯大街交汇的十字路口中央，高空中悬着一个雕塑，底下刷成灰色的塑像，凝视状。街边的的花满楼里，歌妓一刻不停，欢歌轻舞，北上的旅人在肉欲慰藉里且度今宵。 一个神经病突然发出声，左边，右边，上头，后面，四周的人头朝向了一个声音，他开始搜索眼神，轻率，讶异；唇角，僵硬，微微收拢；线条轮廓，百无聊赖的平淡，妇女的乳房变得苍白，持续的缓慢的最终鲜明确凿的凸现：抑郁寡欢。 格格不入，对峙，退却。他捂住耳朵，周围的表情幻化成压抑的声源，他没有停止搜寻的迹象，那是一件危险的事。 那是两天前发生的事，他来到了夜幕下的哈尔滨。 其二为了躲开原本那座城市刺眼的眼神，他才开始北上。几天前，他读毕一本书，八百余页，一整天，于是就再也看不清路人了，他没有变瞎，慢跑到紫禁城红墙下，已是深夜，红墙看不明颜色，但他知道颜色恰巧到中古悠然，也知道这是最契合这座城市意蕴的地方，他不准备停留，沿着府右街过景山前街，在南北池子大街的交汇处的东华门处停下，路上一对夜归的老夫妻互相搀扶前行，凝视十分钟，他没有意识到自己正对着他们傻笑，老夫妻被吓得赶忙加快脚步，一瘸一拐，他对着空白的街景继续傻笑。 十分钟时间不长，对他的外在来讲也不长，只是意识流开始缠上身，前两分钟，唐宋时代的风俗人情，三四分钟；来自柏拉图的洞穴的幻影浮现；五六分钟，霸王别姬，哪吒传奇；六七分钟，竖排繁体的《脂砚斋重评石头记》，优雅笃定的当下感，博尔赫斯在图书馆，沈从文的边城；最后三分钟，《洛丽塔》，《人间失格》，《一个陌生女人的来信》，《2666》，《荒原狼》，《树上的男爵》，《瓦尔登湖》，《伊豆的舞女》，《到灯塔去》，《都柏林人》，若即若离。 他累了，找个旅馆睡下，一头扎进了一个没有亮光的胡同，那是一家老店。沦落为蜗居在老城区角落的廉价旅馆，早已徒有虚名。窄小巷子中的灰白色混凝土小楼，如同所有以临时心态搭建的建筑，苟且度日。接待处服务员，胖而迟钝的中年妇女，磕瓜子看着面前的电视屏幕，麻木的表情。走廊上铺陈一长条红色地毯，清洗。睡觉对他来说还不算是一件煎熬的事，可以在十分钟内很快睡去。夜深了。 其三这是他常会有的那几天，业已习惯了这样的日子，例如还有几次，他一个人乘着火车，火车往南开，去的是一个中部城市，城市很大。 火车上，乘客不多。一些时间说话，一些时间睡觉，一些时间喝水与看望黑夜，一些时间思考不着边际的问题。10个小时后，火车抵达值夏的城市。下车，出地道。出站口两扇敞开木门，一角灰蓝色天空。微风缭绕。广场上出租汽车和三轮车颇显冷落，生意寥寥。在清晨的光景里，这个城略显闹腾，低矮旧楼被雨水洗刷成暗色，路边耸立广告牌上，词汇带有时光倒退30年的落伍气息。他的精神一振，知道来到正确的地方，稍许加快了脚步，往前。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十二封情书]]></title>
    <url>%2F2016%2F12%2F05%2F%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B0%81%E6%83%85%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[我睡眼蒙眬的躺在地铺上，已经好久没有写信给你了，每次写字最多的就是给你写的信了，我想你应该已经躺在温暖的有点过头的床上睡着了吧。时间已经那么晚了，我可能写着写着就睡着了。 回想这些天发生在我们之间的事，争吵，不合拍，嫌弃，甚至厌恶，我不知道有一些情感是否来的真实，但它确实从一张我们内心生发出来了，每次回味那些言语，我都会有些无助的感觉，夹杂在其中，我深切的感觉到心情的起伏已经无法被自己控制，就像河水猛兽般汹涌的侵蚀自己敏感外露的但又极力克制的心，无法逆转，无法平复，根本无法像平常那样和周围的人说话。我总会在想：爱情是不痛苦的，它是纯快乐，不该掺进别的，尤其不该掺进痛苦，记得一首外国诗这样说：“啊！“爱情”！他们大大的误解了你！他们说你的甜蜜是痛苦，当你丰富的果实比任何果实都甜蜜。” 然而，我们真切的在爱情里尝到过痛苦，撕心裂肺的感觉。面对离别，我们却只能远望对方的背影渐渐离去；面对不理解，脑海里只会倍增对对方的不满；如若行为与本人价值观不符，便会用自己的意志控制对方，使对方的自由关进牢笼；如若争辩，我们也异常坚定的秉持自己的立场，尝试使用任何办法说服，如果对方的乖张让你感到舒适，战斗才停歇下来，否则就是一场无穷无尽的无聊的口舌之争，对，我们也常常使用这个套数让对方处于劣势（我不想在这里和你发生这些无聊的口舌之争）；服软的那一方看似更爱对方一些，不服输的总是多爱自己一些。 但爱情里面哪有那么多的算术题啊，我们都在做一些回过头来我们也无法理解的事，我们沉浸在不理性，肆意飞扬的情绪里，甚至是荒谬的，怪异的形式才促成了爱情那奇怪的模样呀。我们要接受它，冷静的意识到爱情需要一定程度的自我牺牲，但这绝不是允许控制与被控制的发生，自由是爱情的首要目的，幸福也来源于此。 “什么是爱，爱是恒久忍耐，爱是彼此包容，爱是相对付出”这句话出自圣经，高中的时候，和班上的同学你问我答，说得是那么轻巧，但当注入真实体验的时候，我们已经开始怀疑，我们所经历的爱情，到底是不是真正的爱情了。在现实的情境与企盼的盛景有落差时，我常有幻灭的感觉，爱情难道不是那样吗？爱情不该是这样的，它不会辜负良苦用心的我们的吧？忍耐，包容，付出，多么简单的词，我却做不到。 罗兰巴特对爱情的不同阶段的场景有这么一段描述： 尽管恋人的表述仅仅是纷纭的情境，他们骚动起来全无秩序可言，不比在屋子里胡飞乱舞的苍蝇的轨迹更有规律，我还是能——至少是在回忆或者想像中——给爱情的发展找出一定的规律来。爱情的旅程似乎分为三个阶段（或者三幕戏）：首先是一见钟情，是闪电般的“迷上”“被俘虏”（我被一个形象迷住了）；然后便是一连串的相逢（约会、电话、情书、短途旅行），在此期间，我如痴如醉地发掘着情偶的完美，也就是说，对象与我的欲望之间那种完全出乎我意料的契合：这是初时的柔情，田园诗一般的光阴。在这幸福时光之后便是“一连串”恋爱的麻烦——持续不断的痛苦、创伤、焦虑、忧愁、怨恨、失望、窘迫还有陷阱——我们都成了里面的困兽，老是提心吊胆，生怕爱情衰退，怕这衰退不仅会毁了对方和我，还会毁了当初的缘分，那种神奇的情投意合。从这漫长的隧道中走出来，我又能重见天日了：这也许是因为我成功的找到了解决了不幸爱情的辩证出路——维持爱情、但脱离梦幻，冷静现实的面对它。） 我想我们正在经历如他所述的爱情低谷，我们眼前一片黑暗，对方的形象也因此而模糊、歪曲。青春的时限或许可以做出解释，爱情在这个年纪注定是不能风平浪静的，它该有它狂傲不羁的模样，也该有它不突显成熟爱情本质的局限，我们的一半是孩子，一半又竭力挣脱这样的形象。 但总有一天会磨合出一个更为安静的环境供爱情生长，愿爱情如我们所期盼的那样，我们会脱离梦境、从那漫长的黑暗隧道中走出来，如你所说：内心笃定，爱则长久。我们为了彼此，做最好的自己，然后留给岁月。在长久的陪伴中，我们越来越相似于彼此，直到连微笑的弧度都一样。直到看着越来越像彼此的自己埋怨不起来。我想，这就是我们的爱情，它来源的没有任何预兆，也没有任何可以消失的理由。它并不逊色生离死别的爱情，相比之下更多了青春奋斗的最美好的回忆。 晚安。]]></content>
      <categories>
        <category>红楼记事</category>
      </categories>
      <tags>
        <tag>红楼记事</tag>
      </tags>
  </entry>
</search>